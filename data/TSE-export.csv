"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Measuring the Impact of Code Dependencies on Software Architecture Recovery Techniques","T. Lutellier; D. Chollak; J. Garcia; L. Tan; D. Rayside; N. Medvidović; R. Kroeger","University of Waterloo, Waterloo, ON, Canada; University of Waterloo, Waterloo, ON, Canada; University of California, Irvine, CA; University of Waterloo, Waterloo, ON, Canada; University of Waterloo, Waterloo, ON, Canada; University of Southern California, Los Angeles, CA; Google Inc., Mountain View, CA","IEEE Transactions on Software Engineering","","2018","44","2","159","181","Many techniques have been proposed to automatically recover software architectures from software implementations. A thorough comparison among the recovery techniques is needed to understand their effectiveness and applicability. This study improves on previous studies in two ways. First, we study the impact of leveraging accurate symbol dependencies on the accuracy of architecture recovery techniques. In addition, we evaluate other factors of the input dependencies such as the level of granularity and the dynamic-bindings graph construction. Second, we recovered the architecture of a large system, Chromium, that was not available previously. Obtaining the ground-truth architecture of Chromium involved two years of collaboration with its developers. As part of this work, we developed a new submodule-based technique to recover preliminary versions of ground-truth architectures. The results of our evaluation of nine architecture recovery techniques and their variants suggest that (1) using accurate symbol dependencies has a major influence on recovery quality, and (2) more accurate recovery techniques are needed. Our results show that some of the studied architecture recovery techniques scale to very large systems, whereas others do not.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2671865","Natural Sciences and Engineering Research Council of Canada; Google Faculty Research Award; Ontario Ministry of Research and Innovation; U.S. National Science Foundation; Infosys Technologies, Ltd.; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7859416","Software architecture;empirical software engineering;maintenance and evolution;program comprehension","Computer architecture;Software architecture;Software;Heuristic algorithms;Chromium;Software algorithms;Manuals","software architecture;software quality;system recovery","symbol dependencies;accurate recovery techniques;recovery quality;submodule-based technique;ground-truth architecture;input dependencies;software implementations;software architectures;software architecture recovery techniques;code dependencies","","3","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Foreword","R. B. Bunt","University of Saskatchewan","IEEE Transactions on Software Engineering","","1987","SE-13","3","362","362","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233167","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702222","","Queueing analysis;Fault tolerant systems;FDDI;Token networks;Protocols","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Operations and implementation of complex objects","W. Kim; H. -. Chou; J. Banerjee","Microelectron. & Comput. Technol. Corp., Austin, TX, USA; Microelectron. & Comput. Technol. Corp., Austin, TX, USA; Microelectron. & Comput. Technol. Corp., Austin, TX, USA","IEEE Transactions on Software Engineering","","1988","14","7","985","996","A model of a complex object is presented and a set of meaningful operations, both basic and advanced, on a single complex object and on a configuration of complex objects is defined. A set of requirements is presented for storage subsystems that support complex objects. Implementation of complex objects and operations on a single complex object are described, and a detailed performance analysis is provided which establishes the merit of complex objects. Finally, storage techniques are proposed for supporting advanced operations on a configuration of complex objects.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42739","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42739","","Computer aided manufacturing;CADCAM;Artificial intelligence;Performance analysis;Spatial databases;Multimedia databases;Multimedia systems;Database systems;Microelectronics;Information systems","data structures;query languages;relational databases;storage management","relational databases;data structures;query languages;storage management;complex objects;storage subsystems;performance analysis","","23","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Coverage Prediction for Accelerating Compiler Testing","J. Chen; G. Wang; D. Hao; Y. Xiong; H. Zhang; L. Zhang; B. XIE","EECS, Peking University, Beijing, Beijing China (e-mail: chenjunjie@pku.edu.cn); EECS, Peking University, 12465 Beijing, Beijing China (e-mail: amocywang@gmail.com); EECS,Peking University, Institute of Software, Beijing, Beijing China 100871 (e-mail: haodan@pku.edu.cn); EECS, Peking University, Beijing, Beijing China 100871 (e-mail: xiongyf@pku.edu.cn); School of Electrical Engineering and Computing, The University of Newcastle, Newcastle, New South Wales Australia (e-mail: hongyu.zhang@newcastle.edu.au); EECS,Peking University, Institute of Software, Beijing, Beijing China (e-mail: zhanglucs@pku.edu.cn); Software Engineering Institute, Peking University, Beijing, Beijing China (e-mail: xiebing@pku.edu.cn)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Compilers are one of the most fundamental software systems. Compiler testing is important for assuring the quality of compilers. Due to the crucial role of compilers, they have to be well tested. Therefore, automated compiler testing techniques (those based on randomly generated programs) tend to run a large number of test programs (which are test inputs of compilers). The cost for compilation and execution for these test programs is significant. These techniques can take a long period of testing time to detect a relatively small number of compiler bugs. That may cause many practical problems, e.g., bringing a lot of costs including time costs and financial costs, and delaying the development/release cycle. Recently, some approaches have been proposed to accelerate compiler testing by executing test programs that are more likely to trigger compiler bugs earlier according to some criteria. However, these approaches ignore an important aspect in compiler testing: different test programs may have similar test capabilities (i.e., testing similar functionalities of a compiler, even detecting the same compiler bug), which may largely discount their acceleration effectiveness if the test programs with similar test capabilities are executed all the time. Test coverage is a proper approximation to help distinguish them, but collecting coverage dynamically is infeasible in compiler testing since most test programs are generated on the fly by automatic test-generation tools like Csmith. In this paper, we propose the first method to predict test coverage statically for compilers, and then propose to prioritize test programs by clustering them according to the predicted coverage information. The novel approach to accelerating compiler testing through coverage prediction is called COP (short for COverage Prediction). Our evaluation on GCC and LLVM demonstrates that COP significantly accelerates compiler testing, achieving an average of 51.01% speedup in test execution time on the existing constructed dataset and achieving an average of 68.74% speedup on the new dataset including 12 latest release versions of GCC and LLVM. Moreover, COP outperforms the state-of-the-art acceleration approach significantly by improving 17.16%~82.51% speedups in different settings on average.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2889771","National Key Research and Development Program; National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8588375","Compiler Testing;Test Prioritization;Machine Learning","Testing;Program processors;Computer bugs;Life estimation;Acceleration;Optimization;Electromagnetic interference","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Empirical Software Engineering","D. R. Jeffery; L. G. Votta","CAESAR; NA","IEEE Transactions on Software Engineering","","1999","25","4","435","437","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1999.799935","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=799935","","Software engineering;Biological system modeling;Humans;Biology;Biological information theory;Moon;Process design;Laboratories;In vitro;In vivo","","","","3","","","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental design of a power transformer station controller using a controller synthesis methodology","H. Marchand; M. Samaan","IRISA/INRIA, Rennes, France; NA","IEEE Transactions on Software Engineering","","2000","26","8","729","741","The authors describe the incremental specification of a power transformer station controller using a controller synthesis methodology. They specify the main requirements as simple properties, named control objectives, that the controlled plant has to satisfy. Then, using algebraic techniques, the controller is automatically derived from this set of control objectives. In our case, the plant is specified at a high level, using the data-flow synchronous SIGNAL language, and then by its logical abstraction, called polynomial dynamical system. The control objectives are specified as invariance, reachability, ...properties, as well as partial order relations to be checked by the plant. The control objectives equations are synthesized using algebraic transformations.","0098-5589;1939-3520;2326-3881","","10.1109/32.879811","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879811","","Power transformers;Automatic control;Signal synthesis;Polynomials;Optimal control;Equations;Control system synthesis;Data structures;Boolean functions;Circuit faults","power transformers;power system control;parallel languages;control system synthesis","incremental design;power transformer station controller;controller synthesis methodology;incremental specification;simple properties;named control objectives;controlled plant;algebraic techniques;control objectives;high level specification;data-flow synchronous SIGNAL language;logical abstraction;polynomial dynamical system;invariance;reachability;partial order relations;control objectives equations;algebraic transformations","","20","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Absolute bounds on set intersection and union sizes from distribution information","N. C. Rowe","Dept. of Comput. Sci., US Naval Postgraduate Sch., Monterey, CA, USA","IEEE Transactions on Software Engineering","","1988","14","7","1033","1048","A catalog of quick closed-form bounds on set intersection and union sizes is presented; they can be expressed as rules, and managed by a rule-based system architecture. These methods use a variety of statistics precomputed on the data, and exploit homomorphisms (onto mappings) of the data items onto distributions that can be more easily analyzed. The methods can be used anytime, but tend to work best when there are strong or complex correlations in the data. This circumstance is poorly handled by the standard independence-assumption and distributional-assumption estimates.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42743","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42743","","Information retrieval;Distributed computing;Frequency estimation;Algorithm design and analysis;Transaction databases;Knowledge based systems;Computer architecture;Statistical analysis;Statistical distributions;Algebra","Boolean algebra;database theory;file organisation;set theory;statistical analysis","Boolean algebra;file organisation;database access;set theory;statistical analysis;set intersection;union sizes;distribution information;closed-form bounds;rule-based system architecture","","6","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical data modeling in software engineering using radial basis functions","Miyoung Shin; A. L. Goel","Electron. & Telecommun. Res. Inst., Taejon, South Korea; NA","IEEE Transactions on Software Engineering","","2000","26","6","567","576","Many empirical studies in software engineering involve relationships between various process and product characteristics derived via linear regression analysis. We propose an alternative modeling approach using radial basis functions (RBFs) which provide a flexible way to generalize linear regression function. Further, RBF models possess strong mathematical properties of universal and best approximation. We present an objective modeling methodology for determining model parameters using our recent SG algorithm, followed by a model selection procedure based on generalization ability. Finally, we describe a detailed RBF modeling study for software effort estimation using a well-known NASA dataset.","0098-5589;1939-3520;2326-3881","","10.1109/32.852743","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=852743","","Software engineering;Linear regression;Data analysis;Mathematical model;NASA;Programming;Signal processing algorithms;Predictive models;Inspection;Computer aided software engineering","software development management;data models;radial basis function networks;statistical analysis","empirical data modeling;software engineering;radial basis functions;product characteristics;linear regression analysis;alternative modeling approach;linear regression function;RBF models;mathematical properties;best approximation;objective modeling methodology;model parameters;SG algorithm;model selection procedure;generalization ability;RBF modeling study;software effort estimation;NASA dataset","","66","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Correcton to ""SOFL"": A formal engineering methodology for industrial applicatoins","S. Liu; A. J. Offutt; C. Ho-Stuart","NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","5","390","390","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1998.685261","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=685261","","Biographies;Software engineering;Computer science;Software safety;Typesetting;Educational institutions;Fellows;Programming;Software testing;Computer Society","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"A quantitative model of the security intrusion process based on attacker behavior","E. Jonsson; T. Olovsson","Dept. of Comput. Eng., Chalmers Univ. of Technol., Goteborg, Sweden; NA","IEEE Transactions on Software Engineering","","1997","23","4","235","245","The paper is based on a conceptual framework in which security can be split into two generic types of characteristics, behavioral and preventive. Here, preventive security denotes the system's ability to protect itself from external attacks. One way to describe the preventive security of a system is in terms of its interaction with the alleged attacker, i.e., by describing the intrusion process. To our knowledge, very little is done to model this process in quantitative terms. Therefore, based on empirical data collected from intrusion experiments, we have worked out a hypothesis on typical attacker behavior. The hypothesis suggests that the attacking process can be split into three phases: the learning phase, the standard attack phase, and the innovative attack phase. The probability for successful attacks during the learning and innovative phases is expected to be small, although for different reasons. During the standard attack phase it is expected to be considerably higher. The collected data indicates that the breaches during the standard attack phase are statistically equivalent and that the times between breaches are exponentially distributed. This would actually imply that traditional methods for reliability modeling could be applicable.","0098-5589;1939-3520;2326-3881","","10.1109/32.588541","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=588541","","Data security;Protection;Probability;Particle measurements;Control system synthesis;Testing","computer crime;social aspects of automation;authorisation;message authentication","quantitative model;security intrusion process;attacker behavior;conceptual framework;preventive security;external attacks;alleged attacker;quantitative terms;empirical data;intrusion experiments;attacking process;learning phase;standard attack phase;innovative attack phase;reliability modeling;computer security;operational security","","140","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing error-prone system structure","R. W. Selby; V. R. Basili","Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA; NA","IEEE Transactions on Software Engineering","","1991","17","2","141","152","Using measures of data interaction called data bindings, the authors quantify ratios of coupling and strength in software systems and use the ratios to identify error-prone system structures. A 148000 source line system from a prediction environment was selected for empirical analysis. Software error data were collected from high-level system design through system testing and from field operation of the system. The authors use a set of five tools to calculate the data bindings automatically and use a clustering technique to determine a hierarchical description of each of the system's 77 subsystems. A nonparametric analysis of variance model is used to characterize subsystems and individual routines that had either many or few errors or high or low error correction effort. The empirical results support the effectiveness of the data bindings clustering approach for localizing error-prone system structure.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67595","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67595","","Error analysis;Software measurement;Software systems;Data analysis;Computer errors;Software testing;System testing;Error correction;Inspection;Computer science","error analysis;program diagnostics;software metrics","error-prone system structure;data interaction;data bindings;software systems;prediction environment;empirical analysis;clustering technique;nonparametric analysis of variance model","","80","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Software process representation and analysis for framework instantiation","H. C. Jiau; Chia Hung Kao; Kuo-Feng Ssu","Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","IEEE Transactions on Software Engineering","","2004","30","10","707","","When studying [T.C. Oliveira et al. (2004)], we found an error and some ambiguous parts. In this paper, we state those errors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339280","","Redundancy","software process improvement","software process representation;framework instantiation;instantiation layer commands","","1","","1","","","","","","IEEE","IEEE Journals & Magazines"
"A transformation approach to derive efficient parallel implementations","T. Rauber; G. Runger","Inst. fur Inf., Univ. Halle-Wittenberg, Germany; NA","IEEE Transactions on Software Engineering","","2000","26","4","315","339","The construction of efficient parallel programs usually requires expert knowledge in the application area and a deep insight into the architecture of a specific parallel machine. Often, the resulting performance is not portable, i.e., a program that is efficient on one machine is not necessarily efficient on another machine with a different architecture. Transformation systems provide a more flexible solution. They start with a specification of the application problem and allow the generation of efficient programs for different parallel machines. The programmer has to give an exact specification of the algorithm expressing the inherent degree of parallelism and is released from the low-level details of the architecture. We propose such a transformation system with an emphasis on the exploitation of the data parallelism combined with a hierarchically organized structure of task parallelism. Starting with a specification of the maximum degree of task and data parallelism, the transformations generate a specification of a parallel program for a specific parallel machine. The transformations are based on a cost model and are applied in a predefined order, fixing the most important design decisions like the scheduling of independent multitask activations, data distributions, pipelining of tasks, and assignment of processors to task activations. We demonstrate the usefulness of the approach with examples from scientific computing.","0098-5589;1939-3520;2326-3881","","10.1109/32.844492","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=844492","","Parallel processing;Parallel machines;Programming profession;Microprocessors;Scientific computing;Message passing;Computer architecture;Costs;Processor scheduling;Pipeline processing","bibliographies;parallel programming;formal specification;parallel machines;message passing","transformation approach;parallel implementations;parallel program design;expert knowledge;application area;parallel machine;transformation systems;parallel machines;exact specification;data parallelism;hierarchically organized structure;task parallelism;cost model;predefined order;design decisions;independent multitask activations;data distributions;task pipelining;processor assignment;task activations;scientific computing","","18","","58","","","","","","IEEE","IEEE Journals & Magazines"
"A group-select operation for relational algebra and implications for database machine design","J. Bradley","Dept. of Comput. Sci., Calgary Univ., Alta., Canada","IEEE Transactions on Software Engineering","","1988","14","1","126","129","A group-select operation has been defined for relational algebra. This operation is found to be useful for efficiently reducing expressions of nonprocedural relational languages that permit natural quantifiers. Conceptually, the operation first partitions a relation into blocks of tuples that have the same value for an attribute or attribute concatenation. It then extracts each block for which a specified number of tuples meet a specified condition. The quantity of tuples for the operation is specified by means of a natural quantifier. Performance of the group-select operation will be poor with conventional file processing, making the operation more suitable for use with a database machine with an associative memory.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4630","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4630","","Algebra;Database machines;Relational databases;Remuneration;Calculus;EMP radiation effects;Associative memory;ANSI standards;Standards development;Natural languages","relational databases","relational algebra;database machine design;group-select operation;nonprocedural relational languages;natural quantifiers;tuples;associative memory","","2","","22","","","","","","IEEE","IEEE Journals & Magazines"
"On the Value of Ensemble Effort Estimation","E. Kocaguneli; T. Menzies; J. W. Keung","West Virginia University, Morgantown; West Virginia University, Morgantown; The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Software Engineering","","2012","38","6","1403","1416","Background: Despite decades of research, there is no consensus on which software effort estimation methods produce the most accurate models. Aim: Prior work has reported that, given M estimation methods, no single method consistently outperforms all others. Perhaps rather than recommending one estimation method as best, it is wiser to generate estimates from ensembles of multiple estimation methods. Method: Nine learners were combined with 10 preprocessing options to generate 9 × 10 = 90 solo methods. These were applied to 20 datasets and evaluated using seven error measures. This identified the best n (in our case n = 13) solo methods that showed stable performance across multiple datasets and error measures. The top 2, 4, 8, and 13 solo methods were then combined to generate 12 multimethods, which were then compared to the solo methods. Results: 1) The top 10 (out of 12) multimethods significantly outperformed all 90 solo methods. 2) The error rates of the multimethods were significantly less than the solo methods. 3) The ranking of the best multimethod was remarkably stable. Conclusion: While there is no best single effort estimation method, there exist best combinations of such effort estimation methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.111","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6081882","Software cost estimation;ensemble;machine learning;regression trees;support vector machines;neural nets;analogy;k-NN","Costs;Software performance;Measurement uncertainty;Taxonomy;Machine learning;Regression tree analysis;Support vector machines;Neural networks","software development management","ensemble effort estimation;software effort estimation;single method;multiple estimation method;error measures","","79","","82","","","","","","IEEE","IEEE Journals & Magazines"
"The effectiveness of control structure diagrams in source code comprehension activities","D. Hendrix; J. H. Cross; S. Maghsoodloo","Dept. of Comput. Sci. & Software Eng., Auburn Univ., AL, USA; Dept. of Comput. Sci. & Software Eng., Auburn Univ., AL, USA; NA","IEEE Transactions on Software Engineering","","2002","28","5","463","477","Recently, the first two in a series of planned comprehension experiments were performed to measure the effect of the control structure diagram (CSD) on program comprehensibility. Upper- and lower-division computer science and software engineering students were asked to respond to questions regarding the structure and execution of one source code module of a public domain graphics library. The time taken for each response and the correctness of each response was recorded. Statistical analysis of the data collected from these two experiments revealed that the CSD was highly significant in enhancing the subjects' performance in this program comprehension task. The results of these initial experiments promise to shed light on fundamental questions regarding the effect of software visualizations on program comprehensibility.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000450","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000450","","Performance evaluation;Computer science;Software engineering;Computer graphics;Software libraries;Statistical analysis;Data visualization","diagrams;reverse engineering;program visualisation;program control structures","control structure diagrams;source code comprehension;experiments;program comprehensibility;computer science students;software engineering students;public domain graphics library;statistical analysis;data analysis;software visualizations","","19","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Inference of message sequence charts","R. Alur; K. Etessami; M. Yannakakis","Dept. of Comput. & Inf. Sci., Pennsylvania Univ., Philadelphia, PA, USA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","7","623","633","Software designers draw message sequence charts for early modeling of the individual behaviors they expect from the concurrent system under design. Can they be sure that precisely the behaviors they have described are realizable by some implementation of the components of the concurrent system? If so, can we automatically synthesize concurrent state machines realizing the given MSCs? If, on the other hand, other unspecified and possibly unwanted scenarios are ""implied"" by their MSCs, can the software designer be automatically warned and provided the implied MSCs? In this paper, we provide a framework in which all these questions are answered positively. We first describe the formal framework within which one can derive implied MSCs and then provide polynomial-time algorithms for implication, realizability, and synthesis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214326","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214326","","System recovery;Software design;Polynomials;Algorithm design and analysis;Automata;Formal verification;Unified modeling language;Pattern analysis;Timing;Pattern matching","formal verification;system recovery;program testing","message sequence charts;concurrent system;software designer;polynomial-time algorithms;deadlock freedom;concurrent state machines;formal verification;requirements analysis","","69","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Mechanically verifying concurrent programs with the Boyer-Moore prover","D. M. Goldschlag","Comp. Logic, Inc., Austin, TX, USA","IEEE Transactions on Software Engineering","","1990","16","9","1005","1023","A proof system suitable for the mechanical verification of concurrent programs is described. This proof system is based on Unity, and may be used to specify and verify both safety and liveness properties. However, it is defined with respect to an operational semantics of the transition system model of concurrency. Proof rules are simply theorems of this operational semantics. This methodology makes a clear distinction between the theorems in the proof system and the logical inference rules and syntax which define the underlying logic. Since this proof system essentially encodes Unity in another sound logic, and this encoding has been mechanically verified, this encoding proves the soundness of this formalization of Unity. This proof system has been mechanically verified by the Boyer-Moore prover. This proof system has been used to mechanically verify the correctness of a distributed algorithm that computes the minimum node value in a tree.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58787","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58787","","Logic;Concurrent computing;Encoding;Distributed algorithms;Distributed computing;Computer architecture;Safety;Formal specifications;Computer languages;Runtime","encoding;inference mechanisms;parallel programming;program verification;theorem proving","mechanically verifying concurrent programs;Boyer-Moore prover;proof system;Unity;safety;liveness;operational semantics;transition system model;concurrency;inference rules;encoding;distributed algorithm","","20","","33","","","","","","IEEE","IEEE Journals & Magazines"
"An optimal algorithm for scheduling soft aperiodic tasks in dynamic-priority preemptive systems","I. Ripoll; A. Crespo; A. Garcia-Fornes","Dept. de Ingenieria de Sistemas, Comput. y Autom., Univ. Politecnica de Valencia, Spain; NA; NA","IEEE Transactions on Software Engineering","","1997","23","6","388","400","The paper addresses the problem of jointly scheduling tasks with both hard and soft real time constraints. We present a new analysis applicable to systems scheduled using a priority preemptive dispatcher, with priorities assigned dynamically according to the EDF policy. Further, we present a new efficient online algorithm (the acceptor algorithm) for servicing aperiodic work load. The acceptor transforms a soft aperiodic task into a hard one by assigning a deadline. Once transformed, aperiodic tasks are handled in exactly the same way as periodic tasks with hard deadlines. The proposed algorithm is shown to be optimal in terms of providing the shortest aperiodic response time among fixed and dynamic priority schedulers. It always guarantees the proper execution of periodic hard tasks. The approach is composed of two parts: an offline analysis and a run time scheduler. The offline algorithm runs in pseudopolynomial time O(mn), where n is the number of hard periodic tasks and m is the hyperperiod/min deadline.","0098-5589;1939-3520;2326-3881","","10.1109/32.601081","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=601081","","Scheduling algorithm;Dynamic scheduling;Processor scheduling;Real time systems;Testing;Delay;Timing;Computer Society;Runtime","scheduling;real-time systems;minimisation;computational complexity;distributed algorithms","optimal algorithm;soft aperiodic task scheduling;dynamic priority preemptive systems;soft real time constraints;priority preemptive dispatcher;EDF policy;online algorithm;acceptor algorithm;aperiodic work load servicing;hard deadlines;aperiodic response time;dynamic priority schedulers;periodic hard tasks;offline analysis;run time scheduler;pseudopolynomial time;earliest deadline first","","25","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Using Larch to specify Avalon/C++ objects","J. M. Wing","Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA","IEEE Transactions on Software Engineering","","1990","16","9","1076","1088","A formal specification of three base Avalon/C++ classes - recoverable, atomic, and subatomic - is given. Programmers derive from class recoverable to define persistent objects, and from either class atomic or class subatomic to define atomic objects. The specifications, written in Larch, provide the means for showing that classes derived from the base classes implement objects that are persistent or atomic and thus exemplify the applicability of an existing specification method to specifying nonfunctional properties. Writing these formal specifications for Avalon/C++'s built-in classes has helped to clarify places in the programming language where features interact, to make unstated assumptions explicit, and to characterize complex properties of objects.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58791","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58791","","Formal specifications;Security;Hardware;Software safety;Writing;Programming profession;Computer languages;Object oriented programming;Specification languages;Sorting","formal specification","Larch;Avalon/C++ objects;formal specification;recoverable;atomic;subatomic;nonfunctional properties;complex properties","","3","","29","","","","","","IEEE","IEEE Journals & Magazines"
"A Bayesian estimation method for the failure rate of a possibly correct program","G. Becker; L. Camarinopoulos","Inst. fuer Prozess & Anlagentechnik. Tech. Univ. of Berlin, West Germany; Inst. fuer Prozess & Anlagentechnik. Tech. Univ. of Berlin, West Germany","IEEE Transactions on Software Engineering","","1990","16","11","1307","1310","An extension of software reliability modeling is introduced to account for the possibility of programs which, after some debugging, contain no more errors. This is achieved by a repetitive application of the Bayes law, each time taking the posterior of the last step as a prior for the next one. A class of conjugate priors considerably facilitates this modeling. The resulting model includes an estimator for the probability that a program still contains errors, which is an upper bound for the failure probability.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60318","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60318","","Bayesian methods;Software reliability;Error correction;Debugging;Application software;Upper bound;Error probability;Artificial intelligence;Statistics","Bayes methods;probability;software reliability","prior step;Bayesian estimation method;failure rate;possibly correct program;software reliability modeling;Bayes law;posterior;last step;conjugate priors;estimator;upper bound;failure probability","","15","","5","","","","","","IEEE","IEEE Journals & Magazines"
"The call-return tree and its application to program performance analysis","S. Kundu","Department of Computer Science, Louisiana State University, Baton Rouge, LA 70803","IEEE Transactions on Software Engineering","","1986","SE-12","11","1096","1098","The notion of a call-return tree is defined to describe the dynamic calling relationship of the procedure and functions in a program execution. It is shown how the call-return tree can be used to compute the live times and the execution times of the various calls made during the execution. The call-return tree can also be used to compute other behavioral metrics such as the depth and height of a call and the number of direct and indirect calls generated from any point. The technique applies uniformly for both nonrecursive and recursive calls. The algorithms take linear time in the length of the source code and the number of calls made during an execution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313000","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313000","Algorithm;live and execution times;performance;software probe","Vegetation;Software;Silicon;Software algorithms;Probes;Monitoring","program testing;trees (mathematics)","call-return tree;program performance analysis;dynamic calling relationship;program execution;behavioral metrics","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Software reuse by specialization of generic procedures through views","G. S. Novak","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA","IEEE Transactions on Software Engineering","","1997","23","7","401","417","A generic procedure can be specialized, by compilation through views, to operate directly on concrete data. A view is a computational mapping that describes how a concrete type implements an abstract type. Clusters of related views are needed for specialization of generic procedures that involve several types or several views of a single type. A user interface that reasons about relationships between concrete types and abstract types allows view clusters to be created easily. These techniques allow rapid specialization of generic procedures for applications.","0098-5589;1939-3520;2326-3881","","10.1109/32.605759","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=605759","","Concrete;Software algorithms;Clustering algorithms;Programming profession;Object oriented programming;User interfaces;Application software;Costs;Production;Libraries","software reusability;abstract data types;partial evaluation (compilers);program compilers;user interfaces","software reuse;generic procedure specialization;views;computational mapping;concrete type;abstract data type;user interface;view clusters;partial evaluation;direct manipulation editor","","20","","77","","","","","","IEEE","IEEE Journals & Magazines"
"Design recovery for distributed systems","L. J. Holtzblatt; R. L. Piazza; H. B. Reubenstein; S. N. Roberts; D. R. Harris","Reasoning Inc., Mountain View, CA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","7","461","472","Two factors limit the utility of reverse engineering technology for many distributed software systems. First, with the exception of tools that support Ada and its explicit tasking constructs, reverse engineering tools fail to capture information concerning the flow of information between tasks. Second, relatively few reverse engineering tools are available for programming languages in which many older legacy applications were written (e.g., Jovial, CMS-2, and various assembly languages). We describe approaches that were developed for overcoming these limitations. In particular, we have implemented an approach for automatically extracting task flow information from a command and control system written in CMS-2. Our approach takes advantage of a small amount of externally provided design knowledge in order to recover design information relevant to the distributed nature of the target system.","0098-5589;1939-3520;2326-3881","","10.1109/32.605763","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=605763","","Software maintenance;Reverse engineering;Software systems;Data mining;Navigation;Productivity;Displays;Software tools;Computer languages;Application software","distributed processing;reverse engineering;software tools;Ada;command and control systems","design recovery;distributed systems;reverse engineering;tools;Ada;tasking constructs;information flow;programming languages;legacy applications;Jovial;CMS-2;assembly languages;command and control system;program understanding","","20","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Model-Based Adaptation of Behavioral Mismatching Components","C. Canal; P. Poizat; G. Salaün","Universidad de Málaga, Málaga; INRIA, Rocquencourt; Universidad de Málaga, Málaga","IEEE Transactions on Software Engineering","","2008","34","4","546","563","Component-Based Software Engineering focuses on the reuse of existing software components. In practice, most components cannot be integrated directly into an application-to-be, because they are incompatible. Software Adaptation aims at generating, as automatically as possible, adaptors to compensate mismatch between component interfaces, and is therefore a promising solution for the development of a real market of components promoting software reuse. In this article, we present our approach for software adaptation which relies on an abstract notation based on synchronous vectors and transition systems for governing adaptation rules. Our proposal is supported by dedicated algorithms that generate automatically adaptor protocols. These algorithms have been implemented in a tool, called Adaptor, that can be used through a user-friendly graphical interface.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527252","Software Engineering;Requirements/Specifications;Design Tools and Techniques;Software Architectures;Interoperability;Interface definition languages;Software Construction;Software Engineering;Requirements/Specifications;Design Tools and Techniques;Software Architectures;Interoperability;Interface definition languages;Software Construction","Adaptation model;Irrigation;Protocols;Middleware;Software engineering;Proposals;Assembly systems;Software tools;Contracts;Petri nets","application program interfaces;object-oriented programming;software reusability","model-based software adaptation;behavioral mismatching software component;component-based software engineering;component interface;software reusability;abstract notation;synchronous vector;transition system;automatic adaptor protocol generation","","73","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Formal specification of user interfaces: a comparison and evaluation of four axiomatic approaches"" by U.H. Chi","H. Alexander","STC Technol. Ltd., Newcastle-under-Lyme, UK","IEEE Transactions on Software Engineering","","1988","14","4","438","439","A recent paper (see ibid., vol.11, no.8, p.671-88, Aug. 1985) compared several axiomatic methods of formal specification, one of which was Z. As a result of a comparison made with Z and a similar, but executable, specification language called Me too, it was pointed out that one of the Z specifications given was not correct. In this response, the erroneous function is described and some conclusions are drawn about the process of formal specification.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4665","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4665","","Formal specifications;User interfaces;Displays;Prototypes;Mathematical model","software engineering;specification languages;user interfaces","software engineering;user interfaces;formal specification;specification language;Me too","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Diagnosing rediscovered software problems using symptoms","I. Lee; R. K. Iyer","Dept. of Electr. Eng., Hanyang Univ., Seoul, South Korea; NA","IEEE Transactions on Software Engineering","","2000","26","2","113","127","This paper presents an approach to automatically diagnosing rediscovered software failures using symptoms, in environments in which many users run the same procedural software system. The approach is based on the observation that the great majority of field software failures are rediscoveries of previously reported problems and that failures caused by the same defect often share common symptoms. Based on actual data, the paper develops a small software failure fingerprint, which consists of the procedure call trace, problem detection location, and the identification of the executing software. The paper demonstrates that over 60 percent of rediscoveries can be automatically diagnosed based on fingerprints; less than 10 percent of defects are misdiagnosed. The paper also discusses a pilot that implements the approach. Using the approach not only saves service resources by eliminating repeated data collection for and diagnosis of reoccurring problems, but it can also improve service response time for rediscoveries.","0098-5589;1939-3520;2326-3881","","10.1109/32.841113","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=841113","","Software systems;Fingerprint recognition;Databases;Computer Society;Delay;Software measurement;Operating systems;Computer crashes;Software testing;Costs","program diagnostics","symptoms;automatic rediscovered software failure diagnosis;procedural software system;software failure fingerprint;procedure call trace;problem detection location;executing software identification;service response time","","11","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Determining the Cause of a Design Model Inconsistency","A. Reder; A. Egyed","Johannes Kepler University Linz, Linz; Johannes Kepler University Linz, Linz","IEEE Transactions on Software Engineering","","2013","39","11","1531","1548","When a software engineer finds an inconsistency in a model, then the first question is why? What caused it? Obviously, there must be an error. But where could it be? Or is the design rule erroneous and if yes then which part? The cause of an inconsistency identifies the part of the model or design rule where the error must be. We believe that the visualization of an inconsistency ought to visualize the cause. Understanding the cause is of vital importance before a repair can even be formulated. Indeed, any automation (e.g., code generation, refactoring) has to be considered with caution if it involves model elements that cause inconsistencies. This paper analyzes the basic structure of inconsistent design rules as well as their behavior during validation and presents an algorithm for computing its cause. The approach is fully automated, tool supported, and was evaluated on 14,111 inconsistencies across 29 design models. We found that our approach computes correct causes for inconsistencies, these causes are nearly always a subset of the model elements investigated by the design rules' validation (a naive cause computation approximation), and the computation is very fast (99.8 percent of the causes are computable in <; 100 ms).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6560054","Design tools and techniques;programming environments/construction tools;validation","Unified modeling language;Computational modeling;Context;Maintenance engineering;Visualization;Context modeling;Light emitting diodes","software development management","design model inconsistency;software engineer;code generation;refactoring;model elements;inconsistent design rules","","10","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A formal security model for microprocessor hardware","V. Lotz; V. Kessler; G. H. Walter","Corp. Technol., Siemens AG, Munich, Germany; NA; NA","IEEE Transactions on Software Engineering","","2000","26","8","702","712","The paper introduces a formal security model for a microprocessor hardware system. The model has been developed as part of the evaluation process of the processor product according to ITSEC assurance level E4. Novel aspects of the model are the need for defining integrity and confidentiality objectives on the hardware level without the operating system or application specification and security policy being given, and the utilization of an abstract function and data space. The security model consists of a system model given as a state transition automaton on infinite structures and the formalization of security objectives by means of properties of automaton behaviors. Validity of the security properties is proved. The paper compares the model with published ones and summarizes the lessons learned throughout the modeling process.","0098-5589;1939-3520;2326-3881","","10.1109/32.879809","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879809","","Microprocessors;Hardware;Operating systems;Data security;Application software;Learning automata;Quality assurance;Formal specifications;Context modeling;Access control","microcomputers;security of data;data integrity;automata theory;theorem proving;formal verification;quality management","microprocessor hardware;formal security model;evaluation process;processor product;ITSEC assurance level;E4;integrity;confidentiality objectives;hardware level;operating system;application specification;security policy;abstract function;data space;system model;state transition automaton;infinite structures;security objectives;automaton behaviors;security properties;modeling process;validity proving","","2","","","","","","","","IEEE","IEEE Journals & Magazines"
"Applying Concept Analysis to User-Session-Based Testing of Web Applications","S. Sampath; S. Sprenkle; E. Gibson; L. Pollock; A. Souter Greenwald","IEEE Computer Society; NA; NA; IEEE Computer Society; IEEE Computer Society","IEEE Transactions on Software Engineering","","2007","33","10","643","658","The continuous use of the Web for daily operations by businesses, consumers, and the government has created a great demand for reliable Web applications. One promising approach to testing the functionality of Web applications leverages the user-session data collected by Web servers. User-session-based testing automatically generates test cases based on real user profiles. The key contribution of this paper is the application of concept analysis for clustering user sessions and a set of heuristics for test case selection. Existing incremental concept analysis algorithms are exploited to avoid collecting and maintaining large user-session data sets and to thus provide scalability. We have completely automated the process from user session collection and test suite reduction through test case replay. Our incremental test suite update algorithm, coupled with our experimental study, indicates that concept analysis provides a promising means for incrementally updating reduced test suites in response to newly captured user sessions with little loss in fault detection capability and program coverage.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70723","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302777","Software testing;Web applications;User-session-based testing;Test suite reduction;Concept analysis;Incremental test suite reduction","Application software;Automatic testing;Web server;Computer Society;Government;Algorithm design and analysis;Software testing;Clustering algorithms;Scalability;Fault detection","fault tolerant computing;Internet;program testing;user modelling","program coverage;fault detection;incremental test suite update algorithm;incremental test suite reduction;test case selection;user session clustering;user profile;test case generation;Web server;reliable Web application;user-session based testing;incremental concept analysis algorithm","","61","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Using patterns to design rules in workflows","F. Casati; S. Castano; M. Fugini; I. Mirbel; B. Pernici","Dipt. di Elettronica e Inf., Politecnico di Milano, Italy; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","8","760","785","In order to design workflows in changing and dynamic environments, a flexible, correct, and rapid realization of models of the activity flow is required. In particular, techniques are needed to design workflows capable of adapting themselves effectively when exceptional situations occur during process execution. The authors present an approach to flexible workflow design based on rules and patterns developed in the framework of the WIDE project. Rules allow a high degree of flexibility during workflow design by modeling exceptional aspects of the workflow separately from the main activity flow. Patterns model frequently occurring exceptional situations in a generalized way by providing the designer with skeletons of rules and suggestions about their instantiation, together with indications on relationships with other rules, with the activity flow, and with related information. Pattern based design relies on a pattern catalog containing patterns to be reused and on a formal basis for specializing and instantiating available patterns.","0098-5589;1939-3520;2326-3881","","10.1109/32.879813","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879813","","Computer Society;Proposals;Skeleton;Process control;Control systems;Business communication","bibliographies;workflow management software;object-oriented programming;software reusability;knowledge based systems","rule design;dynamic environments;activity flow;exceptional situations;process execution;flexible workflow design;WIDE project;exceptional aspects;main activity flow;frequently occurring exceptional situations;rule instantiation;pattern based design;pattern catalog;pattern reuse;formal basis","","34","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Formally verified on-line diagnosis","C. J. Walter; P. Lincoln; N. Suri","WW Technol. Group, Ellicott City, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","11","684","721","A reconfigurable fault tolerant system achieves the attributes of dependability of operations through fault detection, fault isolation and reconfiguration, typically referred to as the FDIR paradigm. Fault diagnosis is a key component of this approach, requiring an accurate determination of the health and state of the system. An imprecise state assessment can lead to catastrophic failure due to an optimistic diagnosis, or conversely, result in underutilization of resources because of a pessimistic diagnosis. Differing from classical testing and other off-line diagnostic approaches, we develop procedures for maximal utilization of the system state information to provide for continual, on-line diagnosis and reconfiguration capabilities as an integral part of the system operations. Our diagnosis approach, unlike existing techniques, does not require administered testing to gather syndrome information but is based on monitoring the system message traffic among redundant system functions. We present comprehensive on-line diagnosis algorithms capable of handling a continuum of faults of varying severity at the node and link level. Not only are the proposed algorithms on-line in nature, but are themselves tolerant to faults in the diagnostic process. Formal analysis is presented for all proposed algorithms. These proofs offer both insight into the algorithm operations and facilitate a rigorous formal verification of the developed algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/32.637385","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637385","","Fault diagnosis;Fault tolerant systems;Costs;System testing;Fault detection;Monitoring;Algorithm design and analysis;Formal verification;Resource management","online operation;reconfigurable architectures;software fault tolerance;program diagnostics;program verification;program testing","online diagnosis;formal verification;reconfigurable fault tolerant system;operation dependability;fault detection;fault isolation;FDIR paradigm;fault diagnosis;optimistic diagnosis;pessimistic diagnosis;testing;system state information;system message traffic monitoring;redundant system functions","","33","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Automatically ‘Verifying’ Discrete-Time Complex Systems through Learning, Abstraction and Refinement","J. Wang; J. Sun; S. Qin; C. Jegourel","Information Systems Technology and Design, Singapore University of Technology and Design, 233793 Singapore, Singapore Singapore 487372 (e-mail: wangjyee@gmail.com); Computer Science, Singapore University of Technology and Design, SINGAPORE, Singapore Singapore 487372 (e-mail: sunjun@sutd.edu.sg); School of Computing, University of Teesside, Middlesbrough, Cleveland United Kingdom of Great Britain and Northern Ireland TS1 3BA (e-mail: s.qin@tees.ac.uk); ISTD, Singapore University of Technology and Design, 233793 Singapore, Singapore Singapore (e-mail: cyrille.jegourel@gmail.com)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Precisely modeling complex systems like cyber-physical systems is challenging, which often render model-based system verification techniques like model checking infeasible. To overcome this challenge, we propose a method called LAR to automatically ‘verify’ such complex systems through a combination of learning, abstraction and refinement from a set of system log traces. We assume that log traces and sampling frequency are adequate to capture ‘enough’ behaviour of the system. Given a safety property and the concrete system log traces as input, LAR automatically learns and refines system models, and produces two kinds of outputs. One is a counterexample with a bounded probability of being spurious. The other is a probabilistic model based on which the given property is ‘verified’. The model can be viewed as a proof obligation, i.e., the property is verified if the model is correct. It can also be used for subsequent system analysis activities like runtime monitoring or model-based testing. Our method has been implemented as a self-contained software toolkit. The evaluation on multiple benchmark systems as well as a real-world water treatment system shows promising results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2886898","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8576657","Verification;model learning;abstraction refinement;Cyber-physical system","Probabilistic logic;Model checking;Analytical models;Safety;Complex systems;System analysis and design","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A study of the effect of imperfect debugging on software development cost","Min Xie; Bo Yang","Dept. of Ind. & Syst. Eng., Nat. Univ. of Singapore, Singapore; NA","IEEE Transactions on Software Engineering","","2003","29","5","471","473","It is widely recognized that the debugging processes are usually imperfect. Software faults are not completely removed because of the difficulty in locating them or because new faults might be introduced. Hence, it is of great importance to investigate the effect of the imperfect debugging on software development cost, which, in turn, might affect the optimal software release time or operational budget. In this paper, a commonly used cost model is extended to the case of imperfect debugging. Based on this, the effect of imperfect debugging is studied. As the probability of perfect debugging, termed testing level here, is expensive to be increased, but manageable to a certain extent with additional resources, a model incorporating this situation is presented. Moreover, the problem of determining the optimal testing level is considered. This is useful when the decisions regarding the test team composition, testing strategy, etc., are to be made for more effective testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199075","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199075","","Programming;Cost function;Software debugging;Software testing;Resource management;Software reliability;Fault diagnosis;Software systems;Personnel","software engineering;program debugging;software reliability;software development management","debugging processes;imperfect debugging;testing optimization;software development cost;optimal software release time;software reliability;testing level","","54","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Using a Protean language to enhance expressiveness in specification","B. Bloom; A. Cheng; A. Dsouza","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","4","224","234","A Protean specification language (B. Bloom, 1995) based on structured operational semantics (SOS) allows the user to invent appropriate operations to improve abstraction and readability. This is in contrast to traditional specification languages, where the set of operations is fixed. An efficient algorithm, described by A. Dsouza and B. Bloom (1995), uses binary decision diagrams (BDDs) to verify properties of finite specifications written in a Protean language and provides the basis for a model checker we have developed. The paper provides a synthesis of our work on Protean languages and relates the work to other specification techniques. We show how abstraction and refinement in the Protean framework can improve the effectiveness of model checking. We rewrite and verify properties of an existing Z specification by defining suitable operations. We also show how a Protean language can be used to model restricted I/O automata, action refinement, and 1-safe and k-bounded Petri nets.","0098-5589;1939-3520;2326-3881","","10.1109/32.588539","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=588539","","Specification languages;Algebra;Data structures;Boolean functions;Petri nets;Automata;Process design;Protocols;ISO standards","specification languages;formal specification;rewriting systems;program verification;process algebra","Protean language;expressiveness enhancement;Protean specification language;structured operational semantics;abstraction;readability;specification languages;efficient algorithm;binary decision diagrams;finite specifications;model checker;specification techniques;model checking;Z specification;restricted I/O automata;action refinement;k-bounded Petri nets","","3","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Extracting reusable functions by flow graph based program slicing","F. Lanubile; G. Visaggio","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA","IEEE Transactions on Software Engineering","","1997","23","4","246","259","An alternative approach to developing reusable components from scratch is to recover them from existing systems. We apply program slicing, a program decomposition method, to the problem of extracting reusable functions from ill structured programs. As with conventional slicing first described by M. Weiser (1984), a slice is obtained by iteratively solving data flow equations based on a program flow graph. We extend the definition of program slice to a transform slice, one that includes statements which contribute directly or indirectly to transform a set of input variables into a set of output variables. Unlike conventional program slicing, these statements do not include either the statements necessary to get input data or the statements which test the binding conditions of the function. Transform slicing presupposes the knowledge that a function is performed in the code and its partial specification, only in terms of input and output data. Using domain knowledge we discuss how to formulate expectations of the functions implemented in the code. In addition to the input/output parameters of the function, the slicing criterion depends on an initial statement, which is difficult to obtain for large programs. Using the notions of decomposition slice and concept validation we show how to produce a set of candidate functions, which are independent of line numbers but must be evaluated with respect to the expected behavior. Although human interaction is required, the limited size of candidate functions makes this task easier than looking for the last function instruction in the original source code.","0098-5589;1939-3520;2326-3881","","10.1109/32.588543","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=588543","","Software reusability;Data mining;Software quality;Programming profession;Computer Society;Flow graphs;Transforms;Software systems;Equations;Input variables","program diagnostics;flow graphs;software reusability;reverse engineering","reusable function extraction;flow graph based program slicing;reusable components;program slicing;program decomposition method;ill structured programs;data flow equations;program flow graph;transform slice;output variables;transform slicing;domain knowledge;input/output parameters;slicing criterion;decomposition slice;concept validation;human interaction;candidate functions","","72","","44","","","","","","IEEE","IEEE Journals & Magazines"
"A unified high-level Petri net formalism for time-critical systems","C. Ghezzi; D. Mandrioli; S. Morasca; M. Pezze","Dipartimento di Elettronica, Politecnico di Milano, Italy; Dipartimento di Elettronica, Politecnico di Milano, Italy; Dipartimento di Elettronica, Politecnico di Milano, Italy; NA","IEEE Transactions on Software Engineering","","1991","17","2","160","172","The authors introduce a high-level Petri net formalism-environment/relationship (ER) nets-which can be used to specify control, function, and timing issues. In particular, they discuss how time can be modeled via ER nets by providing a suitable axiomatization. They use ER nets to define a time notation that is shown to generalize most time Petri-net-based formalisms which appeared in the literature. They discuss how ER nets can be used in a specification support environment for a time-critical system and, in particular, the kind of analysis supported.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67597","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67597","","Time factors;Real time systems;Petri nets;Erbium;Timing;Formal specifications;Mice;Power system modeling;Software prototyping;Prototypes","formal specification;Petri nets;software tools","environment relationship nets;high-level Petri net;time-critical systems;timing;ER nets;time notation;specification support environment","","208","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Automated tuning of parallel I/O systems: an approach to portable I/O performance for scientific applications","Ying Chen; M. Winslett","IBM Almaden Res. Center, San Jose, CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","4","362","383","Parallel I/O systems typically consist of individual processors, communication networks, and a large number of disks. Managing and utilizing these resources to meet performance, portability, and usability goals of high performance scientific applications has become a significant challenge. For scientists, the problem is exacerbated by the need to retune the I/O portion of their code for each supercomputer platform where they obtain access. We believe that a parallel I/O system that automatically selects efficient I/O plans for user applications is a solution to this problem. The authors present such an approach for scientific applications performing collective I/O requests on multidimensional arrays. Under our approach, an optimization engine in a parallel I/O system selects high quality I/O plans without human intervention, based on a description of the application I/O requests and the system configuration. To validate our hypothesis, we have built an optimizer that uses rule based and randomized search based algorithms to tune parameter settings in Panda, a parallel I/O library for multidimensional arrays. Our performance results obtained from an IBM SP using an out-of-core matrix multiplication application show that the Panda optimizer is able to select high quality I/O plans and deliver high performance under a variety of system configurations with a small total optimization overhead.","0098-5589;1939-3520;2326-3881","","10.1109/32.844494","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=844494","","Application software;Humans;Computer Society;Communication networks;Multidimensional systems;Analytical models;Cost function;Database systems;Resource management;Usability","bibliographies;parallel programming;software portability;natural sciences computing;program diagnostics;input-output programs;matrix multiplication;software libraries","automated tuning;parallel I/O systems;portable I/O performance;scientific applications;usability goals;supercomputer platform;parallel I/O system;user applications;collective I/O requests;multidimensional arrays;optimization engine;high quality I/O plans;human intervention;system configuration;randomized search based algorithms;parameter tuning;parameter settings;Panda;parallel I/O library;IBM SP;out-of-core matrix multiplication application;system configurations;total optimization overhead","","1","","57","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of software reuse with special attention to Ada","Nam-Yong Lee; C. R. Litecky","Inf. Syst. Directorate, Korea Inst. for Defense Inf. Syst., Seoul, South Korea; NA","IEEE Transactions on Software Engineering","","1997","23","9","537","549","Over the past several decades, numerous software technologies have been developed for overcoming the software crisis. Among these technologies, reuse has been recognized as one of the most important software technologies. Recently, it has gained substantial attention as a possible solution to the software crisis in Ada and other software communities. The purpose of this empirical study is to examine how organizations actually exploit reuse technologies and evaluates how reuse factors affect the rate of reuse in an organization. This study is an attempt to enhance the measurement of the rate of reuse and the effectiveness of reuse by establishing conceptual foundations in the literature for reuse and conducting an empirical investigation of organizations using Ada technology. This study differentiated software reuse into six criteria: domain, human, tool, organization, software metrics, and environment. The results of this study show that the rate of reuse significantly depends upon reuse capability, software development effort, object-oriented design capability, repository development effort, Ada technology capability, and domain capability.","0098-5589;1939-3520;2326-3881","","10.1109/32.629492","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=629492","","Programming;Software metrics;Software tools;Humans;Software engineering;Regression analysis;Government;Computer industry;Software standards;Standards development","Ada;object-oriented languages;object-oriented programming;software reusability;software metrics;software tools","software reuse;Ada;software crisis;organizations;measurement;literature;domain capability;human factors;software tool;software metrics;software environment;software development effort;object-oriented design;repository development;factor analysis;multiple regression analysis","","26","","91","","","","","","IEEE","IEEE Journals & Magazines"
"Exploring Community Smells in Open-Source: An Automated Approach","D. A. A. Tamburri; F. Palomba; R. Kazman","Computer Science, Eindhoven University of Technology, Eindhoven, Netherlands Netherlands (e-mail: dtamburri@acm.org); Department of Informatics, University of Zurich, Zurich, Zurich Switzerland 8050 (e-mail: palomba@ifi.uzh.ch); Department of Information Technology Management, University of Hawaii, Honolulu, Hawaii United States 96822 (e-mail: kazman@hawaii.edu)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Software engineering is now more than ever a community effort. Its success often weighs on balancing distance, culture, global engineering practices and more. In this scenario many unforeseen socio-technical events may result into additional project cost or ?social"" debt, e.g., sudden, collective employee turnover. With industrial research we discovered community smells, that is, sub-optimal patterns across the organisational and social structure in a software development community that are precursors of such nasty socio-technical events. To understand the impact of community smells at large, in this paper we first introduce CodeFace4Smells, an automated approach able to identify four community smell types that reflect socio-technical issues that have been shown to be detrimental both the software engineering and organisational research fields. Then, we perform a large-scale empirical study involving over 100 years worth of releases and communication structures data of 60 open-source communities: we evaluate (i) their diffuseness, i.e., how much are they distributed in open-source, (ii) how developers perceive them, to understand whether practitioners recognize their presence and their negative effects in practice, and (iii) how community smells relate to existing socio-technical factors, with the aim of assessing the inter-relations between them. The key findings of our study highlight that community smells are highly diffused in open-source and are perceived by developers as relevant problems for the evolution of software communities. Moreover, a number of state-of-the-art socio-technical indicators (e.g., socio-technical congruence) can be used to monitor how healthy a community is and possibly avoid the emergence of social debt.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2901490","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8651329","Software Organisational Structures;Software Community Smells;Human Aspects in Software Engineering;Social Software Engineering;Empirical Software Engineering","Software engineering;Open source software;Organizational aspects;Social networking (online);Tools;Microstructure","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Debugging Larch shared language specifications","S. J. Garland; J. V. Guttag; J. J. Horning","Lab. for Comput. Sci., MIT, Cambridge, MA, USA; Lab. for Comput. Sci., MIT, Cambridge, MA, USA; NA","IEEE Transactions on Software Engineering","","1990","16","9","1044","1057","The checkability designed into the LSL (Larch shared language) is described, and two tools that help perform the checking are discussed. LP (the Larch power) is the principal debugging tool. Its design and development have been motivated primarily by work on LSL, but it also has other uses (e.g. reasoning about circuits and concurrent algorithms). Because of these other uses, and because they also tend to use LP to analyze Larch interface specifications, the authors have tried not to make LP too LSL-specific. Instead, they have chosen to build a second tool, LSLC (the LSL checker), to serve as a front-end to LP. LSLC checks the syntax and static semantics of LSL specifications and generates LP proof obligations from their claims. These proof obligations fall into three categories: consistency (that a specification does not contradict itself), theory containment (that a specification has intended consequences), and relative completeness (that a set of operators is adequately defined). An extended example illustrating how LP is used to debug LSL specifications is presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58789","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58789","","Debugging;Formal specifications;Specification languages;Logic testing;Costs;Buildings;Modular construction;Crops;Laboratories;Algorithm design and analysis","formal specification;inference mechanisms;parallel programming;program debugging","Larch shared language specifications;debugging;checkability;Larch power;design;development;concurrent algorithms;static semantics;consistency;theory containment","","24","","19","","","","","","IEEE","IEEE Journals & Magazines"
"A highly available local leader election service","C. Fetzer; F. Cristian","AT&T, Florham-Park, NJ, USA; NA","IEEE Transactions on Software Engineering","","1999","25","5","603","618","We define the highly available local leader election problem (G. LeLann, 1977), a generalization of the leader election problem for partitionable systems. We propose a protocol that solves the problem efficiently and give some performance measurements of our implementation. The local leader election service has been proven useful in the design and implementation of several fail-aware services for partitionable systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.815321","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=815321","","Nominations and elections;Protocols;Broadcasting;Measurement;Bridges;Local area networks;Availability;Clocks;Synchronization","distributed processing;protocols;fault tolerant computing","highly available local leader election service;partitionable systems;protocol;performance measurements;fail-aware services;timed asynchronous systems","","28","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Software testing based on SDL specifications with save","G. Luo; A. Das; G. v. Bochmann","Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada; NA; NA","IEEE Transactions on Software Engineering","","1994","20","1","72","87","The signal save construct is one of the features distinguishing SDL from traditional high-level specification and programming languages. However, this feature increases the difficulties of testing SDL-specified software. We present a testing approach consisting of the following three phases: SDL specifications are first abstracted into finite state machines with save constructs, called SDL-machines; the resulting SDL-machines are then transformed into equivalent finite state machines without save constructs if this is possible; and, finally, test cases are selected from the resulting finite state machines. Since there are many existing methods for the first and third phases, we mainly concentrate upon the second phase and come up with a method of transforming SDL-machines into equivalent finite state machines, which preserve the same input/output relationship as in the original SDL-machines. The transformation method is useful not only for testing but also for verifying SDL-specified software.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.263756","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=263756","","Software testing;Automata;Protocols;Computer languages;Formal specifications;Communication standards;Software standards;Application software;Communication industry;Computer science","program testing;finite state machines;formal specification;specification languages;program verification","SDL specifications;software testing;signal save construct;high-level specification;programming languages;finite state machines;SDL-machines;SDL-machine transformation;input/output relationship;software verification","","21","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Where Should We Fix This Bug? A Two-Phase Recommendation Model","D. Kim; Y. Tao; S. Kim; A. Zeller","The Hong Kong University of Science and Technology, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong; Saarland University, Saarland","IEEE Transactions on Software Engineering","","2013","39","11","1597","1610","To support developers in debugging and locating bugs, we propose a two-phase prediction model that uses bug reports' contents to suggest the files likely to be fixed. In the first phase, our model checks whether the given bug report contains sufficient information for prediction. If so, the model proceeds to predict files to be fixed, based on the content of the bug report. In other words, our two-phase model ""speaks up"" only if it is confident of making a suggestion for the given bug report; otherwise, it remains silent. In the evaluation on the Mozilla ""Firefox"" and ""Core"" packages, the two-phase model was able to make predictions for almost half of all bug reports; on average, 70 percent of these predictions pointed to the correct files. In addition, we compared the two-phase model with three other prediction models: the Usual Suspects, the one-phase model, and BugScout. The two-phase model manifests the best prediction performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6517844","Bug reports;machine learning;patch file prediction","Predictive models;Feature extraction;Computer bugs;Software;Computational modeling;Data mining;Noise","formal verification;program debugging","two-phase recommendation model;debugging;two-phase prediction model;bug report;two-phase model;speaks up;Mozilla packages;Firefox packages;Core packages;BugScout","","34","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Nontraversible Paths in a Program","M. Chellappa","Flight Control Systems Division, Aeronautical Development Establishment","IEEE Transactions on Software Engineering","","1987","SE-13","6","751","756","A finite-state machine representation of a program graph is shown to have the property of exposing nontraversible paths in a program. A minimal covering set of paths for such a program may fail to yield realizable test cases, as the nontraversible paths have inconsistent path predicates.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233480","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702280","Directed graph;finite state machine;path analysis;path predicates;test case selection;test cover","Testing;Flowcharts;Computer errors;Performance evaluation;Imaging phantoms;Control systems","","Directed graph;finite state machine;path analysis;path predicates;test case selection;test cover","","","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Matching and Merging of Variant Feature Specifications","S. Nejati; M. Sabetzadeh; M. Chechik; S. Easterbrook; P. Zave","Simula Research Laboratory, Lysaker; Simula Research Laboratory, Lysaker; University of Toronto, Toronto; University of Toronto, Toronto; AT&T Laboratories-Research, Florham Park","IEEE Transactions on Software Engineering","","2012","38","6","1355","1375","Model Management addresses the problem of managing an evolving collection of models by capturing the relationships between models and providing well-defined operators to manipulate them. In this paper, we describe two such operators for manipulating feature specifications described using hierarchical state machine models: Match, for finding correspondences between models, and Merge, for combining models with respect to known or hypothesized correspondences between them. Our Match operator is heuristic, making use of both static and behavioral properties of the models to improve the accuracy of matching. Our Merge operator preserves the hierarchical structure of the input models, and handles differences in behavior through parameterization. This enables us to automatically construct merges that preserve the semantics of hierarchical state machines. We report on tool support for our Match and Merge operators, and illustrate and evaluate our work by applying these operators to a set of telecommunication features built by AT&T.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.112","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6086550","Model management;match;merge;hierarchical state machines;statecharts;behavior preservation;variability modeling;parameterization","Computational modeling;Semantics;Hierarchical systems;Pragmatics;Parameterization;Electronic mail;Voice mail","finite state machines;formal specification","variant feature specification;model management;hierarchical state machine model;match operator;static property;behavioral property;merge operator;hierarchical structure;tool support;telecommunication feature","","8","","83","","","","","","IEEE","IEEE Journals & Magazines"
"Eager Reclamation","Ching-Chy Wang; M. L. Soffa","IBM T. J. Watson Research Center; NA","IEEE Transactions on Software Engineering","","1985","SE-11","4","437","439","Reclamation policies are presented which permit the reuse of storage for procedure instances before the procedures terminate. These eager reclamation policies are applicable under certain conditions and can reduce the total amount of memory needed for the execution of a program. The policies essentially differ in the amount of run-time checks needed and the reclamation opportunities. This eager reclamation strategy would be particularly useful when instances are implemented using register banks or in a microprocessor environment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232232","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702025","Implementation;optimization;procedure reclamation;reclamation;storage management","Runtime;Iterative algorithms;Programming profession;Computer science;Registers;Microprocessors;Iterative methods;Automatic control;Optimizing compilers;Program processors","","Implementation;optimization;procedure reclamation;reclamation;storage management","","","","10","","","","","","IEEE","IEEE Journals & Magazines"
"A practical method for specification and analysis of exception handling-a Java/JVM case study","E. Borger; W. Schulte","Dipartimento di Inf., Pisa Univ., Italy; NA","IEEE Transactions on Software Engineering","","2000","26","9","872","887","We provide a rigorous framework for language and platform independent design and analysis of exception handling mechanisms in modern programming languages and their implementations. To illustrate the practicality of the method we develop it for the exception handling mechanism of Java and show that its implementation on the Java Virtual Machine (JVM) Is correct. For this purpose we define precise abstract models for exception handling in Java and in the JVM and define a compilation scheme of Java to JVM code which allows us to prove that, in corresponding runs, Java and the JVM throw the same exceptions and with equivalent effect. Thus, the compilation scheme can, with reasonable confidence, be used as a standard reference for Java exception handling compilation.","0098-5589;1939-3520;2326-3881","","10.1109/32.877847","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877847","","Java;Computer aided software engineering;Object oriented modeling;Virtual machining;Programming profession;Computer languages;Runtime;Program processors;Concurrent computing;Mathematical model","exception handling;Java;program compilers;formal specification","exception handling specification;exception handling analysis;programming languages;Java;Java Virtual Machine;precise abstract models;compilation scheme","","4","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Dealing with Burstiness in Multi-Tier Applications: Models and Their Parameterization","G. Casale; N. Mi; L. Cherkasova; E. Smirni","Imperial College London, London; Northeastern University, Boston; Hewlett-Packard Laboratories, Palo Alto; College of William and Mary, Williamsburg","IEEE Transactions on Software Engineering","","2012","38","5","1040","1053","Workloads and resource usage patterns in enterprise applications often show burstiness resulting in large degradation of the perceived user performance. In this paper, we propose a methodology for detecting burstiness symptoms in multi-tier applications but, rather than identifying the root cause of burstiness, we incorporate this information into models for performance prediction. The modeling methodology is based on the index of dispersion of the service process at a server, which is inferred by observing the number of completions within the concatenated busy times of that server. The index of dispersion is used to derive a Markov-modulated process that captures burstiness and variability of the service process at each resource well and that allows us to define queueing network models for performance prediction. Experimental results and performance model predictions are in excellent agreement and argue for the effectiveness of the proposed methodology under both bursty and nonbursty workloads. Furthermore, we show that the methodology extends to modeling flash crowds that create burstiness in the stream of requests incoming to the application.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311395","Capacity planning;multi-tier applications;bursty workload;bottleneck switch;index of dispersion","Servers;Indexes;Dispersion;Switches;Predictive models;Estimation","client-server systems;Markov processes;queueing theory;resource allocation;software architecture","multitier applications;resource usage patterns;workload patterns;enterprise applications;user performance degradation;burstiness symptom detection;burstiness root cause identification;performance prediction model;server busy times;Markov-modulated process;service process variability;queueing network models;nonbursty workloads;flash crowd model","","22","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting (individual) software productivity","W. S. Humphrey; N. D. Singpurwalla","Software Eng. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA; Software Eng. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA","IEEE Transactions on Software Engineering","","1991","17","2","196","207","A method for projecting software productivity with reasonable accuracy which uses the statistical techniques of time series analysis is described. The measure of productivity is the development time required per line of code. In making productivity projections, the key issue is the need to achieve a balance between forecasting stability and responsiveness to changing conditions. An integrated moving average process of order one, using exponential smoothing of all the previous observations, is judged appropriate for software productivity analysis, particularly where there are limited data available or where conditions are sufficiently varied to make much of the available data inapplicable. Empirical evidence suggests that most commonly encountered time series can be reasonably well described by such methods. The methods for computing the weights used for exponential smoothing are described, as are the means for determining prediction intervals, or measures of forecast uncertainty. This data analytic approach uses historical data alone, unlike structural methods where learning curves as well as prior data are used to define the predictive process.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67600","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67600","","Productivity;Time series analysis;Time measurement;Smoothing methods;Software engineering;Programming profession;Meeting planning;Stability;Delay;Measurement uncertainty","DP management;software engineering;statistical analysis","management;software productivity;statistical techniques;time series analysis;development time;moving average process;limited data;exponential smoothing;forecast uncertainty;historical data","","19","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Factors that impact implementing a system development methodology"" [with reply]","S. B. Yadav; N. G. Shaw; L. Webb; C. Sutcu; T. L. Roberts; M. L. Gibson; R. K. Rainer; K. T. Fields","Dept. of ISQS, Texas Tech. Univ., Lubbock, TX, USA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","279","286","In this correspondence, we point out some of the major shortcomings that we have discovered in the article titled ""Factors that Impact Implementing a System Development Methodology"" by Roberts et al. (1998). The article was published in the August 1998 issue of IEEE Transactions of Software Engineering. The article contains multiple problems that, if not pointed out, have the potential to lead to a state of confusion among researchers and practitioners alike. In particular, the article has the following problems: (1) The authors claim that the lack of theoretical basis for their factors is due to the fact that SDM implementation has never been studied in the literature. In fact, there is a multitude of studies very similar to that of Roberts at al. (2) The study does not meet commonly accepted standards for factor analysis procedures such as a minimum sample size ratio of 5:1, a ratio mandated even by the authors' own citations. This and other factor analysis problems lead to results that are questionable. In order to make the published article more useful to researchers and practitioners, this paper corrects some of the inaccuracies in the Roberts et al.'s article by: providing a brief literature review of some articles that are similar to the Roberts et al., study; and noting some of the technical inaccuracies in the data analysis procedures used by the authors so that the results can be interpreted in the proper context.","0098-5589;1939-3520;2326-3881","","10.1109/32.910863","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910863","","Data analysis;Software engineering;Citation analysis;Design methodology","software engineering;data analysis","system development methodology;factor analysis procedures;minimum sample size ratio;literature review;data analysis","","5","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Timed Communicating Object Z","B. Mahony; Jin Song Dong","Div. of Inf. Technol., Defence Sci. & Technol. Organ., Salisbury, SA, Australia; NA","IEEE Transactions on Software Engineering","","2000","26","2","150","177","This paper describes a timed, multithreaded object modeling notation for specifying real-time, concurrent, and reactive systems. The notation Timed Communicating Object Z (TCOZ) builds on Object Z's strengths in modeling complex data and algorithms, and on Timed CSP's strengths in modeling process control and real-time interactions. TCOZ Is novel in that it includes timing primitives, properly separates process control and data/algorithm issues and supports the modeling of true multithreaded concurrency. TCOZ is particularly well-suited for specifying complex systems whose components have their own thread of control. The expressiveness of the notation is demonstrated by a case study in specifying a multilift system that operates in real-time.","0098-5589;1939-3520;2326-3881","","10.1109/32.841115","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=841115","","Object oriented modeling;Process control;Real time systems;Concurrent computing;Timing;Formal specifications;Control systems;Algorithm design and analysis;Carbon capture and storage","formal specification;real-time systems;object-oriented languages;object-oriented methods","timed multithreaded object modeling notation;real-time systems;concurrent systems;reactive systems;specification;Timed Communicating Object Z;Timed CSP;real-time interactions;process control;complex data;algorithms;timing primitives;multithreaded concurrency;multilift system","","74","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Reducing inspection interval in large-scale software development","D. E. Perry; A. Porter; M. W. Wade; L. G. Votta; J. Perpich","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","7","695","705","We have found that, when software is developed by multiple, geographically separated teams, the cost-benefit trade-offs of software inspection change. In particular, this situation can significantly lengthen the inspection interval (calendar time needed to complete an inspection). Our research goal was to find a way to reduce the inspection interval without reducing inspection effectiveness. We believed that Internet technology offered some potential solutions, but we were not sure which technology to use nor what effects it would have on effectiveness. To conduct this research, we drew on the results of several empirical studies we had previously performed. These results clarified the role that meetings and individuals play in inspection effectiveness and interval. We conducted further studies showing that manual inspections without meetings were just as effective as manual inspections with them. On the basis of these and other findings and our understanding of Internet technology, we built an economical and effective tool that reduced the interval without reducing effectiveness. This tool, Hypercode, supports meetingless software inspections with geographically distributed reviewers. HyperCode is a platform-independent tool, developed on top of an Internet browser, that integrates seamlessly into the current development process. By seamless, we mean the tool produces a paper flow that is almost identical to the current inspection process. HyperCode's acceptance by its user community has been excellent. Moreover, we estimate that using HyperCode has reduced the inspection interval by 20 to 25 percent. We believe that, had we focused solely on technology (without considering the information our studies had uncovered), we would have created a more complex, but not necessarily more effective tool. We probably would have supported group meetings, restricted each participant's access to review comments, and supported a wider variety of inspection methods. In other words, the principles derived from our empirical studies dramatically and successfully directed our search for a technological solution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019483","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019483","","Inspection;Large-scale systems;Programming;Internet;Calendars;Costs;Computer Society;Software tools;Quality assurance;Collaborative software","software tools;computer aided software engineering;software quality;inspection;groupware;Internet;online front-ends;cost-benefit analysis","software inspection interval;large-scale software development;software development teams;cost-benefit tradeoffs;software inspection effectiveness;Internet technology;group meetings;manual inspections;Hypercode;meetingless software inspections;geographically distributed reviewers;platform-independent tool;Internet browser;paper flow;user community acceptance;participant access;review comments;World Wide Web-based code inspections;asynchronous communication;automated support;information flow;workflow","","26","","21","","","","","","IEEE","IEEE Journals & Magazines"
"On the applicability of Weyuker Property 9 to object-oriented structural inheritance complexity metrics","Gursaran; G. Roy","Dept. of Math., Dayalbagh Educ. Inst., Agra, India; NA","IEEE Transactions on Software Engineering","","2001","27","4","381","384","In the metric suite for object oriented design put forward by S.R. Chidamber and C.F. Kemerer (1994), it is observed that E. Weyuker's (1988) Property 9 is not satisfied by any of the structural inheritance complexity metrics. The same is also observed for candidate structural inheritance complexity metrics proposed by A.F. Brito and R. Carapuca (1994). The authors formally show that particular classes of inheritance metrics (that include the above proposals) that are defined on a directed graph abstraction of the inheritance structure and that are contrived on the assumptions and definitions given by Chidamber and Kemerer, can never satisfy Property 9. Furthermore, it is also argued that the formalisation can be generalized to include other classes of structural metrics that are not necessarily inheritance metrics.","0098-5589;1939-3520;2326-3881","","10.1109/32.917526","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917526","","Proposals;Measurement standards;Memory management;Runtime","object-oriented programming;software metrics;inheritance;directed graphs","Weyuker Property 9;object oriented structural inheritance complexity metrics;metric suite;object oriented design;directed graph abstraction;inheritance metrics","","8","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Adaptive Service Composition in Flexible Processes","D. Ardagna; B. Pernici","NA; NA","IEEE Transactions on Software Engineering","","2007","33","6","369","384","In advanced service oriented systems, complex applications, described as abstract business processes, can be executed by invoking a number of available Web services. End users can specify different preferences and constraints and service selection can be performed dynamically identifying the best set of services available at runtime. In this paper, we introduce a new modeling approach to the Web service selection problem that is particularly effective for large processes and when QoS constraints are severe. In the model, the Web service selection problem is formalized as a mixed integer linear programming problem, loops peeling is adopted in the optimization, and constraints posed by stateful Web services are considered. Moreover, negotiation techniques are exploited to identify a feasible solution of the problem, if one does not exist. Experimental results compare our method with other solutions proposed in the literature and demonstrate the effectiveness of our approach toward the identification of an optimal solution to the QoS constrained Web service selection problem","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1011","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181707","Web services;quality of service;service composition;integer programming.","Web services;Quality of service;Constraint optimization;Context awareness;Mixed integer linear programming;Linear programming;Runtime environment;Context-aware services;Grid computing;Fluctuations","integer programming;linear programming;quality of service;Web services","adaptive service composition;service oriented system;business process;Web services;QoS constraints;integer linear programming problem;loop peeling;optimization;negotiation technique;quality of service","","509","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Inference graphs: a computational structure supporting generation of customizable and correct analysis components","L. K. Dillon; R. E. K. Stirewalt","Dept. of Comput. Sci. & Eng., Michigan State Univ., USA; Dept. of Comput. Sci. & Eng., Michigan State Univ., USA","IEEE Transactions on Software Engineering","","2003","29","2","133","150","Amalia is a generator framework for constructing analyzers for operationally defined formal notations. These generated analyzers are components that are designed for customization and integration into a larger environment. The customizability, and efficiency of Amalia analyzers owe to a computational structure called an inference graph. This paper describes this structure, how inference graphs enable Amalia to generate analyzers for operational specifications, and how we build in assurance. On another level, this paper illustrates how to balance the need for assurance, which typically implies a formal proof obligation, against other design concerns, whose solutions leverage design techniques that are not (yet) accompanied by mature proof methods. We require Amalia-generated designs to be transparent with respect to the formal semantic models upon which they are based. Inference graphs are complex structures that incorporate many design optimizations. While not formally verifiable, their fidelity with respect to a formal operational semantics can be discharged by inspection.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178052","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178052","","Design optimization;Design engineering;Computer Society;Object oriented modeling;Inspection;Assembly;Software design;Design methodology;Software engineering;Computer science","program verification;program diagnostics;graphs","operationally defined formal notations;Amalia analyzers;computational structure;inference graph;operational specifications;assurance;formal proof obligation;mature proof methods;formal semantic models;correctness","","6","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Estimating software project effort using analogies","M. Shepperd; C. Schofield","Dept. of Comput., Bournemouth Univ., Poole, UK; NA","IEEE Transactions on Software Engineering","","1997","23","11","736","743","Accurate project effort prediction is an important goal for the software engineering community. To date most work has focused upon building algorithmic models of effort, for example COCOMO. These can be calibrated to local environments. We describe an alternative approach to estimation based upon the use of analogies. The underlying principle is to characterize projects in terms of features (for example, the number of interfaces, the development method or the size of the functional requirements document). Completed projects are stored and then the problem becomes one of finding the most similar projects to the one for which a prediction is required. Similarity is defined as Euclidean distance in n-dimensional space where n is the number of project features. Each dimension is standardized so all dimensions have equal weight. The known effort values of the nearest neighbors to the new project are then used as the basis for the prediction. The process is automated using a PC-based tool known as ANGEL. The method is validated on nine different industrial datasets (a total of 275 projects) and in all cases analogy outperforms algorithmic models based upon stepwise regression. From this work we argue that estimation by analogy is a viable technique that, at the very least, can be used by project managers to complement current estimation techniques.","0098-5589;1939-3520;2326-3881","","10.1109/32.637387","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637387","","Project management;Costs;Software engineering;Euclidean distance;Nearest neighbor searches;Programming;Software development management;Centralized control;Accuracy;Particle measurements","software development management;project management;software cost estimation;software tools;software metrics","software project effort estimation;project effort prediction;software engineering;algorithmic models;COCOMO;estimation by analogy;software development method;functional requirements document;Euclidean distance;nearest neighbors;personal computer-based tool;ANGEL;industrial datasets;stepwise regression;project management","","438","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""More success and failure factors in software reuse""","M. Morisio; M. Ezran; C. Tully","Dipt. di Automatica e Informatica, Politecnico di Torino, Italy; NA; NA","IEEE Transactions on Software Engineering","","2003","29","5","478","","For original paper see ibid., p. 474. This is a clear example of how research in software engineering can progress when empirical methods are applied. Menzies and Di Stefano apply a number of data mining tools to the data set. While, inmost cases, their results are in agreement with ours, in some cases they are not. Our first and main observation is that our interpretation of the data set is based not only on the data set itself but also on the knowledge gathered during the interviews with project members. The main problem with the data set is its size: 23 data points. Although this data set is the largest one available about reuse projects, it is too limited to base analysis only on data mining techniques; data mining is usually applied to data sets with thousands if not millions of data points.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199077","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199077","","Data mining;Predictive models;Testing;Data analysis;Human factors;Computer Society;Software engineering;Production;Association rules","data mining;software reusability","software reuse;software engineering;data set;data mining","","2","","2","","","","","","IEEE","IEEE Journals & Magazines"
"The imposition of protocols over open distributed systems","N. H. Minsky","Dept. of Comput. Sci., Rutgers Univ., New Brunswick, NJ, USA","IEEE Transactions on Software Engineering","","1991","17","2","183","195","In order to facilitate enforcement of protocols, an architecture for distributed systems is introduced under which all interactions between objects are governed by an explicit and strictly enforced set of rules, called the law of the system. This law is global in the sense that all the objects of the system are made to obey it, but the maintenance of the law and its enforcement are performed locally, at each object (or node). The term law is used to emphasized that it not only provides the specification of protocols, but actually governs the system by enforcing them. In other words, under this architecture a protocol can be established simply by writing it into the law of a system, without having to worry about the programs that drive the various objects that might populate that system. The law, then, is the enforced specification of protocols. It is shown that various familiar protocols can be established under this architecture. A technique for online distributed updating of the global law of a system is presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67599","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67599","","Protocols;Computer architecture;Writing;Distributed computing;Clocks;Computer science","distributed processing;protocols","protocols;open distributed systems;enforced specification;online distributed updating;global law","","39","","20","","","","","","IEEE","IEEE Journals & Magazines"
"On the Semantics of Associations and Association Ends in UML","D. Milicev","IEEE Computer Society","IEEE Transactions on Software Engineering","","2007","33","4","238","251","Association is one of the key concepts in UML that is intensively used in conceptual modeling. Unfortunately, in spite of the fact that this concept is very old and is inherited from other successful modeling techniques, a fully unambiguous understanding of it, especially in correlation with other newer concepts connected with association ends, such as uniqueness, still does not exist. This paper describes a problem with one widely assumed interpretation of the uniqueness of association ends, the restrictive interpretation, and proposes an alternative, the intentional interpretation. Instead of restricting the association from having duplicate links, uniqueness of an association end in the intentional interpretation modifies the way in which the association end maps an object of the opposite class to a collection of objects of the class at that association end. If the association end is unique, the collection is a set obtained by projecting the collection of all linked objects. In that sense, the uniqueness of an association end modifies the view to the objects at that end, but does not constrain the underlying object structure. This paper demonstrates how the intentional interpretation improves expressiveness of the modeling language and has some other interesting advantages. Finally, this paper gives a completely formal definition of the concepts of association and association ends, along with the related notions of uniqueness, ordering, and multiplicity. The semantics of the UML actions on associations are also defined formally","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123326","Object-oriented modeling;Unified Modeling Language (UML);association;association end;formal semantics;conceptual modeling;model-driven development.","Unified modeling language;Object oriented modeling;Application software;Switches;Erbium;Computer Society;Visualization;Software systems;Computer industry;Object oriented databases","entity-relationship modelling;formal specification;object-oriented programming;programming language semantics;Unified Modeling Language","UML;conceptual modeling;intentional interpretation;association end;object-oriented modeling;Unified Modeling Language;formal semantics;formal specification","","12","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Specification of realtime systems using ASTRAL","A. Coen-Porisini; C. Ghezzi; R. A. Kemmerer","Dipartimento di Elettronica, Politecnico di Milano, Italy; NA; NA","IEEE Transactions on Software Engineering","","1997","23","9","572","598","ASTRAL is a formal specification language for real-time systems. It is intended to support formal software development and, therefore, has been formally defined. The structuring mechanisms in ASTRAL allow one to build modularized specifications of complex systems with layering. A real-time system is modeled by a collection of state machine specifications and a single global specification. This paper discusses the rationale of ASTRAL's design. ASTRAL's specification style is illustrated by discussing a telephony example. Composability of one or more ASTRAL system specifications is also discussed by the introduction of a composition section, which provides the needed information to combine two or more ASTRAL system specifications.","0098-5589;1939-3520;2326-3881","","10.1109/32.629494","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=629494","","Formal specifications;Programming;Application software;Aerospace electronics;Testing;Specification languages;Telephony;Logic;Timing;Aircraft","formal specification;specification languages;real-time systems;telephony;telecommunication computing;program verification;temporal logic;finite state machines","real-time systems specification;ASTRAL;formal specification language;formal software development;structuring mechanisms;modularized specifications;complex systems;state machine specifications;global specification;telephony;program verification;temporal logic","","31","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Comments, with reply, on ""On the projection method for protocol verification"" by T.-Y. Cheung","Y. Hirakawa","NTT Electr. Commun. Lab., Tokyo, Japan","IEEE Transactions on Software Engineering","","1990","16","3","370","371","A counterexample to a theorem given in a paper by T.-Y. Cheung (see ibid., vol.12, no.11, p.1088-9, 1986) is given. In a reply, the author of the original paper argues that the counterexample is not correct.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.48933","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=48933","","Protocols;Safety;Sufficient conditions;Communication systems","program verification;protocols","projection method;protocol verification;theorem","","","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""On the applicability of Weyuker property 9 to object-oriented structural inheritance complexity metrics""","Lu Zhang; Dan Xie","Dept. of Comput. Sci., Liverpool Univ., UK; NA","IEEE Transactions on Software Engineering","","2002","28","5","526","527","In this paper, we point out some discrepancies in a correspondence published in this journal recently by Gursaran and G. Roy (see ibid., vol. 27, no. 4, p. 381-4 (2001)). Due to the discrepancies, the central two ""theorems"" and two ""corollaries"" claimed in that correspondence may not be held true in some extreme circumstances and, therefore, its main conclusion cannot be drawn for all the possible cases.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000454","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000454","","Software metrics;Object oriented programming","software metrics;object-oriented programming","Weyuker property 9;object-oriented structural inheritance complexity metrics;software complexity metrics","","7","","6","","","","","","IEEE","IEEE Journals & Magazines"
"An extensible system for source code analysis","G. Canfora; A. Cimitile; U. De Carlini; A. De Lucia","Fac. of Eng., Sannio Univ., Piazza Roma, Italy; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","9","721","740","Constructing code analyzers may be costly and error prone if inadequate technologies and tools are used. If they are written in a conventional programming language, for instance, several thousand lines of code may be required even for relatively simple analyses. One way of facilitating the development of code analyzers is to define a very high-level domain-oriented language and implement an application generator that creates the analyzers from the specification of the analyses they are intended to perform. This paper presents a system for developing code analyzers that uses a database to store both a no-loss fine-grained intermediate representation and the results of the analyses. The system uses an algebraic representation, called F(p), as the user-visible intermediate representation. Analyzers are specified in a declarative language, called F(p)-l, which enables an analysis to be specified in the form of a traversal of an algebraic expression, with access to, and storage of, the database information the algebraic expression indices. A foreign language interface allows the analyzers to be embedded in C programs. This is useful for implementing the user interface of an analyzer, for example, or to facilitate interoperation of the generated analyzers with pre-existing tools. The paper evaluates the strengths and limitations of the proposed system, and compares it to other related approaches.","0098-5589;1939-3520;2326-3881","","10.1109/32.713328","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=713328","","Performance analysis;Information analysis;Reverse engineering;Databases;Data mining;Computer languages;Software maintenance;Software systems;Programming profession;Testing","reverse engineering;software maintenance;application generators;formal specification;specification languages;user interfaces","extensible system;source code analysis;code analyzers;programming language;domain-oriented language;application generator;specification;database;algebraic representation;declarative language;algebraic expression;foreign language interface;C programs;user interface;reverse engineering;software maintenance","","16","","55","","","","","","IEEE","IEEE Journals & Magazines"
"On Weyuker's axioms for software complexity measures","J. C. Cherniavsky; C. H. Smith","Nat. Sci. Found., Washington, DC, USA; NA","IEEE Transactions on Software Engineering","","1991","17","6","636","638","Properties for software complexity measures are discussed. It is shown that a collection of nine properties suggested by E.J. Weyuker is inadequate for determining the quality of a software complexity measure. (see ibid., vol.14, p.1357-65, 1988). A complexity measure which satisfies all nine of the properties, but which has absolutely no practical utility in measuring the complexity of a program is presented. It is concluded that satisfying all of the nine properties is a necessary, but not sufficient, condition for a good complexity measure.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.87287","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=87287","","Software measurement;Software quality;Software maintenance;Fluid flow measurement;Computer languages;Programming;Metrology;Arithmetic;Software testing;Cost function","computational complexity;software metrics","software complexity measures","","31","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Using machine learning for estimating the defect content after an inspection","F. Padberg; T. Ragg; R. Schoknecht","Karlsruhe Univ., Germany; NA; NA","IEEE Transactions on Software Engineering","","2004","30","1","17","28","We view the problem of estimating the defect content of a document after an inspection as a machine learning problem: The goal is to learn from empirical data the relationship between certain observable features of an inspection (such as the total number of different defects detected) and the number of defects actually contained in the document. We show that some features can carry significant nonlinear information about the defect content. Therefore, we use a nonlinear regression technique, neural networks, to solve the learning problem. To select the best among all neural networks trained on a given data set, one usually reserves part of the data set for later cross-validation; in contrast, we use a technique which leaves the full data set for training. This is an advantage when the data set is small. We validate our approach on a known empirical inspection data set. For that benchmark, our novel approach clearly outperforms both linear regression and the current standard methods in software engineering for estimating the defect content, such as capture-recapture. The validation also shows that our machine learning approach can be successful even when the empirical inspection data set is small.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265733","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265733","","Machine learning;Inspection;Neural networks;Software engineering;Curve fitting;Linear regression;Software standards;Software testing;Quality assurance;Estimation error","learning (artificial intelligence);regression analysis;neural nets;program verification;program testing","defect content estimation;machine learning;nonlinear regression technique;neural network;empirical methods;software engineering;software inspection;program validation","","11","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Experimenting with quantitative evaluation tools for monitoring operational security","R. Ortalo; Y. Deswarte; M. Kaaniche","Lab. d'Autom. et d'Anal. des Syst., CNRS, Toulouse, France; NA; NA","IEEE Transactions on Software Engineering","","1999","25","5","633","650","This paper presents the results of an experiment in security evaluation. The system is modeled as a privilege graph that exhibits its security vulnerabilities. Quantitative measures that estimate the effort an attacker might expend to exploit these vulnerabilities to defeat the system security objectives are proposed. A set of tools has been developed to compute such measures and has been used in an experiment to monitor a large real system for nearly two years. The experimental results are presented and the validity of the measures is discussed. Finally, the practical usefulness of such tools for operational security monitoring is shown and a comparison with other existing approaches is given.","0098-5589;1939-3520;2326-3881","","10.1109/32.815323","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=815323","","Information security;Power system security;Computerized monitoring;Computer security;Computer Society;Data security;Computer networks;Collaborative software;Collaborative work;Power system interconnection","security of data;graph theory","quantitative evaluation tools;operational security monitoring;experiment;privilege graph","","189","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Risk Assessment of Computer Controlled Systems","M. O. Fryer","Engineering Design Division of EG&amp;G Idaho, Inc.","IEEE Transactions on Software Engineering","","1985","SE-11","1","125","129","Software is increasingly being used to control and monitor systems for which safety and reliability are critical. When comparing software designs for such systems, an evaluation of how each design can contribute to the risk of system failure is desirable. Unfortunately, the science of risk assessment of combined hardware and software systems is in its infancy. Risk assessment of combined hardware/software systems is often based on oversimplified assumptions about software behavior.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231849","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701904","Fault trees;probability of system failure;relative failure probabilities;risk assessment;software reliability","Risk management;Control systems;Hardware;Fault trees;Application software;Safety;Software design;Software systems;Computerized monitoring;Condition monitoring","","Fault trees;probability of system failure;relative failure probabilities;risk assessment;software reliability","","1","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Detection of Mutual Inconsistency in Distributed Systems""","K. V. S. Ramarao","Department of Computer Science, University of Pittsburgh","IEEE Transactions on Software Engineering","","1987","SE-13","6","759","760","In the above paper,<sup>1</sup>authors have given a technique for the detection of mutual inconsistency among the copies of a file, in presence of network partitioning. This technique involves maintaining a ""version vector"" with each file in a partition. Here, we show that it is impossible to maintain the version vectors when arbitrary reconfigurations can occur after a network partitioning.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233482","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702282","Database consistency;fault-tolerance;network partitioning;optimistic protocols","Protocols;Computer science;Fault tolerance;Merging;Broadcasting","","Database consistency;fault-tolerance;network partitioning;optimistic protocols","","","","3","","","","","","IEEE","IEEE Journals & Magazines"
"The Use of Multilegged Arguments to Increase Confidence in Safety Claims for Software-Based Systems: A Study Based on a BBN Analysis of an Idealized Example","B. Littlewood; D. Wright","NA; NA","IEEE Transactions on Software Engineering","","2007","33","5","347","365","The work described here concerns the use of so-called multilegged arguments to support dependability claims about software-based systems. The informal justification for the use of multilegged arguments is similar to that used to support the use of multiversion software in pursuit of high reliability or safety. Just as a diverse 1-out-of-2 system might be expected to be more reliable than each of its two component versions, so might a two-legged argument be expected to give greater confidence in the correctness of a dependability claim (for example, a safety claim) than would either of the argument legs alone. Our intention here is to treat these argument structures formally, in particular, by presenting a formal probabilistic treatment of ""confidence,"" which will be used as a measure of efficacy. This will enable claims for the efficacy of the multilegged approach to be made quantitatively, answering questions such as, ""How much extra confidence about a system's safety will I have if I add a verification argument leg to an argument leg based upon statistical testing?"" For this initial study, we concentrate on a simplified and idealized example of a safety system in which interest centers upon a claim about the probability of failure on demand. Our approach is to build a ""Bayesian belief network"" (BBN) model of a two-legged argument and manipulate this analytically via parameters that define its node probability tables. The aim here is to obtain greater insight than what is afforded by the more usual BBN treatment, which involves merely numerical manipulation. We show that the addition of a diverse second argument leg can indeed increase confidence in a dependability claim; in a reasonably plausible example, the doubt in the claim is reduced to one-third of the doubt present in the original single leg. However, we also show that there can be some unexpected and counterintuitive subtleties here; for example, an entirely supportive second leg can sometimes undermine an original argument, resulting, overall, in less confidence than what came from this original argument. Our results are neutral on the issue of whether such difficulties will arise in real life $that is, when real experts judge real systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1002","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160972","Safety claims;safety arguments;software safety;software reliability;Bayesian belief networks.","Leg;Software safety;Uncertainty;Battery powered vehicles;Particle measurements;System testing;Probability;Software reliability;Bayesian methods;Control systems","Bayes methods;belief networks;probability;software reliability","multilegged argument;software-based system;multiversion software;probabilistic treatment;Bayesian belief network;software safety;software reliability","","40","","26","","","","","","IEEE","IEEE Journals & Magazines"
"A decision-analytic stopping rule for validation of commercial software systems","T. Chavez","Rapt Technol. Corp., San Francisco, CA, USA","IEEE Transactions on Software Engineering","","2000","26","9","907","918","The decision of when to release a software product commercially is not a question of when the software has attained some objectively justifiable degree of correctness. It is, rather, a question of whether the software achieves a reasonable balance among engineering objectives, market demand, customer requirements, and marketing directives of the software organization. We present a rigorous framework for addressing this important decision. Conjugate distributions from statistical decision theory provide an attractive means of modeling the cost and rate of bugs given information acquired during software testing, as well as prior information provided by software engineers about the fidelity of the software before testing begins. In contrast to other methods, the stopping analysis yields a computationally simple rule for deciding when to release a commercial software product based on information revealed to engineers during software testing-complicated numerical procedures are not needed. Our method has the added benefits that it is sequential: it measures explicitly the costs of customer dissatisfaction associated with bugs as well as the costs of declining market position while the testing process continues; and it incorporates a practical framework for cost-criticality assessment that makes sense to professional software developers.","0098-5589;1939-3520;2326-3881","","10.1109/32.877849","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877849","","Software systems;Software testing;Programming;Computer bugs;Costs;Business;Decision theory;Software tools;Design engineering;Engineering management","program testing;software development management;program debugging;formal verification","decision-analytic stopping rule;commercial software system validation;engineering objectives;market demand;customer requirements;marketing directives;software organization;statistical decision theory;conjugate distributions;software testing;bugs;customer dissatisfaction;declining market position;cost-criticality assessment;professional software developers","","12","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Featured Transition Systems: Foundations for Verifying Variability-Intensive Systems and Their Application to LTL Model Checking","A. Classen; M. Cordy; P. Schobbens; P. Heymans; A. Legay; J. Raskin","University of Namur (FUNDP), Namur; University of Namur (FUNDP), Namur; University of Namur (FUNDP), Namur; University of Namur (FUNDP), Namur and INRIA Lille-Nord Europe, France; IRISA/INRIA Rennes, France Université de Liège, Rennes Liège; Université Libre de Bruxelles (ULB), Brussels","IEEE Transactions on Software Engineering","","2013","39","8","1069","1089","The premise of variability-intensive systems, specifically in software product line engineering, is the ability to produce a large family of different systems efficiently. Many such systems are critical. Thorough quality assurance techniques are thus required. Unfortunately, most quality assurance techniques were not designed with variability in mind. They work for single systems, and are too costly to apply to the whole system family. In this paper, we propose an efficient automata-based approach to linear time logic (LTL) model checking of variability-intensive systems. We build on earlier work in which we proposed featured transitions systems (FTSs), a compact mathematical model for representing the behaviors of a variability-intensive system. The FTS model checking algorithms verify all products of a family at once and pinpoint those that are faulty. This paper complements our earlier work, covering important theoretical aspects such as expressiveness and parallel composition as well as more practical things like vacuity detection and our logic feature LTL. Furthermore, we provide an in-depth treatment of the FTS model checking algorithm. Finally, we present SNIP, a new model checker for variability-intensive systems. The benchmarks conducted with SNIP confirm the speedups reported previously.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6389685","Formal methods;model checking;verification;variability;features;software product lines","Unified modeling language;Semantics;Software;Labeling;Automata;Quality assurance","automata theory;formal logic;formal verification;software quality","featured transition systems;variability-intensive system verification;LTL model checking;software product line engineering;quality assurance techniques;automata-based approach;linear time logic model checking;mathematical model;FTS model checking algorithm;SNIP;model checker","","62","","74","","","","","","IEEE","IEEE Journals & Magazines"
"The Tinkertoy graphical programming environment","M. Edel","Illinois Univ., Urbana, IL, USA","IEEE Transactions on Software Engineering","","1988","14","8","1110","1115","The Tinkertoy graphical interface to Lisp is described, in which programs are 'built' rather than written, out of icons and flexible interconnections. It represents a computer/user interface that can easily exceed the interaction speed of the best text-based language editors and command languages. It also provides a consistent framework for interaction across both editing and command execution. Moreover, because programs are represented graphically, structures that do not naturally conform to the text medium can be clearly described, the new kinds of information can be incorporated into programs and program elements.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.7621","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7621","","Programming environments;Computer graphics;Command languages;Power engineering and energy;Computer interfaces;User interfaces;Computer languages;Hardware;Mice","computer graphics;LISP;programming environments;text editing;user interfaces","graphical programming environment;Tinkertoy graphical interface;Lisp;icons;user interface","","12","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Multiprocessor online scheduling of hard-real-time tasks","M. L. Dertouzos; A. K. Mok","Lab. for Comput. Sci., MIT, Cambridge, MA, USA; NA","IEEE Transactions on Software Engineering","","1989","15","12","1497","1506","The problems of hard-real-time task scheduling in a multiprocessor environment are discussed in terms of a scheduling game representation of the problem. It is shown that optimal scheduling without a priori knowledge is impossible in the multiprocessor case even if there is no restriction on preemption owing to precedence or mutual exclusion constraints. Sufficient conditions that permit a set of tasks to be optimally scheduled at run time are derived.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58762","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58762","","Optimal scheduling;Processor scheduling;Sufficient conditions;Runtime;Computer science;Microprocessors;Multiprocessing systems;Time factors;Robot control","multiprocessing systems;multiprogramming;real-time systems;scheduling","hard-real-time task scheduling;multiprocessor environment;scheduling game representation;optimal scheduling;a priori knowledge;mutual exclusion constraints","","250","","12","","","","","","IEEE","IEEE Journals & Magazines"
"On conditions for defining a closed cover to verify progress for communicating finite state machines","A. Chung; D. P. Sidhu","Dept. of Comput. Sci., Maryland Univ., Baltimore, MD, USA; Dept. of Comput. Sci., Maryland Univ., Baltimore, MD, USA","IEEE Transactions on Software Engineering","","1989","15","11","1491","1494","The closed-cover technique for verifying progress for two communicating finite-state machines exchanging messages over two lossless, FIFO channels is considered. The authors point out that the definition of a closed cover in M.G. Gouda (ibid., vol.SE-10, no.6, p.846-55, Nov. 1984) may be too restrictive, while that in M.G. Gouda and C.K. Chang (ACM Trans. Prog. Lang., vol.8, no.1, p.154-82, Jan. 1986) is not correct. They then show how a condition of the closed-cover definition can be modified to relax restriction to various degrees. They also discuss the similarities and relationship between the structural partition technique and the closed-cover technique.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41341","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41341","","Automata;Protocols;Computer science;EMP radiation effects","finite automata;protocols","progress verification;closed cover;communicating finite state machines;lossless;FIFO channels;structural partition technique","","1","","4","","","","","","IEEE","IEEE Journals & Magazines"
"BDD-based safety-analysis of concurrent software with pointer data structures using graph automorphism symmetry reduction","Farn Wang; K. Schmidt; Fang Yu; Geng-Dian Huang; Bow-Yaw Wang","Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","6","403","417","Dynamic data-structures with pointer links, which are heavily used in real-world software, cause extremely difficult verification problems. Currently, there is no practical framework for the efficient verification of such software systems. We investigated symmetry reduction techniques for the verification of software systems with C-like indirect reference chains like x/spl rarr/y/spl rarr/z/spl rarr/w. We formally defined the model of software with pointer data structures and developed symbolic algorithms to manipulate conditions and assignments with indirect reference chains using BDD technology. We relied on two techniques, inactive variable elimination and process-symmetry reduction in the data-structure configuration, to reduce time and memory complexity. We used binary permutation for efficiency, but we also identified the possibility of an anomaly of false image reachability. We implemented the techniques in tool Red 5.0 and compared performance with Mur/spl phi/ and SMC against several benchmarks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321062","Symbolic model checking;pointers;data structure;address manipulation;symmetry reduction;experiments.","Data structures;Boolean functions;Software safety;Software systems;Hardware;Vehicle dynamics;Formal verification;Computer Society;Software algorithms;Binary decision diagrams","binary decision diagrams;data structures;symbol manipulation;program verification;distributed programming;computational complexity;graph theory","BDD-based safety-analysis;concurrent software;pointer data structure;graph automorphism symmetry reduction;software system verification;indirect reference chain;symbolic algorithm;inactive variable elimination;process-symmetry reduction;false image reachability;Red 5.0;symbolic model checking;address manipulation","","2","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Small Errors in ""Toward Formalizing Domain Modeling Semantics in Language Syntax'","R. J. Botting","IEEE Computer Society","IEEE Transactions on Software Engineering","","2005","31","10","911","911","A recent paper on domain modeling had State Charts with semantic errors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.116","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542071","Index Terms- UML;semantics;state charts.","Computer errors;Rails;Computer science","","Index Terms- UML;semantics;state charts.","","","","2","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical approach to studying software evolution","C. F. Kemerer; S. Slaughter","Pittsburgh Univ., PA, USA; NA","IEEE Transactions on Software Engineering","","1999","25","4","493","509","With the approach of the new millennium, a primary focus in software engineering involves issues relating to upgrading, migrating, and evolving existing software systems. In this environment, the role of careful empirical studies as the basis for improving software maintenance processes, methods, and tools is highlighted. One of the most important processes that merits empirical evaluation is software evolution. Software evolution refers to the dynamic behaviour of software systems as they are maintained and enhanced over their lifetimes. Software evolution is particularly important as systems in organizations become longer-lived. However, evolution is challenging to study due to the longitudinal nature of the phenomenon in addition to the usual difficulties in collecting empirical data. We describe a set of methods and techniques that we have developed and adapted to empirically study software evolution. Our longitudinal empirical study involves collecting, coding, and analyzing more than 25000 change events to 23 commercial software systems over a 20-year period. Using data from two of the systems, we illustrate the efficacy of flexible phase mapping and gamma sequence analytic methods, originally developed in social psychology to examine group problem solving processes. We have adapted these techniques in the context of our study to identify and understand the phases through which a software system travels as it evolves over time. We contrast this approach with time series analysis. Our work demonstrates the advantages of applying methods and techniques from other domains to software engineering and illustrates how, despite difficulties, software evolution can be empirically studied.","0098-5589;1939-3520;2326-3881","","10.1109/32.799945","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=799945","","Software maintenance;Software systems;Time series analysis;Software engineering;Dynamic programming;Psychology;Problem-solving;Costs;Software performance;Error correction","software prototyping;software maintenance","empirical approach;software evolution;software engineering;empirical studies;software maintenance processes;dynamic behaviour;longitudinal nature;empirical data;change events;commercial software systems;flexible phase mapping;gamma sequence analytic methods","","147","","44","","","","","","IEEE","IEEE Journals & Magazines"
"A validation of the component-based method for software size estimation","J. J. Dolado","Fac. de Inf., Pais Vasco Univ., San Sebastian, Spain","IEEE Transactions on Software Engineering","","2000","26","10","1006","1021","Estimation of software size is a crucial activity among the tasks of software management. Work planning and subsequent estimations of the effort required are made based on the estimate of the size of the software product. Software size can be measured in several ways: lines of code (LOC) is a common measure and is usually one of the independent variables in equations for estimating several methods for estimating the final LOC count of a software system in the early stages. We report the results of the validation of the component-based method (initially proposed by Verner and Tate, 1988) for software sizing. This was done through the analysis of 46 projects involving more than 100,000 LOC of a fourth-generation language. We present several conclusions concerning the predictive capabilities of the method. We observed that the component-based method behaves reasonably, although not as well as expected for ""global"" methods such as Mark II function points for software size prediction. The main factor observed that affects the performance is the type of component.","0098-5589;1939-3520;2326-3881","","10.1109/32.879821","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879821","","Lab-on-a-chip;Size measurement;Software measurement;Equations;Software systems;Project management;Life estimation;Linear regression;Neural networks;Genetic programming","software reusability;software development management;high level languages;software metrics","software component-based method;software size estimation;software management;work planning;lines of code;fourth-generation language;Mark II function points;software size prediction;neural networks;genetic programming","","73","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Quality-Aware Service Selection for Service-Based Systems Based on Iterative Multi-Attribute Combinatorial Auction","Q. He; J. Yan; H. Jin; Y. Yang","Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Information Systems and Technology, University of Wollongong, Wollongong, Australia; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Anhui University, Hefei, China","IEEE Transactions on Software Engineering","","2014","40","2","192","215","The service-oriented paradigm offers support for engineering service-based systems (SBSs) based on service composition where existing services are composed to create new services. The selection of services with the aim to fulfil the quality constraints becomes critical and challenging to the success of SBSs, especially when the quality constraints are stringent. However, none of the existing approaches for quality-aware service composition has sufficiently considered the following two critical issues to increase the success rate of finding a solution: 1) the complementarities between services; and 2) the competition among service providers. This paper proposes a novel approach called combinatorial auction for service selection (CASS) to support effective and efficient service selection for SBSs based on combinatorial auction. In CASS, service providers can bid for combinations of services and apply discounts or premiums to their offers for the multi-dimensional quality of the services. Based on received bids, CASS attempts to find a solution that achieves the SBS owner's optimisation goal while fulfilling all quality constraints for the SBS. When a solution cannot be found based on current bids, the auction iterates so that service providers can improve their bids to increase their chances of winning. This paper systematically describes the auction process and the supporting mechanisms. Experimental results show that by exploiting the complementarities between services and the competition among service providers, CASS significantly outperforms existing quality-aware service selection approaches in finding optimal solutions and guaranteeing system optimality. Meanwhile, the duration and coordination overhead of CASS are kept at satisfactory levels in scenarios on different scales.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.2297911","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6702520","Service-based system;combinatorial auction;quality of service;service composition;service selection;web services;integer programming","Scattering;Quality of service;Optimization;Contracts;Abstracts;Time factors","combinatorial mathematics;iterative methods;quality of service;service-oriented architecture;software quality;Web services","system optimality;optimal solutions;auction mechanisms;auction process;service providers;service quality;CASS approach;quality-aware service composition;quality constraints;service creation;SBS;service-oriented paradigm;iterative multiattribute combinatorial auction;service-based systems;quality-aware service selection","","36","","61","","","","","","IEEE","IEEE Journals & Magazines"
"Partition testing vs. random testing: the influence of uncertainty","W. J. Gutjahr","Dept. of Stat. Oper. Res. & Comput. Sci., Wien Univ., Austria","IEEE Transactions on Software Engineering","","1999","25","5","661","674","This paper compares partition testing and random testing on the assumption that program failure rates are not known with certainty before testing and are, therefore, modeled by random variables. It is shown that under uncertainty, partition testing compares more favorably to random testing than suggested by prior investigations concerning the deterministic case: the restriction to failure rates that are known with certainty systematically favors random testing. In particular, we generalize a result by Weyuker and Jeng (1991) stating equal fault detection probabilities for partition testing and random testing in the case where the failure rates in the subdomains defined by the partition are equal. It turns out that for independent random failure rates with equal expectation, the case above is a boundary case (the worst case for partition testing), and the fault detection probability of partition testing can be up to k times higher than that of random testing, where k is the number of subdomains. Also in a related model for dependent failure rates, partition testing turns out to be consistently better than random testing. The dominance can also be verified for the expected (weighted) number of detected faults as an alternative comparison criterion.","0098-5589;1939-3520;2326-3881","","10.1109/32.815325","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=815325","","Uncertainty;Fault detection;Software testing;Acoustic testing;System testing;Random variables;Genetic mutations;Partitioning algorithms;Pain","program testing;program debugging","partition testing;random testing;program failure rates;random variables;fault detection;decisions under uncertainty;program testing","","68","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on “number of faults per line of code”","F. Stetter","Lehrstuhl fur Praktische Informatik 1, Universitat Mannheim, A5, 6800 Mannheim 1, West Germany","IEEE Transactions on Software Engineering","","1986","SE-12","12","1145","1145","In a recent paper (see ibid., vol.SE-8, p.437-9, July 1982) an approximate formula for the number of faults per line of code was developed. It is shown that there is an approximation which is easier to develop, more accurate, and simpler to use.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313010","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313010","Program error estimates;program predictions;software reliability;software science","Approximation methods;Software;Indexes;Software reliability","programming theory;software reliability","program error estimates;software reliability;program predictions;faults per line of code","","2","","","","","","","","IEEE","IEEE Journals & Magazines"
"Some consideration on real-time behavior of concurrent programs","A. Fuggetta; C. Ghezzi; D. Mandrioli","CEFRIEL, Milan, Italy; NA; NA","IEEE Transactions on Software Engineering","","1989","15","3","356","359","Some basic semantic issues of a language for a reliable and provably correct real-time programs are discussed. The language is based on E.W. Dijkstra's guarded commands and on a proposal by V.H. Haase (1981). Haase's proposal is assessed, its semantic consistencies are shown, and corrections are proposed that give a sound basis for a real-time language based on guarded commands.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21763","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21763","","Proposals;Real time systems;Formal specifications;Production","formal languages;high level languages;parallel programming;program verification;programming theory;real-time systems","real-time behavior;concurrent programs;semantic issues;real-time programs;guarded commands;real-time language","","","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Rebooting Research on Detecting Repackaged Android Apps: Literature Review and Benchmark","L. Li; T. F. Bissyande; J. Klein","Faculty of Information Technology, Monash University, 2541 Clayton, Victoria Australia (e-mail: li.li@monash.edu); SnT, Universite du Luxembourg, 81872 Luxembourg, Luxembourg Luxembourg L-1855 (e-mail: tegawende.bissyande@uni.lu); SnT, University of Luxembourg, 81872 Luxembourg, Luxembourg Luxembourg (e-mail: jacques.klein@uni.lu)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Repackaging is a serious threat to the Android ecosystem as it deprives app developers of their benefits, contributes to spreading malware on users' devices, and increases the workload of market maintainers. In the space of six years, the research around this specific issue has produced 57 approaches which do not readily scale to millions of apps or are only evaluated on private datasets without, in general, tool support available to the community. Through a systematic literature review of the subject, we argue that the research is slowing down, where many state-of-the-art approaches have reported high-performance rates on closed datasets, which are unfortunately difficult to replicate and to compare against. In this work, we propose to reboot the research in repackaged app detection by providing a literature review that summarises the challenges and current solutions for detecting repackaged apps and by providing a large dataset that supports replications of existing solutions and implications of new research directions. We hope that these contributions will re-activate the direction of detecting repackaged apps and spark innovative approaches going beyond the current state-of-the-art.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2901679","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8653409","Android;Repackaging;Clone;Literature Review;Benchmark","Bibliographies;Malware;Cloning;Systematics;Tools;Aging;Libraries","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A Learning-Based Framework for Engineering Feature-Oriented Self-Adaptive Software Systems","N. Esfahani; A. Elkhodary; S. Malek","George Mason University, Fairfax; George Mason University, Fairfax; George Mason University, Fairfax","IEEE Transactions on Software Engineering","","2013","39","11","1467","1493","Self-adaptive software systems are capable of adjusting their behavior at runtime to achieve certain functional or quality-of-service goals. Often a representation that reflects the internal structure of the managed system is used to reason about its characteristics and make the appropriate adaptation decisions. However, runtime conditions can radically change the internal structure in ways that were not accounted for during their design. As a result, unanticipated changes at runtime that violate the assumptions made about the internal structure of the system could degrade the accuracy of the adaptation decisions. We present an approach for engineering self-adaptive software systems that brings about two innovations: 1) a feature-oriented approach for representing engineers' knowledge of adaptation choices that are deemed practical, and 2) an online learning-based approach for assessing and reasoning about adaptation decisions that does not require an explicit representation of the internal structure of the managed software system. Engineers' knowledge, represented in feature-models, adds structure to learning, which in turn makes online learning feasible. We present an empirical evaluation of the framework using a real-world self-adaptive software system. Results demonstrate the framework's ability to accurately learn the changing dynamics of the system while achieving efficient analysis and adaptation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6574860","Self-adaptive software;autonomic computing;feature-orientation;machine learning","Software systems;Runtime;Adaptation models;Quality of service;Authentication;Measurement","inference mechanisms;learning (artificial intelligence);quality of service;software engineering","feature-models;adaptation decision reasoning;adaptation decision assessment;online learning-based approach;runtime conditions;quality-of-service goals;engineering feature-oriented self-adaptive software systems;learning-based framework","","35","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Thorough investigation into ""An improved algorithm based on subset closures for synthesizing a relational database scheme""","Lin Lin Wang","Dept. of Comput. Sci., Chongqing Inst. of Posts & Telecommun., Sichuan, China","IEEE Transactions on Software Engineering","","1996","22","4","271","274","The paper describes a technical improvement of a synthesis algorithm for relational database scheme design, which was proposed by Yang et al. (1988). The improvement is based on the observation that the original algorithm may lose attributes in a decomposition in special cases, due to an insufficient handling of representatives of equivalence classes formed over a given set of functional dependencies. The paper then proposes a correction of the original algorithm in which this problem disappears.","0098-5589;1939-3520;2326-3881","","10.1109/32.491651","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=491651","","Relational databases;Artificial intelligence;Algorithm design and analysis","relational databases;database theory;software engineering;equivalence classes;algorithm theory","relational database scheme synthesis;synthesis algorithm;decomposition;attribute loss;equivalence classes;functional dependencies;algorithm correction","","","","7","","","","","","IEEE","IEEE Journals & Magazines"
"A binary Markov process model for random testing","Sanping Chen; S. Mills","Stat. Consulting Centre, Carleton Univ., Ottawa, Ont., Canada; NA","IEEE Transactions on Software Engineering","","1996","22","3","218","223","A binary Markov process model is proposed for the random testing of software. This model is suggested for replacing the standard binomial distribution model, which is based on the easily-violated assumption of test runs being statistically independent of each other. In addition to a general result on the probability of having any specific number of software failures during testing, practical implications of the new model are also discussed. In particular, we demonstrate that, in general, the effect of a possible correlation between test runs cannot be ignored in estimating software reliability.","0098-5589;1939-3520;2326-3881","","10.1109/32.489081","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=489081","","Software testing;Markov processes;Statistics;Application software;Milling machines;Graphics","program testing;software reliability;binomial distribution;Markov processes","binary Markov process model;random software testing;binomial distribution model;statistically independent test runs;software failure probability;correlation;software reliability estimation;dependent test runs;statistical testing;ultra-reliability application","","18","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Well-defined generalized stochastic Petri nets: a net-level method to specify priorities","E. Teruel; G. Franceschinis; M. De Pierro","Departamento de Informatica e Ingenieria de Sistemas, Zaragoza Univ., Spain; NA; NA","IEEE Transactions on Software Engineering","","2003","29","11","962","973","Generalized stochastic Petri nets (GSPN), with immediate transitions, are extensively used to model concurrent systems in a wide range of application domains, particularly including software and hardware aspects of computer systems, and their interactions. These models are typically used for system specification, logical and performance analysis, or automatic code generation. In order to keep modeling separate from the analysis and to gain in efficiency and robustness of the modeling process, the complete specification of the stochastic process underlying a model should be guaranteed at the net level, without requiring the generation and exploration of the state space. In this paper, we propose a net-level method that guides the modeler in the task of defining the priorities (and weights) of immediate transitions in a GSPN model, to deal with confusion and conflict problems. The application of this method ensures well-definition without reducing modeling flexibility or expressiveness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245298","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245298","","Stochastic processes;Petri nets;State-space methods;Application software;Robustness;Software performance;Software systems;Software tools;Computer aided manufacturing;Computer networks","Petri nets;formal specification;stochastic processes;performance evaluation;concurrency theory","well-defined generalized stochastic Petri nets;net-level method;priorities specification;immediate transitions;concurrent systems;software aspects;hardware aspects;computer systems;stochastic process;specification;logical analysis;performance analysis;automatic code generation","","9","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Defining software by continuous smooth functions"" by R.A. De Millo and R.J. Lipton","N. Y. Foo","Basser Dept. of Comput. Sci., Sydney Univ., NSW, Australia","IEEE Transactions on Software Engineering","","1993","19","3","307","309","The commenter agrees with R.A. De Millo and R.S. Lipton's assertion in the above-titled work (see ibid., vol.17, no.4, p.383-4, 1991) that continuous, smooth functions of comparable complexity can be used to represent Boolean functions computed by programs. The commenter reconciles this assertion with a referee's objection that the discontinuity of software resides in the large changes of behavior that results from small changes of code, which was countered by the authors by arguing that this notion of discontinuity is vague. It is suggested that this vagueness can be remedied if discontinuity is replaced by a more natural measure that nevertheless captures a similar intuition.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.221140","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=221140","","Q measurement;Mathematics;Length measurement;Software measurement;Reasoning about programs;Mathematical model;Boolean functions;Proposals;Australia Council;Knowledge based systems","Boolean functions;computational complexity;software metrics","smooth functions;comparable complexity;Boolean functions;programs;natural measure","","1","","6","","","","","","IEEE","IEEE Journals & Magazines"
"A linear algorithm for generating random numbers with a given distribution","M. D. Vose","Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA","IEEE Transactions on Software Engineering","","1991","17","9","972","975","Let xi be a random variable over a finite set with an arbitrary probability distribution. Improvements to a fast method of generating sample values for xi in constant time are suggested. The proposed modification reduces the time required for initialization to O(n). For a simple genetic algorithm, this improvement changes an O(g n 1n n) algorithm into an O(g n) algorithm (where g is the number of generations, and n is the population size).<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92917","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92917","","Random number generation;Random variables;Probability distribution;Genetic algorithms;Roundoff errors;Computational modeling;Computer science","genetic algorithms;probability;random number generation","linear algorithm;random numbers;random variable;finite set;arbitrary probability distribution;simple genetic algorithm","","41","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Recognizing immediacy in an N-tree hierarchy and its application to protection groups","R. S. Sandhu","Dept. of Inf. Syst. & Syst. Eng., George Mason Univ., Fairfax, VA, USA","IEEE Transactions on Software Engineering","","1989","15","12","1518","1525","The benefits of providing access control with groups of users as the unit of granularity are enhanced if the groups are organized in a hierarchy (partial order) by the subgroup relation <or=, where g<or=h signifies that every member of group g is thereby also a member of group h. It is often useful to distinguish the case when g is an immediate subgroup of h, that is when g<h and there is no group k such that g<k<h. The class of partial orders called n-trees was recently defined by using rooted trees and inverted rooted trees as basic partial orders and combining these recursively by refinement. Any n-tree hierarchy can be expressed as the intersection of two linear orderings, so it is possible to assign a pair of integers l(x) and r(x) to each group x such that g<or=h if and only if l(g)<or=l(h) and r(g)<or=r(h). The author shows how to extend this representation of n-trees by assigning four additional integers to each group so that it is also easily determined whether or not g is an immediate subgroup of h.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58764","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58764","","Protection;Access control;Authorization;Security;Tires;Information systems","data structures;multi-access systems;security of data;set theory;trees (mathematics)","immediacy;N-tree hierarchy;protection groups;access control;granularity;partial order;subgroup relation;inverted rooted trees;recursively;refinement;linear orderings;integers","","","","12","","","","","","IEEE","IEEE Journals & Magazines"
"A Theoretical and Empirical Analysis of Program Spectra Diagnosability","A. Perez; R. Abreu; A. Van Deursen","Faculty of Engineering, University of Porto, Porto, Porto Portugal 4200-465 (e-mail: alexandre.perez@fe.up.pt); Computer Science and Engineering, Instituto Superior Tácnico, University of Lisbon, Lisbon, Lisbon Portugal 1049-001 (e-mail: rui@computer.org); EEMCS / ST / SE, Delft University of Technology, Delft, ZH Netherlands 2628 CD (e-mail: arie.vandeursen@tudelft.nl)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Current metrics for assessing the adequacy of a test-suite plainly focus on the number of components (be it lines, branches, paths) covered by the suite, but do not explicitly check how the tests actually exercise these components and whether they provide enough information so that spectrum-based fault localization techniques can perform accurate fault isolation. We propose a metric, called, aimed at complementing adequacy measurements by quantifying a test-suite's diagnosability, i.e., the effectiveness of applying spectrum-based fault localization to pinpoint faults in the code in the event of test failures. Our aim is to increase the value generated by creating thorough test-suites, so they are not only regarded as error detection mechanisms but also as effective diagnostic aids that help widely-used fault-localization techniques to accurately pinpoint the location of bugs in the system. We have performed a topology-based simulation of thousands of spectra and have found that DDU can effectively establish an upper bound on the effort to diagnose faults. Furthermore, our empirical experiments using the Defects4J dataset show that optimizing a test suite with respect to DDU yields a 34% gain in spectrum-based fault localization report accuracy when compared to the standard branch-coverage metric.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2895640","Fundacao para a Ciencia e a Tecnologia; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8627980","Testing;Coverage;Diagnosability","Software;Measurement uncertainty;Diversity reception;Cognition;Current measurement;Tools","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"The impact on software development costs of using HOL's","J. E. Gaffney","Department of Computer Science, Polytechnic Institute of New York, Brooklyn, NY 11201; Federal Systems Division, IBM, Gaithersburg, MD 20817","IEEE Transactions on Software Engineering","","1986","SE-12","3","496","499","The author quantifies the relative reduction in software development costs provided by the use of a higher order coding language (HOL). Paradoxically, as T. C. Jones (1978) has pointed out, when productivity is measured as source lines of code per labor month, there is `an apparent productivity lowering for the whole development cycle with high level (coding) languages, even though development costs have actually been reduced.' The problem here is the unit of measure for the amount of the software function produced per labor month. It is believed that the amount of machine code produced (after taking compiler inefficiency into account) may provide a better quantification of function than the lines of source coding language used.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312890","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312890","Higher order coding language;software development costs","Software;Productivity;Encoding;Assembly;Software engineering;Testing;Programming","software engineering","HOL;software development costs;higher order coding language;productivity;source lines of code per labor month;development cycle;machine code;compiler inefficiency","","2","","","","","","","","IEEE","IEEE Journals & Magazines"
"Handling of irregularities in human centered systems: a unified framework for data and processes","T. Murata; A. Borgida","Dept. of Comput. Sci., Rutgers Univ., Piscataway, NJ, USA; NA","IEEE Transactions on Software Engineering","","2000","26","10","959","977","Practical process-support and workflow systems should be built to describe the simple, normal flow of events and then deal easily with irregularities, including tolerating deviations. Similarly, these systems should describe the normal format and constraints concerning the large amounts of data that are usually stored, but then deal with abnormalities and possibly accommodate exceptional values. We offer a framework for treating both kinds of irregularities uniformly by using the notion of exception handling (with human agents as potential online exception handlers) and applying it to processes that have been reified as objects in classes with steps as attributes. As a result, only a small number of new constructs, which can be applied orthogonally, need to be introduced. Special runtime checks are used to deal with the consequences of permitting deviations from the norm to persist as violations of constraints. A logical semantics of process coordination and deviations is presented as a specification for implementations.","0098-5589;1939-3520;2326-3881","","10.1109/32.879819","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879819","","Humans;Runtime;Safety;Engines;Information management;Information retrieval;Databases;Control systems;Error correction;Data models","exception handling;workflow management software;software engineering;human factors","human centered systems;process-support systems;workflow systems;exception handling;runtime checks;logical semantics;process coordination","","6","","50","","","","","","IEEE","IEEE Journals & Magazines"
"On the Distribution of Software Faults","H. Zhang","NA","IEEE Transactions on Software Engineering","","2008","34","2","301","302","The Pareto principle is often used to describe how faults in large software systems are distributed over modules. A recent paper by Andersson and Runeson again confirmed the Pareto principle of fault distribution. In this paper, we show that the distribution of software faults can be more precisely described as the Weibull distribution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70771","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4407730","Product metrics;Measurement applied to SQA and V&V;Product metrics;Measurement applied to SQA and V&V","Weibull distribution;Software systems;Packaging;Probability distribution;Pareto analysis;Distribution functions;Reliability engineering;Java;Application software;Databases","Pareto analysis;Weibull distribution","software faults distribution;Pareto principle;large software systems;Weibull distribution","","30","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing partition and random testing via majorization and Schur functions","P. J. Boland; H. Singh; B. Cukic","Dept. of Stat., Nat. Univ. of Ireland, Dublin, Ireland; NA; NA","IEEE Transactions on Software Engineering","","2003","29","1","88","94","The comparison of partition and random sampling methods for software testing has received considerable attention in the literature. A standard criterion for comparisons between random and partition testing based on their expected efficacy in program debugging is the probability of detecting at least one failure causing input in the program's domain. We investigate the relative effectiveness of partition testing versus random testing through the powerful mathematical technique of majorization, which was introduced by Hardy et al. (1952). The tools of majorization and the concepts of Schur (convex and concave) functions (1923) enable us to derive general conditions under which partition testing is superior to random testing and, consequently, to give further insights into the value of partition testing strategies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166591","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166591","","Software testing;Sampling methods;Software debugging;Arithmetic;Software reliability;System testing;Software design;Software measurement","program debugging;program testing","partition methods;random sampling methods;software testing;program debugging;majorization;Schur functions;partition testing;random testing","","16","","26","","","","","","IEEE","IEEE Journals & Magazines"
"A controlled experiment to assess the benefits of estimating with analogy and regression models","I. Myrtveit; E. Stensrud","Norwegian Sch. of Manage., Sandvika, Norway; NA","IEEE Transactions on Software Engineering","","1999","25","4","510","525","To have general validity, empirical results must converge. To be credible, an experimental science must understand the limitations and be able to explain the disagreements of empirical results. We describe an experiment to replicate previous studies which claim that estimation by analogy outperforms regression models. In the experiment, 68 experienced practitioners each estimated a project from a dataset of 48 industrial COTS projects. We applied two treatments, an analogy tool and a regression model, and we used the estimating performance when aided by the historical data as the control. We found that our results do not converge with previous results. The reason is that previous studies have used other datasets and partially different data analysis methods, and last but not least, the tools have been validated in isolation from the tool users. This implies that the results are sensitive to the experimental design: the characteristics of the dataset, the norms for removing outliers and other data points from the original dataset, the test metrics, significance levels, and the use of human subjects and their level of expertise. Thus, neither our results nor previous results are robust enough to claim any general validity.","0098-5589;1939-3520;2326-3881","","10.1109/32.799947","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=799947","","Enterprise resource planning;Testing;Humans;Costs;Convergence;Physics;Data analysis;Design for experiments;Robustness;Software performance","software cost estimation;project management;statistical analysis","controlled experiment;regression models;experienced practitioners;empirical results;experimental science;estimation by analogy;industrial COTS projects;analogy tool;regression model;estimating performance;historical data;partially different data analysis methods;tool users;experimental design;outliers;data points;dataset;test metrics;significance levels;human subjects;software cost estimation;commercial off-the-shelf software projects;multivariate regression analysis;human performance;enterprise resource planning","","89","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Toward constraint-object-oriented development","T. Bolognesi","Istituto di Elaborazione dell'Inf., CNR, Pisa, Italy","IEEE Transactions on Software Engineering","","2000","26","7","594","616","In this paper, we propose to conservatively extend object-oriented decomposition by letting it affect also operations (methods). Different objects may support different parts of the same operation. The responsibility of defining an operation, in terms of enabling conditions and effects on the state, is distributed over several interacting objects, which act as constraints and express different, partial views about the system behavior. Constraint-oriented reasoning has already been explored and applied in the context of formal specification languages for concurrent and reactive systems, and is sufficiently different from object-oriented reasoning to be considered as a paradigm in itself, with its own specific advantages. Nevertheless, the paper shows that the two approaches are sufficiently compatible to be profitably integrated. We introduce a constraint-oriented style for an object-oriented programming language (JAVA).","0098-5589;1939-3520;2326-3881","","10.1109/32.859530","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859530","","Formal specifications;Java;Programming profession;Object oriented programming;Design methodology;Computer languages;Software engineering;Engineering management;Isolation technology;Data encapsulation","formal specification;object-oriented programming;constraint handling;Java","object-oriented decomposition;constraint-object-oriented development;constraints;partial views;interacting objects;object-oriented reasoning;constraint-oriented reasoning;constraint-oriented specification;LOTOS;multi-object operation;JAVA programming","","4","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluation of several nonparametric bootstrap methods to estimate confidence intervals for software metrics","Skylar Lei; M. R. Smith","Gen. Dynamics Canada, Calgary, Alta., Canada; NA","IEEE Transactions on Software Engineering","","2003","29","11","996","1004","Sample statistics and model parameters can be used to infer the properties, or characteristics, of the underlying population in typical data-analytic situations. Confidence intervals can provide an estimate of the range within which the true value of the statistic lies. A narrow confidence interval implies low variability of the statistic, justifying a strong conclusion made from the analysis. Many statistics used in software metrics analysis do not come with theoretical formulas to allow such accuracy assessment. The Efron bootstrap statistical analysis appears to address this weakness. In this paper, we present an empirical analysis of the reliability of several Efron nonparametric bootstrap methods in assessing the accuracy of sample statistics in the context of software metrics. A brief review on the basic concept of various methods available for the estimation of statistical errors is provided, with the stated advantages of the Efron bootstrap discussed. Validations of several different bootstrap algorithms are performed across basic software metrics in both simulated and industrial software engineering contexts. It was found that the 90 percent confidence intervals for mean, median, and Spearman correlation coefficients were accurately predicted. The 90 percent confidence intervals for the variance and Pearson correlation coefficients were typically underestimated (60-70 percent confidence interval), and those for skewness and kurtosis overestimated (98-100 percent confidence interval). It was found that the Bias-corrected and accelerated bootstrap approach gave the most consistent confidence intervals, but its accuracy depended on the metric examined. A method for correcting the under-/ overestimation of bootstrap confidence intervals for small data sets is suggested, but the success of the approach was found to be inconsistent across the tested metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245301","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245301","","Software metrics;Statistical analysis;Statistics;Estimation error;State estimation;Software algorithms;Context modeling;Computer industry;Footwear industry;Software engineering","software metrics;software reliability","nonparametric bootstrap methods;software metrics;sample statistics;model parameters;Efron bootstrap statistical analysis;industrial software engineering;Spearman correlation coefficients;confidence intervals estimation","","27","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Cognitive Biases in Software Engineering: A Systematic Mapping Study","R. Mohanani; I. Salman; B. Turhan; P. Rodríguez; P. Ralph","Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Oulu Finland (e-mail: rahul.mohanani@oulu.fi); M3S, Oulun Yliopisto Teknillinen Tiedekunta, 101225 Oulu, Northern Ostrobothnia Finland 90014 (e-mail: iflaah.salman@oulu.fi); Faculty of Information Technology, Monash University, Melbourne, Victoria Australia (e-mail: turhanb@computer.org); Department of Information Processing Sciences, University of Oulu, Oulu, Oulu Finland 90014 (e-mail: pilar.rodriguez@oulu.fi); Department of Computer Science, University of Auckland, 1415 Auckland, Auckland New Zealand (e-mail: p.ralph@auckland.ac.nz)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","One source of software project challenges and failures is the systematic errors introduced by human cognitive biases. Although extensively explored in cognitive psychology, investigations concerning cognitive biases have only recently gained popularity in software engineering research. This paper therefore systematically maps, aggregates and synthesizes the literature on cognitive biases in software engineering to generate a comprehensive body of knowledge, understand state of the art research and provide guidelines for future research and practise. Focusing on bias antecedents, effects and mitigation techniques, we identified 65 articles (published between 1990 and 2016), which investigate 37 cognitive biases. Despite strong and increasing interest, the results reveal a scarcity of research on mitigation techniques and poor theoretical foundations in understanding and interpreting cognitive biases. Although bias-related research has generated many new insights in the software engineering community, specific bias mitigation techniques are still needed for software professionals to overcome the deleterious effects of cognitive biases on their work.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2877759","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8506423","Antecedents of cognitive bias;cognitive bias;debiasing;effects of cognitive bias;software engineering;systematic mapping","","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Automatic Identification and Classification of Software Development Video Tutorial Fragments","L. Ponzanelli; G. Bavota; A. Mocci; R. Oliveto; M. Di Penta; S. C. Haiduc; B. Russo; M. Lanza","Software Institute, Faculty of Informatics, Universit&#x00E0; della Svizzera italiana, Lugano, Ticino Switzerland (e-mail: luca.ponzanelli@usi.ch); Software Institute, Faculty of Informatics, Universita della Svizzera Italiana, 27216 Lugano, Lugano Switzerland (e-mail: gabriele.bavota@usi.ch); Software Institute, Faculty of Informatics, Universit&#x00E0; della Svizzera italiana, Lugano, Ticino Switzerland (e-mail: andrea.mocci@gmail.com); Department of Bioscience and Territory, University of Molise, Pesche, Isernia Italy 86090 (e-mail: rocco.oliveto@unimol.it); Dept. of Engineering, University of Sannio, Benevento, _ Italy 82100 (e-mail: dipenta@unisannio.it); Department Of Computer Science, Wayne State University, Detroit, Michigan United States 48202 (e-mail: shaiduc@cs.fsu.edu); Faculty of Computer Science, University of Bolzano-Bozen, Bolzano-Bozen, Bolzano Italy 39100 (e-mail: barbara.russo@unibz.it); Software Institute, Faculty of Informatics, University of Lugano, Lugano, TI Switzerland (e-mail: michele.lanza@usi.ch)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Software development video tutorials have seen a steep increase in popularity in recent years. Their main advantage is that they thoroughly illustrate how certain technologies, programming languages, etc. are to be used. However, they come with a caveat: there is currently little support for searching and browsing their content. This makes it difficult to quickly find the useful parts in a longer video, as the only options are watching the entire video, leading to wasted time, or fast-forwarding through it, leading to missed information. We present an approach to mine video tutorials found on the web and enables developers to query their contents as opposed to just their metadata. The video tutorials are processed and split into coherent fragments, such that only relevant fragments are returned in response to a query. Fragments are automatically classified according to their purpose, such as introducing theoretical concepts, explaining code implementation steps, or dealing with errors. This allows developers to set filters in their search to target a specific type of video fragment. In addition, the video fragments in CodeTube are complemented with information from other sources, such as Stack Overflow discussions, giving more context and useful information for understanding the concepts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2779479","Schweizerischer Nationalfonds zur Forderung der Wissenschaftlichen Forschung; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8128506","Recommender Systems;Mining Unstructured Data;Video Tutorials","Tutorials;Java;Software;YouTube;Indexes;Androids;Humanoid robots","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"Bounding completion times of jobs with arbitrary release times, variable execution times and resource sharing","J. Sun; M. K. Gardner; J. W. S. Liu","Geoworks Inc., Berkeley, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","10","603","615","The workload of many real time systems can be characterized as a set of preemptable jobs with linear precedence constraints. Typically their execution times are only known to lie within a range of values. In addition, jobs share resources and access to the resources must be synchronized to ensure the integrity of the system. The paper is concerned with the schedulability of such jobs when scheduled on a priority driven basis. It describes three algorithms for computing upper bounds on the completion times of jobs that have arbitrary release times and priorities. The first two are simple but do not yield sufficiently tight bounds, while the last one yields the tightest bounds but has the greatest complexity.","0098-5589;1939-3520;2326-3881","","10.1109/32.637144","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637144","","Iterative algorithms;Processor scheduling;Upper bound;Scheduling algorithm;Resource management;Real time systems;Algorithm design and analysis;Timing;Sun;Data analysis","real-time systems;resource allocation;scheduling;computational complexity;data integrity","bounding completion times;arbitrary release times;variable execution times;resource sharing;real time systems;preemptable jobs;linear precedence constraints;system integrity;schedulability;priority driven basis;complexity","","12","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Effects of field service on software reliability","C. T. Baker","IBM Corp., Poughkeepsie, NY, USA","IEEE Transactions on Software Engineering","","1988","14","2","254","258","The reliability of a program, when many copies are run in a multisite environment with the support of a software service organization, depends on the inherent reliability of the program and certain characteristics of the service organization. Here, a small number of parameters that determine the relevant characteristics of the service organization are identified, and their effects on the reliability of the program as it is experienced at an average site are analyzed. This is done with two software reliability models, a first (defect) discovery model and a total (defect) discovery model.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4642","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4642","","Software reliability;Software maintenance;Stability","programming theory;software reliability","field service;software reliability;multisite environment;software service organization","","7","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Performance and stability analysis of multilevel data structures with deferred reorganization","I. -. Chen; S. A. Banawan","Dept. of Comput. Sci., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA; NA","IEEE Transactions on Software Engineering","","1999","25","5","690","700","We develop a methodology for analyzing the performance and stability of a server that maintains a multilevel data structure to service a set of access operations for (key, value) records. A subset of the operations executed by the server (e.g., insert and delete) require the multilevel data structure be reorganized so that the sewer can execute all subsequent requests efficiently. We study how often the server should carry out data reorganization (i.e., maintenance) to maximize its performance. If the server is frequently idle then there is no need to impose the reorganization overhead on the operation requests. The reorganization overhead may be completely eliminated by utilizing server-idling periods. If the server is frequently busy, then the reorganization overhead can be minimized by performing a complete reorganization only after the server has served a sufficient number of insert/delete operations so that the amortized cost per operation is small. Therefore, the issue of how often one should perform data reorganization to minimize the average service time depends not only on the multilevel data structure maintained by the server but also on the type and intensity of the system workload. The proposed methodology is exemplified with a two-level sorted file with deferred maintenance. The performance and stability results are compared with those of a single-level binary tree data structure with on-the-fly maintenance. It is shown that deferred maintenance of the two-level sorted file outperforms on-the-fly maintenance of the single-level binary tree in both open and closed systems. Furthermore, deferred maintenance can sustain higher workload intensities without risking system stability.","0098-5589;1939-3520;2326-3881","","10.1109/32.815327","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=815327","","Stability analysis;Data structures;Network servers;Performance analysis;Binary trees;File servers;Costs;Computer networks;Switches;Tree data structures","software performance evaluation;tree data structures;client-server systems;software maintenance","performance evaluation;stability analysis;multilevel data structures;deferred data reorganization;client server systems;access operations;server-idling periods;system workload;two-level sorted file;binary tree data structure;on-the-fly maintenance","","2","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Total variance approach to software reliability estimation","T. Adams","110 Waterloo Station Dr., Cary, NC, USA","IEEE Transactions on Software Engineering","","1996","22","9","687","688","This paper presents a mathematical model for a new approach to calculating the confidence intervals for software reliability projections. Unlike those calculated by current methods, these confidence intervals account for any uncertainty concerning the operational profile of the system.","0098-5589;1939-3520;2326-3881","","10.1109/32.541438","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=541438","","Software reliability;Uncertainty;System testing;Mathematical model;Bayesian methods;Prediction methods;Certification;Probability density function;Computer Society;Algorithms","software reliability;statistical analysis;Bayes methods","total variance approach;software reliability estimation;mathematical model;confidence intervals;software reliability projections;operational profile uncertainty;random testing;Bayesian methods","","14","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Bristlecone: Language Support for Robust Software Applications","B. Demsky; S. Sundaramurthy","University of California, Irvine, Irvine; University of California, Irvine, Irvine","IEEE Transactions on Software Engineering","","2011","37","1","4","23","We present Bristlecone, a programming language for robust software systems. Bristlecone applications have two components: a high-level organization specification that describes how the application's conceptual operations interact and a low-level operational specification that describes the sequence of instructions that comprise an individual conceptual operation. Bristlecone uses the high-level organization specification to recover the software system from an error to a consistent state and to reason how to safely continue the software system's execution after the error. We have implemented a compiler and runtime for Bristlecone. We have evaluated this implementation on three benchmark applications: a Web crawler, a Web server, and a multiroom chat server. We developed both a Bristlecone version and a Java version of each benchmark application. We used injected failures to evaluate the robustness of each version of the application. We found that the Bristlecone versions of the benchmark applications more successfully survived the injected failures. The Bristlecone compiler contains a static analysis that operates on the organization specification to generate a set of diagrams that graphically present the task interactions in the application. We have used the analysis to help understand the high-level structure of three Bristlecone applications: a game server, a Web server, and a chat server.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5416725","Software robustness.","Robustness;Application software;Software systems;Switches;Runtime;Computer languages;Costs;Web server;Java;Software tools","Java;program compilers;program diagnostics;programming languages;software fault tolerance;specification languages","language support;robust software applications;programming language;robust software systems;high-level organization specification;low-level operational specification;runtime;benchmark applications;Web crawler;Web server;multiroom chat server;Java version;injected failures;Bristlecone compiler;static analysis;task interactions;high-level structure;game server","","2","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Model-checking algorithms for continuous-time Markov chains","C. Baier; B. Haverkort; H. Hermanns; J. -. Katoen","Inst. fur Informatik I, Bonn Univ., Germany; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","6","524","541","Continuous-time Markov chains (CTMCs) have been widely used to determine system performance and dependability characteristics. Their analysis most often concerns the computation of steady-state and transient-state probabilities. This paper introduces a branching temporal logic for expressing real-time probabilistic properties on CTMCs and presents approximate model checking algorithms for this logic. The logic, an extension of the continuous stochastic logic CSL of Aziz et al. (1995, 2000), contains a time-bounded until operator to express probabilistic timing properties over paths as well as an operator to express steady-state probabilities. We show that the model checking problem for this logic reduces to a system of linear equations (for unbounded until and the steady-state operator) and a Volterra integral equation system (for time-bounded until). We then show that the problem of model-checking time-bounded until properties can be reduced to the problem of computing transient state probabilities for CTMCs. This allows the verification of probabilistic timing properties by efficient techniques for transient analysis for CTMCs such as uniformization. Finally, we show that a variant of lumping equivalence (bisimulation), a well-known notion for aggregating CTMCs, preserves the validity of all formulas in the logic.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205180","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205180","","Steady-state;Stochastic processes;Probabilistic logic;Transient analysis;System performance;Timing;Integral equations;Performance analysis;Throughput;Production systems","Markov processes;formal verification;probability;temporal logic;bisimulation equivalence;timing;Volterra equations","continuous-time Markov chains;model-checking algorithms;system performance;system dependability;transient-state probabilities;steady-state probabilities;branching temporal logic;real-time probabilistic properties;continuous stochastic logic;time-bounded until operator;probabilistic timing properties;linear equations;Volterra integral equation system;uniformization;lumping equivalence;bisimulation","","340","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Symbolic parametric safety analysis of linear hybrid systems with BDD-like data-structures","Farn Wang","Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan","IEEE Transactions on Software Engineering","","2005","31","1","38","51","We introduce a new BDD-like data structure called hybrid-restriction diagrams (HRDs) for the representation and manipulation of linear hybrid automata (LHA) state-spaces and present algorithms for weakest precondition calculations. This permits us to reason about the valuations of parameters that make safety properties satisfied. Advantages of our approach include the ability to represent discrete state information and concave polyhedra in a unified scheme, as well as to save both memory consumptions and manipulation times when processing the same substructures in state-space representations. Our experimental results document its efficiency in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392719","Index Terms- Data-structures;BDD;hybrid automata;verification;model-checking.","Safety;Data structures;Boolean functions;Binary decision diagrams;Automata;Cost accounting;Computer Society;Systems engineering and theory;Character generation;Protocols","binary decision diagrams;formal verification;automata theory;data structures;formal specification;directed graphs;symbol manipulation","BDD-like data structure;hybrid-restriction diagrams;linear hybrid automata;symbolic parametric safety analysis;formal verification;model-checking","","22","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Introduction to the Special Section on Software Maintenance","N. F. Schneidewind","NA","IEEE Transactions on Software Engineering","","1987","SE-13","3","301","301","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233159","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702214","","Special issues and sections;Software maintenance;Software engineering;Software systems;Software performance;Employment;Large-scale systems;Computer science;Integrated circuit modeling;Software standards","","","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to 'Certifying the reliability of software' (Jan. 1986 3-11)","P. A. Curritt; M. Dyer; H. D. Mills","IBM Corp., Bethesda, MD, USA; IBM Corp., Bethesda, MD, USA; IBM Corp., Bethesda, MD, USA","IEEE Transactions on Software Engineering","","1989","15","3","362","","The authors correct several typographical errors, and misinterpretations in their abovementioned paper (see ibid., vol.SE-12, no.1, p.3-11, Jan. 1986).<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21765","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21765","","Switches;History;Sampling methods;Error correction;Certification;Electric breakdown;Databases;Software packages;Packaging;Software design","software reliability","software reliability","","55","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Composition validation and subjectivity in GenVoca generators","D. Batory; B. J. Geraci","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA","IEEE Transactions on Software Engineering","","1997","23","2","67","82","GenVoca generators synthesize software systems by composing components from reuse libraries. GenVoca components are designed to export and import standardized interfaces, and thus be plug-compatible, interchangeable, and interoperable with other components. We examine two different but important issues in software system synthesis. First, not all syntactically correct compositions of components are semantically correct. We present simple, efficient, and domain-independent algorithms for validating compositions of GenVoca components. Second, components that export and import immutable interfaces are too restrictive for software system synthesis. We show that the interfaces and bodies of GenVoca components are subjective, i.e., they mutate and enlarge upon instantiation. This mutability enables software systems with customized interfaces to be composed from components with ""standardized"" interfaces.","0098-5589;1939-3520;2326-3881","","10.1109/32.585497","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585497","","Software systems;Software libraries;Application software;Programming;Buildings;Software tools;Logic;Debugging;Algorithm design and analysis;Encoding","software tools;computer aided software engineering;application generators;software reusability;software libraries;program verification","composition validation;subjectivity;GenVoca generators;software system synthesis;reuse libraries;standardized interfaces;syntactically correct compositions;semantic correctness;domain-independent algorithms;mutability;customized interfaces;software generators;design rule checking;high-level specifications;code synthesis","","60","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to 'Protocol conversion'","S. S. Lam","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA","IEEE Transactions on Software Engineering","","1988","14","9","1376","","In previously giving a summary of the theory of protocol projection (ibid., vol.14, no.3, p.353-62, Mar. 1988), an incorrect statement was made. A given statement is claimed to be false, and is corrected accordingly.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6181","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6181","","Protocols;Sufficient conditions;Logic;Testing;Cities and towns","protocols","protocol projection","","6","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Interface Grammars for Modular Software Model Checking","G. Hughes; T. Bultan","University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara","IEEE Transactions on Software Engineering","","2008","34","5","614","632","Verification techniques relying on state enumeration (e.g., model checking) face two important challenges: the state-space explosion, an exponential increase in the state space as the number of components increases; and environment generation, modeling components that are either not available for analysis, or that cannot be handled by the verification tool in use. We propose a semi-automated approach for attacking these problems. In our approach, interfaces for the components not under analysis are specified using a specification language based on grammars. Specifically, an interface grammar for a component specifies the sequences of method invocations that are allowed by that component. We have built an compiler that takes the interface grammar for a component as input and generates a stub for that component. The stub thus generated can be used to replace that component during state space exploration, either to moderate state space explosion, or to provide an executable environment for the component under verification. We conducted a case study by writing an interface grammar for the Enterprise JavaBeans (EJB) persistence interface, and using the resulting stub to check EJB clients using the JPF model checker. Our results show that EJB clients can be verified efficiently with JPF using our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.72","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4609388","Model checking;Languages;interface grammars;modular verification;Model checking;Languages;interface grammars;modular verification","State-space methods;Java;Explosions;Software tools;Specification languages;Space exploration;Automata;Production;Writing;Hardware","application program interfaces;finite state machines;formal specification;grammars;program compilers;program verification;specification languages","interface grammar;modular software model checking;state enumeration;state-space explosion;environment generation;semiautomated approach;specification language;compiler;program verification;Enterprise JavaBeans;finite state machine","","8","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Formal requirements analysis of an avionics control system","B. Dutertre; V. Stavridou","Dept. of Comput. Sci., Queen Mary & Westfield Coll., London, UK; NA","IEEE Transactions on Software Engineering","","1997","23","5","267","278","The authors report on a formal requirements analysis experiment involving an avionics control system. They describe a method for specifying and verifying real-time systems with PVS. The experiment involves the formalization of the functional and safety requirements of the avionics system as well as its multilevel verification. First level verification demonstrates the consistency of the specifications whilst the second level shows that certain system safety properties are satisfied by the specification. They critically analyze methodological issues of large scale verification and propose some practical ways of structuring verification activities for optimizing the benefits.","0098-5589;1939-3520;2326-3881","","10.1109/32.588520","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=588520","","Aerospace electronics;Control systems;Software safety;Aircraft;Aerospace control;Real time systems;Large-scale systems;Optimization methods;Formal verification;Collaborative work","formal specification;formal verification;safety-critical software;real-time systems;military avionics;military aircraft;military computing;aircraft control","avionics control system;formal requirements analysis;real-time system specification;real-time system verification;PVS;formalized functional requirements;formalized safety requirements;multilevel verification;system safety properties;large scale verification;verification activity structuring;fighter aircraft","","29","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Heap-filter merge join: a new algorithm for joining medium-size inputs","G. Graefe","Dept. of Comput. Sci., Colorado Univ., Boulder, CO, USA","IEEE Transactions on Software Engineering","","1991","17","9","979","982","A novel algorithm for relational equijoin is presented. The algorithm is a modification of merge join, but promises superior performance for medium-size inputs. In many cases it even compares favorably with both merge join and hybrid hash join, which is shown using analytic cost functions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92919","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92919","","Sorting;Relational databases;Merging;Cost function;Performance analysis;Database systems;Computer science;Writing;Probes","database theory;relational databases","heap filter merge join;medium-size inputs;novel algorithm;relational equijoin;merge join;medium-size inputs;hybrid hash join;analytic cost functions","","","","11","","","","","","IEEE","IEEE Journals & Magazines"
"An incremental version of iterative data flow analysis","L. L. Pollock; M. L. Soffa","Dept. of Comput. Sci., Rice Univ., Houston, TX, USA; NA","IEEE Transactions on Software Engineering","","1989","15","12","1537","1549","A technique is presented for incrementally updating solutions to both union and intersection data-flow problems in response to program edits and transformations. For generality, the technique is based on the iterative approach to computing data-flow information. The authors show that for both union and intersection problems, some changes can be incrementally incorporated immediately into the data-flow sets while others are handled by a two-phase approach. The first phase updates the data-flow sets to overestimate the effect of the program change, enabling the second phase to incrementally update the affected data-flow sets to reflect the actual program change. An important problem that is addressed is the computation of the data-flow changes that need to be propagated throughout a program, based on different local code changes. The technique is compared to other approaches to incremental data-flow analysis.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58766","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58766","","Data analysis;Data flow computing;Information analysis;Application software;Iterative methods;Computer science;Optimizing compilers;Software testing;Optimization methods;Programming environments","iterative methods;parallel programming;set theory;systems analysis","union data flow problems;incremental version;iterative data flow analysis;intersection data-flow problems;program edits;iterative approach;data-flow information;data-flow sets;two-phase approach;data-flow sets;local code changes","","33","","19","","","","","","IEEE","IEEE Journals & Magazines"
"e-Transactions: end-to-end reliability for three-tier architectures","S. Frolund; R. Guerraoui","Hewlett-Packard Labs., Palo Alto, CA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","4","378","395","A three-tier application is organized as three layers: human users interact with front-end clients (e.g., browsers), middle-tier application servers (e.g., Web servers) contain the business logic of the application, and perform transactions against back-end databases. Although three-tier applications are becoming mainstream, they usually fail to provide sufficient reliability guarantees to the users. Usually, replication and transaction-processing techniques are applied to specific parts of the application, but their combination does not provide end-to-end reliability. The aim of this paper is to provide a precise specification of a desirable, yet realistic, end-to-end reliability contract in three-tier applications. The paper presents the specification in the form of the Exactly-Once Transaction (e-Transaction) abstraction: an abstraction that encompasses both safety and liveness properties in three-tier environments. It gives an example implementation of that abstraction and points out alternative implementations and tradeoffs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995430","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995430","","Humans;Web server;Logic;Transaction databases;Contracts;Safety","software reliability;client-server systems;transaction processing;software architecture;formal specification;database management systems","end-to-end reliability;three-tier architectures;front-end clients;middle-tier application servers;business logic;back-end databases;replication;transaction-processing;specification;Exactly-Once Transaction;e-Transaction;safety;liveness;software fault-tolerance;client server systems","","32","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Sufficient condition for a communication deadlock and distributed deadlock detection","B. E. Wojcik; Z. M. Wojcik","Beechcraft Co., Wichita, KS, USA; NA","IEEE Transactions on Software Engineering","","1989","15","12","1587","1595","The necessary and sufficient condition for deadlock in a distributed system and an algorithm for detection of a distributed deadlock based on the sufficient condition are formulated. The protocol formulated, checks all wait-for contiguous requests in one iteration. A cycle is detected when a query message reaches the initiator. A wait-for cycle is only the necessary condition for the distributed deadlock. A no-deadlock message is expected by the query initiator to infer a deadlock-free situation if at least one wait-for cycle is present. A no-deadlock message is issued by a dependent (query intercessor) that is not waiting-for. No no-deadlock message implies a deadlock, and processes listed in the received query messages are the processes involved in a distributed deadlock. Properties of the protocol are discussed. The authors show that a replication of a requested higher-priority (or older) process can prevent a distributed deadlock (in a continuous deadlock treatment). A replication is shown to recover (in a periodical deadlock handling) a sequence of processes from an indefinite wait-die scheme.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58770","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58770","","System recovery;Sufficient conditions;Protocols;Network servers;Resource management;Imaging phantoms;Mathematics;Computer science;Statistics;Hardware","distributed processing;system recovery","communication deadlock;distributed deadlock detection;sufficient condition;protocol;wait-for contiguous requests;query message;wait-for cycle;no-deadlock message;query initiator;deadlock-free situation;query intercessor;replication;periodical deadlock handling;indefinite wait-die scheme","","6","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional Dependability Evaluation for STATEMATE","E. Bode; M. Herbstritt; H. Hermanns; S. Johr; T. Peikenkamp; R. Pulungan; J. Rakow; R. Wimmer; B. Becker","OFFIS Institute for Information Technology, Oldenburg; Albert-Ludwigs-University, Freiburg im Breisgau; Saarland University, Saarbrücken; Saarland University, Saarbrücken; OFFIS Institute for Information Technology, Oldenburg; Saarland University, Saarbrücken; Carl von Ossietzky University, Oldenburg; Albert-Ludwigs-University, Freiburg im Breisgau; Albert-Ludwigs-University, Freiburg im Breisgau","IEEE Transactions on Software Engineering","","2009","35","2","274","292","Software and system dependability is getting ever more important in embedded system design. Current industrial practice of model-based analysis is supported by state-transition diagrammatic notations such as Statecharts. State-of-the-art modelling tools like STATEMATE support safety and failure-effect analysis at design time, but restricted to qualitative properties. This paper reports on a (plug-in) extension of STATEMATE enabling the evaluation of quantitative dependability properties at design time. The extension is compositional in the way the model is augmented with probabilistic timing information. This fact is exploited in the construction of the underlying mathematical model, a uniform continuous-time Markov decision process, on which we are able to check requirements of the form: ""The probability to hit a safety-critical system configuration within a mission time of 3 hours is at most 0.01."" We give a detailed explanation of the construction and evaluation steps making this possible, and report on a nontrivial case study of a high-speed train signalling system where the tool has been applied successfully.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.102","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4711060","Real-time and embedded systems;Fault tolerance;Modeling techniques;Reliability;availability;and serviceability;Model checking;Reliability;Design notations and documentation;State diagrams;Real-time and embedded systems;Fault tolerance;Modeling techniques;Reliability;availability;and serviceability;Model checking;Reliability;Design notations and documentation;State diagrams","Embedded system;Stochastic processes;Safety;Failure analysis;Timing;Mathematical model;Communication system signaling;Fault tolerant systems;Availability;Documentation","decision theory;embedded systems;failure analysis;fault tolerance;formal specification;Markov processes;probability;safety-critical software;software performance evaluation;system monitoring","compositional dependability evaluation;embedded system design;model-based analysis;state-transition diagrammatic notation;failure-effect analysis;probabilistic timing information;uniform continuous-time Markov decision process;safety-critical system configuration;model checking;fault tolerance;statemate;functional specification","","17","","43","","","","","","IEEE","IEEE Journals & Magazines"
"A modeling framework to implement preemption policies in non-Markovian SPNs","A. Bobbio; A. Puliafito; M. Tekel","Fac. di Sci., Univ. del Piemonte, Alessandrai, Italy; NA; NA","IEEE Transactions on Software Engineering","","2000","26","1","36","54","Petri nets represent a useful tool for performance, dependability, and performability analysis of complex systems. Their modeling power can be increased even more if nonexponentially distributed events are considered. However, the inclusion of nonexponential distributions destroys the memoryless property and requires to specify how the marking process is conditioned upon its past history. We consider, in particular, the class of stochastic Petri nets whose marking process can be mapped into a Markov regenerative process. An adequate mathematical framework is developed to deal with the considered class of Markov Regenerative Stochastic Petri Nets (MRSPN). A unified approach for the solution of MRSPNs where different preemption policies can be defined in the same model is presented. The solution is provided both in steady-state and in transient condition. An example concludes the paper.","0098-5589;1939-3520;2326-3881","","10.1109/32.825765","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=825765","","Power system modeling;Stochastic processes;Petri nets;Steady-state;Transient analysis;Computer Society;Performance analysis;History;Specification languages;Stochastic systems","Petri nets;Markov processes;software performance evaluation;specification languages;formal specification","modeling framework;preemption policies;performance analysis;dependability analysis;nonexponentially distributed events;mathematical framework;Markov Regenerative Stochastic Petri Nets;steady-state condition;transient condition;specification language","","34","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Implementing atomic actions in Ada 95","A. Wellings; A. Burns","Dept. of Comput. Sci., York Univ., UK; NA","IEEE Transactions on Software Engineering","","1997","23","2","107","123","Atomic actions are an important dynamic structuring technique that aid the construction of fault-tolerant concurrent systems. Although they were developed some years ago, none of the well-known commercially-available programming languages directly support their use. This paper summarizes software fault tolerance techniques for concurrent systems, evaluates the Ada 95 programming language from the perspective of its support for software fault tolerance, and shows how Ada 95 can be used to implement software fault tolerance techniques. In particular, it shows how packages, protected objects, requeue, exceptions, asynchronous transfer of control, tagged types, and controlled types can be used as building blocks from which to construct atomic actions with forward and backward error recovery, which are resilient to deserter tasks and task abortion.","0098-5589;1939-3520;2326-3881","","10.1109/32.585500","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585500","","Fault tolerant systems;Computer languages;Fault tolerance;Redundancy;Fault detection;Computer Society;Packaging;Protection;Error correction;Abortion","Ada;software fault tolerance;system recovery;exception handling;concurrency control;parallel programming","atomic actions;Ada 95;dynamic structuring technique;fault-tolerant concurrent systems;software fault tolerance techniques;packages;protected objects;requeue;exceptions;asynchronous transfer of control;tagged types;controlled types;error recovery;task abortion;deserter tasks;exception handling;recovery blocks","","9","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling the effects of combining diverse software fault detection techniques","B. Littlewood; P. T. Popov; L. Strigini; N. Shryane","Centre for Software Reliability, City Univ., London, UK; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","12","1157","1167","Considers what happens when several different fault-finding techniques are used together. The effectiveness of such multi-technique approaches depends upon a quite subtle interplay between their individual efficacies. The modeling tool we use to study this problem is closely related to earlier work on software design diversity which showed that it would be unreasonable even to expect software versions that were developed truly independently to fail independently of one another. The key idea was a ""difficulty function"" over the input space. Later work extended these ideas to introduce a notion of ""forced"" diversity. In this paper, we show that many of these results for design diversity have counterparts in diverse fault detection in a single software version. We define measures of fault-finding effectiveness and diversity, and show how these might be used to give guidance for the optimal application of different fault-finding procedures to a particular program. The effects on reliability of repeated applications of a particular fault-finding procedure are not statistically independent; such an incorrect assumption of independence will always give results that are too optimistic. For diverse fault-finding procedures, it is possible for effectiveness to be even greater than it would be under an assumption of statistical independence. Diversity of fault-finding procedures is a good thing and should be applied as widely as possible. The model is illustrated using some data from an experimental investigation into diverse fault-finding on a railway signalling application.","0098-5589;1939-3520;2326-3881","","10.1109/32.888629","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888629","","Diversity reception;Fault detection;Software design;Application software;Redundancy;Hardware;Aerospace control;Battery powered vehicles;Software engineering;Particle measurements","system recovery;program diagnostics;software reliability;signalling;railways","software fault detection techniques;multi-technique approach;modeling tool;software design diversity;independently developed software versions;system failure;difficulty function;forced diversity;fault-finding effectiveness;repeated application reliability;diverse fault-finding procedures;statistical independence;railway signalling application;fault removal;software testing;software reliability growth","","22","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Data Mining Static Code Attributes to Learn Defect Predictors""","H. Zhang; X. Zhang","NA; NA","IEEE Transactions on Software Engineering","","2007","33","9","635","637","In this correspondence, we point out a discrepancy in a recent paper, ""data mining static code attributes to learn defect predictors,"" that was published in this journal. Because of the small percentage of defective modules, using probability of detection (pd) and probability of false alarm (pf) as accuracy measures may lead to impractical prediction models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70706","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288196","defect prediction;accuracy measures;static code attributes;empirical","Data mining;Predictive models;Accuracy;Area measurement;Q measurement;Machine learning;Training data;Information retrieval;Resource management","data mining;learning (artificial intelligence)","data mining static code attributes;defect predictors;detection probability;false alarm probability","","54","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Distributed agreement in the presence of processor and communication faults","K. J. Perry; S. Toueg","IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598; Department of Computer Science, Cornell University, Ithaca, NY 14853","IEEE Transactions on Software Engineering","","1986","SE-12","3","477","482","A model of distributed computation is proposed in which processes may fail by not sending or receiving the message specified by a protocol. The solution to the Byzantine generals problem for this model is presented. The algorithm exhibits early stopping under conditions of less than maximum failure and is as efficient as the algorithm developed for the more restrictive crash-fault model in terms of time, message, and bit complexity. The authors show extant models to underestimate resiliency when faults in the communication medium are considered; the model outlined here is more accurate in this regard.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312888","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312888","Byzantine agreement;distributed computing;early stopping;fault tolerance;protocols","Protocols;Computer crashes;Complexity theory;Computational modeling;Relays;Fault tolerance;Fault tolerant systems","distributed processing;fault tolerant computing;protocols","software engineering;processor faults;communication faults;distributed computation;protocol;Byzantine generals problem;early stopping;crash-fault model;bit complexity;resiliency","","18","","","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to ""Technology for testing nondeterministic client/server database applications""","Gwan-Hwan Hwang; Sheng-Jen Chang; Huey-Der Chu","National Taiwan Normal University; NA; NA","IEEE Transactions on Software Engineering","","2004","30","4","278","278","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274046","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274046","","Testing;Transaction databases;Telecommunication computing;Computer science education;Laboratories;Management information systems","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"STRPN: a Petri-net approach for modeling spatial-temporal relations between moving multimedia objects","Ping-Yu Hsu; Yuan-Bin Chang; Yen-Liang Chen","Dept. of Bus. Adm., Nat. Central Univ., Chung-li, Taiwan; Dept. of Bus. Adm., Nat. Central Univ., Chung-li, Taiwan; Dept. of Bus. Adm., Nat. Central Univ., Chung-li, Taiwan","IEEE Transactions on Software Engineering","","2003","29","1","63","76","A multimedia presentation model provides designers a tool to formally specify the temporal and spatial relationships of objects. The formality helps designers to communicate with others, to check the integrity of designs, and provides a chance to simulate the designs. Although much research has been devoted to this subject, to the best of our knowledge, no multimedia models are able to describe the spatial-temporal relations of moving objects that may refer to each other for computing displaying addresses. The addresses may be recomputed several times during the objects' lifetimes to reflect their movements. Without a formal model, designers are forced to specify the relationships in an ad hoc manner that causes misunderstanding and hampers integrity check. The check includes if an object gets its addresses in time from another object, if an object is displayed in the right places, etc. The difficulty of designing such a formal model lies in integrating temporal constraints of objects with a real-time address transferring mechanism. In this paper, we present an extended Petri-net model, which models concurrent relationships of objects with new places, transitions, and firing rules to transfer and transform addresses in real time. Its descriptive power and correctness is demonstrated by five patterns of multimedia presentations and a sample play scripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166589","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166589","","Computational modeling;Multimedia computing;Computer displays;Power engineering and energy;Video sharing;Natural languages;Humans;Petri nets","multimedia computing;Petri nets;temporal logic;synchronisation","STRPN;Petri-net approach;spatial-temporal relations;moving multimedia objects;multimedia presentation model;temporal relationships;spatial relationships;integrity check;extended Petri-Net model;concurrent relationships;descriptive power;sample play scripts","","12","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Selected papers from the second IFIP Int'l conference on formal methods for open object based distributed systems, 1997","H. Bowman; J. Derrick; E. Brinksma","The University of Kent at Canterbury; NA; NA","IEEE Transactions on Software Engineering","","2000","26","7","577","578","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.859528","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859528","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Allocating programs containing branches and loops within a multiple processor system"" by D. Towsley","F. Kaudel","Dept of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada","IEEE Transactions on Software Engineering","","1990","16","4","471","","The assignment algorithms proposed by D. Towsley (see ibid., vol.12, no.10, p.1018-24 (1986)) may not minimize the total communication and execution costs as stated. An example where the proposed restricted assignment algorithm fails to find an optimal assignment is given, and modifications that allow the algorithm to properly consider execution costs are proposed. Additional changes needed to model communication costs correctly in many assignment problems are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.54298","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=54298","","Cost function;Distributed processing;Processor scheduling;Scheduling algorithm","operating systems (computers);scheduling","programs allocation;branches;loops;multiple processor system;assignment algorithms","","1","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Temporal logic-based deadlock analysis for Ada"" by G.M. Karam and R.J.A. Burh","M. Young; D. L. Levine; R. N. Taylor","Dept. of Comput. Science, Purdue Univ., West Lafayette, IN, USA; NA; NA","IEEE Transactions on Software Engineering","","1993","19","2","198","199","The commenters discuss several flaws they found in the above-titled paper by G.M. Koran and R.J.A. Burh (see ibid., vol.17, no.10, p.109-1125, (1991)). The commenters argue that the characterization of operational and axiomatic proof method is modified and inaccurate; the classification of modeling techniques for concurrent systems confuses the distinction between state-based and event-based models with the essential distinction between explicit enumeration of behaviors and symbolic manipulation of properties; the statements about the limitations of linear-time temporal logic in relation to nondeterminism are inaccurate; and the characterization of the computational complexity of the analysis technique is overly optimistic.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.214836","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=214836","","System recovery;Logic;Computational complexity;Computer science;Interleaved codes;Testing;Heart;Software engineering;Safety;Reachability analysis","Ada;computational complexity;concurrency control;symbol manipulation;temporal logic","temporal logic-based deadlock analysis;state-based models;Ada;axiomatic proof method;event-based models;symbolic manipulation;nondeterminism;computational complexity","","","","14","","","","","","IEEE","IEEE Journals & Magazines"
"A multiframe model for real-time tasks","A. K. Mok; D. Chen","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA","IEEE Transactions on Software Engineering","","1997","23","10","635","645","The well known periodic task model of C.L. Liu and J.W. Layland (1973) assumes a worst case execution time bound for every task and may be too pessimistic if the worst case execution time of a task is much longer than the average. We give a multiframe real time task model which allows the execution time of a task to vary from one instance to another by specifying the execution time of a task in terms of a sequence of numbers. We investigate the schedulability problem for this model for the preemptive fixed priority scheduling policy. We show that a significant improvement in the utilization bound can be established in our model.","0098-5589;1939-3520;2326-3881","","10.1109/32.637146","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637146","","Processor scheduling;Scheduling algorithm;Vehicles;Computational modeling;Adaptive scheduling;Timing","real-time systems;scheduling;multiprogramming;computational complexity","execution time;real time tasks;periodic task model;worst case execution time bound;multiframe real time task model;schedulability problem;preemptive fixed priority scheduling policy;utilization bound","","95","","16","","","","","","IEEE","IEEE Journals & Magazines"
"The Adoption of JavaScript Linters in Practice: A Case Study on ESLint","K. F. Tómasdóttir; M. Aniche; A. Van Deursen","Software Engineering Research Group, Technische Universiteit Delft, 2860 Delft, South Holland Netherlands (e-mail: kristinfjolato@gmail.com); Software Engineering Research Group, Technische Universiteit Delft, 2860 Delft, South Holland Netherlands 2628CD (e-mail: mauricioaniche@gmail.com); EEMCS / ST / SE, Delft University of Technology, Delft, ZH Netherlands 2628 CD (e-mail: arie.vandeursen@tudelft.nl)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","A linter is a static analysis tool that warns software developers about possible code errors or violations to coding standards. By using such a tool, errors can be surfaced early in the development process when they are cheaper to fix. For a linter to be successful, it is important to understand the needs and challenges of developers when using a linter.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2871058","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8468105","","Tools;Static analysis;Interviews;Encoding;Standards;Software;Face","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"An experimental investigation of software metrics and their relationship to software development effort","R. K. Lind; K. Vairavan","Gen. Electr. Med. Syst., Milwaukee, WI, USA; NA","IEEE Transactions on Software Engineering","","1989","15","5","649","653","The results are reported of an experimental study of software metrics for a fairly large software system used in a real-time application. A number of issues are examined, including the mutual relationship between various software metrics and, more importantly, the relationship between metrics and the development effort. Some interesting connections are reported between metrics and the software development effort.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24715","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24715","","Software metrics;Programming;Bandwidth;Application software;Software maintenance;Software systems;Real time systems;Software quality;Computer industry","software engineering","software metrics;software development;software system;real-time application;mutual relationship","","55","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Variability Mining: Consistent Semi-automatic Detection of Product-Line Features","C. Kästner; A. Dreiling; K. Ostermann","School of Computer Science, Carnegie Mellon University; University of Magdeburg and Deutsche Bank AG, Germany; Department of Mathematics and ComputerScience at Philipps University Marburg, Germany","IEEE Transactions on Software Engineering","","2014","40","1","67","82","Software product line engineering is an efficient means to generate a set of tailored software products from a common implementation. However, adopting a product-line approach poses a major challenge and significant risks, since typically legacy code must be migrated toward a product line. Our aim is to lower the adoption barrier by providing semi-automatic tool support-called variability mining -to support developers in locating, documenting, and extracting implementations of product-line features from legacy code. Variability mining combines prior work on concern location, reverse engineering, and variability-aware type systems, but is tailored specifically for the use in product lines. Our work pursues three technical goals: (1) we provide a consistency indicator based on a variability-aware type system, (2) we mine features at a fine level of granularity, and (3) we exploit domain knowledge about the relationship between features when available. With a quantitative study, we demonstrate that variability mining can efficiently support developers in locating features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6613490","Variability;reverse engineering;mining;feature;software product line;LEADT;feature location","Feature extraction;Software;Context;Data mining;Java;Companies;Educational institutions","data mining;reverse engineering;software product lines","variability mining;consistent semi automatic detection;product line features;software product line engineering;product line approach;legacy code;semi automatic tool support;reverse engineering;variability aware type systems","","25","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Defining and applying measures of distance between specifications","L. L. Jilani; J. Desharnais; A. Mili","Inst. de Recherches en Sci., Inf. et Telecommun., Ariana, Tunisia; NA; NA","IEEE Transactions on Software Engineering","","2001","27","8","673","703","Echoing Louis Pasteur's quote, we submit the premise that it is advantageous to define measures of distance between requirements specifications because such measures open up a wide range of possibilities both in theory and in practice. The authors present a mathematical basis for measuring distances between specifications and show how their measures of distance can be used to address concrete problems that arise in the practice of software engineering.","0098-5589;1939-3520;2326-3881","","10.1109/32.940565","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=940565","","Software measurement;Application software;Computer Society;Software engineering;Lattices;Software libraries;Concrete;Upper bound;Kernel;Arithmetic","formal specification;software metrics;systems analysis","specification distance measures;requirements specifications;mathematical basis;concrete problems;software engineering","","17","","61","","","","","","IEEE","IEEE Journals & Magazines"
"Algorithms for scheduling real-time tasks with input error and end-to-end deadlines","Wu-Chun Feng; J. W. -. Liu","Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; NA","IEEE Transactions on Software Engineering","","1997","23","2","93","106","This paper describes algorithms for scheduling preemptive, imprecise, composite tasks in real-time. Each composite task consists of a chain of component tasks, and each component task is made up of a mandatory part and an optional part. Whenever a component task uses imprecise input, the processing times of its mandatory and optional parts may become larger. The composite tasks are scheduled by a two-level scheduler. At the high level, the composite tasks are scheduled preemptively on one processor, according to an existing algorithm for scheduling simple imprecise tasks. The low-level scheduler then distributes the time budgeted for each composite task across its component tasks so as to minimize the output error of the composite task.","0098-5589;1939-3520;2326-3881","","10.1109/32.585499","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585499","","Scheduling algorithm;Timing;Processor scheduling;Error correction;Computer applications;Distributed computing;Real time systems;Robustness;Degradation;Video compression","real-time systems;programming theory;scheduling","real-time task scheduling algorithms;input error;end-to-end deadlines;composite task;component tasks;two-level scheduler;imprecise tasks;transient overload;imprecise-computation technique","","34","","22","","","","","","IEEE","IEEE Journals & Magazines"
"A simple mechanism for type security across compilation units","M. L. Scott; R. A. Finkel","Dept. of Comput. Sci., Rochester Univ., NY, USA; NA","IEEE Transactions on Software Engineering","","1988","14","8","1238","1239","A simple technique is described that detects structural-type clashes across compilation units with an arbitrarily high degree of confidence. The type of each external object is described in canonical form. A hash function compresses the description into a short code. If the code is embedded in a symbol-table name, then consistency can be checked by an ordinary linker. For distributed programs, run-time checking of message types can be performed with very little overhead.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.7631","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7631","","Runtime;Scholarships;Computer science;File servers","data structures;program compilers","compilers;data structures;type security;compilation units;structural-type clashes;hash function;symbol-table name;ordinary linker;distributed programs;run-time checking;message types","","3","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Quality, productivity, and learning in framework-based development: an exploratory case study","Chia Hung Kao","National Cheng Kung University","IEEE Transactions on Software Engineering","","2003","29","3","288","288","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183941","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183941","","Productivity;Computer aided software engineering;Statistical analysis;Performance evaluation;Testing;Error correction;Cities and towns","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Estimating bounds on the reliability of diverse systems","P. Popov; L. Strigini; J. May; S. Kuball","Centre for Software Reliability, City Univ., London, UK; Centre for Software Reliability, City Univ., London, UK; NA; NA","IEEE Transactions on Software Engineering","","2003","29","4","345","359","We address the difficult problem of estimating the reliability of multiple-version software. The central issue is the degree of statistical dependence between failures of diverse versions. Previously published models of failure dependence described what behavior could be expected ""on average"" from a pair of ""independently generated"" versions. We focus instead on predictions using specific information about a given pair of versions. The concept of ""variation of difficulty"" between situations to which software may be subject is central to the previous models cited, and it turns out to be central for our question as well. We provide new understanding of various alternative imprecise estimates of system reliability and some results of practical use, especially with diverse systems assembled from pre-existing (e.g., ""off-the-shelf"") subsystems. System designers, users, and regulators need useful bounds on the probability of system failure. We discuss how to use reliability data about the individual diverse versions to obtain upper bounds and other useful information for decision making. These bounds are greatly affected by how the versions' probabilities of failure vary between subdomains of the demand space or between operating regimes-it is even possible in some cases to demonstrate, before operation, upper bounds that are very close to the true probability of failure of the system-and by the level of detail with which these variations are documented in the data.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1191798","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1191798","","Software safety;Regulators;Upper bound;Computer errors;Application software;Hardware;Reliability;Assembly systems;Decision making;Fault tolerant systems","redundancy;software reliability","diverse system reliability bounds estimation;multiple-version software reliability;multiversion software reliability;statistical dependence;software failures;failure dependence;difficulty variation;system designers;system users;system regulators;decision making","","23","","24","","","","","","IEEE","IEEE Journals & Magazines"
"How Practitioners Perceive Automated Bug Report Management Techniques","W. Zou; D. Lo; Z. Chen; X. Xia; Y. Feng; B. Xu","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu China (e-mail: wqzou@smail.nju.edu.cn); School of Information Systems, Singapore Management University, Singapore, Singapore Singapore 17890 (e-mail: davidlo@smu.edu.sg); State Key Laboratory for Novel Software Technology, Nanjing University, Jiangsu, Jiangsu China 210093 (e-mail: zychen@nju.edu.cn); Faculty of Information Technology, Monash University, Melbourne, Victoria Australia (e-mail: xxkidd@zju.edu.cn); Department of Informatics, University of California, Irvine, Irvine, California United States (e-mail: yang.feng@uci.edu); Computer, Nanjing University, Nanjing, Jiangsu China (e-mail: bwxu@nju.edu.cn)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Bug reports play an important role in the process of debugging and fixing bugs. To reduce the burden of bug report managers and facilitate the process of bug fixing, a great amount of software engineering research has been invested into automated bug report management techniques. However, the verdict is still open whether such techniques are actually required and applicable outside of the theoretical research domain. To fill this gap, in this paper, we conducted a survey among 327 practitioners to gain their insights into various categories of automated bug report management techniques. Specifically, in the survey, we asked them to rate the importance of such techniques and provide the rationales of their ratings. To get deeper insight into practitioners' perspective, we conducted follow-up interviews with 25 interviewees selected from the survey respondents. Through the survey and the interviews, we gained a better understanding of the perceived usefulness (or its lack) of different categories of automated bug report management techniques. Based on the survey and interview results, we summarized some potential research directions in developing techniques to help developers better manage bug reports.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2870414","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8466000","Bug Report;Developer Perception","Computer bugs;Software;Software engineering;Bibliographies;Conferences;Interviews;Maintenance engineering","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"EVA: a flexible programming method for evolving systems","S. Matsuura; H. Kuruma; S. Honiden","Lab. for New Software Archit., Inf. Technol. Promotion Agency, Tokyo, Japan; NA; NA","IEEE Transactions on Software Engineering","","1997","23","5","296","313","The authors' goal is to establish a flexible programming support for evolving systems that will enable one to modify programs using less labor, while maintaining good quality during service life. EVA (evolution mechanism for flexible agent) was developed to allow a flexible programming support system to be constructed based on their programming method for evolving systems. They consider that programming methods for evolving systems need to satisfy the following essential conditions. First, they need to make it easy to specify changes in a system in terms of new requirements. Second, they need to have a procedure for transmitting the new requirements to a program. Third, they need to be able to guarantee that the resultant program will meet the new requirements. Finally, because of the repetitive nature of much evolving systems work, they need to provide for the reuse of similar modifications during programming. In order to overcome limitations in program modification techniques which have only considered programming products, programming processes have been introduced into the method. To achieve their goal, they have formulated programming products and programming processes using such formal techniques as functional programming, type theory, modules, parameterized programming and natural deduction, and have constructed a mechanism of reusing these formal programming processes. The paper explains a case study which shows how to develop an evolving system using EVA and it discusses how one can use EVA's mechanism effectively.","0098-5589;1939-3520;2326-3881","","10.1109/32.588522","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=588522","","Functional programming;Genetic programming;Programming profession;High level languages;Automatic programming;Application software;Laboratories;Software architecture","software reusability;formal specification;programming environments;software maintenance;functional programming;type theory","flexible programming method;evolving systems;EVA;program modification;quality;evolution mechanism for flexible agent;change specification;new requirements;modification reuse;formal techniques;functional programming;type theory;modules;parameterized programming;natural deduction;formal programming processes","","5","","27","","","","","","IEEE","IEEE Journals & Magazines"
"On the projection method for protocol verification","T. Cheung","Distributed Computing Research Group, Department of Computer Science, University of Ottawa, Ottawa, Ont., Canada K1N 9B4","IEEE Transactions on Software Engineering","","1986","SE-12","11","1088","1089","S.S. Lam and A.U. Shankar (1982) have proposed a projection method for protocol verification. They claim that the method guarantees the faithfulness of the safety and liveness properties of a protocol system. Although not clearly defined, `faithfulness' appears to mean that `the image protocol system is live (respectively, safe) if and only if the original protocol system is live (respectively, safe). It is shown that the `only if' part is not true for certain liveness properties, and a remedy is suggested.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312998","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312998","Projection method;protocol verification;safety and liveness properties","Protocols;Real-time systems;Timing;Software reliability;Fault tolerance;Fault tolerant systems","protocols","projection method;protocol verification;image protocol system","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Integrating formal verification and conformance testing for reactive systems","C. Constant; T. Jéron; H. Marchand; V. Rusu","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","8","558","574","In this paper, we describe a methodology integrating verification and conformance testing. A specification of a system - an extended input-output automaton, which may be infinite-state - and a set of safety properties (""nothing bad ever happens"") and possibility properties (""something good may happen"") are assumed. The properties are first tentatively verified on the specification using automatic techniques based on approximated state-space exploration, which are sound, but, as a price to pay for automation, are not complete for the given class of properties. Because of this incompleteness and of state-space explosion, the verification may not succeed in proving or disproving the properties. However, even if verification did not succeed, the testing phase can proceed and provide useful information about the implementation. Test cases are automatically and symbolically generated from the specification and the properties and are executed on a black-box implementation of the system. The test execution may detect violations of conformance between implementation and specification; in addition, it may detect violation/satisfaction of the properties by the implementation and by the specification. In this sense, testing completes verification. The approach is illustrated on simple examples and on a bounded retransmission protocol.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70707","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267026","","Formal verification;System testing;Safety;Formal specifications;Automata;Automation;Explosions;Automatic testing;Protocols;Performance evaluation","automata theory;conformance testing;formal specification;formal verification","formal verification;conformance testing;reactive systems;formal specification;extended input-output automaton;infinite-state;safety properties;possibility properties;approximated state-space exploration;bounded retransmission protocol","","28","","33","","","","","","IEEE","IEEE Journals & Magazines"
"An architecture for high performance engineering information systems","N. Roussopoulos; L. Mark; T. Sellis; C. Faloutsos","Maryland Univ., College Park, MD, USA; Maryland Univ., College Park, MD, USA; Maryland Univ., College Park, MD, USA; Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1991","17","1","22","33","Commercially available database systems do not meet the information and processing needs of design and manufacturing environments. A new generation of systems-engineering information systems-must be built to meet these needs. The architectural and computational aspects of such systems are addressed, and solutions are proposed. The authors argue that a mainframe-workstation architecture is needed to provide distributed functionality while ensuring high availability and low communication overhead, that explicit control of metaknowledge is needed to support extendibility and evolution, that large rule bases are needed to make the knowledge of the systems active, and that incremental computation models are needed to achieve the required performance of such engineering information systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67576","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67576","","Computer architecture;Database systems;Process design;Manufacturing processes;Availability;Communication system control;Distributed computing;High performance computing;Computational modeling;Knowledge engineering","database management systems;manufacturing data processing","design environments;engineering information systems;database systems;manufacturing environments;mainframe-workstation architecture;distributed functionality;metaknowledge;extendibility;evolution;rule bases;knowledge;incremental computation models;performance","","4","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Cost-effective analysis of in-place software processes","J. E. Cook; L. G. Votta; A. L. Wolf","Dept. of Comput. Sci., New Mexico State Univ., Las Cruces, NM, USA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","8","650","663","Process studies and improvement efforts typically call for new instrumentation on the process in order to collect the data they have deemed necessary. This can be intrusive and expensive, and resistance to the extra workload often foils the study before it begins. The result is neither interesting new knowledge nor an improved process. In many organizations, however, extensive historical process and product data already exist. Can these existing data be used to empirically explore what process factors might be affecting the outcome of the process? If they can, organizations would have a cost-effective method for quantitatively, if not causally, understanding their process and its relationship to the product. We present a case study that analyzes an in-place industrial process and takes advantage of existing data sources. In doing this, we also illustrate and propose a methodology for such exploratory empirical studies. The case study makes use of several readily-available repositories of process data in the industrial organization. Our results show that readily available data can be used to correlate both simple aggregate metrics and complex process metrics with defects in the product. Through the case study, we give evidence supporting the claim that exploratory empirical studies can provide significant results and benefits while being cost-effective in their demands on the organization.","0098-5589;1939-3520;2326-3881","","10.1109/32.707700","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=707700","","Instruments;Computer Society;Software engineering;Industrial relations;Aggregates;Software measurement;Software quality;Computer industry;History;Electrical equipment industry","software engineering","cost-effective analysis;in-place software processes;process improvement;historical data;retrospective case study;industrial process;existing data sources;exploratory empirical studies;process data repositories;aggregate metrics;complex process metrics;software product defects;organizational demands;process measurement;process model validation","","7","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Query optimization for nontraditional database applications","T. K. Sellis; L. Shapiro","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA","IEEE Transactions on Software Engineering","","1991","17","1","77","86","Database query languages and their use for programming nontraditional applications, such as engineering and artificial intelligence applications, are discussed. In such environments, database programs are used to code applications that work over large data sets residing in databases. Optimizing such programs then becomes a necessity. An examination is made of various optimization techniques, and transformations are suggested for improving the performance of database programs. These transformations result in new equivalent database programs with better space and time performance. Several of these techniques apply to classical query languages, although extended query languages which include an iteration operator are specifically discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67580","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67580","","Query processing;Database languages;Relational databases;Data engineering;Artificial intelligence;Computer science;Deductive databases;Indexes;Database systems;Data processing","database management systems;query languages","query optimization;database query languages;space performance;nontraditional database applications;programming;engineering;artificial intelligence;environments;large data sets;equivalent database programs;time performance;extended query languages;iteration operator","","1","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Primitives for distributed computing in a heterogeneous local area network environment","G. Bernard; A. Duda; Y. Haddad; G. Harrus","ISEM, Univ. Paris-Sud, Orsay, France; NA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","12","1567","1578","Epsilon is a testbed for monitoring distributed applications involving heterogeneous computers, including microcomputers, interconnected by a local area network. Such a hardware configuration is usual but raises difficulties for the programmer. First, the interprocess communication mechanisms provided by the operating systems are rather cumbersome to use. Second, they are different from one system to another. Third, the programmer of distributed applications should not worry about system and/or network aspects that are not relevant for the application level. The authors present the solution chosen in Epsilon. A set of high-level communication primitives has been designed and implemented to provide the programmer with an interface independent of the operating system and of the underlying interprocess communications facilities. A program participating in a distributed application can be executed on any host without any change in the source code except for host names.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58768","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58768","","Distributed computing;Programming profession;Application software;Operating systems;Testing;Computerized monitoring;Computer applications;Computer networks;Microcomputers;LAN interconnection","computer communications software;local area networks;software engineering","distributed computing;heterogeneous local area network environment;distributed applications;heterogeneous computers;microcomputers;hardware configuration;interprocess communication mechanisms;operating systems;network aspects;Epsilon;high-level communication primitives;source code;host names","","7","","52","","","","","","IEEE","IEEE Journals & Magazines"
"A decentralized self-adaptation mechanism for service-based applications in the cloud","V. Nallur; R. Bahsoon","University of Birmingham, Birmingham; University of Birmingham, Birmingham","IEEE Transactions on Software Engineering","","2013","39","5","591","612","Cloud computing, with its promise of (almost) unlimited computation, storage, and bandwidth, is increasingly becoming the infrastructure of choice for many organizations. As cloud offerings mature, service-based applications need to dynamically recompose themselves to self-adapt to changing QoS requirements. In this paper, we present a decentralized mechanism for such self-adaptation, using market-based heuristics. We use a continuous double-auction to allow applications to decide which services to choose, among the many on offer. We view an application as a multi-agent system and the cloud as a marketplace where many such applications self-adapt. We show through a simulation study that our mechanism is effective for the individual application as well as from the collective perspective of all applications adapting at the same time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6249687","Self-adaptation;market based;multi-agent systems","Quality of service;Pricing;Reliability;Resource management;Measurement;Adaptation models;Cloud computing","cloud computing;electronic commerce;multi-agent systems;quality of service","decentralized self-adaptation mechanism;service-based applications;cloud computing;QoS requirements;market-based heuristics;continuous double-auction;multiagent system","","35","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Performance measurement for parallel and distributed programs: a structured and automatic approach","C. -. Yang; B. P. Miller","Dept. of Comput. Sci., North Texas Univ., Denton, TX, USA; NA","IEEE Transactions on Software Engineering","","1989","15","12","1615","1629","Novel approaches are presented for designing performance measurement systems for parallel and distributed programs. The first approach involves unifying performance information into a single, regular structure that reflects the structure of programs under measurement. The authors define a hierarchical model for the execution of parallel and distributed programs as a framework for the performance measurement. A complete different levels of detail in the hierarchy. The second approach is based on the development of automatic guidance techniques that can direct users to the location of performance problems in the program. Guidance information from such techniques supplies facts about problems in the program and provides possible answers for the further improvement of program efficiency. A performance measurement system, called IPS, has been developed as a prototype of the authors' model and design. Some of the test results from IPS are also discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58772","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58772","","Operating systems;Prototypes;Testing;Distributed computing;Concurrent computing;Delay;Peer to peer computing;Computer science;Current measurement;Computer languages","multiprocessing programs;performance evaluation;program testing;software reliability","parallel programs;automatic approach;performance measurement systems;distributed programs;performance information;hierarchical model;automatic guidance techniques;performance problems;program efficiency;IPS","","28","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Using coverage information to predict the cost-effectiveness of regression testing strategies","D. S. Rosenblum; E. J. Weyuker","Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA; NA","IEEE Transactions on Software Engineering","","1997","23","3","146","156","Selective regression testing strategies attempt to choose an appropriate subset of test cases from among a previously run test suite for a software system, based on information about the changes made to the system to create new versions. Although there has been a significant amount of research in recent years on the design of such strategies, there has been very little investigation of their cost-effectiveness. The paper presents some computationally efficient predictors of the cost-effectiveness of the two main classes of selective regression testing approaches. These predictors are computed from data about the coverage relationship between the system under test and its test suite. The paper then describes case studies in which these predictors were used to predict the cost-effectiveness of applying two different regression testing strategies to two software systems. In one case study, the TESTTUBE method selected an average of 88.1 percent of the available test cases in each version, while the predictor predicted that 87.3 percent of the test cases would be selected on average.","0098-5589;1939-3520;2326-3881","","10.1109/32.585502","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585502","","System testing;Software testing;Costs;Software systems;Computer Society;Information analysis;Personnel","statistical analysis;software cost estimation;program testing;system monitoring","coverage information;cost effectivenes prediction;selective regression testing strategies;test cases;test suite;software system;system changes;computationally efficient predictors;coverage relationship;TESTTUBE method","","43","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""A distributed scheme for detecting communication deadlocks"" by N. Natarajan","Liu Lingzhong","Dept. of Electron., Inner Mongolia Univ., Huhehot, China","IEEE Transactions on Software Engineering","","1989","15","7","926","","A distributed scheme for detecting communication deadlocks and a correctness proof of the algorithm were given by N. Natarajan (see ibid., vol.SE-12, p.531-7 (1986)). It is shown in this correspondence that the proof is not strict.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.29492","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=29492","","System recovery;Microcomputers;Software design","distributed processing","communication deadlocks detection;distributed scheme;correctness proof","","","","2","","","","","","IEEE","IEEE Journals & Magazines"
"An Aspect of Aesthetics in Human-Computer Communications: Pretty Windows","J. Gait","Computer Research Laboratory, Tektronix, Inc.","IEEE Transactions on Software Engineering","","1985","SE-11","8","714","717","Aesthetics in user interfaces addresses font definitions, typesetting conventions, color combinations, graphics design considerations, high resolution for viewscreens, and the shapes of windows. Computer viewscreens are evolving into pictorial media, communicating information with visual immediacy. The more interesting interfaces make use of multiple windows, menus, icons, and other visual effects to waken and sustain user interest and effectiveness. A pretty window is a viewscreen window with the dimensions of a golden rectangle, a rectangle whose width and height form the golden ratio of Euclid. Psychologists believe golden rectangles are aesthetically more pleasing than arbitrary rectangles, and subjects tend to select them in preference to other rectangles in tests.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232520","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702080","Aesthetics;human factors;man-machine communications;psychological preferences;viewscreen windows","Computer interfaces;Application software;User interfaces;Typesetting;Computer graphics;Shape;Psychology;Software safety;Workstations;Communication system control","","Aesthetics;human factors;man-machine communications;psychological preferences;viewscreen windows","","10","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction to the Special Section from the International Conference on Software Maintenance and Evolution","D. Binkley; R. Koschke; S. Mancoridis","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","797","798","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70765","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4375378","","Software maintenance;Testing;Conferences;Computer industry;Government;Predictive models;Software engineering;Software systems;Buildings;Computer architecture","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"A classification of noncircular attribute grammars based on the look-ahead behavior","Wuu Yang","Dept. of Comput. & Inf. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan","IEEE Transactions on Software Engineering","","2002","28","3","210","227","We propose a family of static evaluators for subclasses of the well-defined (i.e., noncircular) attribute grammars. These evaluators augment the evaluator for the absolutely noncircular attribute grammars with look-ahead behaviors. Because this family covers exactly the set of all well-defined attribute grammars, well-defined attribute grammars may be classified into a hierarchy, called the NC hierarchy, according to their evaluators in the family. The location of a noncircular attribute grammar in the NC hierarchy is an intrinsic property of the grammar. The NC hierarchy confirms a result of Riis and Skyum (1981), which says that all well-defined attribute grammars allow a (static) pure multivisit evaluator by actually constructing such an evaluator. We also show that, for any finite m, an NC(m) attribute grammar can be transformed to an equivalent NC(0) grammar.","0098-5589;1939-3520;2326-3881","","10.1109/32.991318","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=991318","","Formal languages","attribute grammars;trees (mathematics)","static evaluators;noncircular attribute grammars;look-ahead behavior;syntax tree;NC hierarchy;pure multivisit evaluator;ordered attribute grammars;grammar classification","","","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Supertotal function definition in mathematics and software engineering","R. Boute","INTEC, Ghent Univ., Belgium","IEEE Transactions on Software Engineering","","2000","26","7","662","672","In engineering (including computing), mathematics and logic, expressions can arise that contain function applications where the argument is outside the function's domain. Such a situation need not represent a conceptual error, for instance, in conditional expressions, but it is traditionally considered a type error. Various solutions can be found in the literature based on the notion of partial function and/or a distinguished value undefined. However, these have rather pervasive effects, complicating function definition, sacrificing convenient algebraic laws of logical operators and/or Leibniz's rule, one of the most valuable assets in formal reasoning (especially in the calculational style). Other solutions have in common the realization that well-structured mathematical arguments are always guarded by conditions and that the value of A/spl rArr/B is not affected by domain violations in B in case-A. These solutions preserve Leibniz's rule and the standard meaning of the logical operators. In this second category, we propose the simplest possible solution, called supertotal function definition, and consisting of assigning the value false (or 0, depending on the preferred formalism) to any function application where the argument is outside the domain. This approach assumes the notion of function with which a domain is associated as a part of its specification. Ramifications regarding formal reasoning, use in software engineering (such as Parnas's predicate calculus) and in mathematical formulation in general are discussed. The proposed solution justifies formal reasoning as usual, but with increased freedom in expressions regarding types of function arguments. Hence, it can be adopted in existing formalisms with very minor changes to the latter, As a bonus, this discussion includes a very simple new view on conditional expressions, yielding unusually powerful and convenient calculational properties. Finally, differences and advantages w.r.t. other approaches are pointed out.","0098-5589;1939-3520;2326-3881","","10.1109/32.859534","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859534","","Mathematics;Software engineering;Application software;Calculus;Logic;Software standards","formal specification","function definition;software engineering;mathematics;formal methods;software specification;predicate calculus;calculational reasoning;functional mathematics;guarded formulas;conditional expressions;undefinedness;type correctness;subtyping","","1","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Formal methods application: an empirical tale of software development","A. E. K. Sobel; M. R. Clarkson","Dept. of Comput. Sci. & Syst. Anal., Miami Univ., Oxford, OH, USA; NA","IEEE Transactions on Software Engineering","","2002","28","3","308","320","The development of an elevator scheduling system by undergraduate students is presented. The development was performed by 20 teams of undergraduate students, divided into two groups. One group produced specifications by employing a formal method that involves only first-order logic. The other group used no formal analysis. The solutions of the groups are compared using the metrics of code correctness, conciseness, and complexity. Particular attention is paid to a subset of the formal methods group which provided a full verification of their implementation. Their results are compared to other published formal solutions. The formal methods group's solutions are found to be far more correct than the informal solutions.","0098-5589;1939-3520;2326-3881","","10.1109/32.991322","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=991322","","Application software;Programming","formal specification;object-oriented programming;computer science education;formal logic;software metrics","formal methods;software development;elevator scheduling system;undergraduate students;formal specifications;first-order logic;software metrics;object oriented design;code correctness;software engineering curriculum","","26","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Mathematical notation in formal specification: too difficult for the masses?","K. Finney","Sch. of Comput. & Math. Sci., Greenwich Univ., London, UK","IEEE Transactions on Software Engineering","","1996","22","2","158","159","The phrase ""not much mathematics required"" can imply a variety of skill levels. When this phrase is applied to computer scientists, software engineers, and clients in the area of formal specification, the word ""much"" can be widely misinterpreted with disastrous consequences. A small experiment in reading specifications revealed that students already trained in discrete mathematics and the specification notation performed very poorly; much worse than could reasonably be expected if formal methods proponents are to be believed.","0098-5589;1939-3520;2326-3881","","10.1109/32.485225","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=485225","","Formal specifications;Mathematics;Programming profession;Mathematical programming;Education;Software engineering;Computer science;Natural languages;Set theory;Logic","formal specification;computer science education;teaching","mathematical notation;formal specification;skill levels;computer scientists;software engineers;students;discrete mathematics;specification notation;formal methods","","33","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Using CSP to detect errors in the TMN protocol","G. Lowe; B. Roscoe","Dept. of Math. & Comput. Sci., Leicester Univ., UK; NA","IEEE Transactions on Software Engineering","","1997","23","10","659","669","We use FDR (Failures Divergence Refinement), a model checker for CSP, to detect errors in the TMN protocol (M. Tatebayashi et al., 1990). We model the protocol and a very general intruder as CSP processes, and use the model checker to test whether the intruder can successfully attack the protocol. We consider three variants on the protocol, and discover a total of 10 different attacks leading to breaches of security.","0098-5589;1939-3520;2326-3881","","10.1109/32.637148","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637148","","Cryptography;Cryptographic protocols;State-space methods;Testing;Communication system security;Mobile communication;Algebra;Authentication;Redundancy","protocols;formal verification;program verification;cryptography;communicating sequential processes","security breaches;TMN protocol;error detection;FDR;Failures Divergence Refinement;model checker;general intruder;CSP processes;communicating sequential processes","","67","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Enhancing structured review with model-based verification","I. Traore; D. B. Aredo","Dept. of Electr. & Comput. Eng., Victoria Univ., BC, Canada; NA","IEEE Transactions on Software Engineering","","2004","30","11","736","753","We propose a development framework that extends the scope of structured review by supplementing the structured review with model-based verification. The proposed approach uses the Unified Modeling Language (UML) as a modeling notation. We discuss a set of correctness arguments that can be used in conjunction with formal verification and validation (V&V) in order to improve the quality and dependability of systems in a cost-effective way. Formal methods can be esoteric; consequently, their large scale application is hindered. We propose a framework based on the integration of lightweight formal methods and structured reviews. Moreover, we show that structured reviews enable us to handle aspects of V&V that cannot be fully automated. To demonstrate the feasibility of our approach, we have conducted a study on a security-critical system - a patient document service (PDS) system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359768","Index Terms- Structured review;formal methods;UML;prototype verification system (PVS);OCL;model-based verification;validation and verification.","Quality assurance;Unified modeling language;Software quality;Formal verification;Large-scale systems;Costs;Guidelines;Software prototyping;Prototypes","formal specification;formal verification;Unified Modeling Language;program testing;safety-critical software","structured review;model-based verification;Unified Modeling Language;formal verification;formal validation;security-critical system;patient document service system;prototype verification system;object constraint language","","11","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient expressions for completely and partly unsuccessful batched search of tree-structured files","S. D. Lang; Y. Manolopoulos","Dept. of Comput. Sci., Central Florida Univ., Orlando, FL, USA; NA","IEEE Transactions on Software Engineering","","1990","16","12","1433","1435","Closed-form, nonrecurrent expressions for the cost of completely and partly unsuccessful batched searching are developed for complete j-ary tree files. These expressions are applied to both the replacement and nonreplacement models of the search queries. The expressions provide more efficient formulas than previously reported for calculating the cost of batched searching. The expressions can also be used to estimate the number of block accesses for hierarchical file structures.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.62451","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=62451","","Costs;Database systems;Tiles;Buffer storage;Query processing;Relational databases;Closed-form solution;Computer science;Probability","batch processing (computers);data structures;database management systems;information retrieval systems;search problems;trees (mathematics)","nonrecurrent expressions;partly unsuccessful batched searching;complete j-ary tree files;nonreplacement models;search queries;block accesses;hierarchical file structures","","1","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Factors that impact implementing a system development methodology","T. L. Roberts; M. L. Gibson; K. T. Fields; R. K. Rainer","Dept. of Manage., Central Florida Univ., Orlando, FL, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","8","640","649","Presents the findings of empirical research from 61 companies, mostly from the USA, to identify the factors that may impact implementation of a system development methodology (SDM). The study uses a survey instrument to identify the SDM implementation factors. The survey focused on the perspective of the primary constituents: functional managers, information systems managers, system personnel and external consultants. The study uses an exploratory factor analysis that identifies five factors important to implementing an SDM: organizational SDM transition, functional management involvement/support, SDM transition, the use of models and external support. The research findings have important implications for further research and the practice of system development. For researchers, it points to important measures in the implementation and use of SDMs that may be further verified and extended in subsequent research. For practitioners, it provides a general guide to the important aspects to consider in the implementation and use of an SDM.","0098-5589;1939-3520;2326-3881","","10.1109/32.707699","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=707699","","Computer aided software engineering;Information management;Management information systems;Project management;Software development management;Programming;Information systems;Error correction;Instruments;Personnel","software engineering;statistical analysis","system development methodology implementation factors;USA;survey instrument;functional managers;information systems managers;system personnel;external consultants;exploratory factor analysis;organizational transition;functional management involvement;functional management support;models;external support;project management;system life-cycle","","14","","42","","","","","","IEEE","IEEE Journals & Magazines"
"A simplification of a conversation design scheme using Petri nets","J. Wu; E. B. Fernandez","Dept. of Electr. & Comput. Eng., Florida Atlantic Univ., Boca Raton, FL, USA; Dept. of Electr. & Comput. Eng., Florida Atlantic Univ., Boca Raton, FL, USA","IEEE Transactions on Software Engineering","","1989","15","5","658","660","In the conversation design procedure, the definition of the state of the system is one of the most important aspects. The question is how to identify transitions in Occam programs in order to express them as Petri nets. In the paper, a simplified transition identification method is proposed. Using the robot arm control program of A.M. Tyrrell and D.J. Holding it is shown that the correspondent Petri net graph is simpler than theirs, but the communication state change table is the same. It is also shown that these two methods are equivalent.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24717","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24717","","Petri nets;Fault tolerance;Robot control;Reactive power;Buildings;Software testing","fault tolerant computing;Petri nets;software engineering;software reliability","system state;fault tolerant software;conversation design;Petri nets;transitions;Occam programs;simplified transition identification method;robot arm control program;Tyrrell;Holding;Petri net graph;communication state change table","","2","","5","","","","","","IEEE","IEEE Journals & Magazines"
"An Experimental Study of Software Metrics for Real-Time Software","H. A. Jensen; K. Vairavan","Johnson Controls, Inc.; NA","IEEE Transactions on Software Engineering","","1985","SE-11","2","231","234","The rising costs of software development and maintenance have naturally aroused intere5t in tools and measures to quantify and analyze software complexity. Many software metrics have been studied widely because of the potential usefulness in predicting the complexity and quality of software. Most of the work reported in this area has been related to nonreal-time software. In this paper we report and discuss the results of an experimental investigation of some important metrics and their relationship for a class of 202 Pascal programs used in a real-time distributed processing environment. While some of our observations confirm independent studies, we have noted significant differences. For instance the correlations between McCabe's control complexity measure and Halstead's metrics are low in comparison to a previous study. Studies of the type reported here are important for understanding the relationship between software metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232199","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701992","Program length;real-time software;software complexity;software metrics","Software metrics;Software maintenance;Software tools;Costs;Software quality;Error correction;Operating systems;Programming;Software measurement;Distributed processing","","Program length;real-time software;software complexity;software metrics","","19","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Monitoring Data Usage in Distributed Systems","D. Basin; M. Harvan; F. Klaedtke; E. Zalinescu","ETH Zurich, Zurich; ETH Zurich, Zurich; ETH Zurich, Zurich; ETH Zurich, Zurich","IEEE Transactions on Software Engineering","","2013","39","10","1403","1426","IT systems manage increasing amounts of sensitive data and there is a growing concern that they comply with policies that regulate data usage. In this paper, we use temporal logic to express policies and runtime monitoring to check system compliance. While well-established methods for monitoring linearly ordered system behavior exist, a major challenge is monitoring distributed and concurrent systems where actions are locally observed in the different system parts. These observations can only be partially ordered, while policy compliance may depend on the actions' actual order of appearance. Technically speaking, it is in general intractable to check compliance of partially ordered traces. We identify fragments of our policy specification language for which compliance can be checked efficiently, namely, by monitoring a single representative trace in which the observed actions are totally ordered. Through a case study we show that the fragments are capable of expressing nontrivial policies and that monitoring representative traces is feasible on real-world data.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6493331","Monitors;temporal logic;verification;distributed systems;regulation","Monitoring;Cost accounting;Periodic structures;Semantics;Distributed databases;Standards;Finite element analysis","concurrency control;formal verification;specification languages;temporal logic","data usage monitoring;distributed systems;IT systems;information technology systems;data usage regulation;temporal logic;runtime monitoring;system compliance;concurrent systems;policy compliance;policy specification language","","8","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Performance Specification and Evaluation with Unified Stochastic Probes and Fluid Analysis","R. A. Hayden; J. T. Bradley; A. Clark","Imperial College London, London; Imperial College London, London; University of Edinburgh, Edinburgh","IEEE Transactions on Software Engineering","","2013","39","1","97","118","Rapid and accessible performance evaluation of complex software systems requires two critical features: the ability to specify useful performance metrics easily and the capability to analyze massively distributed architectures, without recourse to large compute clusters. We present the unified stochastic probe, a performance specification mechanism for process algebra models that combines many existing ideas: state and action-based activation, location-based specification, many-probe specification, and immediate signaling. These features, between them, allow the precise and compositional construction of complex performance measurements. The paper shows how a subset of the stochastic probe language can be used to specify common response-time measures in massive process algebra models. The second contribution of the paper is to show how these response-time measures can be analyzed using so-called fluid techniques to produce rapid results. In doing this, we extend the fluid approach to incorporate immediate activities and a new type of response-time measure. Finally, we calculate various response-time measurements on a complex distributed wireless network of O(10<sup>129</sup>) states in size.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6133297","Performance modeling;performance evaluation tools;stochastic process algebra;measurement probes;fluid approximation;passage-time analysis","Probes;Stochastic processes;Analytical models;Algebra;Computational modeling;Semantics;Syntactics","formal specification;process algebra;software metrics;software performance evaluation","performance specification mechanism;performance evaluation mechanism;unified stochastic probes;fluid analysis;software system;performance metrics;process algebra model;state-based activation;action-based activation;location-based specification;many-probe specification;immediate signaling;stochastic probe language;common response-time measure;complex distributed wireless network","","9","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on Program Slicing","H. K. N. Leung; H. K. Reghbati","Department of Computer Science, University of Alberta; NA","IEEE Transactions on Software Engineering","","1987","SE-13","12","1370","1371","This correspondence points out some of the problems with Weiser's algorithm [5] for computing program slices. Corrections are made to Weiser's algorithm. It is shown how Weiser's algorithm can be amended to handle loops. Advantages of the Bergeretti and Carre's approach [1] are discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233147","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702186","Data flow analysis;debugging;information flow;slicing","","","Data flow analysis;debugging;information flow;slicing","","8","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Debugging concurrent Ada programs by deterministic execution","K. -. Tai; R. H. Carver; E. E. Obaid","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA; NA","IEEE Transactions on Software Engineering","","1991","17","1","45","63","A language-based approach to deterministic execution debugging of concurrent Ada programs is presented. The approach is to define synchronization (SYN)-sequences of a concurrent Ada program in terms of Ada language constructs and to replay such SYN-sequences without the need for system-dependent debugging tools. It is shown how to define a SYN-sequence of a concurrent Ada program in order to provide sufficient information for deterministic execution. It is also shown how to transform a concurrent Ada program P so that the SYN-sequences of previous executions of P can be replayed. This transformation adds an Ada task to P that controls program execution by synchronizing with the original tasks in P. A brief description is given of the implementation of tools supporting deterministic execution debugging of concurrent Ada programs.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67578","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67578","","Computer science;Testing;Software debugging;Software engineering;Mathematics","Ada;parallel programming;program debugging","synchronisation sequences;concurrent Ada programs;deterministic execution debugging;Ada language constructs;SYN-sequences;sufficient information;previous executions;transformation;program execution;tools","","91","","32","","","","","","IEEE","IEEE Journals & Magazines"
"A multilayer client-server queueing network model with synchronous and asynchronous messages","Sridhar Ramesh; H. G. Perros","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA","IEEE Transactions on Software Engineering","","2000","26","11","1086","1100","We analyze a multilayered queueing network that models a client-server system where clients and servers communicate via synchronous and asynchronous messages. The servers are organized in groups such that they form a multilayered hierarchical structure. The queueing network is approximately analyzed using a decomposition algorithm. Numerical tests show that the approximation algorithm has a good accuracy.","0098-5589;1939-3520;2326-3881","","10.1109/32.881719","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=881719","","Nonhomogeneous media;Queueing analysis;Client-server systems;Network servers;Software performance;Computer architecture;Software systems;Algorithm design and analysis;Testing;Approximation algorithms","client-server systems;message passing;queueing theory;software performance evaluation;multi-threading","multilayer client-server queueing network model;synchronous messages;asynchronous messages;multilayered queueing network;client-server system;multilayered hierarchical structure;decomposition algorithm;approximation algorithm","","18","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Performance Model Estimation and Tracking Using Optimal Filters","T. Zheng; C. M. Woodside; M. Litoiu","Carleton University, Ottawa; Carleton University, Ottawa; IBM Centre for Advanced Studeis, Toronto","IEEE Transactions on Software Engineering","","2008","34","3","391","406","To update a performance model, its parameter values must be updated, and in some applications (such as autonomic systems) tracked continuously over time. Direct measurement of many parameters during system operation requires instrumentation which is impractical. Kalman filter estimators can track such parameters using other data such as response times and utilizations, which are readily observable. This paper adapts Kalman filter estimators for performance model parameters, evaluates the approximations which must be made, and develops a systematic approach to setting up an estimator. The estimator converges under easily verified conditions. Different queueing-based models are considered here, and the extension for state-based models (such as stochastic Petri nets) is straightforward.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4515874","Modeling techniques;Measurement;Performance model;Kalman filtering;Parameter tracking;Modeling techniques;Measurement;Performance model;Kalman filtering;Parameter tracking","Filters;Parameter estimation;Delay;State estimation;Instruments;Stochastic processes;Time varying systems;Recursive estimation;Predictive models;Petri nets","Kalman filters;parameter estimation;software performance evaluation","optimal filters;performance model estimation;performance model tracking;Kalman filter estimators;queuing-based models","","56","","34","","","","","","IEEE","IEEE Journals & Magazines"
"The mean resequencing delay for M/H/sub K// infinity systems","S. Chowdhury","Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA","IEEE Transactions on Software Engineering","","1989","15","12","1633","1638","The relationship between the mean resequencing delay and variations in packet transmission times is studied. Assuming a Poisson stream of packets, a K-stage hyperexponential transmission time distribution and an infinite number of equal capacity links connecting the source and destination nodes, the authors derive an expression for the mean resequencing delay. This result provides an upper bound on the mean resequencing delay for nodes connected by finitely many links. They observe that for the two-stage and three-stage hyperexponential distribution, the mean resequencing delay varies almost perfectly linearly with the squared coefficient of variation. An asymptotic bound analysis can explain this behavior.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58774","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58774","","H infinity control;Circuits;Computer networks;Joining processes;Delay effects;Queueing analysis;Upper bound;Computer architecture;Computer science;Virtual colonoscopy","computer networks;electronic messaging;packet switching;queueing theory","mean resequencing delay;packet transmission times;Poisson stream;K-stage hyperexponential transmission time distribution;equal capacity links;destination nodes;upper bound;asymptotic bound analysis","","8","","7","","","","","","IEEE","IEEE Journals & Magazines"
"IASTED conference on reliability and quality control","B. Dhillon","Department of Mechanical Engineering, University of Ottawa, Ottawa, Ont. K1N 6N5, Canada","IEEE Transactions on Software Engineering","","1986","SE-12","9","996","996","THE International Association of Science and Technology for Development (IASTED) Conference on Reliability and Quality Control is to be held at the Palais des Congres in Paris, France.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313055","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313055","","Quality control;Analytical models;Predictive models;Software;Power system reliability;Software reliability","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"CSPL: an Ada95-like, Unix-based process environment","Jen-Yen Jason Chen","Dept. of Comput. Sci. & Inf. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan","IEEE Transactions on Software Engineering","","1997","23","3","171","184","The paper presents a new process-centered environment called ""concurrent software process language"" (CSPL). CSPL takes a unique and innovative approach to integrate the object-oriented Ada95-like syntax (for its modeling power) with Unix shell semantics (for its enactment capability) in a software process language. The paper depicts the following new CSPL features: (1) object orientation, (2) multirole and multiuser, and (3) unified object modeling. Language constructs specially designed for software process such as work assignment statement, communication-related statements, role unit, tool unit, relation unit and so on, are, respectively, described. The related work of this diversified field is also surveyed in some depth. The CSPL environment prototype has been built. A CSPL process program for the IEEE Software Process Modeling Example Problem has been developed and enacted to demonstrate the capabilities of this environment.","0098-5589;1939-3520;2326-3881","","10.1109/32.585504","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585504","","Object oriented modeling;Computer languages;Software design;Process design;Software prototyping;Prototypes;Software engineering;Software maintenance;Software tools;Computer industry","Unix;object-oriented languages;programming environments;parallel languages;computational linguistics;operating systems (computers)","Ada95-like Unix-based process environment;CSPL;concurrent software process language;object-oriented Ada95-like syntax;Unix shell semantics;modeling;enactment;object orientation;multirole feature;multiuser feature;unified object modeling;language constructs;work assignment statement;communication-related statements;role unit;tool unit;relation unit;CSPL environment prototype;CSPL process program;IEEE Software Process Modeling Example Problem","","20","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Requirements elicitation and validation with real world scenes","P. Haumer; K. Pohl; K. Weidenhaupt","Tech. Hochschule Aachen, Germany; NA; NA","IEEE Transactions on Software Engineering","","1998","24","12","1036","1054","A requirements specification defines the requirements for the future system at a conceptual level (i.e., class or type level). In contrast, a scenario represents a concrete example of current or future system usage. In early RE phases, scenarios are used to support the definition of high level requirements (goals) to be achieved by the new system. In many cases, those goals can to a large degree be elicited by observing, documenting and analyzing scenarios about current system usage. To support the elicitation and validation of the goals achieved by the existing system and to illustrate problems of the old system, we propose to capture current system usage using rich media (e.g., video, speech, pictures, etc.) and to interrelate those observations with the goal definitions. Thus, we aim at making the abstraction process which leads to the definition of the conceptual models more transparent and traceable. We relate the parts of the observations which have caused the definition of a goal or against which a goal was validated with the corresponding goal. These interrelations provide the basis for: 1) explaining and illustrating a goal model to, e.g., untrained stakeholders and/or new team members; 2) detecting, analyzing, and resolving a different interpretation of the observations; 3) comparing different observations using computed goal annotations; and 4) refining or detailing a goal model during later process phases. Using the PRIME implementation framework, we have implemented the PRIME-CREWS environment, which supports the interrelation of conceptual models and captured system usage observations. We report on our experiences with PRIME-CREWS gained in an experimental case study.","0098-5589;1939-3520;2326-3881","","10.1109/32.738338","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=738338","","Layout;Object oriented modeling;Context modeling;History;Erbium;Unified modeling language;Business process re-engineering;System testing;Knowledge engineering;Concrete","formal specification;program verification;systems analysis;program diagnostics","requirements elicitation;requirements validation;real world scenes;requirements specification;future system;conceptual level;type level;future system usage;early RE phases;high level requirements;new system;current system usage;old system;goal definitions;abstraction process;conceptual models;untrained stakeholders;new team members;computed goal annotations;goal model;PRIME implementation framework;PRIME-CREWS environment;captured system usage observations;experimental case study","","66","","48","","","","","","IEEE","IEEE Journals & Magazines"
"New NP-Complete Problems in Performance Evaluation of Concurrent Systems Using Petri Nets","J. Magott","Institute of Engineering Cybernetics, Technical University of Wroclaw","IEEE Transactions on Software Engineering","","1987","SE-13","5","578","581","Timed Petri nets are useful in performance evaluation of concurrent systems. The maximum computation rate is achieved for minimal cycle time of timed Petri net. It is known that minimal cycle time problem for P-invariant Petri nets is NP-complete. In this paper we prove that the minimal cycle time problem, for non-P-invariant Petri nets and for a small subclass of P-invariant Petri nets called free-choice nets having live and safe marking, is NP-complete.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233462","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702257","Computational complexity;free-choice net;minimal cycle time;non-P-invariant Petri net;performance evaluation;timed Petri net","Petri nets;Time factors;Random variables;Time measurement;Data analysis;Data flow computing;Cybernetics;Fires","","Computational complexity;free-choice net;minimal cycle time;non-P-invariant Petri net;performance evaluation;timed Petri net","","9","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Integration and analysis of use cases using modular Petri nets in requirements engineering","Woo Jin Lee; Sung Deok Cha; Yong Rae Kwon","Dept. of Comput. Sci., Korea Adv. Inst. of Sci. & Technol., Taejon, South Korea; NA; NA","IEEE Transactions on Software Engineering","","1998","24","12","1115","1130","It is well known that requirements engineering plays a critical role in software quality. The use case approach is a requirements elicitation technique commonly used in industrial applications. Software requirements are stated as a collection of use cases, each of which is written in the user's perspective and describes a specific flow of events in the system. The use case approach offers several practical advantages in that use case requirements are relatively easy to describe, understand, and trace. Unfortunately, there are a couple of major drawbacks. Since use cases are often stated in natural languages, they lack formal syntax and semantics. Furthermore, it is difficult to analyze their global system behavior for completeness and consistency, partly because use cases describe only partial behaviors and because interactions among them are rarely represented explicitly. We propose the Constraints-based Modular Petri Nets (CMPNs) approach as an effective way to formalize the informal aspects of use cases. CMPNs, an extension of Place/Transition nets, allow the formal and incremental specification of requirements. The major contributions of the paper, in addition to the formal definitions of CMPNs, are the development of: 1) a systematic procedure to convert use cases stated in natural language to a CMPN model; and 2) a set of guidelines to find inconsistency and incompleteness in CMPNs. We demonstrate an application of our approach using use cases developed for telecommunications services.","0098-5589;1939-3520;2326-3881","","10.1109/32.738342","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=738342","","Computer aided software engineering;Petri nets;Natural languages;Computer Society;Software quality;Application software;Guidelines;Telecommunication services;Software systems","formal specification;systems analysis;software quality;Petri nets;telecommunication computing","requirements elicitation technique;modular Petri nets;requirements engineering;software quality;use case approach;software requirements;use case requirements;natural languages;global system behavior;completeness;consistency;partial behaviors;Constraints-based Modular Petri Nets;CMPNs;informal aspects;Place/Transition nets;incremental specification;formal definitions;systematic procedure;telecommunications services;formal syntax;formal specification","","38","","31","","","","","","IEEE","IEEE Journals & Magazines"
"On formalization of the whole-part relationship in the Unified Modeling Language","Hee Beng Kuan Tan; Lun Hao; Yong Yang","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","IEEE Transactions on Software Engineering","","2003","29","11","1054","1055","F. Barbier et al. (2003) introduced a formal definition for the semantics of the whole-part relationship in the Unified Modeling Language (UML). This paper reports some discrepancies that appeared previously and proposes solutions to these discrepancies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245307","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245307","","Unified modeling language;Mathematics;Qualifications","specification languages","whole-part relationship;Unified Modeling Language","","3","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Minimizing aperiodic response times in a firm real-time environment","G. C. Buttazzo; M. Caccamo","Scuola Superiore S. Anna, Pisa, Italy; NA","IEEE Transactions on Software Engineering","","1999","25","1","22","32","In certain real-time applications, ranging from multimedia to telecommunication systems, timing constraints can be more flexible than scheduling theory usually permits. In this paper, we deal with the problem of scheduling hybrid sets of tasks, consisting of firm periodic tasks (i.e. tasks with deadlines which can occasionally skip one instance) and soft aperiodic requests, which have to be served as soon as possible to achieve good responsiveness. We propose and analyze an algorithm, based on a variant of earliest-deadline-first scheduling, which exploits skips to minimize the response time of aperiodic requests. One of the most interesting features of our algorithm is that it can easily be tuned to balance performance vs. complexity, for adapting it to different application requirements. Extensive simulation experiments show the effectiveness of the proposed approach with respect to existing methods. Schedulability bounds are also derived to perform off-line analysis.","0098-5589;1939-3520;2326-3881","","10.1109/32.748916","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=748916","","Time factors;Scheduling algorithm;Real time systems;Timing;Communication system control;Multimedia systems;Delay;Algorithm design and analysis;Processor scheduling;Quality of service","scheduling;real-time systems;quality of service;timing;minimisation;communication complexity;software performance evaluation","aperiodic response time minimization;hard real-time environment;multimedia;telecommunication systems;flexible timing constraints;hybrid task set scheduling;periodic tasks;task deadlines;soft aperiodic requests;responsiveness;earliest-deadline-first scheduling;skips;performance-complexity balance;application requirements;simulation;schedulability bounds;off-line analysis;service quality","","15","","19","","","","","","IEEE","IEEE Journals & Magazines"
"A unified framework for coupling measurement in object-oriented systems","L. C. Briand; J. W. Daly; J. K. Wust","Fraunhofer Inst. for Exp. Software Eng., Kaiserslautern, Germany; NA; NA","IEEE Transactions on Software Engineering","","1999","25","1","91","121","The increasing importance being placed on software measurement has led to an increased amount of research developing new software measures. Given the importance of object-oriented development techniques, one specific area where this has occurred is coupling measurement in object-oriented systems. However, despite a very interesting and rich body of work, there is little understanding of the motivation and empirical hypotheses behind many of these new measures. It is often difficult to determine how such measures relate to one another and for which application they can be used. As a consequence, it is very difficult for practitioners and researchers to obtain a clear picture of the state of the art in order to select or define measures for object-oriented systems. This situation is addressed and clarified through several different activities. First, a standardized terminology and formalism for expressing measures is provided which ensures that all measures using it are expressed in a fully consistent and operational manner. Second, to provide a structured synthesis, a review of the existing frameworks and measures for coupling measurement in object-oriented systems takes place. Third, a unified framework, based on the issues discovered in the review, is provided and all existing measures are then classified according to this framework. This paper contributes to an increased understanding of the state-of-the-art.","0098-5589;1939-3520;2326-3881","","10.1109/32.748920","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=748920","","Software measurement;Terminology;Software quality;Measurement standards;Area measurement;Decision making;Object oriented modeling;Force measurement;Programming","software metrics;object-oriented programming;software quality","software measurement;object-oriented systems;object-oriented development techniques;coupling measurement;standardized terminology;standardized formalism;structured synthesis;unified framework","","353","","37","","","","","","IEEE","IEEE Journals & Magazines"
"An automatic class generation mechanism by using method integration","K. Maruyama; K. I. Shima","Media Technol. Dev. Center, NTT Commun. Corp., Tokyo, Japan; NA","IEEE Transactions on Software Engineering","","2000","26","5","425","440","The paper presents a mechanism for automatically generating new classes from classes existing in a library by using their modification histories. To generate classes that are likely to meet a programmer's requirements and that are consistent with the existing classes, we propose three actors: a Specifier, a Finder, and an integrator. The Specifier records the history of modifications between methods with the same interface of a parent class and its heir. If the required method is not defined in the existing class which a programmer is referring to, the Finder retrieves classes similar to the referenced class and the Integrator applies the past modifications of similar classes to the referenced class. Classes are determined to be similar, based on their positions in a class hierarchy tree. Both the Specifier and Integrator are achieved by using a method integration algorithm based on object oriented bounded program slicing and class dependence graph matching. This mechanism enables programmers to reuse classes with little or no modification, and thus, easily create object oriented programs.","0098-5589;1939-3520;2326-3881","","10.1109/32.846300","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846300","","Programming profession;History;Mobile communication;Computer Society;Tree graphs;Costs;Object oriented programming;Software libraries;Laboratories","object-oriented programming;software libraries;program slicing;automatic programming;graph theory","automatic class generation mechanism;method integration;library classes;modification histories;Specifier;Finder;integrator;parent class;past modifications;referenced class;class hierarchy tree;method integration algorithm;object oriented bounded program slicing;class dependence graph matching;class reuse;object oriented programs","","1","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""An Interval Logic for Real-Time System Specification""","C. A. Furia; A. Morzenti; M. Pradella; M. G. Rossi","Dipt. di Elettronica e Informazione, Politecnico di Milano, Milan, Italy; Dipt. di Elettronica e Informazione, Politecnico di Milano, Milan, Italy; NA; NA","IEEE Transactions on Software Engineering","","2006","32","6","424","427","The paper ""An Interval Logic for Real-Time System Specification"" (Mattolini and Nesi, IEEE Trans. Software Eng., vol. 27, no. 3, pp. 208-227, Mar. 2001) presents the TILCO specification language and compares it to other existing similar languages. In this comment, we show that several of the logic formulas used for the comparison are flawed and/or overly complicated and we explain why, in this respect, the comparison is moot","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650216","Formal methods;temporal logic;real-time systems.","Logic;Real time systems;Resource management;Specification languages","formal specification;real-time systems;specification languages;temporal logic","interval logic;real-time system specification;TILCO specification language;logic formulas","","","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Comments, with reply, on ""On criteria for module interfaces"" by D. Hoffman","L. L. Constantine","Acton, MA, USA","IEEE Transactions on Software Engineering","","1990","16","12","1440","","The commenter acknowledges that the practical criteria provided in the above-titled paper (see ibid., vol.16, no.5, p.537-42, 1990), offer substantive guidelines for designing module interfaces. He points out that the results obtained can be further improved and certain remaining conflicts resolved through consideration of established principles of structured design and software engineering. He illustrates his point with an example involving the specification of a stack interface that requires two or three separate references to replace one. He presents two designs and argues that the first is better. The author refutes the commenter's arguments and argues that the second design is better.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.62453","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=62453","","Inspection;Computer science","data structures;structured programming;user interfaces","module interfaces;practical criteria;structured design;software engineering;stack interface","","","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Performance analysis of stochastic timed Petri nets using linear programming approach","Zhen Liu","Inst. Nat. de Recherche en Inf. et Autom., Sophia-Antipolis, France","IEEE Transactions on Software Engineering","","1998","24","11","1014","1030","Stochastic timed Petri nets are a useful tool in the performance analysis of concurrent systems such as parallel computers, communication networks and flexible manufacturing systems. In general, performance measures of stochastic timed Petri nets are difficult to obtain for practical problems due to their sizes. In this paper, we provide a method to efficiently compute upper and lower bounds for the throughputs and mean token numbers for a large class of stochastic timed Petri nets. Our approach is based on uniformization technique and linear programming.","0098-5589;1939-3520;2326-3881","","10.1109/32.730548","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=730548","","Performance analysis;Stochastic processes;Petri nets;Stochastic systems;Computer aided manufacturing;Computer networks;Concurrent computing;Communication networks;Flexible manufacturing systems;Size measurement","Petri nets;stochastic systems;linear programming;performance evaluation;telecommunication networks;flexible manufacturing systems;concurrency theory","performance analysis;stochastic timed Petri nets;linear programming;concurrent systems;parallel computers;communication networks;flexible manufacturing systems;performance measures;upper bounds;lower bounds;throughput;mean token number;uniformization technique;performance bounds","","15","","54","","","","","","IEEE","IEEE Journals & Magazines"
"A methodology for feature interaction detection in the AIN 0.1 framework","F. J. Lin; Hong Liu; A. Ghosh","Appl. Res., Bellcore, Morristown, NJ, USA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","10","797","817","We propose an integrated methodology for specifying AIN (advanced intelligent networks) and switch based features and analyzing their interactions in the AIN 0.1 framework. The specification of each individual feature is tied to the AIN call model and requires only a minimum amount of information in terms of control and data for interaction analysis. Once a feature is specified, its specification is then validated for consistency with respect to control and data. Interaction analysis is conducted for a set of features based on the sharing of call variables between the SSP and the SCP. With this approach, one can detect the following interactions involving AIN features: (1) side effects, where a call variable modified by one feature is used by another feature and (2) disabling, where one feature disconnects a call, preventing another feature from execution. We also develop a theory that is based on the computation of sequences of messages exchanged between the SSP and the SCP and their call variable usage. This theory is shown to dramatically reduce the number of cases considered during the analysis. A brief overview of a tool that makes use of this methodology to aid in the task of feature interaction detection is also given.","0098-5589;1939-3520;2326-3881","","10.1109/32.729681","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=729681","","Computer vision;Logic;Telecommunication switching;Information analysis;Data analysis;Switches;Telecommunication services;Telephony;Software engineering;Packaging","intelligent networks;telecommunication computing;knowledge based systems;telecommunication switching;software engineering","feature interaction detection;AIN framework;integrated methodology;advanced intelligent networks;switch based features;AIN call model;interaction analysis;specification;call variables;SSP;SCP;AIN features;side effects;disabling;message sequences;feature specification;telecommunication services","","2","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Foreword","H. K. T. Wong","NA","IEEE Transactions on Software Engineering","","1985","SE-11","10","1038","1039","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231850","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701918","","Data compression;Database languages;Costs;Statistical analysis;Computational modeling;Artificial intelligence;Multidimensional systems;Sampling methods;User interfaces;Data security","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Clarifications on the Construction and Use of the ManyBugs Benchmark","C. Le Goues; Y. Brun; S. Forrest; W. Weimer","School of Computer Science, Carnegie Mellon University, Pittsburgh, PA; College of Information and Computer Science, University of Massachusetts at Amherst, Amherst, MA; Department of Computer Science, University of New Mexico, Albuquerque, NM; Computer Science and Engineering, University of Michigan, Ann Arbor, MI","IEEE Transactions on Software Engineering","","2017","43","11","1089","1090","Automated repair techniques produce variant php interpreters, which should naturally serve as the tested interpreters. However, the answer to the question of what should serve as the testing interpreter is less obvious. php's default test harness configuration uses the same version of the interpreter for both the tested and testing interpreter. However, php may be configured via a command-line argument to use a different interpreter, such as the unmodified defective version, or a separate, manually-repaired version.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2755651","US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8048536","","Maintenance engineering;Benchmark testing;Computer science;Electronic mail;Software engineering;Software","program debugging","ManyBugs benchmark;automated repair techniques;test harness configuration;testing interpreter","","","","7","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Allocating programs containing branches and loops within a multiple processor system","D. Towsley","Department of Computer and Information Science, University of Massachusetts, Amherst, MA 01003","IEEE Transactions on Software Engineering","","1986","SE-12","10","1018","1024","The problem of assigning the modules of distributed program to the processors of a distributed system is addressed. The goal of such an assignment is to minimize the total execution and communication costs. A computational model of a distributed program containing probabilistic branches and loops is described by a directed graph whose edges represent precedence relations between modules. Efficient algorithms based on short-path methods are presented to determine the optimum assignment on a distributed system containing <i>N</i> heterogeneous processors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313018","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313018","Computer networks;distributed processing;multiprocessor system scheduling;shortest path algorithms","Computational modeling;Resource management;Algorithm design and analysis;Computational efficiency;Computational complexity;Context","directed graphs;modules;multiprocessing systems","programs allocation;branches;loops;multiple processor system;modules;distributed program;computational model;probabilistic branches;directed graph;short-path methods;optimum assignment","","25","","","","","","","","IEEE","IEEE Journals & Magazines"
"Perceptions, Expectations, and Challenges in Defect Prediction","Z. Wan; X. Xia; A. E. Hassan; D. Lo; J. Yin; X. Yang","College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang China (e-mail: wanzhiyuan@zju.edu.cn); Faculty of Information Technology, Monash University, Melbourne, Victoria Australia (e-mail: xxkidd@zju.edu.cn); School of Computing, Queen's University, Kingston, Ontario Canada K7L 3N6 (e-mail: ahmed@cs.queensu.ca); School of Information Systems, Singapore Management University, Singapore, Singapore Singapore 17890 (e-mail: davidlo@smu.edu.sg); Computer science and technology, Zhejiang University, Hangzhou, Zhejiang Province China 310027 (e-mail: zjuyjw@cs.zju.edu.cn); College of Computer Science, Zhejiang University, Hangzhou, Zhejiang China (e-mail: yangxh@zju.edu.cn)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Defect prediction has been an active research area for over four decades. Despite numerous studies on defect prediction, the potential value of defect prediction in practice remains unclear. To address this issue, we performed a mixed qualitative and quantitative study to investigate what practitioners think, behave and expect in contrast to research findings when it comes to defect prediction. We collected hypotheses from open-ended interviews and a literature review, followed by a validation survey. We received 395 responses from practitioners. Some of our key findings include: 1) Over 90% of respondents are willing to adopt defect prediction techniques. 2) There exists a disconnect between practitioners' perceptions and well supported research evidence regarding defect density distribution and the relationship between file size and defectiveness. 3) 7.2% of the respondents reveal an inconsistency between their behavior and perception regarding defect prediction. 4) Defect prediction at the feature level is the most preferred level of granularity by practitioners. 5) During bug fixing, more than 40% of the respondents acknowledged that they would make a ""work-around"" fix rather than correct the actual error-causing code. Based on our findings, we highlight future research directions and provide recommendations for practitioners.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2877678","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8502824","Defect Prediction;Empirical Study;Practitioner;Survey","Interviews;Tools;Software;Bibliographies;Computer bugs;Companies;Continents","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Addendum to ""Locating features in source code""","D. Bojic; T. Eisenbarth; R. Koschke; D. Simon; D. Velasevic","Fac. of Electr. Eng., Belgrade Univ., Serbia; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","2","140","","For original paper by T. Eisenbarth et al. see ibid., vol.29, no.3, p.210-24 (2003). We compare three approaches that apply formal concept analysis on execution profiles. This survey extends the discourse of related research by Bojic and Velasevic (2000).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265818","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265818","","Testing;Software architecture;Lattices;Software engineering;Information analysis;Unified modeling language;Software packages;Packaging","program diagnostics;software architecture;formal specification","formal concept analysis;feature location;source coding;program analysis;software architecture recovery","","1","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Achieving high availability in distributed databases","H. Garcia-Molina; B. Kogan","Dept. of Comput. Sci., Princeton Univ., NJ, USA; Dept. of Comput. Sci., Princeton Univ., NJ, USA","IEEE Transactions on Software Engineering","","1988","14","7","886","896","An approach is presented for managing distributed database systems in the face of communication failures and network partitions. The approach is based on the idea of dividing the database into fragments and assigning each fragment a controlling entity called an agent. The goals achieved by this approach include high data availability and the ability to operate without promptly and correctly detecting partitions. A correctness criterion for transaction execution, called fragmentwise serializability, is introduced. It is less strict than the conventional serializability, but provides a valuable alternative for some applications.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42732","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42732","","Availability;Distributed databases;Database systems;Concurrency control;Protocols;Intelligent networks;Face detection;Transaction databases;Communication system control;Fault tolerant systems","database theory;distributed databases;fault tolerant computing;program verification;software reliability","fault tolerant computing;distributed databases;communication failures;network partitions;controlling entity;agent;data availability;correctness criterion;transaction execution;fragmentwise serializability","","12","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Toward the Next Generation of Data Modeling Tools","C. R. Carlson; A. K. Arora","Department of Computer Science, Illinois Institute of Technology, IIT Center; NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","966","970","This paper describes the Update Protocol Model (UPM), a formal language for the expression of database update semantics. UPM has been used primarily to capture and communicate in a precise and uniform notation the plethora of database semantics described by a variety of ""fourth generation"" models, many of which are imprecise when it comes to update semantics. Several computing trends–knowledge-based expert systems, distributed database management systems, and new applications based on higher order semantic models–point to the need for modeling techniques beyond that which current data models such as the relational and entity-relationship models provide.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232831","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702116","Database;data modeling;relational model;update protocol model;update semantics","Protocols;Relational databases;Data models;Distributed databases;Formal languages;Distributed computing;Expert systems;Database systems;Information management","","Database;data modeling;relational model;update protocol model;update semantics","","3","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Structured solution of asynchronously communicating stochastic modules","J. Campos; S. Donatelli; M. Silva","Dept. de Inf. & Ingenieria de Sistemas, Zaragoza Univ., Spain; NA; NA","IEEE Transactions on Software Engineering","","1999","25","2","147","165","Asynchronously communicating stochastic modules (SAM) are Petri nets that can be seen as a set of modules that communicate through buffers, so they are not (yet another) Petri net subclass, but they complement a net with a structured view. This paper considers the problem of exploiting the compositionality of the view to generate the state space and to find the steady-state probabilities of a stochastic extension of SAM in a net-driven, efficient way. Essentially we give an expression of an auxiliary matrix, G, which is a supermatrix of the infinitesimal generator of a SAM. G is a tensor algebra expression of matrices of the size of the components for which it is possible to numerically solve the characteristic steady-state solution equation /spl pi//spl middot/G=0, without the need to explicitly compute G. Therefore, we obtain a method that computes the steady-state solution of a SAM without ever explicitly computing and storing its infinitesimal generator, and therefore without computing and storing the reachability graph of the system. Some examples of application of the technique are presented and compared to previous approaches.","0098-5589;1939-3520;2326-3881","","10.1109/32.761442","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=761442","","Stochastic processes;State-space methods;Tensile stress;Algebra;Steady-state;Petri nets;Equations;Explosions;Computer Society;Matrices","Petri nets;software performance evaluation;matrix algebra","asynchronously communicating stochastic modules;Petri nets;buffer communication;structured solution;structured view;compositionality;state space;steady-state probabilities;auxiliary matrix;supermatrix;infinitesimal generator;tensor algebra expression;numerical solution","","20","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to 'A modified priority based probe algorithm for distributed deadlock detection and resolution' (A.N. Choudhary et al.)","A. N. Choudhary; W. H. Kohler; J. A. Stankovic; D. Towsley","Comput. Syst. Group, Illinois Univ., Urbana, IL, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","12","1644","","A line inadvertently omitted from a section of the pseudocode in the above paper (see ibid., vol.15, no.1, p.10-17, 1989) is provided. The correct reading of the section is given in full.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58776","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58776","","Probes;System recovery;Information science","distributed processing;error correction;system recovery","modified priority based probe algorithm;distributed deadlock detection;pseudocode","","2","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Model Checking Semantically Annotated Services","I. Di Pietro; F. Pagliarecci; L. Spalazzi","Universit&#x0E0; Politecnica delle Marche, Ancona; Universit&#x0E0; Politecnica delle Marche, Ancona; Universit&#x0E0; Politecnica delle Marche, Ancona","IEEE Transactions on Software Engineering","","2012","38","3","592","608","Model checking is a formal verification method widely accepted in the web service world because of its capability to reason about service behavior at process level. It has been used as a basic tool in several scenarios such as service selection, service validation, and service composition. The importance of semantics is also widely recognized. Indeed, there are several solutions to the problem of providing semantics to web services, most of them relying on some form of Description Logic. This paper presents an integration of model checking and semantic reasoning technologies in an efficient way. This can be considered the first step toward the use of semantic model checking in problems of selection, validation, and composition. The approach relies on a representation of services at process level that is based on semantically annotated state transition systems (asts) and a representation of specifications based on a semantically annotated version of computation tree logic (anctl). This paper proves that the semantic model checking algorithm is sound and complete and can be accomplished in polynomial time. This approach has been evaluated with several experiments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680919","Formal methods;model checking;temporal logic;description logic;intelligent web services;semantic web;web services.","Web services;Semantics;Ontologies;Switches;Computational modeling;Biological system modeling;Syntactics","computational complexity;formal verification;semantic Web;temporal logic;trees (mathematics);Web services","formal verification method;Web service;description logic;semantic reasoning technologies;semantic model checking;annotated state transition systems;computation tree logic;polynomial time;temporal logic;semantically annotated services","","11","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Towards a framework for software measurement validation""","S. Morasca; L. C. Briand; V. R. Basili; E. J. Weyuker; M. V. Zelkowitz; B. Kitchenham; S. Lawrence Pfleeger; N. Fenton","Dipartimento di Elettronica, Politecnico di Milano, Italy; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","3","187","189","A view of software measurement that disagrees with the model presented by Kitchenham, Pfleeger, and Fenton (1995), is given. Whereas Kitchenham et al. argue that properties used to define measures should not constrain the scale type of measures, the authors contend that that is an inappropriate restriction. In addition, a misinterpretation of Weyuker's (1988) properties is noted.","0098-5589;1939-3520;2326-3881","","10.1109/32.585506","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585506","","Software measurement;Particle measurements;Computer Society;Temperature measurement;Size measurement;Measurement units;Impedance;Physics","software metrics;program testing;program verification","software measurement validation","","17","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Foreword Software Reliability","A. L. Goel; F. B. Bastani","NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","12","1409","1410","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232175","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701961","","Software reliability;Software measurement;Software testing;Error correction;Condition monitoring;Software systems;Fault tolerance;Probability;History;Programming","","","","4","","","","","","","","IEEE","IEEE Journals & Magazines"
"Ant Colony Optimization for Software Project Scheduling and Staffing with an Event-Based Scheduler","W. Chen; J. Zhang","Sun Yat-sen University, Guangzhou; Sun Yat-sen University, Guangzhou","IEEE Transactions on Software Engineering","","2013","39","1","1","17","Research into developing effective computer aided techniques for planning software projects is important and challenging for software engineering. Different from projects in other fields, software projects are people-intensive activities and their related resources are mainly human resources. Thus, an adequate model for software project planning has to deal with not only the problem of project task scheduling but also the problem of human resource allocation. But as both of these two problems are difficult, existing models either suffer from a very large search space or have to restrict the flexibility of human resource allocation to simplify the model. To develop a flexible and effective model for software project planning, this paper develops a novel approach with an event-based scheduler (EBS) and an ant colony optimization (ACO) algorithm. The proposed approach represents a plan by a task list and a planned employee allocation matrix. In this way, both the issues of task scheduling and employee allocation can be taken into account. In the EBS, the beginning time of the project, the time when resources are released from finished tasks, and the time when employees join or leave the project are regarded as events. The basic idea of the EBS is to adjust the allocation of employees at events and keep the allocation unchanged at nonevents. With this strategy, the proposed method enables the modeling of resource conflict and task preemption and preserves the flexibility in human resource allocation. To solve the planning problem, an ACO algorithm is further designed. Experimental results on 83 instances demonstrate that the proposed method is very promising.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6165315","Software project planning;project scheduling;resource allocation;workload assignment;ant colony optimization (ACO)","Software;Resource management;Planning;Humans;Project management;Job shop scheduling;Search problems","ant colony optimisation;human resource management;planning (artificial intelligence);project management;scheduling;software management","ant colony optimization algorithm;software project scheduling;software project staffing;event-based scheduler;computer aided techniques;software project planning;software engineering;project task scheduling problem;human resource allocation problem;EBS;ACO;task list;planned employee allocation matrix;resource conflict modeling;task preemption modeling","","68","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Corrigendum for 'Constraint-based automatic test data generation' by R.A. DeMillo and A.J. Offutt","M. R. Girgis","Dept. of Comput. Sci., Bahrain Univ., Isa Town, Bahrain","IEEE Transactions on Software Engineering","","1993","19","6","640","","In reference to the above-titled paper by R.A. DeMillo and A.J. Offutt (see ibid., vol.17, no.9, p.900-10, Sept. 1991), the commenter rates that he and M.R. Woodward (1985) implemented a system for FORTRAN-77 programs that integrates weak mutation and data flow analysis. He reports here that experiments have been carried out by them (1986), using the system to compare the error exposing ability of weak mutation, data flow, and control flow testing strategies.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232028","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232028","","Automatic testing;Genetic mutations;System testing;Data analysis;Error correction;Instruments;History;Arithmetic;Monitoring;Control systems","program debugging;program testing","constraint-based test data generation;FORTRAN-77;weak mutation;data flow analysis;control flow testing","","1","","3","","","","","","IEEE","IEEE Journals & Magazines"
"How to improve the calibration of cost models","N. B. Ebrahimi","Div. of Stat., Northern Illinois Univ., DeKalb, IL, USA","IEEE Transactions on Software Engineering","","1999","25","1","136","140","One of software engineering's long-standing problems is to estimate the cost of a software project. Using the volume or size of a program to estimate the cost is a common practice in many software development organizations. However, in many situations one is unable to observe the value of this variable at the beginning of the project; one has to estimate it. In this paper we introduce a fairly general model which permits a surrogate or a proxy variable to be observed instead of the actual size. Under this model we obtain estimation of software cost at the given value of a surrogate variable. A confidence interval is also provided under this model.","0098-5589;1939-3520;2326-3881","","10.1109/32.748922","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=748922","","Calibration;Costs;Measurement errors;Size measurement;Software measurement;Parameter estimation;Volume measurement;Mathematical model;Regression analysis;Gaussian distribution","calibration;software cost estimation;project management;software development management","cost model calibration;software engineering;software project cost estimation;surrogate variable;proxy variable;confidence interval","","6","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Correspondence: Response to Botting's Comments","R. H. Bourdeau; B. H. C. Cheng","Department of Computer Science, Michigan State University, East Lansing, MI 48824; NA","IEEE Transactions on Software Engineering","","1996","22","12","911","911","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1996.553640","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=553640","","Error correction;Object oriented programming;Writing","","","","1","","1","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient state space generation for the analysis of real-time systems","I. Kang; I. Lee; Young-Si Kim","Sch. of Comput., Soongsil Univ., Seoul, South Korea; NA; NA","IEEE Transactions on Software Engineering","","2000","26","5","453","477","State explosion is a well-known problem that impedes analysis and testing based on state-space exploration. This problem is particularly serious in real time systems because unbounded time values cause the state space to be infinite even for simple systems. The author presents an algorithm that produces a compact representation of the reachable state space of a real time system. The algorithm yields a small state space, but still retains enough information for analysis. To avoid the state explosion which can be caused by simply adding time values to states, our algorithm uses history equivalence and transition bisimulation to collapse states into equivalent classes. Through history equivalence, states are merged into an equivalence class with the same untimed executions up to the states. Using transition bisimulation, the states that have the same future behaviors are further collapsed. The resultant state space is finite and can be used to analyze real time properties. To show the effectiveness of our algorithm, we have implemented the algorithm and have analyzed several example applications.","0098-5589;1939-3520;2326-3881","","10.1109/32.846302","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846302","","State-space methods;Real time systems;Automata;Safety;Explosions;Reachability analysis;Logic testing;System testing;Algorithm design and analysis;History","state-space methods;real-time systems;systems analysis;formal specification;bisimulation equivalence;equivalence classes;reachability analysis","state space generation;real time systems analysis;state explosion;state-space exploration;unbounded time values;compact representation;reachable state space;time values;history equivalence;transition bisimulation;equivalent classes;untimed executions;future behaviors;real time properties","","11","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Managing Feature Interactions Telecommunications Software Systems - Guest Editorial","Yow-Jian Lin; M. Jazayeri","University of California; NA","IEEE Transactions on Software Engineering","","1998","24","10","777","778","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1998.729679","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=729679","","Software systems;Testing;Computer vision;Interference;Packaging;Intelligent networks;Internet telephony;Environmental management;Software engineering;Software architecture","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Distributed feature composition: a virtual architecture for telecommunications services","M. Jackson; P. Zave","Res. Labs., AT&T, Florham Park, NJ, USA; NA","IEEE Transactions on Software Engineering","","1998","24","10","831","847","Distributed Feature Composition (DFC) is a new technology for feature specification and composition, based on a virtual architecture offering benefits analogous to those of a pipe-and-filter architecture. In the DFC architecture, customer calls are processed by dynamically assembled configurations of filter-like components: each component implements an applicable feature, and communicates with its neighbors by featureless internal calls that are connected by the underlying architectural substrate.","0098-5589;1939-3520;2326-3881","","10.1109/32.729683","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=729683","","Telecommunication services;Digital-to-frequency converters;Computer architecture;Switches;Assembly;Computer Society;Routing;Communication switching;Productivity","telecommunication computing;telecommunication services;user interfaces;interactive systems","distributed feature composition;virtual architecture;telecommunications services;feature specification;pipe-and-filter architecture;DFC architecture;customer calls;dynamically assembled configurations;filter-like components;featureless internal calls;architectural substrate","","98","","26","","","","","","IEEE","IEEE Journals & Magazines"
"A Note on Concurrent Programming Control","C. M. Davidson","Philips Telecommunicatie en Data Systemen Nederland","IEEE Transactions on Software Engineering","","1987","SE-13","7","865","866","An extension to Dijkstra's solution [1], of the problem of limiting access by multiple processors to a single resource, is described. The solution has similar delay characteristics to Ferguson's solution [3] while using less complex data structures. Some claims in [3] are examined.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233498","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702298","Cooperating sequential processes;cooperative mutual exclusion;critical sections;semaphores","Delay;Data structures;System recovery;Timing;Decision making;Telecommunications","","Cooperating sequential processes;cooperative mutual exclusion;critical sections;semaphores","","1","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Foreword Advances in Distributed Computing Systems","S. F. Lundstrom; E. E. Swartzlander","NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","10","1092","1096","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231856","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701924","","Distributed computing;Database systems;Broadcasting;Communication networks;Routing;Computer networks;Application software;Delay;Distributed databases;Intelligent networks","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient algorithms for selection of recovery points in tree task models","S. K. Mishra; V. V. Raghavan; N. -. Tzeng","Center for Adv. Comput. Studies, Southwestern Louisiana Univ., Lafayette, LA, USA; Center for Adv. Comput. Studies, Southwestern Louisiana Univ., Lafayette, LA, USA; Center for Adv. Comput. Studies, Southwestern Louisiana Univ., Lafayette, LA, USA","IEEE Transactions on Software Engineering","","1991","17","7","731","734","Efficient solutions to the problem of optimally selecting recovery points are developed. The solutions are intended for models of computation in which task precedence has a tree structure and a task may fail due to the presence of faults. An algorithm to minimize the expected computation time of the task system under a uniprocessor environment has been developed for the binary tree model. The algorithm has time complexity of O(N/sub 2/), where N is the number of tasks, while previously reported procedures have exponential time requirements. The results are generalized for an arbitrary tree model.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.83909","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=83909","","Binary trees;Computational modeling;Tree data structures;Dynamic programming;Software systems;Tree graphs;Databases;Software algorithms;Fault tolerant systems;Availability","computational complexity;optimisation;trees (mathematics)","tree task models;recovery points;task precedence;tree structure;uniprocessor environment;binary tree model;time complexity;exponential time requirements;arbitrary tree model","","1","","7","","","","","","IEEE","IEEE Journals & Magazines"
"The Effectiveness of Software Diversity in a Large Population of Programs","M. J. P. van der Meulen; M. A. Revilla","Det Norske Veritas, Høvik; University of Valladolid, Valladolid","IEEE Transactions on Software Engineering","","2008","34","6","753","764","In this paper, we first present an exploratory analysis of the aspects of multiple-version software diversity using 36,123, programs written to the same specification. We do so within the framework of the theories of Eckhardt and Lee and Littlewood and Miller. We analyse programming faults made, explore failure regions and difficulty functions, show how effective 1-out-of-2 diversity is and how language diversity increases this effectiveness. The second part of the paper generalizes the findings about 1-out-of-2 diversity, and its special case language diversity by performing statistical analyses of 89,402 programs written to 60 specifications. Most observations in the exploratory analysis are confirmed; however, although the benefit of language diversity can be observed, its effectiveness appears to be low.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4604670","Programming Techniques;Protection mechanisms;Design concepts;Quality analysis and evaluation;Software and System Safety;Reliability;Reliability;Performance measures;Programming Techniques;Protection mechanisms;Design concepts;Quality analysis and evaluation;Software and System Safety;Reliability;Reliability;Performance measures","Statistical analysis;Software reliability;Failure analysis;Functional programming;Fault tolerance;Software testing;Java;Algorithm design and analysis;Reliability engineering","software fault tolerance;statistical analysis","software diversity;exploratory analysis;programming fault;software failure;language diversity;1-out-of-2 diversity;statistical analysis;software reliability","","15","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Quicksort revisited","C. M. Davidson","Philips Telecommun. en Data Syst. Nederland BV, Apeldoorn, Netherlands","IEEE Transactions on Software Engineering","","1988","14","10","1480","1481","H.D. Mills and R.C. Linger (1986) propose adding the datatype set to existing programming languages. During some investigations using sets, it became apparent that Quicksort can be written without using stacks (or recursion). Using sets can lead to efficient multiprocessor usage, because if the elements of a set can be processed in any order, they can frequently be processed simultaneously. An example of the possibilities is an intelligent disk control unit based on External Quicksort, using four processors and four read/write heads., The control unit can sort a large disk file in about 1/3 of the time taken by the one-processor version.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6193","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6193","","Computer languages;Milling machines;Intelligent control;Sorting;Telecommunications","data structures;sorting","data structures;datatype set;Quicksort;multiprocessor usage;disk control unit;External Quicksort","","1","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Topology-Specific Synthesis of Self-Stabilizing Parameterized Systems With Constant-Space Processes","A. Ebnenasir; A. Klinkhamer","Computer Science, Michigan Technological University, Houghton, Michigan United States 49931 (e-mail: aebnenas@mtu.edu); Research, Google Inc, 93176 Mountain View, California United States (e-mail: apklinkh@mtu.edu)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","This paper investigates the problem of synthesizing parameterized systems that are self-stabilizing by construction. To this end, we present several significant results. First, we show a counterintuitive result that despite the undecidability of verifying self-stabilization for parameterized unidirectional rings, synthesizing self-stabilizing unidirectional rings is decidable! This is surprising because it is known that, in general, the synthesis of distributed systems is harder than their verification. Second, we present a topology-specific synthesis method (derived from our proof of decidability) that generates the state transition system of template processes of parameterized self-stabilizing systems with elementary unidirectional topologies (e.g., rings, chains, trees). We also provide a software tool that implements our synthesis algorithms and generates interesting self-stabilizing parameterized unidirectional rings in less than 50 microseconds on a regular laptop. We validate the proposed synthesis algorithms for decidable cases in the context of several interesting distributed protocols. Third, we show that synthesis of self-stabilizing bidirectional rings remains undecidable.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2901485","Division of Computing and Communication Foundations; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8651426","Self-Stabilization;Distributed Programming;Formal Methods;Program Synthesis","Topology;Protocols;Convergence;Software algorithms;Portable computers;Automata;Transforms","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"An introduction to rapid system prototyping","F. Kordon; Luqi","Comput. Sci. Dept., Univ. Pierre et Marie Curie, Paris, France; NA","IEEE Transactions on Software Engineering","","2002","28","9","817","821","The implementation and maintenance of industrial applications have continuously become more and more difficult. In this context, one problem is the evaluation of complex systems. The IEEE defines prototyping as a development approach promoting the implementation of a pilot version of the intended product. This approach is a potential solution to the early evaluation of a system. It can also be used to avoid the shift between the description/specification of a system and its implementation. This brief introduction to the special section on rapid system prototyping illustrates a current picture of prototyping.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033222","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033222","","Prototypes;Software prototyping;Hardware;Conferences;Application software;Textile industry;Design engineering;Computer industry;Computer bugs","software prototyping","rapid system prototyping;pilot version;system description;system specification","","25","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A queueing network model for a distributed database testbed system","B. -. Jenq; W. H. Kohler; D. Towsley","Massachusetts Univ., Amherst, MA, USA; Massachusetts Univ., Amherst, MA, USA; Massachusetts Univ., Amherst, MA, USA","IEEE Transactions on Software Engineering","","1988","14","7","908","921","A queuing network model for analyzing the performance of a distributed database testbed system with a transaction workload is developed. The model includes the effects of the concurrency control protocol (two-phase locking with distributed deadlock detection), the transaction recovery protocol (write-ahead logging of before-images), and the commit protocol (centralized two-phase commit) used in the testbed system. The queuing model differs from previous analytical models in three major aspects. First, it is a model for a distributed transaction processing system. Second, it is more general and integrated than previous analytical models. Finally, it reflects a functioning distributed database testbed system and is validated against performance measurements.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42734","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42734","","Distributed databases;System testing;Transaction databases;Protocols;Queueing analysis;Analytical models;Performance analysis;Concurrency control;System recovery;Measurement","distributed databases;program testing;protocols;queueing theory;system recovery","queueing network model;distributed database testbed system;transaction workload;concurrency control protocol;two-phase locking;distributed deadlock detection;transaction recovery protocol;write-ahead logging;commit protocol;centralized two-phase commit","","14","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Whole Test Suite Generation","G. Fraser; A. Arcuri","Saarland University, Saarbrücken; Simula Research Laboratory, Lysaker","IEEE Transactions on Software Engineering","","2013","39","2","276","291","Not all bugs lead to program crashes, and not always is there a formal specification to check the correctness of a software test's outcome. A common scenario in software testing is therefore that test data are generated, and a tester manually adds test oracles. As this is a difficult task, it is important to produce small yet representative test sets, and this representativeness is typically measured using code coverage. There is, however, a fundamental problem with the common approach of targeting one coverage goal at a time: Coverage goals are not independent, not equally difficult, and sometimes infeasible-the result of test generation is therefore dependent on the order of coverage goals and how many of them are feasible. To overcome this problem, we propose a novel paradigm in which whole test suites are evolved with the aim of covering all coverage goals at the same time while keeping the total size as small as possible. This approach has several advantages, as for example, its effectiveness is not affected by the number of infeasible targets in the code. We have implemented this novel approach in the EvoSuite tool, and compared it to the common approach of addressing one goal at a time. Evaluated on open source libraries and an industrial case study for a total of 1,741 classes, we show that EvoSuite achieved up to 188 times the branch coverage of a traditional approach targeting single branches, with up to 62 percent smaller test suites.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6152257","Search-based software engineering;length;branch coverage;genetic algorithm;infeasible goal;collateral coverage","Software;Genetic algorithms;Search problems;Arrays;Genetic programming;Software testing","formal specification;program debugging;program testing","whole test suite generation;program crashes;formal specification;software testing;code coverage;EvoSuite tool;program debugging","","136","","52","","","","","","IEEE","IEEE Journals & Magazines"
"MOSES: A Framework for QoS Driven Runtime Adaptation of Service-Oriented Systems","V. Cardellini; E. Casalicchio; V. Grassi; S. Iannucci; F. L. Presti; R. Mirandola","University of Roma &#x0022;Tor Vergata&#x0022;, Roma; University of Roma &#x0022;Tor Vergata&#x0022;, Roma; University of Roma &#x0022;Tor Vergata&#x0022;, Roma; University of Roma &#x0022;Tor Vergata&#x0022;, Roma; University of Roma &#x0022;Tor Vergata&#x0022;, Roma; Politecnico di Milano, Milano","IEEE Transactions on Software Engineering","","2012","38","5","1138","1159","Architecting software systems according to the service-oriented paradigm and designing runtime self-adaptable systems are two relevant research areas in today's software engineering. In this paper, we address issues that lie at the intersection of these two important fields. First, we present a characterization of the problem space of self-adaptation for service-oriented systems, thus providing a frame of reference where our and other approaches can be classified. Then, we present MOSES, a methodology and a software tool implementing it to support QoS-driven adaptation of a service-oriented system. It works in a specific region of the identified problem space, corresponding to the scenario where a service-oriented system architected as a composite service needs to sustain a traffic of requests generated by several users. MOSES integrates within a unified framework different adaptation mechanisms. In this way it achieves greater flexibility in facing various operating environments and the possibly conflicting QoS requirements of several concurrent users. Experimental results obtained with a prototype implementation of MOSES show the effectiveness of the proposed approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5963694","Service-oriented architecture;runtime adaptation;quality of service","Service oriented architecture;Quality of service;Runtime;Concrete;Semiconductor optical amplifiers;Adaptation models;Software systems","service-oriented architecture","MOSES;QoS driven runtime adaptation;service oriented system;software system architecture;service oriented paradigm;runtime self adaptable system;software engineering;self adaptation;QoS-driven adaptation;service-oriented system","","68","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Reply to: ""Property-based software engineering measurement""","H. Zuse","Fachbereich Inf., Tech. Univ. Berlin, Germany","IEEE Transactions on Software Engineering","","1997","23","8","533","","L.C. Briand, S. Morasca and V.R. Basili (ibid., vol. 22, no. 1, pp. 68-85, Jan. 1996) introduced a measurement-theoretic approach to software measurement and criticized (among others) the work of the author, but they misinterpreted his work. The author does not require additive software (complexity) measures as Briand, Morasca and Basili state. The author uses the concept of the extensive structure in order to show the empirical properties behind software measures. Briand, Morasca and Basili use the concept of meaningfulness in order to describe scales and that certain scale levels are not excluded by the Weyuker properties. However, they do not consider that scales and scale types are different things.","0098-5589;1939-3520;2326-3881","","10.1109/32.624309","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=624309","","Software engineering;Software measurement;Additives;Size measurement;Software metrics;Lab-on-a-chip","software metrics","property-based software engineering measurement;measurement-theoretic approach;additive software complexity measures;extensive structure;empirical properties;software measures;meaningfulness;scale levels;Weyuker properties;scale types;software metrics","","9","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing Software Service Quality and Trustworthiness at Selection Time","N. Limam; R. Boutaba","POSTECH-Pohang University of Science and Technology, Pohang; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2010","36","4","559","574","The integration of external software in project development is challenging and risky, notably because the execution quality of the software and the trustworthiness of the software provider may be unknown at integration time. This is a timely problem and of increasing importance with the advent of the SaaS model of service delivery. Therefore, in choosing the SaaS service to utilize, project managers must identify and evaluate the level of risk associated with each candidate. Trust is commonly assessed through reputation systems; however, existing systems rely on ratings provided by consumers. This raises numerous issues involving the subjectivity and unfairness of the service ratings. This paper describes a framework for reputation-aware software service selection and rating. A selection algorithm is devised for service recommendation, providing SaaS consumers with the best possible choices based on quality, cost, and trust. An automated rating model, based on the expectancy-disconfirmation theory from market science, is also defined to overcome feedback subjectivity issues. The proposed rating and selection models are validated through simulations, demonstrating that the system can effectively capture service behavior and recommend the best possible choices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.2","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383370","Software as a service (SaaS);software selection;service utility;review and rating;trust and reputation;risk management;SLA monitoring.","Software quality;Risk management;Software maintenance;Costs;Software performance;Project management;Feedback;Monitoring;Business;Computer industry","quality of service;risk management;software architecture;software quality;software selection","software service quality;selection time;project development;software provider;SaaS model;reputation-aware software service selection;automated rating model;expectancy-disconfirmation theory;software service trustworthiness","","63","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""The Model Checker SPIN""","Ki-Seok Bang; Jin-Young Choi; Chuck Yoo","Dept. of Comput. Sci. & Eng., Korea Univ., Seoul, South Korea; NA; NA","IEEE Transactions on Software Engineering","","2001","27","6","573","576","The paper by G.J. Holzmann (see ibid., vol.23, no.5, p.279-95, 1997) describes how to apply SPIN to the verification of a synchronization algorithm (L.M. Ruane, 1990) in process scheduling of an operating system. We report an error in the verification model presented by G.J. Holzmann and present a revised model with verification result. Our result explains the reason why SPIN found the race condition in the synchronization algorithm. We also show that the suggested fix by G.J. Holzmann is incorrect.","0098-5589;1939-3520;2326-3881","","10.1109/32.926177","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=926177","","Software algorithms;Scheduling algorithm;Operating systems;Availability;Computer science","scheduling;operating systems (computers);synchronisation;hazards and race conditions;program verification","Model Checker;SPIN;verification model;verification result;race condition;synchronization algorithm;ACSR;LTL;process scheduling;operating system","","1","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Modular Information Hiding and Type-Safe Linking for C","S. Srivastava; M. Hicks; J. S. Foster; P. Jenkins","University of Maryland at College Park, College Park; University of Maryland at College Park, College Park; University of Maryland at College Park, College Park; University of Maryland at College Park, College Park","IEEE Transactions on Software Engineering","","2008","34","3","357","376","This paper presents CMod, a novel tool that provides a sound module system for C. CMod works by enforcing four rules that are based on principles of modular reasoning and on current programming practice. CMod's rules flesh out the convention that .h header files are module interfaces and .c source files are module implementations. Although this convention is well-known, existing explanations of it are incomplete, omitting important subtleties needed for soundness. In contrast, we have proven formally that CMod's rules enforce both information hiding and type-safe linking. To use CMod, the programmer develops and builds their software as usual, redirecting the compiler and linker to CMod's wrappers. We evaluated CMod by applying it to 30 open source programs, totaling more than one million LoC. Violations to CMod's rules revealed more than a thousand information hiding errors, dozens of typing errors, and hundreds of cases that, although not currently bugs, make programming mistakes more likely as the code evolves. At the same time, programs generally adhere to the assumptions underlying CMod's rules, and so we could fix rule violations with a modest effort. We conclude that CMod can effectively support modular programming in C: it soundly enforces type-safe linking and information-hiding while being largely compatible with existing practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4497211","Coding Tools and Techniques;Modules;packages;Reliability;Code design;Information hiding;Coding Tools and Techniques;Modules;packages;Reliability;Code design;Information hiding","Joining processes;Programming profession;Program processors;Computer errors;Computer bugs;Open source software;Software packages;Packaging;Software reliability;Software safety","C language;data encapsulation;object-oriented programming;program compilers;public domain software;software reusability","modular information hiding;type-safe linking;CMOD;modular reasoning;compiler;open source programs;C language","","1","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Transient analysis of superposed GSPNs","P. Kemper","Inf. IV, Dortmund Univ., Germany","IEEE Transactions on Software Engineering","","1999","25","2","182","193","The paper considers transient analysis using randomization for superposed generalized stochastic Petri nets (GSPNs). Since state space explosion implies that space is the bottleneck for numerical analysis, superposed GSPNs profit from the structured representation known for its associated Markov chain. This moves the bottleneck for analysis from space for generator matrices to space for iteration vectors. Hence a variation of randomization is presented which allows to reduce space requirements for iteration vectors. An additional and welcome side effect is that during an initial phase, this algorithm avoids useless multiplications involving states with zero probability. Furthermore, it accommodates to adaptive randomization in a natural way. Although the algorithm has been developed for superposed GSPNs, it applies to continuous time Markov chains in a more general setting.","0098-5589;1939-3520;2326-3881","","10.1109/32.761444","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=761444","","Transient analysis;State-space methods;Petri nets;Explosions;Stochastic processes;Numerical analysis;Functional analysis;Algebra;Stochastic systems;Performance analysis","Petri nets;Markov processes;randomised algorithms;matrix algebra;iterative methods;state-space methods","transient analysis;randomization;superposed generalized stochastic Petri nets;state space explosion;numerical analysis bottleneck;structured representation;associated Markov chain;generator matrices;iteration vectors;space requirements;algorithm;adaptive randomization;continuous time Markov chains","","14","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Detection and Update Suggestion for Outdated API Names in Documentation","S. Lee; R. Wu; S. C. Cheung; S. Kang","Department of Aerospace and Software Engineering, Gyeongsang National University, Jinju, South Gyeongsang Province Korea (the Republic of) (e-mail: saleese@gnu.ac.kr); Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, Hong Kong Hong Kong (e-mail: wurongxin@cse.ust.hk); Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon Hong Kong (e-mail: scc@cse.ust.hk); Computer Science, KAIST, Daejeon, Daejeon Korea (the Republic of) 305-701 (e-mail: sungwon.kang@kaist.ac.kr)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Application programming interfaces (APIs) continually evolve to meet ever-changing user needs, and documentation provides an authoritative reference for their usage. However, API documentation is commonly outdated because nearly all of the associated updates are performed manually. Such outdated documentation, especially with regard to API names, causes major software development issues. In this paper, we propose a method for automatically updating outdated API names in API documentation. Our insight is that API updates in documentation can be derived from API implementation changes between code revisions. To evaluate the proposed method, we applied it to four open source projects. Our evaluation results show that our method, FreshDoc, detects outdated API names in API documentation with 48% higher accuracy than the existing state-of-the-art methods do. Moreover, when we checked the updates suggested by FreshDoc against the developers? manual updates in the revised documentation, FreshDoc addressed 82% of the outdated names. When we reported 40 outdated API names found by FreshDoc via issue tracking systems, developers accepted 75% of the suggestions. These evaluation results indicate that FreshDoc can be used as a practical method for the detection and updating of API names in the associated documentation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2901459","National Research Foundation of Korea; Hong Kong RGC GRF; MSRA Collaborative Research Award; National Research Foundation of Korea grant funded by the Korea government; National Research Foundation of Korea NRF grant funded by the Korea government MSIP; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8651318","Application programming interfaces;Documentation;History;Software maintenance","Documentation;Computer bugs;Tools;History;Libraries;Software systems","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Computation of dynamic program slices for unstructured programs","B. Korel","Dept. of Comput. Sci., Illinois Inst. of Technol., Chicago, IL, USA","IEEE Transactions on Software Engineering","","1997","23","1","17","34","A dynamic program slice is an executable part of the program whose behaviour is identical, for the same program input, to that of the original program with respect to a variable(s) of interest at some execution position. The existing algorithms of dynamic slice computation use data and control dependencies to compute dynamic slices. These algorithms are limited to structured programs because they may compute incorrect dynamic slices for unstructured programs, due to the limitations of control dependencies that are used to compute dynamic slices. In this paper, we present a novel approach to dynamic slice computation for unstructured programs. The approach employs the notion of a removable block in finding dynamic program slices. Dynamic slices are derived by identifying not only those parts of program execution that contribute to the computation of the value of a variable of interest, but also those parts of program execution that do not contribute to the computation of the variable value. Data dependencies are used to identify contributing computations, whereas removable blocks are used to identify noncontributing computations. We have proved that the presented dynamic slicing algorithms correctly compute dynamic slices. In addition, these algorithms may compute more accurate dynamic slices compared to existing algorithms that use control dependencies. The presented algorithms have been implemented in a tool that supports dynamic slicing for Pascal programs.","0098-5589;1939-3520;2326-3881","","10.1109/32.581327","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=581327","","Heuristic algorithms;Software testing;Software maintenance;Software debugging;Algorithm design and analysis","program debugging;program control structures;program diagnostics","dynamic program slice computation;unstructured programs;executable part;execution position;data dependencies;control dependencies;removable block;variable value computation;contributing computations;noncontributing computations;Pascal programs;execution trace;debugging","","42","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Errata for ""Discovering Documentation for Java Container Classes"" [Aug 07 526-543]","J. Henkel; C. Reichenbach; A. Diwan","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","303","303","In the above titled paper (ibid., vol. 33, no. 8, pp. 526-543, Aug 07), there were several mistakes. The corrections are presented here.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4476755","","Documentation;Java;Containers;Equations;Computer science","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Property-based software engineering measurement: refining the additivity properties""","G. Poels; G. Dedene; L. C. Briand; S. Morasca; V. R. Basili","Dept. of Appl. Econ. Sci., Katholieke Univ., Leuven, Belgium; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","3","190","197","The measure property set of Briand, Morasca, and Basili (1996) establishes the foundation of a real software measurement theory. Unfortunately, a number of inconsistencies related to additivity properties might hinder its acceptance and further elaboration. The authors show how to remove the ambiguity in the property definitions.","0098-5589;1939-3520;2326-3881","","10.1109/32.585508","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585508","","Software engineering;Software measurement;Area measurement;Time measurement;Software systems;Feedback;Psychology;Additives;Flow graphs","software metrics;program testing","property-based software engineering measurement;additivity properties;measure property set;real software measurement theory;property definitions","","8","","15","","","","","","IEEE","IEEE Journals & Magazines"
"The specification and verified decomposition of system requirements using CSP","A. P. Moore","US Naval Res. Lab., Washington, DC, USA","IEEE Transactions on Software Engineering","","1990","16","9","932","948","A formal method for decomposing the critical requirements of a system into requirements of its component processes and a minimal, possibly empty, set of synchronization requirements is described. The trace model of Hoare's communicating sequential processes (CSP) is the basis for the formal method. The method is applied to an abstract voice transmitter and describes the role that the EHDM verification system plays in the transmitter's decomposition is described. In combination with other verification techniques, it is expected that this method will promote the development of more trustworthy systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58782","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58782","","Formal specifications;Algebra;Computer security;Buildings;Transmitters;Formal verification;Safety;Information security;Process design;Space technology","formal specification;synchronisation;theorem proving","specification;verified decomposition;system requirements;CSP;formal method;synchronization requirements;trace model","","5","","50","","","","","","IEEE","IEEE Journals & Magazines"
"False Deadlock Detection in Distributed Systems","G. T. Wuu; A. J. Bernstein","Department of Computer Science, State University of New York; NA","IEEE Transactions on Software Engineering","","1985","SE-11","8","820","821","Detecting a nonexistent deadlock in distributed systems has been referred to as false deadlock detection. This correspondence shows that false deadlock wi1l never occur in a system of two-phase locking transactions. We also describe an algorithm to avoid false deadlock detection when transactions are not two-phase locking.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232530","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702090","Distributed system;false deadlock;transaction-wait-for graph;two-phase locking","System recovery;TV;Detection algorithms;Detectors;Distributed computing;Resource management;Delay;Protocols;Computer networks;Military computing","","Distributed system;false deadlock;transaction-wait-for graph;two-phase locking","","3","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Learning Assumptions for CompositionalVerification of Timed Systems","S. Lin; É. André; Y. Liu; J. Sun; J. S. Dong","Temasek Laboratories, National University of Singapore, 5A, Engineering Drive 1, #09-02, Singapore; Université Paris 13, Sorbonne Paris Cité, Laboratoire d’Informatique de Paris-Nord (LIPN), A204,, Institut Galilée, 99 avenue Jean-Baptiste Clément, 93430 Villetaneuse, CNRS, UMR 7030, Villetaneuse, France; School of Computer Engineering , Nanyang Technological University, 50 Nanyang Avenue, Singapore; Singapore University of Technology and Design, BLK1, Level 3, West Wing, Room 9, 20 Dover Drive, Singapore; Computer Science Department, School of Computing, National University of Singapore, 13 Computing Drive, Singapore","IEEE Transactions on Software Engineering","","2014","40","2","137","153","Compositional techniques such as assume-guarantee reasoning (AGR) can help to alleviate the state space explosion problem associated with model checking. However, compositional verification is difficult to be automated, especially for timed systems, because constructing appropriate assumptions for AGR usually requires human creativity and experience. To automate compositional verification of timed systems, we propose a compositional verification framework using a learning algorithm for automatic construction of timed assumptions for AGR. We prove the correctness and termination of the proposed learning-based framework, and experimental results show that our method performs significantly better than traditional monolithic timed model checking.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6682903","Automatic assume-guarantee reasoning;model checking;timed systems","Model checking;Educational institutions;Explosions;Learning automata;Atomic clocks;Cognition","formal verification;inference mechanisms;learning (artificial intelligence)","monolithic timed model checking;learning-based framework;timed assumptions;learning algorithm;state space explosion problem;AGR techniques;assume-guarantee reasoning techniques;timed systems;compositional verification framework;learning assumptions","","5","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Tuning Temporal Features within the Stochastic π-Calculus","L. Pauleve; M. Magnin; O. Roux","IRCCyN, École Centrale de Nantes; IRCCyN, École Centrale de Nantes; IRCCyN, École Centrale de Nantes","IEEE Transactions on Software Engineering","","2011","37","6","858","871","The stochastic π-calculus is a formalism that has been used for modeling complex dynamical systems where the stochasticity and the delay of transitions are important features, such as in the case of biochemical reactions. Commonly, durations of transitions within stochastic π-calculus models follow an exponential law. The underlying dynamics of such models are expressed in terms of continuous-time Markov chains, which can then be efficiently simulated and model-checked. However, the exponential law comes with a huge variance, making it difficult to model systems with accurate temporal constraints. In this paper, a technique for tuning temporal features within the stochastic π-calculus is presented. This method relies on the introduction of a stochasticity absorption factor by replacing the exponential distribution with the Erlang distribution, which is a sum of exponential random variables. This paper presents a construction of the stochasticity absorption factor in the classical stochastic π-calculus with exponential rates. Tools for manipulating the stochasticity absorption factor and its link with timed intervals for firing transitions are also presented. Finally, the model-checking of such designed models is tackled by supporting the stochasticity absorption factor in a translation from the stochastic π-calculus to the probabilistic model checker PRISM.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.95","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5611556","Temporal parameters;\pi-calculus;model-checking;Markov processes;stochastic processes.","Stochastic processes;Exponential distribution;Random variables;Analytical models","exponential distribution;formal verification;pi calculus;stochastic processes","temporal feature tuning;stochastic π-calculus;complex dynamical system modeling;biochemical reactions;continuous-time Markov chains;stochasticity absorption factor;exponential distribution;Erlang distribution;exponential random variables;probabilistic model checker;PRISM","","1","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Time domain analysis of non-Markovian stochastic Petri nets with PRI transitions","A. Horvath; M. Telek","Dept. of Telecommun., Budapest Univ. of Technol. & Econ., Hungary; Dept. of Telecommun., Budapest Univ. of Technol. & Econ., Hungary","IEEE Transactions on Software Engineering","","2002","28","10","933","943","The time domain analysis of non-Markovian stochastic Petri nets with pre-emptive repeat identical (PRI) type transitions is considered in this paper. The set of ""time domain"" equations describing the evolution of the marking process is provided. The relation of the time domain and formerly available transform domain description is discussed. Based on the time domain description of the process, a simple numerical procedure is provided to analyze the transient behavior. Two examples are calculated to illustrate the proposed numerical method.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041050","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041050","","Time domain analysis;Stochastic processes;Petri nets;Transient analysis;Terminology;Steady-state;Laplace equations;Transforms;Queueing analysis","Petri nets;time-domain analysis;stochastic processes;queueing theory;differential equations;approximation theory","time domain analysis;nonMarkovian stochastic Petri nets;preemptive repeat identical transitions;marking process;transient behavior;queuing model;differential equations;first order approximation","","3","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Planning models for software reliability and cost","M. E. Helander; Ming Zhao; N. Ohlsson","Dept. of Mech. Eng., Linkoping Univ., Sweden; NA; NA","IEEE Transactions on Software Engineering","","1998","24","6","420","434","This paper presents modeling frameworks for distributing development effort among software components to facilitate cost-effective progress toward a system reliability goal. Emphasis on components means that the frameworks can be used, for example, in cleanroom processes and to set certification criteria. The approach, based on reliability allocation, uses the operational profile to quantify the usage environment and a utilization matrix to link usage with system structure. Two approaches for reliability and cost planning are introduced: Reliability-Constrained Cost-Minimization (RCCM) and Budget-Constrained Reliability-Maximization (BCRM). Efficient solutions are presented corresponding to three general functions for measuring cost-to-attain failure intensity. One of the functions is shown to be a generalization of the basic COCOMO form. Planning within budget, adaptation for other cost functions and validation issues are also discussed. Analysis capabilities are illustrated using a software system consisting of 26 developed modules and one procured module. The example also illustrates how to specify a reliability certification level, and minimum purchase price, for the procured module.","0098-5589;1939-3520;2326-3881","","10.1109/32.689400","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=689400","","Software reliability;Software systems;Certification;Linear programming;Software testing;Cost function;Lagrangian functions;Time measurement;Milling machines;Software measurement","software reliability;software cost estimation;software development management","software reliability;planning models;cost-effective progress;system reliability goal;cleanroom processes;certification criteria;reliability allocation;operational profile;usage environment;utilization matrix;reliability-constrained cost-minimization;budget-constrained reliability-maximization;cost-to-attain failure intensity;basic COCOMO form;cost functions;validation issues;reliability certification level;minimum purchase price;Lagrangian multipliers;linear programming;nonlinear programming","","37","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient interprocedural array data-flow analysis for automatic program parallelization","Junjie Gu; Zhiyuan Li","Sun Microsyst. Inc., Palo Alto, CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","3","244","261","Since sequential languages such as Fortran and C are more machine-independent than current parallel languages, it is highly desirable to develop powerful parallelization tools which can generate parallel codes, automatically or semi-automatically, targeting different parallel architectures. Array data-flow analysis is known to be crucial to the success of automatic parallelization. Such an analysis should be performed interprocedurally and symbolically and it often needs to handle the predicates represented by IF conditions. Unfortunately, such a powerful program analysis can be extremely time-consuming if it is not carefully designed. How to enhance the efficiency of this analysis to a practical level remains an issue largely untouched to date. This paper presents techniques for efficient interprocedural array data-flow analysis and documents experimental results of its implementation in a research parallelizing compiler. Our techniques are based on guarded array regions and the resulting tool runs faster, by one or two orders of magnitude, than other similarly powerful tools.","0098-5589;1939-3520;2326-3881","","10.1109/32.842950","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842950","","Data analysis;Parallel languages;Parallel architectures;Performance analysis;Privatization;Computer Society;Power generation;Concurrent computing;Text analysis;Computer applications","arrays;data structures;data flow analysis;parallelising compilers;automatic programming;parallel programming;software performance evaluation","interprocedural array data-flow analysis;automatic program parallelization;parallel code generation;parallel architectures;symbolic analysis;predicates;IF conditions;efficiency enhancement;parallelizing compiler;guarded array regions","","5","","45","","","","","","IEEE","IEEE Journals & Magazines"
"On the Nature of Merge Conflicts: a Study of 2,731 Open Source Java Projects Hosted by GitHub","G. G. L. Menezes; L. G. P. Murta; M. O. Barros; A. Van Der Hoek","Computing Institute, Universidade Federal Fluminense, 28110 Niteroi, Rio de Janeiro Brazil 24220-900 (e-mail: gleiphgh@gmail.com); Computer Science, Universidade Federal Fluminense, Niterói, RJ Brazil 24210-240 (e-mail: leomurta@ic.uff.br); DIA, UNIRIO, Rio de Janeiro, Rio de Janeiro Brazil 22290-240 (e-mail: marcio.barros@uniriotec.br); Informatics, University of California, Irvine, Irvine, California United States 92697-3440 (e-mail: andre@ics.uci.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","When multiple developers change a software system in parallel, these concurrent changes need to be merged to all appear in the software being developed. Numerous merge techniques have been proposed to support this task, but none of them can fully automate the merge process. Indeed, it has been reported that as much as 10% to 20% of all merge attempts result in a merge conflict, meaning that a developer has to manually complete the merge. To date, we have little insight into the nature of these merge conflicts. What do they look like, in detail? How do developers resolve them? Do any patterns exist that might suggest new merge techniques that could reduce the manual effort? This paper contributes an in-depth study of the merge conflicts found in the histories of 2,731 open source Java projects. Seeded by the manual analysis of the histories of five projects, our automated analysis of all 2,731 projects: (1) characterizes the merge conflicts in terms of number of chunks, size, and programming language constructs involved, (2) classifies the manual resolution strategies that developers use to address these merge conflicts, and (3) analyzes the relationships between various characteristics of the merge conflicts and the chosen resolution strategies. Our results give rise to three primary recommendations for future merge techniques, that - when implemented - could on one hand help in automatically resolving certain types of conflicts and on the other hand provide the developer with tool-based assistance to more easily resolve other types of conflicts that cannot be automatically resolved.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2871083","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8468085","Software Merge;Merge Conflict;Merge Resolution","Tools;History;Electronic mail;Java;Software;Task analysis","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Regeneration of replicated objects: a technique and its Eden implementation","C. Pu; J. D. Noe; A. Proudfoot","Dept. of Comput. Sci., Washington Univ., Seattle, WA, USA; Dept. of Comput. Sci., Washington Univ., Seattle, WA, USA; Dept. of Comput. Sci., Washington Univ., Seattle, WA, USA","IEEE Transactions on Software Engineering","","1988","14","7","936","945","A replicated directory system based on a method called regeneration is designed and implemented. The directory system allows selection of arbitrary object to be replicated, choice of the number of replicas for each object, and placement of the copies on machines with independent failure modes. Copies can become inaccessible due to node crashes, but as long as a single copy survives, the replication level is restored by automatically replacing lost copies on other active machines. The focus is on a regeneration algorithm for replica replacement and its application to a replicated directory structure in the Eden local area network. A simple probabilistic approach is used to compare the availability provided by the algorithm to three other replication techniques.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42736","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42736","","Computer crashes;Voting;Availability;Computer science;Algorithm design and analysis;Data analysis;Local area networks;Degradation;Analytical models;Testing","distributed databases;fault tolerant computing;local area networks;system recovery","availability analysis;data replication;distributed databases;replicated objects;Eden;replicated directory system;regeneration;independent failure modes;replica replacement;local area network","","38","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Special Issue - Mobility and Network-Aware Computing","","","IEEE Transactions on Software Engineering","","1998","24","5","0_1","0_1","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1998.685254","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=685254","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Cautious transaction schedulers for database concurrency control","T. Ibaraki; T. Kameda; N. Katoh","Dept. of Inf. & Comput. Sci., Toyohashi Univ. of Technol., Japan; NA; NA","IEEE Transactions on Software Engineering","","1988","14","7","997","1009","Cautious schedulers, which never resort to rollbacks for the purpose of concurrency control, are investigated. In particular, cautious schedulers for classes WW consisting of schedules serializable under the write-write constraints, and WRW, a superclass of W, are considered. The cautious WW-scheduler has a number of nice properties, one of which is the existence of a polynomial-time scheduling algorithm. Since cautious WRW-scheduling is, in general, NP-complete, some restrictions are introduced which allow polynomial-time scheduling. All of these cautious schedulers are based on the assumption that transaction predeclare their read and write sets on arrival. Anomalies which occur when transaction modify their read sets or write sets during execution are discussed and countermeasures are proposed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42740","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42740","","Transaction databases;Concurrency control;Software testing;Delay;Scheduling algorithm;Polynomials;Database systems;Strontium;Algorithm design and analysis;Councils","computational complexity;database theory;distributed databases;scheduling","computational complexity;distributed databases;transaction schedulers;database concurrency control;cautious schedulers;WW;write-write constraints;WRW;WW-scheduler;polynomial-time scheduling algorithm;NP-complete;read sets;write sets","","10","","19","","","","","","IEEE","IEEE Journals & Magazines"
"A theory-based representation for object-oriented domain models","S. A. DeLoach; T. C. Hartrum","Dept. of Electr. & Comput. Eng., US Air Force Inst. of Technol., Wright-Patterson AFB, OH, USA; NA","IEEE Transactions on Software Engineering","","2000","26","6","500","517","Formal software specification has long been touted as a way to increase the quality and reliability of software; however, it remains an intricate, manually intensive activity. An alternative to using formal specifications directly is to translate graphically based, semiformal specifications into formal specifications. However, before this translation can take place, a formal definition of basic object oriented concepts must be found. The paper presents an algebraic model of object orientation that defines how object oriented concepts can be represented algebraically using an object oriented algebraic specification language O-SLANG. O-SLANG combines basic algebraic specification constructs with category theory operations to capture internal object class structure, as well as relationships between classes.","0098-5589;1939-3520;2326-3881","","10.1109/32.852740","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=852740","","Object oriented modeling;Formal specifications;Software quality;Software systems;Computer Society;Specification languages;Software engineering;Formal languages;Application software;Natural languages","object-oriented programming;object-oriented languages;algebraic specification;category theory","theory based representation;object oriented domain models;formal software specification;graphically based semiformal specification translation;formal definition;basic object oriented concepts;algebraic model;object orientation;object oriented concepts;object oriented algebraic specification language;O-SLANG;basic algebraic specification constructs;category theory operations;internal object class structure","","14","","25","","","","","","IEEE","IEEE Journals & Magazines"
"An industrial strength theorem prover for a logic based on Common Lisp","M. Kaufmann; J. S. Moore","Motorola Inc., Austin, TX, USA; NA","IEEE Transactions on Software Engineering","","1997","23","4","203","213","ACL2 is a reimplemented extended version of R.S. Boyer and J.S. Moore's (1979; 1988) Nqthm and M. Kaufmann's (1988) Pc-Nqthm, intended for large scale verification projects. The paper deals primarily with how we scaled up Nqthm's logic to an industrial strength"" programming language-namely, a large applicative subset of Common Lisp-while preserving the use of total functions within the logic. This makes it possible to run formal models efficiently while keeping the logic simple. We enumerate many other important features of ACL2 and we briefly summarize two industrial applications: a model of the Motorola CAP digital signal processing chip and the proof of the correctness of the kernel of the floating point division algorithm on the AMD5/sub K/86 microprocessor by Advanced Micro Devices, Inc.","0098-5589;1939-3520;2326-3881","","10.1109/32.588534","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=588534","","Logic programming;Logic devices;Digital signal processing chips;Mathematics;Large-scale systems;Functional programming;Kernel;Signal processing algorithms;Microprocessors;Automatic logic units","theorem proving;LISP;program verification;digital signal processing chips;floating point arithmetic","industrial strength theorem prover;Common Lisp;ACL2;reimplemented extended version;Nqthm;Pc-Nqthm;large scale verification projects;industrial strength programming language;large applicative subset;formal models;Motorola CAP digital signal processing chip;proof of correctness;floating point division algorithm;AMD5/sub K/86 microprocessor;Advanced Micro Devices;formal logic","","63","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Automatically Assessing Code Understandability","S. Scalabrino; G. Bavota; C. Vendome; M. Linares-V?squez; D. Poshyvanyk; R. Oliveto","Biosciences and Territory, Universita degli Studi del Molise, 18960 Pesche, IS Italy (e-mail: simone.scalabrino@unimol.it); Faculty of Informatics, Universita della Svizzera Italiana, 27216 Lugano, Lugano Switzerland 6904 (e-mail: gabriele.bavota@usi.ch); Computer Science and Software Engineering, Miami University, 6403 Oxford, Ohio United States (e-mail: cgvendome@email.wm.edu); Systems Engineering and Computing, Universidad de los Andes, 27991 Bogota, Bogota Colombia (e-mail: m.linaresv@uniandes.edu.co); Computer Science Department, College of William and Mary, 8604 Williamsburg, Virginia United States (e-mail: denys@cs.wm.edu); Biosciences and Territory, Universita degli Studi del Molise, 18960 Pesche, IS Italy (e-mail: rocco.oliveto@unimol.it)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Understanding software is an inherent requirement for many maintenance and evolution tasks. Without a thorough understanding of the code, developers would not be able to fix bugs or add new features timely. Measuring code understandability might be useful to guide developers in writing better code, and could also help in estimating the effort required to modify code components. Unfortunately, there are no metrics designed to assess the understandability of code snippets. In this work, we perform an extensive evaluation of 121 existing as well as new code-related, documentation-related, and developer-related metrics. We try to (i) correlate each metric with understandability and (ii) build models combining metrics to assess understandability. To do this, we use 444 human evaluations from 63 developers and we obtained a bold negative result: none of the 121 experimented metrics is able to capture code understandability, not even the ones assumed to assess quality attributes apparently related, such as code readability and complexity. While we observed some improvements while combining metrics in models, their effectiveness is still far from making them suitable for practical applications. Finally, we conducted interviews with five professional developers to understand the factors that influence their ability to understand code snippets, aiming at identifying possible new metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2901468","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8651396","Software metrics;Code understandability;Empirical study;Negative result","Complexity theory;Software;Computer bugs;Readability metrics;Software measurement;Indexes","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Modeling and online scheduling of flexible manufacturing systems using stochastic Petri nets","I. Hatono; K. Yamagata; H. Tamura","Dept. of Precision Eng., Osaka Univ., Japan; Dept. of Precision Eng., Osaka Univ., Japan; Dept. of Precision Eng., Osaka Univ., Japan","IEEE Transactions on Software Engineering","","1991","17","2","126","132","The authors discuss the modeling of flexible manufacturing systems (FMSs) under uncertainty and evaluate a rule base for online scheduling. To represent uncertain events in an FMS, such as failure of machine tools, repair time, and processing time, they develop continuous-time and discrete-time stochastic Petri nets with hierarchical structures for constructing the FMS model. For obtaining an efficient schedule for the FMS with an online real-time basis, they construct a rule base and evaluate its performance using the FMS simulation system proposed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67588","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67588","","Job shop scheduling;Flexible manufacturing systems;Stochastic systems;Petri nets;Stochastic processes;Processor scheduling;Machine tools;Computational modeling;Manufacturing systems;Uncertainty","digital simulation;flexible manufacturing systems;knowledge based systems;Petri nets;scheduling","flexible manufacturing systems;stochastic Petri nets;uncertainty;rule base;online scheduling;machine tools;repair time;processing time;continuous-time;discrete-time stochastic Petri nets;hierarchical structures","","41","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Discrete-event simulation of fluid stochastic Petri nets","G. Ciardo; D. M. Nicol; K. S. Trivedi","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA; NA; NA","IEEE Transactions on Software Engineering","","1999","25","2","207","217","The purpose of this paper is to describe a method for the simulation of the recently introduced fluid stochastic Petri nets. Since such nets result in rather complex system of partial differential equations, numerical solution becomes a formidable task. Because of a mixed (discrete and continuous) state space, simulative solution also poses some interesting challenges, which are addressed in the paper.","0098-5589;1939-3520;2326-3881","","10.1109/32.761446","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=761446","","Discrete event simulation;Stochastic processes;Petri nets;State-space methods;Stochastic systems;Performance analysis;Power system modeling;Partial differential equations;Continuous time systems;Fluid flow","discrete event simulation;Petri nets;partial differential equations;discrete event systems","discrete-event simulation;fluid stochastic Petri nets;partial differential equations;numerical solution;mixed state space;simulative solution","","45","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A note on regeneration with virtual copies","R. J. Hilderman; H. J. Hamilton","Dept. of Comput. Sci., Regina Univ., Sask., Canada; NA","IEEE Transactions on Software Engineering","","1997","23","1","56","59","Regeneration with virtual copies (RVC) is a voting-based consistency control algorithm for replicated data objects in a distributed computing system. Proposed by Adam and Tewari (ibid., vol. 19, no. 6, pp. 594-602, 1993), it utilizes selective regeneration and recovery mechanisms for maintaining the availability and consistency of copies. This paper describes some problems with the original paper and proposes solutions.","0098-5589;1939-3520;2326-3881","","10.1109/32.581329","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=581329","","Maintenance;Distributed computing;Availability;Partitioning algorithms;Performance analysis;Control systems;Voting;Algorithm design and analysis","virtual storage;system recovery;replicated databases;concurrency control;distributed databases","selective regeneration mechanisms;virtual copies;voting-based consistency control algorithm;replicated data objects;distributed computing system;selective recovery mechanisms;copy availability;copy consistency;dynamic voting;mutual consistency;network partitioning;reliability;RVC algorithm","","2","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Embedded processes in stochastic Petri nets","W. Henderson; P. G. Taylor","Dept. of Appl. Math., Adelaide Univ., SA, Australia; NA","IEEE Transactions on Software Engineering","","1991","17","2","108","116","Embedded discrete time processes are used to study a class of SPNs (stochastic Petri nets) which have a closed-form equilibrium distribution. These SPNs have probabilistic output bags, colored tokens, and alternating periods of arbitrarily distributing enabling and firing times (periods of time between transitions becoming enabled and absorption of tokens and between transitions absorbing tokens and depositing them in output places, respectively). In addition, an aggregation procedure is proposed which, in certain nets, not only reduces a complex SPN to a much simpler skeleton SPN but also obtains results for the skeleton SPN with are exact marginal distributions for the original SPN.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67592","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67592","","Stochastic processes;Petri nets;Routing;Skeleton;Equations;Mathematics;Intelligent networks;Absorption;Random variables;Computer networks","performance evaluation;Petri nets;stochastic processes","embedded discrete time processes;stochastic Petri nets;closed-form equilibrium distribution;probabilistic output bags;colored tokens;arbitrarily distributing enabling;firing times;aggregation procedure","","37","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A choice relation framework for supporting category-partition test case generation","T. Y. Chen; Pak-Lok Poon; T. H. Tse","Sch. of Inf. Technol., Swinburne Univ. of Technol., Hawthorn, Vic., Australia; NA; NA","IEEE Transactions on Software Engineering","","2003","29","7","577","593","We describe in this paper a choice relation framework for supporting category-partition test case generation. We capture the constraints among various values (or ranges of values) of the parameters and environment conditions identified from the specification, known formally as choices. We express these constraints in terms of relations among choices and combinations of choices, known formally as test frames. We propose a theoretical backbone and techniques for consistency checks and automatic deductions of relations. Based on the theory, algorithms have been developed for generating test frames from the relations. These test frames can then be used as the basis for generating test cases. Our algorithms take into consideration the resource constraints specified by software testers, thus maintaining the effectiveness of the test frames (and hence test cases) generated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214323","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214323","","Computer aided software engineering;Software testing;Software maintenance;Spine;Software algorithms;Costs;Humans;Information technology;Australia;Computer science","computer aided software engineering;program testing","choice relation framework;category- partition test case generation;category-partition test case generation;environment conditions;test frames;consistency checks;resource constraints;software testing","","34","","20","","","","","","IEEE","IEEE Journals & Magazines"
"A formal model of program dependences and its implications for software testing, debugging, and maintenance","A. Podgurski; L. A. Clarke","Dept. of Comput. Eng. & Sci., Case Western Reserve Univ., Cleveland, OH, USA; NA","IEEE Transactions on Software Engineering","","1990","16","9","965","979","A formal, general model of program dependences is presented and used to evaluate several dependence-based software testing, debugging, and maintenance techniques. Two generalizations of control and data flow dependence, called weak and strong syntactic dependence, are introduced and related to a concept called semantic dependence. Semantic dependence models the ability of a program statement to affect the execution behavior of other statements. It is shown that weak syntactic dependence is a necessary but not sufficient condition for semantic dependence and that strong syntactic dependence is necessary but not sufficient condition for a restricted form of semantic dependence that is finitely demonstrated. These results are used to support some proposed uses of program dependences, to controvert others, and to suggest new uses.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58784","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58784","","Software testing;Sufficient conditions;Fault detection;Software debugging;Software maintenance;Programming;Laboratories;Information science;Data analysis;Computer security","formal specification;program debugging;program testing","formal model;program dependences;software testing;debugging;maintenance;data flow dependence;syntactic dependence;semantic dependence","","182","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Finding Trends in Software Research","G. Mathew; A. Agrawal; T. Menzies","Computer Science, North Carolina State University, 6798 Raleigh, North Carolina United States (e-mail: george.meg91@gmail.com); Computer Science, North Carolina State University, 6798 Raleigh, North Carolina United States (e-mail: aagrawa8@ncsu.edu); Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, West Virginia United States 26501 (e-mail: timm@ieee.org)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","This paper explores the structure of research papers in software engineering. Using text mining, we study 35,391 software engineering (SE) papers from 34 leading SE venues over the last 25 years. These venues were divided, nearly evenly, between conferences and journals. An important aspect of this analysis is that it is fully automated and repeatable. To achieve that automation, we used topic modeling (with LDA) to mine 10 topics that represent much of the structure of contemporary SE. The 10 topics presented here should not be ""set in stone"" as the only topics worthy of study in SE. Rather our goal is to report that (a) text mining methods can detect large scale trends within our community; (b) those topic change with time; so (c) it is important to have automatic agents that can update our understanding of our community whenever new data arrives.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2870388","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8465996","Software Engineering;Bibliometrics;Topic Modeling;Text Mining","Software engineering;Conferences;Software;Analytical models;Data models;Predictive models;Testing","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Empirical studies of a safe regression test selection technique","G. Rothermel; M. J. Harrold","Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA","IEEE Transactions on Software Engineering","","1998","24","6","401","419","Regression testing is an expensive testing procedure utilized to validate modified software. Regression test selection techniques attempt to reduce the cost of regression testing by selecting a subset of a program's existing test suite. Safe regression test selection techniques select subsets that, under certain well-defined conditions, exclude no tests (from the original test suite) that if executed would reveal faults in the modified software. Many regression test selection techniques, including several safe techniques, have been proposed, but few have been subjected to empirical validation. This paper reports empirical studies on a particular safe regression test selection technique, in which the technique is compared to the alternative regression testing strategy of running all tests. The results indicate that safe regression test selection can be cost-effective, but that its costs and benefits vary widely based on a number of factors. In particular, test suite design can significantly affect the effectiveness of test selection, and coverage-based test suites may provide test selection results superior to those provided by test suites that are not coverage-based.","0098-5589;1939-3520;2326-3881","","10.1109/32.689399","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=689399","","Software testing;Costs;Software safety;Algorithm design and analysis;Fault detection;Performance evaluation","program testing;software maintenance;statistical analysis","safe regression test selection technique;empirical validation;test suite design;software maintenance;selective retest","","118","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Architectural-level risk analysis using UML","K. Goseva-Popstojanova; A. Hassan; A. Guedem; W. Abdelmoez; D. E. M. Nassar; H. Ammar; A. Mili","Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; NA","IEEE Transactions on Software Engineering","","2003","29","10","946","960","Risk assessment is an essential part in managing software development. Performing risk assessment during the early development phases enhances resource allocation decisions. In order to improve the software development process and the quality of software products, we need to be able to build risk analysis models based on data that can be collected early in the development process. These models will help identify the high-risk components and connectors of the product architecture, so that remedial actions may be taken in order to control and optimize the development process and improve the quality of the product. In this paper, we present a risk assessment methodology which can be used in the early phases of the software life cycle. We use the Unified Modeling Language (UML) and commercial modeling environment Rational Rose Real Time (RoseRT) to obtain UML model statistics. First, for each component and connector in software architecture, a dynamic heuristic risk factor is obtained and severity is assessed based on hazard analysis. Then, a Markov model is constructed to obtain scenarios risk factors. The risk factors of use cases and the overall system risk factor are estimated using the scenarios risk factors. Within our methodology, we also identify critical components and connectors that would require careful analysis, design, implementation, and more testing effort. The risk assessment methodology is applied on a pacemaker case study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237174","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237174","","Risk analysis;Unified modeling language;Risk management;Connectors;Programming;Software development management;Resource management;Software quality;Computer architecture;Statistics","software architecture;risk management;specification languages;Markov processes","software development;risk assessment;software architecture;dynamic coupling;Unified Modeling Language;UML;software life cycle;severity of failure","","76","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to ""A Concurrency Measure""","M. G. Khayat; W. S. Breger; M. Freiling; T. G. Lewis","Department of Computer Science and Engineering, University of Petroleum &amp; Minerals; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","8","822","822","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232532","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702092","","Concurrent computing;Computer science;Petroleum;Minerals;Laboratories","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editor's Introduction","R. E. Fairley","NA","IEEE Transactions on Software Engineering","","1987","SE-13","11","1141","1142","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232861","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702159","","Software engineering;Educational programs;Computer science education;Computer industry;Software quality;Educational products;Software design;Engineering education;Industrial economics;Educational technology","","","","","","10","","","","","","IEEE","IEEE Journals & Magazines"
"""On-the-fly"" solution techniques for stochastic Petri nets and extensions","D. D. Deavours; W. H. Sanders","Center for Reliable & High Performance Comput., Illinois Univ., Urbana, IL, USA; NA","IEEE Transactions on Software Engineering","","1998","24","10","889","902","High level modeling representations, such as stochastic Petri nets, frequently generate very large state spaces and corresponding state transition rate matrices. We propose a new steady state solution approach that avoids explicit storing of the matrix in memory. This method does not impose any structural restrictions on the model, uses Gauss Seidel and variants as the numerical solver, and uses less memory than current state of the art solvers. An implementation of these ideas shows that one can realistically solve very large, general models in relatively little memory.","0098-5589;1939-3520;2326-3881","","10.1109/32.729691","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=729691","","Stochastic processes;Petri nets;State-space methods;Jacobian matrices;Gaussian processes;Computer Society;Steady-state;Scalability;Explosions;Impedance","Petri nets;stochastic systems;iterative methods;mathematics computing;matrix algebra;modelling","on-the-fly solution techniques;stochastic Petri nets;high level modeling representations;very large state spaces;state transition rate matrices;steady state solution approach;structural restrictions;Gauss Seidel;numerical solver;very large general models","","15","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Formal verification of concurrent programs using the Larch prover","B. Chetali","GIE DYADE, Rocquencourt, France","IEEE Transactions on Software Engineering","","1998","24","1","46","62","The paper describes the use of the Larch prover to verify concurrent programs. The chosen specification environment is UNITY, whose proposed model can be fruitfully applied to a wide variety of problems and modified or extended for special purposes. Moreover, UNITY provides a high level of abstraction to express solutions to parallel programming problems. We investigate how the UNITY methodology can be mechanized within a general purpose first order logic theorem prover like LP, and how we can use the theorem proving methodology to prove safety and liveness properties. Then we describe the formalization and the verification of a communication protocol over faulty channels, using the Larch prover LP. We present the full computer checked proof, and we show that a theorem prover can be used to detect flaws in a system specification.","0098-5589;1939-3520;2326-3881","","10.1109/32.663997","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=663997","","Formal verification;Protocols;Software development management;Engineering management;Concurrent computing;Parallel programming;Logic programming;Safety;Mechanical factors;Fault detection","parallel programming;program verification;formal specification;theorem proving;protocols","formal verification;concurrent programs;Larch prover;specification environment;abstraction;parallel programming problems;UNITY methodology;general purpose first order logic theorem prover;LP;theorem proving methodology;liveness properties;communication protocol;faulty channels;computer checked proof;system specification","","5","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Toward formally-based design of message passing programs","S. Gorlatch","Passau Univ., Germany","IEEE Transactions on Software Engineering","","2000","26","3","276","288","Presents a systematic approach to the development of message passing programs. Our programming model is SPMD, with communications restricted to collective operations: scan, reduction, gather, etc. The design process in such an architecture-independent language is based on correctness-preserving transformation rules that are provable in a formal functional framework. We develop a set of design rules for composition and decomposition. For example, scan followed by reduction is replaced by a single reduction, and global reduction is decomposed into two faster operations. The impact of the design rules on the target performance is estimated analytically and tested in machine experiments. As a case study, we design two provably correct, efficient programs using the Message Passing Interface (MPI) for the famous maximum segment sum problem, starting from an intuitive, but inefficient, algorithm specification.","0098-5589;1939-3520;2326-3881","","10.1109/32.842952","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842952","","Message passing;Algorithm design and analysis;Process design;Skeleton;Design methodology;Performance analysis;Testing;Parallel programming;Programming profession;Parallel architectures","message passing;distributed programming;application program interfaces;formal specification;software performance evaluation","formally-based design;message passing programs;SPMD programming model;collective operations;scanning operation;reduction operation;gathering operation;architecture-independent language;correctness-preserving transformation rules;formal functional framework;design rules;composition;decomposition;performance;efficient programs;Message Passing Interface;MPI;maximum segment sum problem;algorithm specification;program transformations;systematic program design;homomorphisms;skeletons","","12","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Specifying timing constraints and composite events: an application in the design of electronic brokerages","A. K. Mok; P. Konana; Guangtian Liu; Chan-Gun Lee; Honguk Woo","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","12","841","858","Increasingly, business applications need to capture consumers' complex preferences interactively and monitor those preferences by translating them into event-condition-action (ECA) rules and syntactically correct processing specification. An expressive event model to specify primitive and composite events that may involve timing constraints among events is critical to such applications. Relying on the work done in active databases and real-time systems, this research proposes a new composite event model based on real-time logic (RTL). The proposed event model does not require fixed event consumption policies and allows the users to represent the exact correlation of event instances in defining composite events. It also supports a wide-range of domain-specific temporal events and constraints, such as future events, time-constrained events, and relative events. This event model is validated within an electronic brokerage architecture that unbundles the required functionalities into three separable components - business rule manager, ECA rule manager, and event monitor - with well-defined interfaces. A proof-of-concept prototype was implemented in the Java programming language to demonstrate the expressiveness of the event model and the feasibility of the architecture. The performance of the composite event monitor was evaluated by varying the number of rules, event arrival rates, and type of composite events.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.105","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377184","Index Terms- Active databases;real-time databases;electronic brokerages;event specification;timing constraints.","Timing;Event detection;Databases;Monitoring;Real time systems;Logic design;Prototypes;Java;Computer languages;Process control","active databases;electronic trading;Java;real-time systems;formal specification;temporal logic;electronic commerce","timing constraints;electronic brokerages architecture;business application;event-condition-action;correct processing specification;active databases;real-time systems;real-time logic;domain-specific temporal event;time-constrained event;business rule manager;ECA rule manager;event monitor;Java programming language;event arrival rates","","8","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Special issues for fm '99: the first world congress on formal methods in the development of computing systems","J. M. Wing; J. Woodcock","Carnegie Mellon University; NA","IEEE Transactions on Software Engineering","","2000","26","8","673","674","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.879806","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879806","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Providing quality responses with natural language interfaces: the null value problem","M. Kao; N. Cercone; W. -. Luk","Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada; Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada; Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada","IEEE Transactions on Software Engineering","","1988","14","7","959","984","An underlying relational database model and the database query language SQL are assumed, and methods are presented for responding with appropriate answers to null value responses. This is done by using a knowledge base based on RM/T, an extended relational model. The advantages of this approach are described. To demonstrate the utility of the knowledge base model, a simple knowledge base is constructed. The algorithms that provide additional information when a null answer is returned are detailed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42738","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42738","","Natural languages;Information retrieval;Relational databases;Database languages;Database systems;Null value;History;Quality management;Graphics","knowledge engineering;natural languages;query languages;relational databases;user interfaces","user interfaces;knowledge engineering;quality responses;natural language interfaces;relational database model;query language;SQL;null value responses;knowledge base;RM/T;extended relational model","","14","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Defining software by continuous, smooth functions","R. A. DeMillo; R. J. Lipton","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; NA","IEEE Transactions on Software Engineering","","1991","17","4","383","384","A simple proof is given, showing that for every operational description of a software system expressed as a discrete state transition function on a virtual machine, there is a continuous smooth function on the reals that agrees with the state transition function on all legal states and has exactly the same complexity. It is suggested that an implication of this result is that there is no reason, in principle, that the methods of classical analysis cannot be used in software engineering.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90437","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=90437","","Software systems;Virtual machining;Software engineering;Mathematics;Logic;Law;Legal factors;Control systems;Mathematical analysis;State-space methods","computational complexity;software engineering","software system;discrete state transition function;virtual machine;continuous smooth function;legal states;complexity;classical analysis;software engineering","","5","","6","","","","","","IEEE","IEEE Journals & Magazines"
"KLAIM: a kernel language for agents interaction and mobility","R. De Nicola; G. L. Ferrari; R. Pugliese","Dept. of Syst. & Inf., Florence Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","1998","24","5","315","330","We investigate the issue of designing a kernel programming language for mobile computing and describe KLAIM, a language that supports a programming paradigm where processes, like data, can be moved from one computing environment to another. The language consists of a core Linda with multiple tuple spaces and of a set of operators for building processes. KLAIM naturally supports programming with explicit localities. Localities are first-class data (they can be manipulated like any other data), but the language provides coordination mechanisms to control the interaction protocols among located processes. The formal operational semantics is useful for discussing the design of the language and provides guidelines for implementations. KLAIM is equipped with a type system that statically checks access right violations of mobile agents. Types are used to describe the intentions (read, write, execute, etc.) of processes in relation to the various localities. The type system is used to determine the operations that processes want to perform at each locality, and to check whether they comply with the declared intentions and whether they have the necessary rights to perform the intended operations at the specific localities. Via a series of examples, we show that many mobile code programming paradigms can be naturally implemented in our kernel language. We also present a prototype implementation of KLAIM in Java.","0098-5589;1939-3520;2326-3881","","10.1109/32.685256","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=685256","","Kernel;Computer languages;Mobile computing;Buildings;Access protocols;Guidelines;Permission;Mobile agents;Prototypes;Java","parallel languages;parallel programming;type theory;object-oriented languages;process algebra;software portability;software agents","KLAIM;kernel programming language;agent interaction;agent mobility;mobile computing;Linda;multiple tuple spaces;operators;explicit localities;first-class data;coordination mechanisms;interaction protocols;formal operational semantics;type system;access right violations;mobile code programming;Java;process algebra","","228","","41","","","","","","IEEE","IEEE Journals & Magazines"
"A weakest precondition semantics for refinement of object-oriented programs","A. Cavalcanti; D. A. Naumann","Centro de Inf., Univ. Fed. de Pernambuco, Recife, Brazil; NA","IEEE Transactions on Software Engineering","","2000","26","8","713","728","We define a predicate-transformer semantics for an object oriented language that includes specification constructs from refinement calculi. The language includes recursive classes, visibility control, dynamic binding, and recursive methods. Using the semantics, we formulate notions of refinement. Such results are a first step toward a refinement calculus.","0098-5589;1939-3520;2326-3881","","10.1109/32.879810","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879810","","Java;Calculus;Object oriented modeling;Object oriented programming;Computer Society;Formal specifications;Terminology;Logic programming;Testing;Software algorithms","object-oriented programming;object-oriented languages;programming language semantics;formal specification;refinement calculus;type theory","weakest precondition semantics;object oriented program refinement;predicate-transformer semantics;object oriented language;specification constructs;refinement calculi;recursive classes;visibility control;dynamic binding;recursive methods","","24","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A system for specification and rapid prototyping of application command languages","J. Stelovsky; H. Sugaya","Swiss Federal Inst. of Technol., Zurich, Switzerland; NA","IEEE Transactions on Software Engineering","","1988","14","7","1023","1032","The XS-2 system that integrates specification, rapid prototyping, and the actual use of application dialogs is described. The XS-2 command language grammar, a nonprocedural description language based on regular expressions, is used to specify commands for any application program. The syntax of the command specification is visible to the user: command names and their activation rules are displayed as a command tree. Since a small set of tools is provided for the development of the command specification and its automatic translation into a prototype application module in Modula-2, no programming work is necessary to design and evaluate the commands. Experience shows that an advanced end user can develop his or her own prototype application without a programmer's assistance.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42742","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42742","","Prototypes;Command languages;Application software;User interfaces;Operating systems;Software prototyping;Automatic programming;Microcomputers;Workstations;Productivity","grammars;interactive systems;programming environments;software tools;user interfaces","software tools;programming environments;user interfaces;rapid prototyping;application command languages;XS-2 system;application dialogs;command language grammar;nonprocedural description language;regular expressions;command specification;command tree;automatic translation;Modula-2","","","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Validating the ISO/IEC 15504 measure of software requirements analysis process capability","K. El Emam; A. Birk","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; NA","IEEE Transactions on Software Engineering","","2000","26","6","541","566","ISO/IEC 15504 is an emerging international standard on software process assessment. It defines a number of software engineering processes and a scale for measuring their capability. One of the defined processes is software requirements analysis (SRA). A basic premise of the measurement scale is that higher process capability is associated with better project performance (i.e., predictive validity). The paper describes an empirical study that evaluates the predictive validity of SRA process capability. Assessments using ISO/IEC 15504 were conducted on 56 projects world-wide over a period of two years. Performance measures on each project were also collected using questionnaires, such as the ability to meet budget commitments and staff productivity. The results provide strong evidence of predictive validity for the SRA process capability measure used in ISO/IEC 15504, but only for organizations with more than 50 IT staff. Specifically, a strong relationship was found between the implementation of requirements analysis practices as defined in ISO/IEC 15504 and the productivity of software projects. For smaller organizations, evidence of predictive validity was rather weak. This can be interpreted in a number of different ways: that the measure of capability is not suitable for small organizations or that the SRA process capability has less effect on project performance for small organizations.","0098-5589;1939-3520;2326-3881","","10.1109/32.852742","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=852742","","ISO standards;IEC standards;Software measurement;Software quality;Capability maturity model;Software standards;Software engineering;Productivity;Military standards;Design engineering","ISO standards;IEC standards;software standards;software metrics;formal specification;software process improvement;software development management;program verification;human resource management","ISO/IEC 15504 measure validation;software requirements analysis process capability;international standard;software process assessment;software engineering processes;measurement scale;process capability;project performance;predictive validity;SRA process capability;performance measures;budget commitments;staff productivity;SRA process capability measure;IT staff;requirements analysis practices;software projects;small organizations","","72","","92","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing verification systems: interactive consistency in ACL2","W. D. Young","Comput.. Logic Inc., Austin, TX, USA","IEEE Transactions on Software Engineering","","1997","23","4","214","223","Achieving interactive consistency among processors in the presence of faults is an important problem in fault tolerant computing, first cleanly formulated by L. Lamport, R. Pease, and M. Shostak (1980; 1982) and solved in selected cases with their Oral Messages (OM) algorithm. Several machine supported verifications of this algorithm have been presented, including a particularly elegant formulation and proof by John Rushby using EHDM and PVS (S. Owre et al., 1992, 1995; J. Rushby, 1992). Rushby proposes interactive consistency as a benchmark problem for specification and verification systems. We present a formalization of the OM algorithm in the ACL2 logic and compare our formalization and proof to his. We draw some conclusions concerning the range of desirable features for verification systems. In particular, while higher order functions, strong typing, lambda abstraction, and full quantification have some value they come with a cost; moreover, many uses of such features can be easily translated into simpler logical constructs, which facilitate more automated proof discovery. We offer a cautionary note about comparing systems with respect to a small set of problems in a limited domain.","0098-5589;1939-3520;2326-3881","","10.1109/32.588536","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=588536","","Fault tolerance;Fault tolerant systems;Cost function;Automatic logic units;Specification languages;Algorithm design and analysis","program verification;software fault tolerance;formal logic;formal specification;theorem proving","verification systems;interactive consistency;ACL2;fault tolerant computing;Oral Messages algorithm;machine supported verifications;EHDM;PVS;benchmark problem;specification systems;OM algorithm;ACL2 logic;higher order functions;strong typing;lambda abstraction;full quantification;logical constructs;automated proof discovery","","5","","23","","","","","","IEEE","IEEE Journals & Magazines"
"A framework based approach to the development of network aware applications","J. Bolliger; T. Gross","Dept. of Comput. Sci., Fed.. Inst. of Technol., Zurich, Switzerland; NA","IEEE Transactions on Software Engineering","","1998","24","5","376","390","Modern networks provide a QoS (quality of service) model to go beyond best-effort services, but current QoS models are oriented towards low-level network parameters (e.g., bandwidth, latency, jitter). Application developers, on the other hand, are interested in quality models that are meaningful to the end-user and, therefore, struggle to bridge the gap between network and application QoS models. Examples of application quality models are response time, predictability or a budget (for transmission costs). Applications that can deal with changes in the network environment are called network-aware. A network-aware application attempts to adjust its resource demands in response to network performance variations. This paper presents a framework-based approach to the construction of network-aware programs. At the core of the framework is a feedback loop that controls the adjustment of the application to network properties. The framework provides the skeleton to address two fundamental challenges for the construction of network-aware applications: how to find out about dynamic changes in network service quality; and how to map application-centric quality measures (e.g., predictability) to network-centric quality measures (e.g., QoS models that focus on bandwidth or latency). Our preliminary experience with a prototype network-aware image retrieval system demonstrates the feasibility of our approach. The prototype illustrates that there is more to network-awareness than just taking network resources and protocols into account and raises questions that need to be addressed (from a software engineering point of view) to make a general approach to network-aware applications useful.","0098-5589;1939-3520;2326-3881","","10.1109/32.685260","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=685260","","Quality of service;Delay;Bandwidth;Predictive models;Software prototyping;Jitter;Bridges;Costs;Feedback loop;Skeleton","software engineering;distributed processing;computer networks;visual databases","framework based approach;network aware applications;quality of service;QoS models;low-level network parameters;end-user;application quality models;response time;predictability;budget;resource demands;network performance variations;feedback loop;network service quality;prototype;image retrieval system;protocols;software engineering","","80","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Adaptive time warp simulation of timed Petri nets","A. Ferscha","Inst. fur Angewandte Inf., Wien Univ., Austria","IEEE Transactions on Software Engineering","","1999","25","2","237","257","Time warp (TW), although generally accepted as a potentially effective parallel and distributed simulation mechanism for timed Petri nets, can reveal deficiencies in certain model domains. Particularly, the unlimited optimism underlying TW can lead to excessive aggressiveness in memory consumption due to saving state histories, and waste of CPU cycles due to over-optimistically progressing simulations that eventually have to be ""rolled back"". Furthermore, in TW simulations executing in distributed memory environments, the communication overhead induced by the roll-back mechanism can cause pathological overall simulation performance. In this work, an adaptive optimism control mechanism for TW is developed to overcome these shortcomings. By monitoring and statistically analyzing the arrival processes of synchronization messages, TW simulation progress is probabilistically throttled based on the forecasted time stamp of forthcoming messages. Two classes of arrival process characterizations are studied, reflecting that a natural trade-off exists among the computational and space complexity, and the respective prediction accuracy: While forecasts based on metrics of central tendency are computationally cheap but yield inadequate predictions for correlated arrivals (thus negatively affecting performance), time series based forecast methods give higher prediction accuracy, but at higher computational cost.","0098-5589;1939-3520;2326-3881","","10.1109/32.761448","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=761448","","Time warp simulation;Petri nets;High performance computing;Accuracy;History;Discrete event simulation;Pathology;Programmable control;Adaptive control;Communication system control","time warp simulation;Petri nets;synchronisation;statistical analysis;time series;computational complexity;distributed processing","adaptive time warp simulation;timed Petri nets;distributed simulation mechanism;parallel simulation mechanism;model domains;memory consumption;state history saving;CPU cycles;distributed memory environments;communication overhead;roll back mechanism;pathological overall simulation performance;arrival process monitoring;statistical analysis;synchronization messages;forecasted time stamp;computational complexity;space complexity;prediction accuracy;metrics","","9","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Stability, availability, and response in network file service","J. Gait","Tekronix Inc., Beaverton, OR, USA","IEEE Transactions on Software Engineering","","1991","17","2","133","140","A network file system called Multifile is described. It meets response, availability, and stability requirements as primitive functions. Multifile has a high degree of responsiveness because its component parts compete among themselves to service file requests; it has high availability because it maintains multiple copies of files; and it exhibits stable behavior over wise range of system parameters. The responsiveness of Multifile to read requests improves as the number of pages per request rises, implying that read ahead pages can profitably be cached at client sites. The throughput of Multifile improves as the request size increases and as the number of clients increases. As server load increases, the responsiveness of Multifile to read requests is stable in most configurations. The throughput of writes is unstable as the number of pages in the wire request rises, implying that write back pages should not be cached at client sites. The scale of events in file service is dominated by disk activity, so lost message exceptions do not occur frequently enough to affect file service; however, duplicate message exceptions are a factor in performance.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67594","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67594","","Stability;Availability;Intelligent networks;Delay;Workstations;Data engineering;File systems;Throughput;Application software;Automotive engineering","file servers;network operating systems","network file service;Multifile;stable behavior;responsiveness;read ahead pages;request size;disk activity;duplicate message exceptions","","","","31","","","","","","IEEE","IEEE Journals & Magazines"
"A C++ data model supporting reachability analysis and dead code detection","Yih-Fam Chen; E. R. Gansner; E. Koutsofios","AT&T Labs. Res., Florham Park, NJ, USA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","9","682","694","A software repository provides a central information source for understanding and reengineering code in a software project. Complex reverse engineering tools can be built by analyzing information stored in the repository without reparsing the original source code. The most critical design aspect of a repository is its data model, which directly affects how effectively the repository supports various analysis tasks. This paper focuses on the design rationales behind a data model for a C++ software repository that supports reachability analysis and dead code detection at the declaration level. These two tasks are frequently needed in large software projects to help remove excess software baggage, select regression tests and support software reuse studies. The language complexity introduced by class inheritance, friendship, and template instantiation in C++ requires a carefully designed model to catch all necessary dependencies for correct reachability analysis. We examine the major design decisions and their consequences in our model and illustrate how future software repositories can be evaluated for completeness at a selected abstraction level. Examples are given to illustrate how our model also supports variants of reachability analysis: impact analysis, class visibility analysis, and dead code detection. Finally, we discuss the implementation and experience of our analysis tools on a few C++ software projects.","0098-5589;1939-3520;2326-3881","","10.1109/32.713323","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=713323","","Reachability analysis;Relational databases;Reverse engineering;Data models;Information analysis;Software testing;Software maintenance;Software tools;Buildings;Tree graphs","object-oriented languages;object-oriented programming;C language;reachability analysis;software libraries;software reusability;systems re-engineering;reverse engineering;inheritance;data structures","C++;data model;reachability analysis;dead code detection;software repository;program understanding;system reengineering;software project;reverse engineering tools;large software projects;regression tests;software reuse;language complexity;class inheritance;friendship;template instantiation;impact analysis;class visibility analysis","","26","","29","","","","","","IEEE","IEEE Journals & Magazines"
"A safe algorithm for resolving OR deadlocks","J. Villadangos; F. Farina; J. R. Gonzalez de Mendivil; J. R. Garitagoitia; A. Cordoba","Dept. de Automatica y Computacion, Univ. Publica de Navarra, Spain; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","7","608","622","Deadlocks in the OR model are usually resolved by aborting a deadlocked process. Prior algorithms for the same model sometimes abort nodes needlessly wasting computing resources. This paper presents a new deadlock resolution algorithm for the OR model that satisfies the following correctness criteria: (Safety) the algorithm does not resolve false deadlocks; (Liveness) the algorithm resolves all deadlocks in finite time. The communication cost of the algorithm is similar to that of previous nonsafe proposals. The theoretical cost has been validated by simulation. In addition, different algorithm initiation alternatives have been analyzed in order to reduce the latency of deadlocks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214325","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214325","","System recovery;Abortion;Computer Society;Costs;Algorithm design and analysis;Delay;Safety;Proposals;Distributed algorithms;Throughput","system recovery;distributed algorithms;program verification","OR model;OR deadlocks;deadlock resolution algorithm;correctness criteria;distributed algorithms","","5","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Mechanizing CSP trace theory in higher order logic","A. J. Camilleri","Hewlett-Packard Lab., Stoke Gifford, UK","IEEE Transactions on Software Engineering","","1990","16","9","993","1004","How a mechanized tool for reasoning about CSP (communicating sequential processes) can be developed by customizing an existing general-purpose theorem prover based on higher-order logic is described. How the trace semantics of CSP operators can be mechanized in higher-order logic is investigated, and how the laws associated with these operators can be proved from their semantic definitions is shown. The resulting system is one in which natural-deduction style proofs can be conducted using the standard CSP laws.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58786","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58786","","Algebra;Logic functions;Concurrent computing;Humans;Formal verification;System recovery;Information systems","formal logic;formal specification;theorem proving","mechanising CSP trace theory;higher order logic;communicating sequential processes;general-purpose theorem prover","","34","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Formal verification of Ada programs","D. Guaspari; C. Marceau; W. Polak","Odyssey Res. Associates Inc., Ithaca, NY, USA; Odyssey Res. Associates Inc., Ithaca, NY, USA; Odyssey Res. Associates Inc., Ithaca, NY, USA","IEEE Transactions on Software Engineering","","1990","16","9","1058","1075","The Penelope verification editor and its formal basis are described. Penelope is a prototype system for the interactive development and verification of programs that are written in a rich subset of sequential Ada. Because it generates verification conditions incrementally, Penelope can be used to develop a program and its correctness proof in concert. If an already-verified program is modified, one can attempt to prove the modified version by replaying and modifying the original sequence of proof steps. Verification conditions are generated by predicate transformers whose logical soundness can be proven by establishing a precise formal connection between predicate transformation and denotational definitions in the style of continuation semantics. Penelope's specification language, Larch/Ada, belongs to the family of Larch interface languages. It scales up properly, in the sense that one can demonstrate the soundness of decomposing an implementation hierarchically and reasoning locally about the implementation of each node in the hierarchy.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58790","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58790","","Formal verification;Specification languages;Formal specifications;Prototypes;Transformers;Programming;Mathematical model;Error correction;Buildings;Manuals","Ada;program verification;software engineering","Ada programs;Penelope verification editor;formal basis;prototype system;interactive development;correctness proof;predicate transformers;logical soundness;interface languages","","34","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Foundations of the trace assertion method of module interface specification","R. Janicki; E. Sekerinski","McMaster Univ., Hamilton, Ont., Canada; NA","IEEE Transactions on Software Engineering","","2001","27","7","577","598","The trace assertion method is a formal state machine based method for specifying module interfaces. A module interface specification treats the module as a black-box, identifying all the module's access programs (i.e., programs that can be invoked from outside of the module) and describing their externally visible effects. In the method, both the module states and the behaviors observed are fully described by traces built from access program invocations and their visible effects. A formal model for the trace assertion method is proposed. The concept of step-traces is introduced and applied. The stepwise refinement of trace assertion specifications is considered. The role of nondeterminism, normal and exceptional behavior, value functions, and multiobject modules are discussed. The relationship with algebraic specifications is analyzed. A tabular notation for writing trace specifications to ensure readability is adapted.","0098-5589;1939-3520;2326-3881","","10.1109/32.935852","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=935852","","Algebra;Computer Society;Equations;Commutation;Writing;History;Tail","formal specification;automata theory","trace assertion method;formal state machine based method;module interface specification;black-box;access programs;module states;module behavior;access program invocations;externally visible effects;step traces;stepwise refinement;trace assertion specifications;nondeterminism;normal behavior;exceptional behavior;value functions;multiobject modules;algebraic specifications;tabular notation;readability;trace specification writing","","17","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Storing and retrieving software components: a refinement based system","R. Mili; A. Mili; R. T. Mittermeir","Southwestern Med. Center, Texas Univ., Dallas, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","7","445","460","Software libraries are repositories which contain software components; as such, they represent a precious resource for the software engineer. As software libraries grow in size, it becomes increasingly difficult to maintain adequate precision and recall with informal retrieval algorithms. In this paper, we discuss the design and implementation of a storage and retrieval structure for software components that is based on formal specifications and on the refinement ordering between specifications.","0098-5589;1939-3520;2326-3881","","10.1109/32.605762","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=605762","","Software libraries;Formal specifications;Software algorithms;Information science;Application software;Information retrieval;Software maintenance;Humans;Maintenance engineering;Computer languages","software libraries;software reusability;information retrieval;formal specification","software component storage;software component retrieval;refinement based system;software libraries;software repositories;informal retrieval algorithms;recall;precision;formal specifications;refinement ordering;software reuse","","65","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Components of software development risk: how to address them? A project manager survey","J. Ropponen; K. Lyytinen","Finnish Evangelical Lutheran Mission, Helsinki, Finland; NA","IEEE Transactions on Software Engineering","","2000","26","2","98","112","Software risk management can be defined as an attempt to formalize risk oriented correlates of development success into a readily applicable set of principles and practices. By using a survey instrument we investigate this claim further. The investigation addresses the following questions: 1) What are the components of software development risk? 2) how does risk management mitigate risk components, and 3) what environmental factors if any influence them? Using principal component analysis we identify six software risk components: 1) scheduling and timing risks, 2) functionality risks, 3) subcontracting risks, 4) requirements management, 5) resource usage and performance risks, and 6) personnel management risks. By using one-way ANOVA with multiple comparisons we examine how risk management (or the lack of it) and environmental factors (such as development methods, manager's experience) influence each risk component. The analysis shows that awareness of the importance of risk management and systematic practices to manage risks have an effect on scheduling risks, requirements management risks, and personnel management risks. Environmental contingencies were observed to affect all risk components. This suggests that software risks can be best managed by combining specific risk management considerations with a detailed understanding of the environmental context and with sound managerial practices, such as relying on experienced and well-educated project managers and launching correctly sized projects.","0098-5589;1939-3520;2326-3881","","10.1109/32.841112","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=841112","","Programming;Risk management;Environmental management;Project management;Environmental factors;Personnel;Analysis of variance;Instruments;Principal component analysis;Software performance","risk management;software development management;principal component analysis","software development risk management;project manager survey;environmental factors;principal component analysis;timing risks;scheduling risks;functionality risks;subcontracting risks;requirements management;resource usage;performance risks;personnel management risks;one-way ANOVA;multiple comparisons;environmental contingencies","","162","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on “estimating the number of faults in code” and two corrections to published data","M. Lipow","Hughes Aircraft Company, Radar Systems Group, P.O. Box 92426, Los Angeles, CA 90009","IEEE Transactions on Software Engineering","","1986","SE-12","4","584","585","The subject paper<sup>1</sup>concludes that number of faults per line of code is independent of whether Assembly language or a high order language (HOL) is used, contrary to previously published results by the author of this correspondence. However, the subject paper contains some technical errors and uses erroneous data. An explanation of the source of the erroneous data is given. Also, previously published information by the author of this correspondence on average number of operators plus operands per line of Assembly language code is corrected.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312907","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312907","Software error rate;software reliability","Assembly;Software;Software reliability;Compass;Computer bugs;Earth Observing System;Registers","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"The parallel assignment problem redefined","C. May","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","1989","15","6","821","824","The parallel assignment problem is slightly redefined using a subtler cost function that tends to reduce the number of extra assignments required. It is shown that the new problem, like the classical, is NP-hard. The new problem is then solved for the restricted case of assignment from invertible functions of single variables. For this restricted case and optimum solution can be found in linear time for both the classical problem and the new problem. However, the number of extra assignments required for the classical problem is equal to the number of cycles in the dependency graph, while in the new problem it is equal to the number of isolated cycles in the dependency graph which may be less.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24735","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24735","","Cost function;Couplings;Artificial intelligence","computational complexity;parallel algorithms;parallel programming","parallel assignment problem;cost function;NP-hard;invertible functions;single variables;restricted case;optimum solution;linear time;classical problem;dependency graph;isolated cycles","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Evolution of object behavior using context relations","L. M. Seiter; J. Palsberg; K. J. Lieberherr","Comput. Eng. Dept., Santa Clara Univ., CA, USA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","1","79","92","A collection of design patterns was described by E. Gamma et al. (1994). Each pattern ensures that a certain system aspect can vary over time, for example the operations that can be applied to an object or the algorithm of a method. The patterns are described by constructs such as the inheritance and reference relations, attempting to emulate more dynamic relationships. As a result, the design patterns demonstrate how awkward it is to program natural concepts of evolution when using a traditional object oriented language. We present a new relation between classes: the context relation. It directly models dynamic evolution, and it is meaningful at both the design and implementation level. At the design level we extend the Unified Modeling Language (UML) to include the context relation as a new form of arrow between classes. At the implementation level we present a small extension of Java. The context relation introduces a new form of dynamic binding that serves as a replacement to delegation. We demonstrate how the context relation can be used to easily model and program numerous design patterns.","0098-5589;1939-3520;2326-3881","","10.1109/32.663999","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=663999","","Unified modeling language;Software engineering;Java;Computer science;Computer Society;Context modeling;Production;Software reusability;Design engineering","object-oriented programming;object-oriented languages;software reusability;inheritance","object behavior evolution;context relations;design patterns;system aspect;inheritance;reference relations;dynamic relationships;natural concepts;evolution;object oriented language;context relation;dynamic evolution;Unified Modeling Language;implementation level;Java extension;dynamic binding;delegation","","23","","37","","","","","","IEEE","IEEE Journals & Magazines"
"The reference model for smooth growth of software systems revisited","W. M. Turski","Inst. of Inf., Warsaw Univ., Poland","IEEE Transactions on Software Engineering","","2002","28","8","814","815","Terms-Software evolution,The difference equation determining evolutionary growth of (some) software systems is generalized to a differential one. A hypothetical geometric model is derived and its possible uses are illustrated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027802","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027802","","Software systems;Difference equations;Solid modeling;History;Application software;Closed-form solution;Size measurement;Software measurement;Calendars;Shape","software engineering;difference equations","difference equation;evolutionary growth model;software evolution;system complexity;software systems;hypothetical geometric model","","38","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Program Structure Charts for Applicative Languages","F. G. Pagan","Department of Computer Science, California State University","IEEE Transactions on Software Engineering","","1987","SE-13","4","490","493","A framework for a system of charts compatible with the use of applicative programming languages is proposed and illustrated. The charts are similar in spirit to the structured flowcharts that have sometimes been used with algorithmic languages. They are based on a method of representing the structure of nested expressions of arbitrary complexity. This method is adaptable to the incorporation of graphical devices for the depiction of local identifier bindings, conditional expressions, recursive function definitions, and the various functional combining forms employed in the FP-style of applicative programming.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233185","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702240","Applicative languages;functional programming;program design and documentation;program structure charts;structured flowcharts","Computer languages;Functional programming;Flowcharts;Documentation;Computer science;History;Diversity reception;Computer displays;Writing","","Applicative languages;functional programming;program design and documentation;program structure charts;structured flowcharts","","","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Specification and Verification of Normative Texts Using C-O Diagrams","G. Díaz; M. E. Cambronero; E. Martínez; G. Schneider","Department of Computer Science , University of Castilla-La Mancha, Albacete, Spain; Department of Computer Science , University of Castilla-La Mancha, Albacete, Spain; Department of Computer Science , University of Castilla-La Mancha, Albacete, Spain; Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Sweden","IEEE Transactions on Software Engineering","","2014","40","8","795","817","C-O diagrams have been introduced as a means to have a more visual representation of normative texts and electronic contracts, where it is possible to represent the obligations, permissions and prohibitions of the different signatories, as well as the penalties resulting from non-fulfillment of their obligations and prohibitions. In such diagrams we are also able to represent absolute and relative timing constraints. In this paper we present a formal semantics for C-O diagrams based on timed automata extended with information regarding the satisfaction and violation of clauses in order to represent different deontic modalities. As a proof of concept, we apply our approach to two different case studies, where the method presented here has successfully identified problems in the specification.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6657668","Normative documents;electronic contracts;deontic logic;formal verification;visual models;timed automata;C-O diagrams","Automata;Clocks;Contracts;Semantics;Cost accounting;Synchronization;Formal languages","automata theory;formal specification;formal verification;text analysis","normative texts;formal specification;formal verification;C-O diagrams;visual representation;electronic contracts;timing constraints;formal semantics;timed automata;deontic modalities","","3","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Formal development and verification of a distributed railway control system","A. E. Haxthausen; J. Peleska","Dept. of Inf. Technol., Tech. Univ., Lyngby, Denmark; NA","IEEE Transactions on Software Engineering","","2000","26","8","687","701","The authors introduce the concept for a distributed railway control system and present the specification and verification of the main algorithm used for safe distributed control. Our design and verification approach is based on the RAISE method, starting with highly abstract algebraic specifications which are transformed into directly implementable distributed control processes by applying a series of refinement and verification steps. Concrete safety requirements are derived from an abstract version that can be easily validated with respect to soundness and completeness. Complexity is further reduced by separating the system model into a domain model and a controller model. The domain model describes the physical system in absence of control and the controller model introduces the safety-related control mechanisms as a separate entity monitoring observables of the physical system to decide whether it is safe for a train to move or for a point to be switched.","0098-5589;1939-3520;2326-3881","","10.1109/32.879808","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879808","","Rail transportation;Distributed control;Control systems;Communication system control;Switches;Railway safety;Centralized control;Formal specifications;Mobile communication;Concrete","railways;rail traffic;traffic control;algebraic specification;program verification;distributed control;safety-critical software","formal development;distributed railway control system verification;formal specification;safe distributed control;verification approach;RAISE method;highly abstract algebraic specifications;directly implementable distributed control processes;verification steps;safety requirements;abstract version;soundness;completeness;domain model;controller model;safety-related control mechanisms","","52","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Documentation driven development for complex real-time systems","Luqi; L. Zhang; V. Berzins; Y. Qiao","Dept. of Comput. Sci., US Naval Postgraduate Sch., USA; Dept. of Comput. Sci., US Naval Postgraduate Sch., USA; Dept. of Comput. Sci., US Naval Postgraduate Sch., USA; Dept. of Comput. Sci., US Naval Postgraduate Sch., USA","IEEE Transactions on Software Engineering","","2004","30","12","936","952","This work presents a novel approach for development of complex real-time systems, called the documentation-driven development (DDD) approach. This approach can enhance integration of computer aided software development activities, which encompass the entire life cycle. DDD will provide a mechanism to monitor and quickly respond to changes in requirements and provide a friendly communication and collaboration environment to enable different stakeholders to be easily involved in development processes and, therefore, significantly improve the agility of software development for complex real-time systems. DDD will also support automated software generation based on a computational model and some relevant techniques. DDD includes two main parts: a documentation management system (DMS) and a process measurement system (PMS). DMS will create, organize, monitor, analyze, and transform all documentation associated with the software development process. PMS will monitor the frequent changes in requirements and assess the effort and success possibility of development. A case study was conducted by a tool set that realized part of the proposed approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.100","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377190","Index Terms- Software development;documentation;agility;information representation;complex systems;real-time systems.","Documentation;Real time systems;Programming;Software systems;Computerized monitoring;Collaborative software;Computational modeling;Information representation;Mission critical systems;Availability","real-time systems;computer aided software engineering;user interfaces;formal specification;formal verification","documentation driven development approach;real-time system;computer aided software development;software generation;computational model;documentation management system;process measurement system","","5","","56","","","","","","IEEE","IEEE Journals & Magazines"
"LEILA: formaL tool for idEntifying mobIle maLicious behAviour","G. Canfora; F. Martinelli; F. Mercaldo; V. Nardone; A. Santone; C. A. Visaggio","Dept. of Engineering, University of Sannio, Benevento, BN Italy (e-mail: canfora@unisannio.it); ICT, National Research Council of Italy, pisa, Italy Italy 56123 (e-mail: fabio.martinelli@iit.cnr.it); IIT, Istituto di Informatica e Telematica Consiglio Nazionale delle Ricerche, 215080 Pisa, Italy Italy 56124 (e-mail: francesco.mercaldo@iit.cnr.it); Ingegneria, University of Sannio, Benevento, Benevento Italy (e-mail: vnardone@unisannio.it); Ingegneria, University of Sannio, Benevento, Benevnto Italy 82100 (e-mail: santone@unisannio.it); Research Centre on Software Technology, Univeristy of Sannio, Benevento, Italy Italy 82100 (e-mail: visaggio@unisannio.it)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","With the increasing diffusion of mobile technologies, nowadays mobile devices represent an irreplaceable tool to perform several operations, from posting a status on a social network to transfer money between bank accounts. As a consequence, mobile devices store a huge amount of private and sensitive information and this is the reason why attackers are developing very sophisticated techniques to extort data and money from our devices. This paper presents the design and the implementation of LEILA (formaL tool for idEntifying mobIle maLicious behAviour), a tool targeted at Android malware families detection. LEILA is based on a novel approach that exploits model checking to analyse and verify the Java Bytecode that is produced when the source code is compiled. After a thorough description of the method used for Android malware families detection, we report the experiments we have conducted using LEILA. The experiments demonstrated that the tool is effective in detecting malicious behaviour and, especially, in localizing the payload within the code: we evaluated real-world malware belonging to several widespread families obtaining an accuracy ranging between 0.97 and 1.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2834344","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8356128","Security;malware;model checking;testing;Android","Malware;Androids;Humanoid robots;Payloads;Tools;Model checking;Automata","","","","4","","","","","","","","IEEE","IEEE Early Access Articles"
"The model multiplicity problem: experimenting with real-time specification methods","M. Peleg; D. Dori","Sch. of Med., Stanford Univ., CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","8","742","759","The object-process methodology (OPM) specifies both graphically and textually the system's static-structural and behavioral-procedural aspects through a single unifying model. This model singularity is contrasted with the multimodel approach applied by existing object oriented system analysis methods. These methods usually employ at least three distinct models for specifying various system aspects: mainly structure, function, and behavior. Object modeling technique (OMT), the main ancestor of the unified modeling language (UML), extended with timed statecharts, represents a family of such multimodal object oriented methods. Two major open questions related to model multiplicity vs. model singularity have been: 1) whether or not a single model, rather than a combination of several models, enables the synthesis of a better system specification; and 2) which of the two alternative approaches yields a specification that is easier to comprehend. The authors address these questions through a double-blind controlled experiment. To obtain conclusive results, real time systems, which exhibit a more complex dynamic behavior than nonreal time systems were selected as the focus of the experiment. We establish empirically that a single model methodology, OPM, is more effective than a multimodel one, OMT, in terms of synthesis. We pinpoint specific issues in which significant diiferences between the two methodologies were found. The specification comprehension results show that there were significant differences between the two methods in specific issues.","0098-5589;1939-3520;2326-3881","","10.1109/32.879812","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879812","","Object oriented modeling;Real time systems;Unified modeling language;Control system synthesis;Biomedical engineering;Design methodology;Software engineering;Computer aided software engineering;Manuals;Information systems","formal specification;real-time systems;object-oriented methods;temporal logic","model multiplicity problem;real time specification methods;object-process methodology;OPM;behavioral-procedural aspects;static-structural aspects;unifying model;multimodel approach;object oriented system analysis methods;distinct models;system aspects;object modeling technique;unified modeling language;timed statecharts;multimodal object oriented methods;model multiplicity;model singularity;system specification;double-blind controlled experiment;real time systems;complex dynamic behavior;nonreal time systems;single model methodology;OMT;specification comprehension results","","40","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Reliability of systems with Markov transfer of control","K. Siegrist","Dept. of Math. & Stat., Alabama Univ., Huntsville, AL, USA","IEEE Transactions on Software Engineering","","1988","14","7","1049","1053","Software/hardware systems are considered which can be decomposed into a finite number of modules. It is assumed that control of the system is transferred among the modules according to a Markov process. Each module has an associated reliability which gives the probability that the module will operate correctly when called and will transfer control successfully when finished. The system will eventually either fail or complete its task successfully and enter a terminal state. The reliability of the system is studied in terms of the module reliabilities and the transition probabilities. Improved methods of predicting system reliability, allocating module reliability, and determining module sensitivity are developed. Special branching and sequential systems are studied in detail.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42744","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42744","","Control systems;State-space methods;Probability;Mathematical model;Markov processes;Software reliability;Software systems;Mathematics;Statistics","Markov processes;probability;software reliability","software reliability;branching systems;Markov process;probability;module reliabilities;transition probabilities;sequential systems","","38","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Some conservative stopping rules for the operational testing of safety critical software","B. Littlewood; D. Wright","Centre for Software Reliability, City Univ., London, UK; NA","IEEE Transactions on Software Engineering","","1997","23","11","673","683","Operational testing, which aims to generate sequences of test cases with the same statistical properties as those that would be experienced in real operational use, can be used to obtain quantitative measures of the reliability of software. In the case of safety critical software it is common to demand that all known faults are removed. This means that if there is a failure during the operational testing, the offending fault must be identified and removed. Thus an operational test for safety critical software takes the form of a specified number of test cases (or a specified period of working) that must be executed failure-free. This paper addresses the problem of specifying the numbers of test cases (or time periods) required for a test, when the previous test has terminated as a result of a failure. It has been proposed that, after the obligatory fix of the offending fault, the software should be treated as if it were completely novel, and be required to pass exactly the same test as originally specified. The reasoning here claims to be conservative, in as much as no credit is given for any previous failure-free operation prior to the failure that terminated the test. We show that, in fact, this is not a conservative approach in all cases, and propose instead some new Bayesian stopping rules. We show that the degree of conservatism in stopping rules depends upon the precise way in which the reliability requirement is expressed. We define a particular form of conservatism that seems desirable on intuitive grounds, and show that the stopping rules that exhibit this conservatism are also precisely the ones that seem preferable on other grounds.","0098-5589;1939-3520;2326-3881","","10.1109/32.637384","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637384","","Software testing;Software safety;System testing;Licenses;Bayesian methods;Protection;Regulators;Phase frequency detector;Battery powered vehicles;Software measurement","safety-critical software;program testing;statistical analysis;software fault tolerance;Bayes methods","conservative stopping rules;operational software testing;safety critical software;test case generation;statistical testing;quantitative measures;software reliability;failure-free;Bayesian stopping rules","","62","","6","","","","","","IEEE","IEEE Journals & Magazines"
"An architecture for exporting environment awareness to mobile computing applications","G. Welling; B. R. Badrinath","C&C Res. Labs., NEC-USA Inc., Princeton, NJ, USA; NA","IEEE Transactions on Software Engineering","","1998","24","5","391","400","In mobile computing, factors such as add-on hardware components and heterogeneous networks result in an environment of changing resource constraints. An application in such a constrained environment must adapt to these changes so that available resources are properly utilized. We propose an architecture for exporting awareness of the mobile computing environment to an application. In this architecture, a change in the environment is modeled as an asynchronous event that includes information related to the change. Events are typed and are organized as an extensible class hierarchy so that they can be handled at different levels of abstraction according to the requirement of each application. We also compare two approaches to structure an adaptive application. One addresses the problem of incorporating adaptiveness into legacy applications, while the other considers the design of an application with adaptiveness in mind.","0098-5589;1939-3520;2326-3881","","10.1109/32.685262","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=685262","","Computer architecture;Mobile computing;Computer applications;Application software;Resource management;Availability;Computer networks;Operating systems;Hardware;Protocols","wireless LAN;software engineering;distributed processing;portable computers;resource allocation","mobile computing applications;application environment awareness;mobile computing;add-on hardware components;heterogeneous networks;changing resource constraints;asynchronous event;extensible class hierarchy;abstraction levels;adaptive application;legacy applications;application design;event delivery framework","","16","","25","","","","","","IEEE","IEEE Journals & Magazines"
"An environment for developing fault-tolerant software","J. M. Purtilo; P. Jalote","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA","IEEE Transactions on Software Engineering","","1991","17","2","153","159","An environment that supports execution of programs using both N-version programming and recovery blocks in a uniform manner is described. For N-version programming, the system offers an easy and flexible way of specifying the target machines for the separate versions. The basic unit of fault tolerance supported by this system is at the procedure or function level. Each such program unit can be packaged as its own task, and different fault tolerance techniques can subsequently be employed, even within the same application. The environment also allows versions to be written in different programming languages and executed on different machines. This enhances the independence between the different versions, making the fault tolerance techniques more effective. This environment has been developed for use on Unix-based hosts and currently runs on a network of Sun and DEC workstations.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67596","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67596","","Fault tolerance;Computer languages;Testing;Application software;Fault tolerant systems;Voting;Programming profession;Computer science;Packaging machines;Sun","fault tolerant computing;programming environments;software reliability;system recovery","environment;fault-tolerant software;N-version programming;recovery blocks;programming languages;Unix-based hosts;Sun;DEC workstations","","18","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Response to Jiau et al.'s comments","T. C. Oliveira; P. S. C. Alencar; I. M. Filho; C. J. P. de Lucena; D. D. Cowan","Dept. de Inf., Pontificia Univ. Catolica do Rio de Janeiro, Brazil; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","10","708","","The consistency problems in UML models and related software processes can be very complex. First, although UML supports a modeling process that should yield inter and intraconsistent models, the meaning of the UML dependencies and their specializations is not precisely defined and, for this reason, many inconsistencies may appear in their models and processes. Precise definitions would form a basis for methods to detect and analyze consistency problems related to UML dependencies and relationships, as well as problems related to software processes described in UML. In addition, we are using the UML object constraint language (OCL) to describe constraints related to the framework instantiation process. OCL is recognized as a limited language in some aspects for expressing well-formedness rules. We have presented a method (T.C. Oliveira et al., 2004) that works in the case of consistent rules and consistent instantiation processes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339281","","Unified modeling language;Aggregates;Documentation","specification languages;formal specification","consistency problems;UML dependencies;UML object constraint language;instantiation process;software processes","","","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Clustering algorithm for parallelizing software systems in multiprocessors environment","D. Kadamuddi; J. J. P. Tsai","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; NA","IEEE Transactions on Software Engineering","","2000","26","4","340","361","A variety of techniques and tools exist to parallelize software systems on different parallel architectures (SIMD, MIMD). With the advances in high-speed networks, there has been a dramatic increase in the number of client/server applications. A variety of client/server applications are deployed today, ranging from simple telnet sessions to complex electronic commerce transactions. Industry standard protocols, like Secure Socket Layer (SSL), Secure Electronic Transaction (SET), etc., are in use for ensuring privacy and integrity of data, as well as for authenticating the sender and the receiver during message passing. Consequently, a majority of applications using parallel processing techniques are becoming synchronization-centric, i.e., for every message transfer, the sender and receiver must synchronize. However, more effective techniques and tools are needed to automate the clustering of such synchronization-centric applications to extract parallelism. The authors present a new clustering algorithm to facilitate the parallelization of software systems in a multiprocessor environment. The new clustering algorithm achieves traditional clustering objectives (reduction in parallel execution time, communication cost, etc.). Additionally, our approach: 1) reduces the performance degradation caused by synchronizations, and 2) avoids deadlocks during clustering. The effectiveness of our approach is depicted with the help of simulation results.","0098-5589;1939-3520;2326-3881","","10.1109/32.844493","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=844493","","Clustering algorithms;Software algorithms;Software systems;Application software;Network servers;Parallel architectures;High-speed networks;Electronic commerce;Electronics industry;Protocols","parallel programming;multiprocessing systems;workstation clusters;client-server systems;message passing;synchronisation;concurrency control","clustering algorithm;software system parallelization;multiprocessor environment;parallel architectures;high-speed networks;client/server applications;telnet sessions;electronic commerce transactions;industry standard protocols;Secure Socket Layer;Secure Electronic Transaction;data privacy;data integrity;message passing;user authentication;parallel processing techniques;message transfer;synchronization-centric applications;traditional clustering objectives;performance degradation;deadlocks","","11","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Integration of sequential scenarios","J. Desharnais; M. Frappier; R. Khedri; A. Mili","Dept. d'Inf., Laval Univ., Que., Canada; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","9","695","708","We give a formal relation-based definition of scenarios and we show how different scenarios can be integrated to obtain a more global view of user-system interactions. We restrict ourselves to the sequential case, meaning that we suppose that there is only one user (thus, the scenarios we wish to integrate cannot occur concurrently). Our view of scenarios is state-based, rather than event-based, like most of the other approaches, and can be grafted to the well-established specification language Z. Also, the end product of scenario integration, the specification of the functional aspects of the system, is given as a relation; this specification can be refined using independently developed methods. Our formal description is coupled with a diagram-based, transition-system like, presentation of scenarios, which is better suited to communication between clients and specifiers.","0098-5589;1939-3520;2326-3881","","10.1109/32.713325","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=713325","","Specification languages;Refining;Automata;Humans;Matrix decomposition;Mathematics","formal specification;specification languages;user interfaces;diagrams","sequential scenario integration;formal relation;user-system interactions;state-based approach;specification language;Z;requirements specification;functional aspects;formal description;diagram;relational approach","","15","","44","","","","","","IEEE","IEEE Journals & Magazines"
"A Quantitative Approach to Input Generation in Real-Time Testing of Stochastic Systems","L. Carnevali; L. Ridi; E. Vicario","Università degli Studi di Firenze, Firenze; Università degli Studi di Firenze, Firenze; Università degli Studi di Firenze, Firenze","IEEE Transactions on Software Engineering","","2013","39","3","292","304","In the process of testing of concurrent timed systems, input generation identifies values of temporal parameters that let the Implementation Under Test (IUT) execute selected cases. However, when some parameters are not under control of the driver, test execution may diverge from the selected input and produce an inconclusive behavior. We formulate the problem on the basis of an abstraction of the IUT which we call partially stochastic Time Petri Net (psTPN), where controllable parameters are modeled as nondeterministic values and noncontrollable parameters as random variables with general (GEN) distribution. With reference to this abstraction, we derive the analytical form of the probability that the IUT runs along a selected behavior as a function of choices taken on controllable parameters. In the applicative perspective of real-time testing, this identifies a theoretical upper limit on the probability of a conclusive result, thus providing a means to plan the number of test repetitions that are necessary to guarantee a given probability of test-case coverage. It also provides a constructive technique for an optimal or suboptimal approach to input generation and a way to characterize the probability of conclusive testing under other suboptimal strategies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6226426","Real-time testing;input generation;Time Petri Nets;non-Markovian Stochastic Petri Nets;stochastic processes;Difference Bound Matrix","Stochastic processes;Timing;Real time systems;Testing;Tin;Vectors;Automata","Petri nets;program testing;real-time systems","quantitative approach;input generation;real-time testing;stochastic systems;concurrent timed systems;temporal parameters;implementation under test;IUT;test execution;inconclusive behavior;partially stochastic Time Petri Net;psTPN;controllable parameters;nondeterministic values;GEN distribution","","2","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Bayesian Extensions to a Basic Model of Software Reliability","W. S. Jewell","Department of Industrial Engineering and Operations Research, University of California","IEEE Transactions on Software Engineering","","1985","SE-11","12","1465","1471","A Bayesian analysis of the software reliability model of Jelinski and Moranda is given, based upon Meinhold and Singpurwalla. Important extensions are provided to the stopping rule and prior distribution of the number of defects, as well as permitting uncertainty in the failure rate. It is easy to calculate the predictive distribution of unfound errors at the end of software testing, and to see the relative effects of uncertainty in the number of errors and in the detection efficiency. The behavior of the predictive mode and mean over time are examined as possible point estimators, but are clearly inferior to calculating the full predictive distribution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231890","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701969","Bayesian analysis;program testing;software reliability","Bayesian methods;Software reliability;Software testing;Uncertainty;Computer errors;Computer science;Stochastic processes;Computer bugs;Failure analysis;Performance analysis","","Bayesian analysis;program testing;software reliability","","35","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Unpredication, unscheduling, unspeculation: reverse engineering Itanium executables","N. Snavely; S. Debray; G. R. Andrews","Dept. of Comput. Sci., Washington Univ., Seattle, WA, USA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","2","99","115","EPIC (explicitly parallel instruction computing) architectures, exemplified by the Intel Itanium, support a number of advanced architectural features, such as explicit instruction-level parallelism, instruction predication, and speculative loads from memory. However, compiler optimizations that take advantage of these features can profoundly restructure the program's code, making it potentially difficult to reconstruct the original program logic from an optimized Itanium executable. This paper describes techniques to undo some of the effects of such optimizations and thereby improve the quality of reverse engineering such executables.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401927","Index Terms- Reverse engineering;EPIC architectures;speculation;predication;code optimization.","Reverse engineering;Computer architecture;Parallel processing;Optimizing compilers;Computer aided instruction;Concurrent computing;Logic;Pipelines;Delay;Program processors","parallel architectures;reverse engineering;optimising compilers;parallel programming;scheduling;instruction sets","reverse engineering;Intel Itanium;EPIC architecture;explicitly parallel instruction computing;instruction-level parallelism;instruction predication;compiler optimization","","1","","23","","","","","","IEEE","IEEE Journals & Magazines"
"KIDS: a semiautomatic program development system","D. R. Smith","Kestrel Inst., Palo Alto, CA, USA","IEEE Transactions on Software Engineering","","1990","16","9","1024","1043","The Kestrel Interactive Development System (KIDS), which provides automated support for the development of correct and efficient programs from formal specifications, is described. The system has components for performing algorithm design, deductive inference, program simplification, partial evaluation, finite differencing optimizations, data type refinement, compilation, and other development operations. Although their application is interactive, all of the KIDS operations are automatic except the algorithm design tactics, which require some interaction at present. Dozens of programs have been derived using the system, and it is believed that KIDS could be developed to the point where it becomes economical to use for routine programming. To illustrate the use of KIDS, the author traces the derivation of an algorithm for enumerating solutions to the k-queens problem. The initial algorithm that KIDS designed takes about 60 minutes on a SUN-4/110 to find all 92 solutions to the 8-queens problem instance. The final optimized version finds the same solutions in under one second.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58788","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58788","","Inference algorithms;Algorithm design and analysis;Formal specifications;Finite difference methods;Performance evaluation;Design optimization;Data structures;Environmental economics;Automatic programming","inference mechanisms;optimisation;software engineering","KIDS;semiautomatic program development system;Kestrel Interactive Development System;formal specifications;algorithm design;deductive inference;program simplification;partial evaluation;finite differencing optimizations;data type refinement;compilation;k-queens problem;SUN-4/110","","186","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""The confounding effect of class size on the validity of object-oriented metrics""","W. M. Evanco","Coll. of Inf. Sci. & Technol., Drexel Univ., Philadelphia, PA, USA","IEEE Transactions on Software Engineering","","2003","29","7","670","672","It has been proposed by El Emam et al. (ibid. vol.27 (7), 2001) that size should be taken into account as a confounding variable when validating object-oriented metrics. We take issue with this perspective since the ability to measure size does not temporally precede the ability to measure many of the object-oriented metrics that have been proposed. Hence, the condition that a confounding variable must occur causally prior to another explanatory variable is not met. In addition, when specifying multivariate models of defects that incorporate object-oriented metrics, entering size as an explanatory variable may result in misspecified models that lack internal consistency. Examples are given where this misspecification occurs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214331","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214331","","Size measurement;Software measurement;Object oriented modeling;Size control;Computer languages;Volume measurement;Veins;Software testing;Statistical analysis;Information science","software metrics;software reliability;object-oriented programming","object-oriented metrics;software detects;defect-proneness;statistical modeling;multivariate models;class size;validity","","14","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Formal specification of a look manager","K. T. Narayana; S. Dharap","Dept. of Comput. Sci., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci., Pennsylvania State Univ., University Park, PA, USA","IEEE Transactions on Software Engineering","","1990","16","9","1089","1103","A formal specification of the look manager of a dialog system is presented. The look manager deals with the presentation of visual aspects of objects and the editing of those visual aspects. A formal model for specifying the look of objects based on the notion of texturing objects is presented. The texturing model is built from the observed real-life use of overlays of slides. The specification takes as a given hypothesis an invariant relation between the logical display of objects and their layout on the physical screen. The look on the screen is characterized as an invariant ideal show relation. The formalization achieves modularity for the look manager. The specifications are written using the Z notation. The experiment is an integral part of a larger effort in the formal design of a dialog system. It shows that the state-based specification methodology Z is very well suited for description of graphical interface software. Further, the formal specification yields insight into the inherent complexity of building graphical interfaces and their associated displays.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58792","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58792","","Formal specifications;Design methodology;Geometry;Shape;Large screen displays;Buildings;User interfaces;Windows;Software design","computer graphics;formal specification;interactive systems;user interfaces","look manager;formal specification;dialog system;formal model;texturing model;modularity;Z notation;graphical interface software","","5","","13","","","","","","IEEE","IEEE Journals & Magazines"
"On the properties of extended inclusion dependences","H. Arisawa; T. Miura","Yokohama National University, Tokiwadai, Hodogayaku, Yokohama, Japan; Mitsui Engineering Co. Ltd., Tsukiji, Chuo-ku, Tokyo, Japan","IEEE Transactions on Software Engineering","","1986","SE-12","11","1098","1101","New classes of inclusion dependences are proposed as an extension of `generalization' based on the entity and association concept. Various kinds of extensions are discussed, and four classes (IND, IXG, UXG, and co-EXD) are evaluated from the viewpoint of database design. The complete inference axioms for each class and the polynomial of inference problems are presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313001","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313001","AIS diagram;AIS model;coexclusion dependence;database design;entity-association model;exclusion dependence;generalization;inclusion dependence","Databases;Data models;Computational modeling;Adaptation models;Software;Polynomials;Testing","database theory","properties;extended inclusion dependences;IND;IXG;UXG;co-EXD;database design;inference axioms;polynomial","","5","","","","","","","","IEEE","IEEE Journals & Magazines"
"Coverage-based Greybox Fuzzing as Markov Chain","M. B&#x00F6;hme; V. Pham; A. Roychoudhury","Computer Science, National University of Singapore, Singapore, Singapore Singapore 117417 (e-mail: marcel.boehme@acm.org); Computer Science, National University of Singapore, Singapore, Singapore Singapore (e-mail: thuanpv@comp.nus.edu.sg); Computer Science, National University of Singapore, Singapore, Singapore Singapore 117417 (e-mail: abhik@comp.nus.edu.sg)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few ""high-frequency"" paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path <formula><tex>$i$</tex></formula> generates an input that exercises path <formula><tex>$j$</tex></formula>. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases monotonically every time that seed is chosen. Energy is controlled with a power schedule. We implemented several schedules by extending AFL. In 24 hours, AFLFast exposes 3 previously unreported CVEs that are not exposed by AFL and exposes 6 previously unreported CVEs <formula><tex>$7 \times$</tex></formula> faster than AFL. AFLFast produces at least an order of magnitude more unique crashes than AFL. We compared AFLFast to the symbolic executor Klee. In terms of vulnerability detection, AFLFast is significantly more effective than Klee on the same subject programs that were discussed in the original Klee paper. In terms of code coverage, AFLFast only slightly outperforms Klee while a combination of both tools achieves best results by mitigating the individual weaknesses.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2785841","National Research Foundation Singapore; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8233151","vulnerability detection;fuzzing;path exploration;symbolic execution;automated testing","Schedules;Markov processes;Computer crashes;Search problems;Tools;Systematics","","","","2","","","","","","","","IEEE","IEEE Early Access Articles"
"Applicability of Weyuker's Property 9 to object oriented metrics","Naveen Sharma; Padmaja Joshi; R. K. Joshi","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Mumbai, India; Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Mumbai, India; Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Mumbai, India","IEEE Transactions on Software Engineering","","2006","32","3","209","211","Weyuker's Property 9 has received a mixed response regarding its applicability to object oriented software metrics. Contrary to past beliefs, the relevance of this property to object oriented systems is brought out. In support of the new argument, counterexamples to earlier claims are formulated and two new metrics highlighting a notion of complexity that is capturable through Property 9 are also presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610611","Software metrics;object oriented design;Weyuker's properties;interaction complexity.","Software metrics;Software measurement;Testing;Q measurement;Corporate acquisitions;Ontologies;Programming profession;Memory management;Runtime","object-oriented programming;object-oriented methods;software metrics","Weyuker Property 9;object oriented software metrics;object oriented system design;software complexity measures","","11","","7","","","","","","IEEE","IEEE Journals & Magazines"
"An interval logic for real-time system specification","R. Mattolini; P. Nesi","Hewlett Packard, Italy; NA","IEEE Transactions on Software Engineering","","2001","27","3","208","227","Formal techniques for the specification of real time systems must be capable of describing system behavior as a set of relationships expressing the temporal constraints among events and actions, including properties of invariance, precedence, periodicity, liveness, and safety conditions. The paper describes a Temporal-Interval Logic with Compositional Operators (TILCO) designed expressly for the specification of real time systems. TILCO is a generalization of classical temporal logics based on the operators, eventually and henceforth; it allows both qualitative and quantitative specification of time relationships. TILCO is based on time intervals and can concisely express temporal constraints with time bounds, such as those needed to specify real time systems. This approach can be used to verify the completeness and consistency of specifications, as well as to validate system behavior against its requirements and general properties. TILCO has been formalized by using the theorem prover Isabelle/HOL. TILCO specifications satisfying certain properties are executable by using a modified version of the Tableaux algorithm. The paper defines TILCO and its axiomatization, highlights the tools available for proving properties of specifications and for their execution, and provides an example of system specification and validation.","0098-5589;1939-3520;2326-3881","","10.1109/32.910858","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910858","","Real time systems;Safety;Automatic logic units;Logic design;Time factors;Specification languages;Aerospace electronics;Robots;Process control;Gas insulated transmission lines","bibliographies;formal specification;real-time systems;temporal logic;program verification;theorem proving","interval logic;real time system specification;formal techniques;system behavior;temporal constraints;safety conditions;Temporal-Interval Logic with Compositional Operators;TILCO;classical temporal logics;eventually;henceforth;quantitative specification;time relationships;time intervals;completeness;consistency;theorem prover;Isabelle/HOL;TILCO specifications;Tableaux algorithm","","21","","69","","","","","","IEEE","IEEE Journals & Magazines"
"Other comments on 'Optimization algorithms for distributed queries' by P.M.G. Apears","W. Cellary; Z. Krolikowski; T. Morzy","Inst. of Control Eng., Tech. Univ. of Poznan, Poland; Inst. of Control Eng., Tech. Univ. of Poznan, Poland; Inst. of Control Eng., Tech. Univ. of Poznan, Poland","IEEE Transactions on Software Engineering","","1988","14","4","439","441","An erroneous fact concerning the assumption of irreducibility of nonjoining attributes of the distributed query optimization algorithm called GENERAL presented in the above paper (see ibid., vol.SE-9, no.1, p.57-68, Jan. 1983) is pointed out. It is shown that it is possible to generate an efficient semijoin program with better response time than the one produced by the GENERAL algorithm. A counterexample that proves this possibility is provided.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4666","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4666","","Delay;Database systems;Scheduling algorithm;Distributed processing;Heuristic algorithms;Control engineering","computational complexity;database theory;distributed databases;optimisation","distributed databases;computational complexity;distributed queries;irreducibility;nonjoining attributes;query optimization algorithm;GENERAL;semijoin program;response time","","","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Semi-Proving: An Integrated Method for Program Proving, Testing, and Debugging","T. Y. Chen; T. H. Tse; Z. Q. Zhou","Swinburne University of Technology, Hawthorn; The University of Hong Kong, Hong Kong; University of Wollongong, Wollongong","IEEE Transactions on Software Engineering","","2011","37","1","109","125","We present an integrated method for program proving, testing, and debugging. Using the concept of metamorphic relations, we select necessary properties for target programs. For programs where global symbolic evaluation can be conducted and the constraint expressions involved can be solved, we can either prove that these necessary conditions for program correctness are satisfied or identify all inputs that violate the conditions. For other programs, our method can be converted into a symbolic-testing approach. Our method extrapolates from the correctness of a program for tested inputs to the correctness of the program for related untested inputs. The method supports automatic debugging through the identification of constraint expressions that reveal failures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5406529","Software/program verification;symbolic execution;testing and debugging.","Software testing;Automatic testing;Computer science;Built-in self-test;Software debugging;Costs;Automation;Australia Council;Communications technology;Software engineering","formal verification;program debugging;program testing","semiproving;program proving;program testing;program debugging;integrated method;metamorphic relation;symbolic evaluation;constraint expression;symbolic testing;automatic debugging;program verification","","37","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Performance evaluation of mobile processes via abstract machines","C. Nottegar; C. Priami; P. Degano","Dipt. di Inf., Verona Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","2001","27","10","867","889","We use a structural operational semantics which drives us in inferring quantitative measures on system evolution. The transitions of the system are labeled and we assign rates to them by only looking at these labels. The rates reflect the possibly distributed architecture on which applications run. We then map transition systems to Markov chains, and performance evaluation is carried out using standard tools. As a working example, we compare the performance of a conventional uniprocessor with a prefetch pipeline machine. We also consider two case studies from the literature involving mobile computation to show that our framework is feasible.","0098-5589;1939-3520;2326-3881","","10.1109/32.962559","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=962559","","Performance analysis;Resource management;Proposals;Computer architecture;Stochastic systems;Quality management;Humans;Calculus;Prefetching;Pipelines","performance evaluation;finite automata;Markov processes;process algebra;pipeline processing;distributed programming","structural operational semantics;quantitative measures;system evolution;distributed architecture;Markov chains;performance evaluation;uniprocessor;prefetch pipeline machine;mobile computation;calculi for mobility;formal methodology;stochastic models;abstract machines;mobile processes","","13","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Generation of execution sequences for modular time critical systems","P. S. Pietro; A. Morzenti; S. Morasca","Dipt. di Elettronica e Inf., Politecnico di Milano, Italy; NA; NA","IEEE Transactions on Software Engineering","","2000","26","2","128","149","We define methods for generating execution sequences for time-critical systems based on their modularized formal specification. An execution sequence represents a behavior of a time critical system and can be used, before the final system is built, to validate the system specification against the user requirements (specification validation) and, after the final system is built, to verify whether the implementation satisfies the specification (functional testing). Our techniques generate execution sequences in the large, in that we focus on the connections among the abstract interfaces of the modules composing a modular specification. Execution sequences in the large are obtained by composing execution sequences in the small for the individual modules. We abstract from the specification languages used for the individual modules of the system, so our techniques can also be used when the modules composing the system are specified with different formalisms. We consider the cases in which connections give rise to either circular or noncircular dependencies among specification modules. We show that execution sequence generation can be carried out successfully under rather broad conditions and we define procedures for efficient construction of execution sequences. These procedures can be taken as the basis for the implementation of (semi)automated tools that provide substantial support to the activity of specification validation and functional testing for industrially-sized time critical systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.841114","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=841114","","System testing;Software testing;Animation;Formal specifications;Sequential analysis;Computer Society;Time factors;Construction industry;Electrical equipment industry;Software prototyping","formal specification;specification languages;formal verification;program testing;sequences","execution sequence generation;modular time critical systems;modularized formal specification;user requirements;specification validation;abstract interfaces;specification languages;noncircular dependencies;circular dependencies;automated tools;functional testing","","3","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Allocating modules to processors in a distributed system","D. Fernandez-Baca","Dept. of Comput. Sci., Iowa State Univ., Ames, IA, USA","IEEE Transactions on Software Engineering","","1989","15","11","1427","1436","The author studies the complexity of the problem of allocating modules to processes in a distributed system to minimize total communication and execution costs. He shows that unless P=NP, there can be no polynomial-time epsilon -approximate algorithm for the problem, nor can there exist a local search algorithm that requires polynomial time per iteration and yields an optimum assignment. Both results hold even if the communication graph is planar and bipartite. On the positive side, it is shown that if the communication graph is a partial k-tree or an almost-tree with parameter k, the module allocation problem can be solved in polynomial time.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41334","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41334","","Polynomials;Tree graphs;Interference;Cost function;Dynamic programming;Processor scheduling;Dynamic scheduling;Hardware;Monitoring","computational complexity;distributed processing;graph theory","distributed system;complexity;execution costs;P=NP;polynomial-time epsilon -approximate algorithm;local search algorithm;polynomial time;iteration;optimum assignment;communication graph;planar;bipartite;partial k-tree;almost-tree;module allocation problem","","239","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Authors’ Reply to “Comments on ‘Researcher Bias: The Use of Machine Learning in Software Defect Prediction’”","M. Shepperd; T. Hall; D. Bowes","Department of Computer Science, Brunel University London, Uxbridge, United Kingdom; Department of Computer Science, Brunel University London, Uxbridge, United Kingdom; University of Hertfordshire, Hatfield, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","11","1129","1131","In 2014 we published a meta-analysis of software defect prediction studies [1] . This suggested that the most important factor in determining results was Research Group, i.e., who conducts the experiment is more important than the classifier algorithms being investigated. A recent re-analysis [2] sought to argue that the effect is less strong than originally claimed since there is a relationship between Research Group and Dataset. In this response we show (i) the re-analysis is based on a small (21 percent) subset of our original data, (ii) using the same re-analysis approach with a larger subset shows that Research Group is more important than type of Classifier and (iii) however the data are analysed there is compelling evidence that who conducts the research has an effect on the results. This means that the problem of researcher bias remains. Addressing it should be seen as a matter of priority amongst those of us who conduct and publish experiments comparing the performance of competing software defect prediction systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2731308","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7990255","Software quality assurance;defect prediction;researcher bias","Software;NASA;Measurement;Analysis of variance;Data models;Predictive models;Analytical models","learning (artificial intelligence);pattern classification;program diagnostics","researcher bias;machine learning;meta-analysis;classifier algorithms;re-analysis approach;software defect prediction systems;research group","","","","5","","","","","","IEEE","IEEE Journals & Magazines"
"CSM: A Distributed Programming Language","Sun Zhongxiu; Li Xining","Department of Computer Science, Nanjing University; NA","IEEE Transactions on Software Engineering","","1987","SE-13","4","497","500","This paper presents an overview of a distributed programming language called Communicating Sequential Modules or CSM, intended to support distributed computing. Developed from Modula-2 and CSP, CSM has been implemented on the ZCZ distributed microcomputer system consisting of several LSI-11 microcomputers. Its implementation is also described.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233187","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702242","Commands;communication;distributed programming;distributed programming languages;modules;networks","Computer languages;Distributed computing;Microcomputers;Concurrent computing;Sun;Proposals;Computer science;Trademarks","","Commands;communication;distributed programming;distributed programming languages;modules;networks","","1","","10","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical investigation of an object-oriented software system","M. Cartwright; M. Shepperd","Dept. of Comput., Bournemouth Univ., UK; NA","IEEE Transactions on Software Engineering","","2000","26","8","786","796","The paper describes an empirical investigation into an industrial object oriented (OO) system comprised of 133000 lines of C++. The system was a subsystem of a telecommunications product and was developed using the Shlaer-Mellor method (S. Shlaer and S.J. Mellor, 1988; 1992). From this study, we found that there was little use of OO constructs such as inheritance, and therefore polymorphism. It was also found that there was a significant difference in the defect densities between those classes that participated in inheritance structures and those that did not, with the former being approximately three times more defect-prone. We were able to construct useful prediction systems for size and number of defects based upon simple counts such as the number of states and events per class. Although these prediction systems are only likely to have local significance, there is a more general principle that software developers can consider building their own local prediction systems. Moreover, we believe this is possible, even in the absence of the suites of metrics that have been advocated by researchers into OO technology. As a consequence, measurement technology may be accessible to a wider group of potential users.","0098-5589;1939-3520;2326-3881","","10.1109/32.879814","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879814","","Software systems;Computer industry;Testing;Lab-on-a-chip;Buildings;Computer languages;Java;Large-scale systems;Particle measurements;Software measurement","object-oriented programming;C++ language;telecommunication computing;software performance evaluation","empirical investigation;object oriented software system;industrial object oriented system;C++ code;telecommunications product;Shlaer-Mellor method;OO constructs;inheritance;polymorphism;defect densities;inheritance structures;prediction systems;local significance;software developers;local prediction systems;OO technology;measurement technology;potential users","","133","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Visual and textual consistency checking tools for graphical user interfaces","R. Mahajan; B. Shneiderman","BDM Int. Inc., McLean, VA, USA; NA","IEEE Transactions on Software Engineering","","1997","23","11","722","735","Designing user interfaces with consistent visual and textual properties is difficult. To demonstrate the harmful effects of inconsistency, we conducted an experiment with 60 subjects. Inconsistent interface terminology slowed user performance by 10 to 25 percent. Unfortunately, contemporary software tools provide only modest support for consistency control. Therefore, we developed SHERLOCK, a family of consistency analysis tools, which evaluates visual and textual properties of user interfaces. It provides graphical analysis tools such as a dialog box summary table that presents a compact overview of visual properties of all dialog boxes. SHERLOCK provides terminology analysis tools including an interface concordance, an interface spellchecker, and terminology baskets to check for inconsistent use of familiar groups of terms. Button analysis tools include a button concordance and a button layout table to detect variant capitalization, distinct typefaces, distinct colors, variant button sizes, and inconsistent button placements. We describe the design, software architecture, and the use of SHERLOCK. We tested SHERLOCK with four commercial prototypes. The outputs, analysis, and feedback from designers of the applications are presented.","0098-5589;1939-3520;2326-3881","","10.1109/32.637386","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637386","","Terminology;User interfaces;Software tools;Color;Software design;Software architecture;Testing;Software prototyping;Prototypes;Output feedback","software tools;graphical user interfaces;user interface management systems;software metrics;human factors","textual consistency checking tools;visual consistency checking tools;graphical user interfaces;experiment;inconsistent interface terminology;user performance;software tools;SHERLOCK;graphical analysis tools;dialog box summary table;terminology analysis tools;interface spell checker;button analysis tools;interface color;software architecture;software metrics","","19","","36","","","","","","IEEE","IEEE Journals & Magazines"
"More success and failure factors in software reuse","T. Menzies; J. S. Di Stefano","Lane Dept. of Comput. Sci., West Virginia Univ., Morgantown, WV, USA; Lane Dept. of Comput. Sci., West Virginia Univ., Morgantown, WV, USA","IEEE Transactions on Software Engineering","","2003","29","5","474","477","Numerous discrepancies exist between expert opinion and empirical data reported in Morisio et al.'s recent TSE article. The differences related to what factors encouraged successful reuse in software organizations. This note describes how those differences were detected and comments on their methodological implications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199076","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199076","","Data mining;Decision trees;Project management;Data analysis;Machine learning;Failure analysis;Stress;Web sites;Association rules","software reusability","software reuse;machine learning;software organizations","","15","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Design of dynamically reconfigurable real-time software using port-based objects","D. B. Stewart; R. A. Volpe; P. K. Khosla","Dept. of Electr. Eng., Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","12","759","776","The port-based object is a new software abstraction for designing and implementing dynamically reconfigurable real-time software. It forms the basis of a programming model that uses domain-specific elemental units to provide specific, yet flexible, guidelines to control engineers for creating and integrating software components. We use a port-based object abstraction, based on combining the notion of an object with the port-automaton algebraic model of concurrent processes. It is supported by an implementation using domain-specific communication mechanisms and templates that have been incorporated into the Chimera real-time operating system and applied to several robotic applications. This paper describes the port-based object abstraction, provides a detailed analysis of communication and synchronization based on distributed shared memory, and describes a programming paradigm based on a framework process and code templates for quickly implementing applications.","0098-5589;1939-3520;2326-3881","","10.1109/32.637390","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637390","","Real time systems;Software design;Operating systems;Application software;Robot programming;Control systems;Robot sensing systems;Guidelines;Laboratories;Communication system software","software reusability;software portability;real-time systems;object-oriented methods;operating systems (computers);synchronisation;shared memory systems;distributed memory systems;robot programming","dynamically reconfigurable real-time software;port-based objects;software abstraction;programming model;domain-specific units;control engineers;software components;port-based object abstraction;port-automaton algebraic model;concurrent processes;Chimera;real-time operating system;robotic applications;synchronization;distributed shared memory;code templates;software reuse","","132","","43","","","","","","IEEE","IEEE Journals & Magazines"
"An approach to the reliability optimization of software with redundancy","F. Belli; P. Jedrzejowicz","Dept. of Electr. & Electron. Eng., Paderborn Univ., Germany; NA","IEEE Transactions on Software Engineering","","1991","17","3","310","312","An approach to the optimization of software reliability is proposed. The emphasis is put on the software redundancy to achieve fault tolerance, i.e. the results of the optimization process are used to determine the optimal structure of the software to be developed. Two optimization models are formulated covering, respectively, modified recovery block scheme and multiversion programming approaches. Both cases are illustrated by simple examples. The models show that it is possible to formulate and solve some software related reliability optimization problems. They further show that the concept of redundancy to achieve fault tolerance (basic for the traditional theory of reliability) can be used in the field of software reliability optimization.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.75419","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=75419","","Redundancy;Software reliability;Power system modeling;Reliability theory;Software systems;Predictive models;Software testing;Power system reliability;Application software;Mathematical model","fault tolerant computing;optimisation;redundancy;software reliability","reliability optimization;software;redundancy;software reliability;fault tolerance;modified recovery block scheme;multiversion programming","","32","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Corrigenda: software size estimation of object-oriented systems","B. Henderson-Sellers","Sch. of Inf. Technol., Swinburne Univ. of Technol., Hawthorn, Vic., Australia","IEEE Transactions on Software Engineering","","1997","23","4","260","261","In an interesting paper, L.A. Laranjeira (see ibid., vol.6, no.5, p.510-22, 1990) describes a first attempt to understand cost estimation within an object oriented environment. While the presented approach presents many interesting and useful ideas, it is, unfortunately, marred by several mathematical errors pertaining to statistics, exponential functions, and the nature of discrete vs. continuous data. These are discussed here and more appropriate correct procedures outlined.","0098-5589;1939-3520;2326-3881","","10.1109/32.588544","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=588544","","Equations;Costs;Mercury (metals);Error analysis;Mathematics;Statistics;Object oriented modeling;Mathematical model;State estimation;Convergence","software cost estimation;object-oriented programming;statistics","software size estimation;object oriented systems;cost estimation;object oriented environment;mathematical errors;statistics;exponential functions;continuous data","","4","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Clarification of two phase locking in concurrent transaction processing","P. -. Leu; B. Bhargava","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA","IEEE Transactions on Software Engineering","","1988","14","1","122","125","The authors propose a formal definition of the two-phase locking class derived from the semantic description of the two-phase locking protocol, and prove that this definition is equivalent to that given by C.H. Papadimitriou (1979). They present: (1) a precise definition of the two phase locking; (2) a clarification of the occurrence and the order of all events such as lock points, unlock points, read operations, and write operations of conflicting transactions; and (3) by relaxing some conditions in the given definition, the derivation of a new class called restricted-non-two-phase locking (RN2PL), which is a superset of the class two-phase locking (2PL) but a subset of the class D-serializable (DSR) given by Papadimitriou.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4629","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4629","","Transaction databases;Concurrency control;Access protocols","database theory;distributed databases;parallel programming;protocols","two phase locking;concurrent transaction processing;semantic description;locking protocol;lock points;unlock points;read operations;write operations;conflicting transactions;RN2PL;D-serializable","","3","","8","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical comparison of software fault tolerance and fault elimination","T. J. Shimeall; N. G. Leveson","Dept. of Comput. Sci., US Naval Postgraduate Sch., Monterey, CA, USA; NA","IEEE Transactions on Software Engineering","","1991","17","2","173","182","The authors compared two major approaches to the improvement of software-software fault elimination and software fault tolerance-by examination of the fault detection (and tolerance, where applicable) of five techniques: run-time assertions, multiversion voting, functional testing augmented by structural testing, code reading by stepwise abstraction, and static data-flow analysis. The focus was on characterizing the sets of faults detected by the techniques and on characterizing the relationships between these sets of faults. Two categories of questions were investigated: (1) comparison between fault elimination and fault tolerance techniques and (2) comparisons among various testing techniques. The results provide information useful for making decisions about the allocation of project resources, show strengths and weaknesses of the techniques studies, and indicate directions for future research.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67598","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67598","","Fault tolerance;Fault detection;Software testing;Software systems;Runtime;Data analysis;Computer science;Fault tolerant systems;Performance evaluation;Software performance","fault tolerant computing;program testing;software reliability","software reliability;software fault tolerance;fault elimination;run-time assertions;multiversion voting;functional testing;structural testing;code reading;stepwise abstraction;static data-flow analysis;project resources","","27","","29","","","","","","IEEE","IEEE Journals & Magazines"
"The Compositional Security Checker: a tool for the verification of information flow security properties","R. Focardi; R. Gorrieri","Dipt. di Matematica Appl. e Inf, Univ. ca Foscari di Venezia, Mestre, Italy; NA","IEEE Transactions on Software Engineering","","1997","23","9","550","571","The Compositional Security Checker (CoSeC for short) is a semantic-based tool for the automatic verification of some compositional information flow properties. The specifications given as inputs to CoSeC are terms of the Security Process Algebra, a language suited for the specification of concurrent systems where actions belong to two different levels of confidentiality. The information flow security properties which can be verified by CoSeC are some of those classified in (Focardi and Gorrieri, 1994). They are derived from some classic notions, e.g., noninterference. The tool is based on the same architecture as the Concurrency Workbench, from which some modules have been imported unchanged. The usefulness of the tool is tested with the significant case-study of an access-monitor, presented in several versions in order to illustrate the relative merits of the various information flow properties that CoSeC can check. Finally, we present an application in the area of network security: we show that the theory (and the tool) can be reasonably applied also for singling out security flaws in a simple, yet paradigmatic, communication protocol.","0098-5589;1939-3520;2326-3881","","10.1109/32.629493","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=629493","","Information security;Access control;Invasive software;Access protocols;Communication system control;Control systems;Computer security;Multilevel systems;Algebra;Concurrent computing","security of data;program verification;algebraic specification;specification languages;process algebra;data privacy;computer networks;protocols;authorisation","Compositional Security Checker;verification tool;information flow security properties;CoSeC;semantic-based tool;specifications;Security Process Algebra;specification language;concurrent systems;data confidentiality;noninterference;Concurrency Workbench;access-monitor;network security;communication protocol","","94","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Subtypes for specifications: predicate subtyping in PVS","J. Rushby; S. Owre; N. Shankar","Comput. Sci. Lab., SRI Int., Menlo Park, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","9","709","720","A specification language used in the context of an effective theorem prover can provide novel features that enhance precision and expressiveness. In particular, type checking for the language can exploit the services of the theorem prover. We describe a feature called ""predicate subtyping"" that uses this capability and illustrate its utility as mechanized in PVS.","0098-5589;1939-3520;2326-3881","","10.1109/32.713327","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=713327","","Specification languages;Computer languages;Logic;Java;Security;Set theory","formal specification;type theory;specification languages;theorem proving","formal specifications;predicate subtyping;PVS;specification language;theorem prover;type checking","","42","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Testability transformation","M. Harman; L. Hu; R. Hierons; J. Wegener; H. Sthamer; A. Baresel; M. Roper","Dept. of Inf. Syst. & Comput., Brunel Univ., Uxbridge, UK; Dept. of Inf. Syst. & Comput., Brunel Univ., Uxbridge, UK; Dept. of Inf. Syst. & Comput., Brunel Univ., Uxbridge, UK; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","1","3","16","A testability transformation is a source-to-source transformation that aims to improve the ability of a given test generation method to generate test data for the original program. We introduce testability transformation, demonstrating that it differs from traditional transformation, both theoretically and practically, while still allowing many traditional transformation rules to be applied. We illustrate the theory of testability transformation with an example application to evolutionary testing. An algorithm for flag removal is defined and results are presented from an empirical study which show how the algorithm improves both the performance of evolutionary test data generation and the adequacy level of the test data so-generated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265732","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265732","","Automatic testing;Application software;Software testing;Computer Society;Software engineering;Vehicles;Impedance;Information systems;Poles and towers","automatic test pattern generation;program testing;evolutionary computation","testability transformation;automated test data generation;evolutionary testing;flag removal algorithm;evolutionary test data generation;search-based software engineering","","114","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Hierarchical simulation approach to accurate fault modeling for system dependability evaluation","Z. Kalbarczyk; R. K. Iyer; G. L. Ries; J. U. Patel; M. S. Lee; Y. Xiao","Center for Reliable & High Performance Comput., Illinois Univ., Urbana, IL, USA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1999","25","5","619","632","This paper presents a hierarchical simulation methodology that enables accurate system evaluation under realistic faults and conditions. In this methodology, effects of low-level (i.e., transistor or circuit level) faults are propagated to higher levels (i.e., system level) using fault dictionaries. The primary fault models are obtained via simulation of the transistor-level effect of a radiation particle penetrating a device. The resulting current bursts constitute the first-level fault dictionary and are used in the circuit-level simulation to determine the impact on circuit latches and flip-flops. The latched outputs constitute the next level fault dictionary in the hierarchy and are applied in conducting fault injection simulation at the chip-level under selected workloads or application programs. Faults injected at the chip-level result in memory corruptions, which are used to form the next level fault dictionary for the system-level simulation of an application running on simulated hardware. When an application terminates, either normally or abnormally, the overall fault impact on the software behavior is quantified and analyzed. The system in this sense can be a single workstation or a network. The simulation method is demonstrated and validated in the case study of Myrinet (a commercial, high-speed network) based network system.","0098-5589;1939-3520;2326-3881","","10.1109/32.815322","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=815322","","Circuit faults;Dictionaries;Circuit simulation;Application software;Analytical models;Power system modeling;Voltage;Space technology;Digital circuits;Latches","circuit simulation;virtual machines;fault tolerant computing;flip-flops","hierarchical simulation;fault modeling;system dependability evaluation;fault dictionaries;transistor-level effect;radiation particle;circuit-level simulation;circuit latches;flip-flops;fault injection simulation;memory corruptions;Myrinet","","25","","17","","","","","","IEEE","IEEE Journals & Magazines"
"The role of inspection in software quality assurance","D. L. Parnas; M. Lawford","Software Quality Res. Lab., Limerick Univ., Ireland; NA","IEEE Transactions on Software Engineering","","2003","29","8","674","676","Due to the complexity of the code, software is released with many errors. In response, both software practitioners and software researchers need to improve the reputation of the software. Inspection is the only way to improve the quality of software. Inspection methods can be more effective but success depends on having a sound and systematic procedure for conducting the inspection. The Workshop on Inspection in Software Engineering (WISE), a satellite event of the 2001 Computer Aided Verification (CAV '01) Conference, brought together researchers, practitioners, and regulators in the hope of finding effective approaches to software inspection. The workshop included invited lectures and paper presentations in the form of panel discussions on all aspects of software inspection. Submissions explained how practitioners and researchers were performing inspections, discussed the relevance of inspections, provided evidence of how inspections could be improved through refinement of the inspection process and computer aided tool support and explained how careful design of software could make inspections easier or more effective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223642","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223642","","Inspection;Software quality;Computer bugs;Software debugging;Software engineering;Satellites;Regulators;Software design;Software performance;Software tools","software quality;software development management;inspection;software process improvement;computer aided software engineering;program verification","software quality assurance;software process improvement;software inspection method;software practitioner;software researcher;Workshop on Inspection in Software Engineering;WISE;computer aided verification;CAV conference;computer aided tool support;software design","","13","","","","","","","","IEEE","IEEE Journals & Magazines"
"Corrections to ""the effectiveness of control structure diagrams in source code comprehension activities""","D. Hendrix; J. H. Cross; S. Maghsoodloo","Auburn University; NA; NA","IEEE Transactions on Software Engineering","","2002","28","6","624","624","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010064","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010064","","Java;Senior members;Back;Visualization;Computer science;Software engineering;Systems engineering and theory","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"IASTED conference on reliability and quality control","B. Dhillon","Department of Mechanical Engineering, University of Ottawa, Ottawa, Ont. K1N 6N5, Canada","IEEE Transactions on Software Engineering","","1986","SE-12","11","1104","1104","THE International Association of Science and Technology for Development (IASTED) Conference on Reliability and Quality Control is to be held at the Palais des Congres in Paris, France.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313003","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313003","","Quality control;Analytical models;Predictive models;Software;Power system reliability;Software reliability","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Interinput and Interoutput Time Distribution in Classical Product-Form Networks","J. Y. Le Boudec","INSA Rennes","IEEE Transactions on Software Engineering","","1987","SE-13","6","756","759","It is shown that, in a general Jackson network of queues, the distribution of interinput and interoutput times are not equal. Equality holds in reversible classical product-form networks with or without blocking. A sufficient condition for reversibility is given.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233481","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702281","Flow-distributions;queueing systems","Intelligent networks;Software testing;Software reliability;Sufficient conditions;Software design;Art;Data analysis;Computer languages;Software measurement","","Flow-distributions;queueing systems","","","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Detecting Arbitrary Stable Properties Using Efficient Snapshots","A. Kshemkalyani; B. Wu","Department of Computer Science, University of Illinois at Chicago, 851 South Morgan Street, Chicago, IL 60607; Department of Computer Science, University of Illinois at Chicago, 851 South Morgan Street, Chicago, IL 60607","IEEE Transactions on Software Engineering","","2007","33","5","330","346","A stable properly continues to hold in an execution once it becomes true. Detecting arbitrary stable properties efficiently in distributed executions is still an open problem. The known algorithms for detecting arbitrary stable properties and snapshot algorithms used to detect such stable properties suffer from drawbacks such as the following: They incur the overhead of a large number of messages per global snapshot, or alter application message headers, or use inhibition, or use the execution history, or assume a strong property such as causal delivery of messages in the system. We solve the problem of detecting an arbitrary stable property efficiently under the following assumptions: P1) the application messages should not be modified, not even by timestamps or message coloring. P2) no inhibition is allowed. P3) the algorithm should not use the message history. P4) any process can initiate the algorithm. This paper proposes a family of nonintrusive algorithms requiring 6(n - 1) control messages, where n is the number of processes. A three-phase strategy of uncoordinated observation of local states is used to give a consistent snapshot from which any stable property can be detected. A key feature of our algorithms is that they do not rely on the processes continually and pessimistically reporting their activity. Only the relevant activity that occurs in the thin slice during the algorithm execution needs to be examined.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1000","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160971","Distributed system;global state;stable predicate;stable property;distributed snapshot.","System recovery;History;Computer vision;Joining processes","distributed processing","distributed system snapshot;stable predicate;arbitrary stable property;nonintrusive algorithm","","10","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Variations on a Method for Representing Data Items of Unlimited Length","F. Luccio","Dipartimento di Informatica, University of Pisa","IEEE Transactions on Software Engineering","","1985","SE-11","4","439","441","A recent encoding method [11 is revisited here, and two simple variations are proposed to reduce the length of the encoded data strings. The new variations also decrease encoding time, and allow numerical data to be represented in any base.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232233","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702026","Data length specification;decoding numbers;encoding numbers;storing data in memory;unlimited length data;variable length data","Encoding;Decoding;Costs;Information retrieval","","Data length specification;decoding numbers;encoding numbers;storing data in memory;unlimited length data;variable length data","","","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Semantics-Based Design for Secure Web Services","M. Bartoletti; P. Degano; G. Ferrari; R. Zunino","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","33","49","We outline a methodology for designing and composing services in a secure manner. In particular, we are concerned with safety properties of service behavior. Services can enforce security policies locally and can invoke other services that respect given security contracts. This call-by-contract mechanism offers a significant set of opportunities, each driving secure ways to compose services. We discuss how we can correctly plan service compositions in several relevant classes of services and security properties. With this aim, we propose a graphical modeling framework based on a foundational calculus called lambda <sup>req</sup> [13]. Our formalism features dynamic and static semantics, thus allowing for formal reasoning about systems. Static analysis and model checking techniques provide the designer with useful information to assess and fix possible vulnerabilities.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70740","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359467","Web services;call-by-contract;language-based security;static analysis;system verification;Web services;call-by-contract;language-based security;static analysis;system verification","Web services;Contracts;History;Information security;Calculus;Distributed computing;Design methodology;Safety;Information analysis;Computer networks","security of data;Web services","semantics-based design;secure Web services;security policies;call-by-contract mechanism;graphical modeling framework;foundational calculus;static semantics;formal reasoning;static analysis;model checking techniques","","36","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Analogy-X: Providing Statistical Inference to Analogy-Based Software Cost Estimation","J. W. Keung; B. A. Kitchenham; D. R. Jeffery","National ICT Australia Ltd. The University of New South Wales, Sydney; National ICT Australia, Sydney; National ICT Australia Ltd. The University of New South Wales, Sydney","IEEE Transactions on Software Engineering","","2008","34","4","471","484","Data-intensive analogy has been proposed as a means of software cost estimation as an alternative to other data intensive methods such as linear regression. Unfortunately, there are drawbacks to the method. There is no mechanism to assess its appropriateness for a specific dataset. In addition, heuristic algorithms are necessary to select the best set of variables and identify abnormal project cases. We introduce a solution to these problems based upon the use of the Mantel correlation randomization test called Analogy-X. We use the strength of correlation between the distance matrix of project features and the distance matrix of known effort values of the dataset. The method is demonstrated using the Desharnais dataset and two random datasets, showing (1) the use of Mantel's correlation to identify whether analogy is appropriate, (2) a stepwise procedure for feature selection, as well as (3) the use of a leverage statistic for sensitivity analysis that detects abnormal data points. Analogy-X, thus, provides a sound statistical basis for analogy, removes the need for heuristic search and greatly improves its algorithmic performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527255","Cost estimation;Management;Statistical methods;Software Engineering;Cost estimation;Management;Statistical methods;Software Engineering","Costs;Sensitivity analysis;Linear regression;Heuristic algorithms;Software algorithms;Input variables;Australia;Computer Society;Testing;Statistical analysis","correlation methods;matrix algebra;software cost estimation;software engineering;statistical analysis","statistical inference;analogy-based software cost estimation;data-intensive analogy;data intensive methods;linear regression;heuristic algorithms;Mantel correlation randomization test;Analogy-X;distance matrix;Desharnais dataset;sensitivity analysis;heuristic search","","61","","37","","","","","","IEEE","IEEE Journals & Magazines"
"On the design of a single-key-lock mechanism based on Newton's interpolating polynomial","Chi-Sung Laih; Lein Harn; Jau-Yien Lee","Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","IEEE Transactions on Software Engineering","","1989","15","9","1135","1137","A single-key-lock (SKL) mechanism used for implementing the access matrix of a computer protection system is proposed. The key selection is very flexible. The lock values are generated recursively using the Newton interpolating polynomial. A new user/file can be inserted into the system without recomputing all locks/keys. Since the computational load of the key-lock operation depends on the key positions in the access matrix, a user-hierarchy structure can be constructed for the mechanism. Thus, the smallest key value is assigned to a user who accesses the information resources more frequently than others, in order to reduce the average computation time. An example is included to illustrate this idea.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31371","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31371","","Polynomials;Protection;Interpolation;Information resources;National security;Councils;Computer science;Cities and towns;Telecommunication computing;Cryptography","interpolation;polynomials;security of data","recursive generation;single-key-lock mechanism;access matrix;computer protection system;key selection;lock values;Newton interpolating polynomial;user/file;computational load;key positions;user-hierarchy structure;information resources","","14","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Cryptographic verification of test coverage claims","P. T. Devanbu; S. G. Stubblebine","Dept. of Comput. Sci., California Univ., Davis, CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","2","178","192","The market for software components is growing, driven on the ""demand side"" by the need for rapid deployment of highly functional products and, on the ""supply side"", by distributed object standards. As components and component vendors proliferate, there is naturally a growing concern about quality and the effectiveness of testing processes. White-box testing, particularly the use of coverage criteria, Is a widely used method for measuring the ""thoroughness"" of testing efforts. High levels of test coverage are used as indicators of good quality control procedures. Software vendors who can demonstrate high levels of test coverage have a credible claim to high quality. However, verifying such claims involves knowledge of the source code, test cases, build procedures, etc. In applications where reliability and quality are critical, it would be desirable to verify test coverage claims without forcing vendors to give up valuable technical secrets. In this paper, we explore cryptographic techniques that can be used to verify such claims. Our techniques have certain limitations, which we discuss in this paper. However, vendors who have done the hard work of developing high levels of test coverage can use these techniques (for a modest additional cost) to provide credible evidence of high coverage, while simultaneously reducing disclosure of intellectual property.","0098-5589;1939-3520;2326-3881","","10.1109/32.841116","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=841116","","Cryptography;Costs;Application software;Software testing;Software standards;Intellectual property;Particle measurements;Quality control;Software quality;Software safety","program testing;formal verification;quality control;software quality;cryptography;safety-critical software;industrial property","software components;test coverage claims;cryptographic verification;distributed object standards;white-box testing;quality control procedures;software vendors;reliability;intellectual property","","5","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to “a rational design process: How and why to fake it”","D. L. Parnas; P. C. Clements","Department of Computer Science, University of Victoria, Victoria, B.C. V8W 2Y2, Canada; Computer Science and Systems Branch, Naval Research Laboratory, Washington, DC 20375; Computer Science and Systems Branch, Naval Research Laboratory, Washington, DC 20375","IEEE Transactions on Software Engineering","","1986","SE-12","8","874","874","A careful reader, Max Stern of Teradata Corporation, has brought to our attention two errors in the above paper.<sup>1</sup>On page 253 in the second paragraph under point 4) the text reads, “A purely digital or purely hybrid computer is a special case of this general module.” It should read, “A purely digital or purely analog computer is a special case of this general model.”","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312991","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312991","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Language design for program manipulation""","W. G. Griswold","Dept. of Comput. Sci. & Eng., California Univ., San Diego, La Jolla, CA, USA","IEEE Transactions on Software Engineering","","1994","20","3","218","219","The paper by E.A.T. Merks et al. (see ibid., vol. 18, p. 19-32, 1992) ""Language design for program manipulation"" identifies design principles for a procedural or object-oriented language whose programs will be easier to manipulate. However, it neglects to relate these design principles to existing, broader, design principles, and in some instances omits good examples of languages meeting their criteria. The author relates the new principles to more fundamental design principles, and provides the needed examples of languages meeting their criteria. Together these additions can better help designers of new programming languages that are amenable to manipulation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.268924","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=268924","","Programming profession;Concrete;Computer languages;Software tools;Buildings;Computer science","object-oriented languages;programming;programming theory","language design;program manipulation;design principles;object-oriented language;new programming languages;procedural language;syntax;semantics","","1","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Foreword Technical Conferences: A Window to the Future","N. L. Marselos","NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","830","831","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232541","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702101","","Application software;Manufacturing;Software maintenance;Telecommunication computing;Hardware;Computer applications;Explosives;Computer science;Statistics;Reflection","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of API Change- and Fault-Proneness on the User Ratings of Android Apps","G. Bavota; M. Linares-Vásquez; C. E. Bernal-Cárdenas; M. D. Penta; R. Oliveto; D. Poshyvanyk","Department of Computer Science, Free University of Bozen-Bolzano, Bolzano, Italy; Department of Computer Science, The College of William and Mary, Williamsburg, VA; Department of Computer Science, The College of William and Mary, Williamsburg, VA; Department of Engineering, University of Sannio, Benevento, Italy; Department of Bioscience and Territory, University of Molise, Pesche (IS), Italy; Department of Computer Science, The College of William and Mary, Williamsburg, VA","IEEE Transactions on Software Engineering","","2015","41","4","384","407","The mobile apps market is one of the fastest growing areas in the information technology. In digging their market share, developers must pay attention to building robust and reliable apps. In fact, users easily get frustrated by repeated failures, crashes, and other bugs; hence, they abandon some apps in favor of their competition. In this paper we investigate how the fault- and change-proneness of APIs used by Android apps relates to their success estimated as the average rating provided by the users to those apps. First, in a study conducted on 5,848 (free) apps, we analyzed how the ratings that an app had received correlated with the fault- and change-proneness of the APIs such app relied upon. After that, we surveyed 45 professional Android developers to assess (i) to what extent developers experienced problems when using APIs, and (ii) how much they felt these problems could be the cause for unfavorable user ratings. The results of our studies indicate that apps having high user ratings use APIs that are less fault- and change-prone than the APIs used by low rated apps. Also, most of the interviewed Android developers observed, in their development experience, a direct relationship between problems experienced with the adopted APIs and the users' ratings that their apps received.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2367027","NSF; NSF; NSF; European Commission; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6945855","Mining software repositories;empirical studies;android;API changes;Mining software repositories;empirical studies;android;API changes","Androids;Humanoid robots;Software;History;Computer bugs;Educational institutions;Electronic mail","application program interfaces;data mining;mobile computing;program debugging;software fault tolerance;system recovery","API change-proneness;API fault-proneness;user ratings;Android Apps;mobile Apps market;information technology;software repository mining","","49","","70","","","","","","IEEE","IEEE Journals & Magazines"
"Modular verification of software components in C","S. Chaki; E. M. Clarke; A. Groce; S. Jha; H. Veith","Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","6","388","402","We present a new methodology for automatic verification of C programs against finite state machine specifications. Our approach is compositional, naturally enabling us to decompose the verification of large software systems into subproblems of manageable complexity. The decomposition reflects the modularity in the software design. We use weak simulation as the notion of conformance between the program and its specification. Following the counterexample guided abstraction refinement (CEGAR) paradigm, our tool MAGIC first extracts a finite model from C source code using predicate abstraction and theorem proving. Subsequently, weak simulation is checked via a reduction to Boolean satisfiability. MAGIC has been interfaced with several publicly available theorem provers and SAT solvers. We report experimental results with procedures from the Linux kernel, the OpenSSL toolkit, and several industrial strength benchmarks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321061","Software engineering;formal methods;verification.","Software engineering;Programming;Protocols;Automata;Software systems;Software design;Linux;Kernel;Unified modeling language;Visualization","formal specification;program verification;C language;finite state machines;theorem proving;computability;Boolean algebra","modular verification;software component;C program;finite state machine specification;software design;counterexample guided abstraction refinement;predicate abstraction;theorem proving;Boolean satisfiability;Linux kernel;software engineering;formal method","","74","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Current trends in exception handling","D. E. Perry; A. Romanovsky; A. Tripathi","University of Texas; NA; NA","IEEE Transactions on Software Engineering","","2000","26","10","921","922","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.879816","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879816","","Computer languages;Fault tolerant systems;Spreadsheet programs;Humans;Process design;Algorithm design and analysis;Computational modeling;Large-scale systems;Maintenance","","","","9","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Assignment and scheduling communicating periodic tasks in distributed real-time systems","Dar-Tzen Peng; K. G. Shin; T. F. Abdelzaher","Dept. of Electr. Eng. & Comput. Sci., Michigan Univ., Ann Arbor, MI, USA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","12","745","758","Presents an optimal solution to the problem of allocating communicating periodic tasks to heterogeneous processing nodes (PNs) in a distributed real-time system. The solution is optimal in the sense of minimizing the maximum normalized task response time, called the system hazard, subject to the precedence constraints resulting from intercommunication among the tasks to be allocated. Minimization of the system hazard ensures that the solution algorithm allocates tasks so as to meet all task deadlines under an optimal schedule, whenever such an allocation exists. The task system is modeled with a task graph (TG), in which computation and communication modules, communication delays and intertask precedence constraints are clearly described. Tasks described by this TG are assigned to PNs by using a branch-and-bound (B&B) search algorithm. The algorithm traverses a search tree whose leaves correspond to potential solutions to the task allocation problem. We use a bounding method that prunes, in polynomial time, nonleaf vertices that cannot lead to an optimal solution, while ensuring that the search path leading to an optimal solution will never be pruned. For each generated leaf vertex, we compute the exact cost using the algorithm developed by Peng and Shin (1993). The lowest-cost leaf vertex (one with the least system hazard) represents an optimal task allocation. Computational experiences and examples are provided to demonstrate the concept, utility and power of the proposed approach.","0098-5589;1939-3520;2326-3881","","10.1109/32.637388","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637388","","Real time systems;Delay;Hazards;Optimal scheduling;Costs;Processor scheduling;Computer Society;Minimization methods;Scheduling algorithm;Tree graphs","distributed processing;real-time systems;processor scheduling;resource allocation;minimisation;tree searching;delays","communicating periodic tasks;task assignment;task scheduling;distributed real-time systems;heterogeneous processing nodes;maximum normalized task response time minimization;system hazard;branch-and-bound search algorithm;task intercommunication;task deadlines;optimal schedule;task graph;computation modules;communication modules;communication delays;intertask precedence constraints;search tree traversal;bounding method;polynomial time algorithm;nonleaf vertex pruning;exact cost computation;lowest-cost leaf vertex","","73","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Conservation of Information: Software’sHidden Clockwork?","L. Hatton","Faculty of Science, Engineering and Computing, Kingston University, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","5","450","460","In this paper it is proposed that the Conservation of Hartley-Shannon Information (hereafter contracted to H-S Information) plays the same role in discrete systems as the Conservation of Energy does in physical systems. In particular, using a variational approach, it is shown that the symmetry of scale-invariance, power-laws and the Conservation of H-S Information are intimately related and lead to the prediction that the component sizes of any software system assembled from components made from discrete tokens always asymptote to a scale-free power-law distribution in the unique alphabet of tokens used to construct each component. This is then validated to a very high degree of significance on some 100 million lines of software in seven different programming languages independently of how the software was produced, what it does, who produced it or what stage of maturity it has reached. A further implication of the theory presented here is that the average size of components depends only on their unique alphabet, independently of the package they appear in. This too is demonstrated on the main data set and also on 24 additional Fortran 90 packages.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2316158","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6784340","Information conservation;component size distribution;power-law;software systems","Software systems;Computer languages;Genomics;Bioinformatics;Genetic communication","information theory;programming languages;software engineering","software hidden clockwork;Hartley-Shannon information conservation;physical systems;discrete systems;energy conservation;variational approach;scale-invariance symmetry;H-S information conservation;software system;scale-free power-law distribution;discrete tokens;Fortran","","3","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Handling obstacles in goal-oriented requirements engineering","A. van Lamsweerde; E. Letier","Dept. d'Ingenierie Inf., Univ. Catholique de Louvain, Belgium; NA","IEEE Transactions on Software Engineering","","2000","26","10","978","1005","Requirements engineering is concerned with the elicitation of high-level goals to be achieved by the envisioned system, the refinement of such goals and their operationalization into specifications of services and constraints and the assignment of responsibilities for the resulting requirements to agents such as humans, devices and software. Requirements engineering processes often result in goals, requirements, and assumptions about agent behavior that are too ideal; some of them are likely not to be satisfied from time to time in the running system due to unexpected agent behavior. The lack of anticipation of exceptional behaviors results in unrealistic, unachievable, and/or incomplete requirements. As a consequence, the software developed from those requirements will not be robust enough and will inevitably result in poor performance or failures, sometimes with critical consequences on the environment. This paper presents formal techniques for reasoning about obstacles to the satisfaction of goals, requirements, and assumptions elaborated in the requirements engineering process. The techniques are based on a temporal logic formalization of goals and domain properties; they are integrated into an existing method for goal-oriented requirements elaboration with the aim of deriving more realistic, complete, and robust requirements specifications. A key principle is to handle exceptions at requirements engineering time and at the goal level, so that more freedom is left for resolving them in a satisfactory way. The various techniques proposed are illustrated and assessed in the context of a real safety-critical system.","0098-5589;1939-3520;2326-3881","","10.1109/32.879820","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879820","","Humans;Robustness;Software systems;Software performance;Logic;Software engineering;Stress;Face;Dispatching","formal specification;systems analysis;temporal logic;safety-critical software;exception handling","goal-oriented requirements engineering;formal specification;unexpected agent behavior;software performance;software failure;temporal logic;requirements specifications;exception handling;safety-critical system","","272","","88","","","","","","IEEE","IEEE Journals & Magazines"
"A parsing methodology for the implementation of visual systems","G. Costagliola; A. De Lucia; S. Orefice; G. Tortora","Dipt. di Inf. ed Applicazioni, Salerno Univ., Italy; NA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","12","777","799","The Visual Language Compiler-Compiler (VLCC) is a grammar-based graphical system for the automatic generation of visual programming environments. In this paper the theoretical and algorithmic issues of VLCC are discussed in detail. The parsing methodology we present is based on the ""positional grammar"" model. Positional grammars naturally extend context-free grammars by considering new relations in addition to string concatenation. Thanks to this, most of the results from LR parsing can be extended to the positional grammars inheriting the well known LR technique efficiency. In particular, we provide algorithms to implement a YACC-like tool embedded in the VLCC system for automatic compiler generation of visual languages described by positional grammars.","0098-5589;1939-3520;2326-3881","","10.1109/32.637392","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637392","","Production;Visual system;Programming environments;Program processors;Multidimensional systems;Flowcharts;Performance analysis","visual programming;programming environments;compiler generators;visual languages;grammars;software tools","parsing methodology;visual systems;Visual Language Compiler-Compiler;VLCC;grammar-based graphical system;visual programming environment generation;positional grammar;context-free grammars;string concatenation;LR parsing;YACC-like tool;automatic compiler generation;visual languages;multidimensional grammars;flow graph languages","","44","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Test Case Generation as a Many-Objective Optimisation Problem with Dynamic Selection of the Targets","A. Panichella; F. M. Kifetew; P. Tonella","SnT, University of Luxembourg, Luxembourg, Esch-sur-Alzette, Luxembourg; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy","IEEE Transactions on Software Engineering","","2018","44","2","122","158","The test case generation is intrinsically a multi-objective problem, since the goal is covering multiple test targets (e.g., branches). Existing search-based approaches either consider one target at a time or aggregate all targets into a single fitness function (whole-suite approach). Multi and many-objective optimisation algorithms (MOAs) have never been applied to this problem, because existing algorithms do not scale to the number of coverage objectives that are typically found in real-world software. In addition, the final goal for MOAs is to find alternative trade-off solutions in the objective space, while in test generation the interesting solutions are only those test cases covering one or more uncovered targets. In this paper, we present Dynamic Many-Objective Sorting Algorithm (DynaMOSA), a novel many-objective solver specifically designed to address the test case generation problem in the context of coverage testing. DynaMOSA extends our previous many-objective technique Many-Objective Sorting Algorithm (MOSA) with dynamic selection of the coverage targets based on the control dependency hierarchy. Such extension makes the approach more effective and efficient in case of limited search budget. We carried out an empirical study on 346 Java classes using three coverage criteria (i.e., statement, branch, and strong mutation coverage) to assess the performance of DynaMOSA with respect to the whole-suite approach (WS), its archive-based variant (WSA) and MOSA. The results show that DynaMOSA outperforms WSA in 28 percent of the classes for branch coverage (+8 percent more coverage on average) and in 27 percent of the classes for mutation coverage (+11 percent more killed mutants on average). It outperforms WS in 51 percent of the classes for statement coverage, leading to +11 percent more coverage on average. Moreover, DynaMOSA outperforms its predecessor MOSA for all the three coverage criteria in 19 percent of the classes with +8 percent more code coverage on average.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2663435","National Research Fund; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7840029","Evolutionary testing;many-objective optimisation;automatic test case generation","Heuristic algorithms;Optimization;Testing;Software algorithms;Algorithm design and analysis;Sorting;Genetic algorithms","optimisation;program testing;search problems;sorting","dynamic target selection;Many-Objective Sorting Algorithm;search-based approaches;test sequence;test input data;branch coverage;many-objective solver;MOAs;many-objective optimisation algorithms;multiple test targets;multiobjective problem;Many-Objective optimisation problem;automated test case generation;DynaMOSA","","8","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Reasoning about the Reliability of Diverse Two-Channel Systems in Which One Channel Is ""Possibly Perfect""","B. Littlewood; J. Rushby","City University, London; SRI International, Menlo Park","IEEE Transactions on Software Engineering","","2012","38","5","1178","1194","This paper refines and extends an earlier one by the first author [1]. It considers the problem of reasoning about the reliability of fault-tolerant systems with two “channels” (i.e., components) of which one, A, because it is conventionally engineered and presumed to contain faults, supports only a claim of reliability, while the other, B, by virtue of extreme simplicity and extensive analysis, supports a plausible claim of “perfection.” We begin with the case where either channel can bring the system to a safe state. The reasoning about system probability of failure on demand (pfd) is divided into two steps. The first concerns aleatory uncertainty about 1) whether channel A will fail on a randomly selected demand and 2) whether channel B is imperfect. It is shown that, conditional upon knowing p<sub>A</sub>(the probability that A fails on a randomly selected demand) and p<sub>B</sub>(the probability that channel B is imperfect), a conservative bound on the probability that the system fails on a randomly selected demand is simply p<sub>A</sub>X p<sub>B</sub>. That is, there is conditional independence between the events “A fails” and “B is imperfect.” The second step of the reasoning involves epistemic uncertainty, represented by assessors' beliefs about the distribution of (p<sub>A</sub>, p<sub>B</sub>), and it is here that dependence may arise. However, we show that under quite plausible assumptions, a conservative bound on system pfd can be constructed from point estimates for just three parameters. We discuss the feasibility of establishing credible estimates for these parameters. We extend our analysis from faults of omission to those of commission, and then combine these to yield an analysis for monitored architectures of a kind proposed for aircraft.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.80","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975177","Software reliability;software fault tolerance;program correctness;assurance case;software diversity","Uncertainty;Software;Phase frequency detector;Cognition;Software reliability;Safety","aircraft;probability;reasoning about programs;software fault tolerance;uncertainty handling","diverse two-channel system;fault tolerant system;reasoning about the reliability;aleatory uncertainty;randomly selected demand;conditional independence;epistemic uncertainty;assessors belief;PFD;aircraft;probability of failure on demand","","9","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Comment on ""A pre-run-time scheduling algorithm for hard real-time systems""","T. F. Abdelzaher; K. G. Shin","Real-Time Comput. Lab., Michigan Univ., Ann Arbor, MI, USA; NA","IEEE Transactions on Software Engineering","","1997","23","9","599","600","In Shepard and Gagne (1991), a branch-and-bound implicit enumeration algorithm is described whose purpose is to generate a feasible schedule, if any, for each processor on a multiprocessing node running hard real-time processes. The optimization criterion is to minimize process lateness defined as the difference between the process completion time and deadline. We show in this correspondence that this algorithm does not always succeed in finding a feasible solution, and describe the reason why the algorithm might fail.","0098-5589;1939-3520;2326-3881","","10.1109/32.629495","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=629495","","Scheduling algorithm;Real time systems;Processor scheduling;Runtime;Application software;Timing;Stochastic processes;Computational modeling;Costs;Software maintenance","processor scheduling;parallel algorithms;real-time systems;tree searching;multiprocessing systems;minimisation","pre-run-time scheduling algorithm;hard real-time systems;branch-and-bound implicit enumeration algorithm;processor scheduling;multiprocessing node;optimization;process lateness minimization;process completion time;deadline","","4","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Alloy Meets the Algebra of Programming: A Case Study","J. N. Oliveira; M. A. Ferreira","University of Minho, Braga; Software Improvement Group, Amsterdam","IEEE Transactions on Software Engineering","","2013","39","3","305","326","Relational algebra offers to software engineering the same degree of conciseness and calculational power as linear algebra in other engineering disciplines. Binary relations play the role of matrices with similar emphasis on multiplication and transposition. This matches with Alloy's lemma “everything is a relation” and with the relational basis of the Algebra of Programming (AoP). Altogether, it provides a simple and coherent approach to checking and calculating programs from abstract models. In this paper, we put Alloy and the Algebra of Programming together in a case study originating from the Verifiable File System mini-challenge put forward by Joshi and Holzmann: verifying the refinement of an abstract file store model into a journaled (Flash) data model catering to wear leveling and recovery from power loss. Our approach relies on diagrams to graphically express typed assertions. It interweaves model checking (in Alloy) with calculational proofs in a way which offers the best of both worlds. This provides ample evidence of the positive impact in software verification of Alloy's focus on relations, complemented by induction-free proofs about data structures such as stores and lists.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6155724","Model checking;algebra of programming;software verification;grand challenges in computing","Metals;Software;Programming;Matrices;Calculus;Cognition","formal verification;mathematics computing;relational algebra;software engineering","relational algebra;software engineering;linear algebra;Alloys lemma;relational basis;algebra of programming;AoP;calculating programs;verifiable file system;model checking;software verification;data structures","","3","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Verifying authentication protocols in CSP","S. Schneider","Dept. of Comput. Sci., R. Holloway Univ. of London, Egham, UK","IEEE Transactions on Software Engineering","","1998","24","9","741","758","This paper presents a general approach for analysis and verification of authentication properties using the theory of Communicating Sequential Processes (CSP). The paper aims to develop a specific theory appropriate to the analysis of authentication protocols, built on top of the general CSP semantic framework. This approach aims to combine the ability to express such protocols in a natural and precise way with the ability to reason formally about the properties they exhibit. The theory is illustrated by an examination of the Needham-Schroeder (1978) public key protocol. The protocol is first examined with respect to a single run and then more generally with respect to multiple concurrent runs.","0098-5589;1939-3520;2326-3881","","10.1109/32.713329","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=713329","","Authentication;Protocols;Security;ISO standards;IEC standards;Computer Society;Public key;Digital signatures;Logic","message authentication;formal verification;protocols;communicating sequential processes;public key cryptography","authentication protocol verification;CSP;Communicating Sequential Processes;semantic framework;public key protocol;Needham-Schroeder protocol","","78","","24","","","","","","IEEE","IEEE Journals & Magazines"
"ARJA: Automated Repair of Java Programs via Multi-Objective Genetic Programming","Y. Yuan; W. Banzhaf","Computer Science and Engineering, Michigan State University, East Lansing, Michigan United States (e-mail: yyxhdy@gmail.com); Computer Science and Engineering, Michigan State University, East Lansing, Michigan United States (e-mail: banzhafw@msu.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Automated program repair is the problem of automatically fixing bugs in programs in order to significantly reduce the debugging costs and improve the software quality. To address this problem, test-suite based repair techniques regard a given test suite as an oracle and modify the input buggy program to make the entire test suite pass. GenProg is well recognized as a prominent repair approach of this kind, which uses genetic programming (GP) to rearrange the statements already extant in the buggy program. However, recent empirical studies show that the performance of GenProg is not fully satisfactory, particularly for Java. In this paper, we propose ARJA, a new GP based repair approach for automated repair of Java programs. To be specific, we present a novel lower-granularity patch representation that properly decouples the search subspaces of likely-buggy locations, operation types and potential fix ingredients, enabling GP to explore the search space more effectively. Based on this new representation, we formulate automated program repair as a multi-objective search problem and use NSGA-II to look for simpler repairs. To reduce the computational effort and search space, we introduce a test filtering procedure that can speed up the fitness evaluation of GP and three types of rules that can be applied to avoid unnecessary manipulations of the code. Moreover, we also propose a type matching strategy that can create new potential fix ingredients by exploiting the syntactic patterns of existing statements. We conduct a large-scale empirical evaluation of ARJA along with its variants on both seeded bugs and real-world bugs in comparison with several state-of-the-art repair approaches. Our results verify the effectiveness and efficiency of the search mechanisms employed in ARJA and also show its superiority over the other approaches. In particular, compared to jGenProg (an implementation of GenProg for Java), an ARJA version fully following the redundancy assumption can generate a test-suite adequate patch for more than twice the number of bugs (from 27 to 59), and a correct patch for nearly four times of the number (from 5 to 18), on 224 real-world bugs considered in Defects4J. Furthermore, ARJA is able to correctly fix several real multi-location bugs that are hard to be repaired by most of the existing repair approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2874648","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8485732","Program repair;patch generation;genetic programming;multi-objective optimization;genetic improvement","Maintenance engineering;Computer bugs;Java;Genetic programming;Search problems;Sociology;Statistics","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Systematic formal verification for fault-tolerant time-triggered algorithms","J. Rushby","Comput. Sci. Lab., SRI Int., Menlo Park, CA, USA","IEEE Transactions on Software Engineering","","1999","25","5","651","660","Many critical real-time applications are implemented as time-triggered systems. We present a systematic way to derive such time-triggered implementations from algorithms specified as functional programs (in which form their correctness and fault-tolerance properties can be formally and mechanically verified with relative ease). The functional program is first transformed into an untimed synchronous system and, then, to its time-triggered implementation. The first step is specific to the algorithm concerned, but the second is generic and we prove its correctness. This proof has been formalized and mechanically checked with the PVS verification system. The approach provides a methodology that can ease the formal specification and assurance of critical fault-tolerant systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.815324","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=815324","","Formal verification;Fault tolerant systems;Synchronization;Upper bound;Control systems;Automobiles;Operating systems;Fault diagnosis;Real time systems;Mechanical factors","program verification;formal specification;software fault tolerance;real-time systems;functional programming","formal verification;fault-tolerant time-triggered algorithms;real-time applications;functional programs;program correctness;untimed synchronous system;PVS verification system;formal specification","","42","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Distributed computing with single read-single write variables","P. M. Lenders","Dept. of Electr. & Comput. Eng., Oregon State Univ., Corvallis, OR, USA","IEEE Transactions on Software Engineering","","1989","15","5","569","574","Single-read-single-write (SRSW) variables are presented for synchronous and asynchronous communication between processes. The operational semantics of the instruction accessing these variables is quite simple: a SRSW variable can be written if it is free, and, once written, it becomes busy. A SRSW variable can be read when busy, and, once read, it becomes free. A process attempting to read a free SRSW variable or write a busy SRSW variable is put in a wait state until the state of the variable changes. The advantages of SRSW variables are multiple. The syntax of a regular sequential language can be used without any change, other than the introduction of a new SRSW data type. Parallel programs tend to be concise and easy to prove correct. The message passing paradigm can be very easily modeled with SRSW variables.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24706","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24706","","Distributed computing;Message passing;Concurrent computing;Asynchronous communication;Command languages;Energy management;Power system management;Condition monitoring;Strontium","distributed processing","distributed computing;synchronous communication;parallel programs;single read-single write variables;asynchronous communication;operational semantics;free;busy;wait state;SRSW variables;syntax;regular sequential language;SRSW data type;message passing","","","","8","","","","","","IEEE","IEEE Journals & Magazines"
"A Model for the Basic Block Protocol of the Cambridge Ring","G. Harrus","ISEM Laboratory, Universite Paris-Sud","IEEE Transactions on Software Engineering","","1985","SE-11","1","130","136","In this paper, we study a model of the Basic Block Protocol of the Cambridge Ring. The model is close to the real behavior so that, with statistically, identical stations and light or moderate load assumptions. we obtain good approximate results of the Ring characteristics. These results are partially validated by a simulation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231537","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701905","Cambridge Ring;distributed systems;local area network;Markov chain;modeling","Local area networks;Hardware;Monitoring;Wire;Access protocols;Costs;Digital communication;Computer vision;Delay","","Cambridge Ring;distributed systems;local area network;Markov chain;modeling","","","","7","","","","","","IEEE","IEEE Journals & Magazines"
"An embedded modeling language approach to interactive 3D and multimedia animation","C. Elliott","Microsoft Corp., Redmond, WA, USA","IEEE Transactions on Software Engineering","","1999","25","3","291","308","While interactive multimedia animation is a very compelling medium, few people are able to express themselves in it. There are too many low-level details that have to do not with the desired content-e.g., shapes, appearance and behavior-but rather how to get a computer to present the content. For instance, behavior such as motion and growth are generally gradual, continuous phenomena. Moreover, many such behaviors go on simultaneously. Computers, on the other hand, cannot directly accommodate either of these basic properties, because they do their work in discrete steps rather than continuously, and they only do one thing at a time. Graphics programmers have to spend much of their effort bridging the gap between what an animation is and how to present it on a computer. We propose that this situation can be improved by a change of language, and present Fran, synthesized by complementing an existing declarative host language, Haskell, with an embedded domain-specific vocabulary for modeled animation. As demonstrated in a collection of examples, the resulting animation descriptions are not only relatively easy to write, but also highly composable.","0098-5589;1939-3520;2326-3881","","10.1109/32.798320","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=798320","","Animation;Vocabulary;Programming profession;Computer languages;Shape;Computer graphics;Domain specific languages;Functional programming;Automatic programming;Writing","computer animation;multimedia computing;simulation languages","embedded modeling language approach;interactive 3D animation;interactive multimedia animation;motion;growth;Fran;declarative host language;Haskell;embedded domain-specific vocabulary;modeled animation","","19","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Debugging Static Analysis","L. Nguyen Quang Do; S. Krüger; P. Hill; K. Ali; E. Bodden","Software Engineering, Universitat Paderborn Institut fur Informatik, 233373 Paderborn, Nordrhein-Westfalen Germany (e-mail: lisa.nguyen@upb.de); Software Engineering, Universitat Paderborn Institut fur Informatik, 233373 Paderborn, Nordrhein-Westfalen Germany (e-mail: stefan.krueger@upb.de); Software Engineering, Universitat Paderborn Institut fur Informatik, 233373 Paderborn, Nordrhein-Westfalen Germany (e-mail: pahill@campus.uni-paderborn.de); Computing Science, University of Alberta, 3158 Edmonton, Alberta Canada T6G 2R3 (e-mail: karim.ali@ualberta.ca); Software Engineering, Universitat Paderborn Fakultat fur Elektrotechnik Informatik und Mathematik, 232601 Paderborn, NRW Germany 33102 (e-mail: eric.bodden@uni-paderborn.de)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Static analysis is increasingly used by companies and individual code developers to detect bugs and security vulnerabilities. As programs grow more complex, the analyses have to support new code concepts, frameworks and libraries. However, static-analysis code itself is also prone to bugs. While more complex analyses are written and used in production systems every day, the cost of debugging and fixing them also increases tremendously. To understand the difficulties of debugging static analysis, we surveyed 115 static-analysis writers. From their responses, we determined the core requirements to build a debugger for static analyses, which revolve around two main issues: abstracting from both the analysis code and the code it analyses at the same time, and tracking the analysis internal state throughout both code bases. Most tools used by our survey participants lack the capabilities to address both issues. Focusing on those requirements, we introduce VisuFlow, a debugging environment for static data-flow analysis. VisuFlow features graph visualizations and custom breakpoints that enable users to view the state of an analysis at any time. In a user study on 20 static-analysis writers, VisuFlow helped identify 25% and fix 50% more errors in the analysis code compared to the standard Eclipse debugging environment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2868349","Fraunhofer-Gesellschaft; Deutsche Forschungsgemeinschaft; Heinz Nixdorf Foundation; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8453858","Testing and Debugging;Program analysis;Development tools;Integrated environments;Graphical environments;Usability testing","Debugging;Static analysis;Tools;Computer bugs;Standards;Writing;Encoding","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Addendum to ""Proof rules for flush channels""","S. Stoller","Dept. of Comput. Sci., Cornell Univ., Ithaca, NY, USA","IEEE Transactions on Software Engineering","","1994","20","8","664","","The logic presented in a previous paper, see ibid., vol. 19, no.4, p.366-78 (1993) for processes that communicate using flush channels is inadequate for reasoning about processes that send multiple identical messages along a channel. A modification to the logic and proof system that remedies this deficiency is described herein.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310675","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310675","","Logic;Mathematical model;Reasoning about programs;Computer science;Costs;Tagging","distributed processing;program verification;message passing;formal logic","flush channels;multiple identical messages;channel;proof system;logic system;proof rules;communicating processes;asynchronous communication;distributed systems;program verification","","1","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Accurate and Scalable Cross-Architecture Cross-OS Binary Code Search with Emulation","Y. Xue; Z. Xu; M. Chandramohan; Y. Liu","School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui China (e-mail: yinxingxue@gmail.com); School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore Singapore (e-mail: XU0001ZI@e.ntu.edu.sg); School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore Singapore 639798 (e-mail: mahintha001@e.ntu.edu.sg); School of Computer Engineering, Nanyang Technological University, Singapore, Singapore Singapore 639798 (e-mail: yangliu@ntu.edu.sg)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Similar code search in binary executables faces big challenges due to the gigantic differences in syntax and structure of binary code that result from different configurations of compilers, architectures and OSs. In our previous study and the tool BINGO, to mitigate the huge gaps in CFG structures due to different compilation scenarios, we propose a selective inlining technique to capture the complete function semantics by inlining relevant library and user-defined functions. However, only features of input/output values are considered in BINGO. In this study, we propose to incorporate features from different categories (e.g., structural features and high-level semantic features) for accuracy improvement and emulation for efficiency improvement. We empirically compare our tool, BINGO-E, with the pervious tool BINGO and the available state-of-the-art tools of binary code search in terms of search accuracy and performance. Results show that BINGO-E achieve significantly better accuracies than BINGO for cross-architecture matching, cross-OS matching, cross-compiler matching and intra-compiler matching. Additionally, in the new task of matching binaries of forked projects, BINGO-E also exhibits a better accuracy than the existing benchmark tool. Meanwhile, BINGO-E takes less time than BINGO during the process of matching.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2827379","National Research Foundation Singapore; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8338420","Binary Code Search;Binary Clone Detection;Vulnerability Matching;Emulation;3D-CFG","Binary codes;Semantics;Tools;Feature extraction;Cloning;Syntactics;Emulation","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"On ""A framework for source code search using program patterns""","P. Devanbu","Software & Syst. Res. Lab., AT&T Bell Labs., Murray Hill, NJ, USA","IEEE Transactions on Software Engineering","","1995","21","12","1009","1010","The need to query and understand source code is an important practical problem for software engineers in large development projects. A paper by Paul and Prakash (1994) proposes a workable solution to this problem. However, there are several previously reported systems that can also address this problem. The relationship of their work to the body of existing work is the subject of the paper.","0098-5589;1939-3520;2326-3881","","10.1109/32.489076","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=489076","","Data mining;Programming profession;Electronic mail;Switches;Performance analysis;Electrical capacitance tomography;Software maintenance;Maintenance engineering;Software systems;Software tools","software maintenance;compiler generators;software tools","source code search;program patterns;source code querying;software engineers;large development projects","","6","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Detection and Removal of Ineffective Mutants for the Mutation Analysis of Relational Database Schemas","P. McMinn; C. J. Wright; C. J. McCurdy; G. Kapfhammer","Computer Science, University of Sheffield, Sheffield, South Yorkshire United Kingdom of Great Britain and Northern Ireland S1 4DP (e-mail: p.mcminn@sheffield.ac.uk); Computer Science, University of Sheffield, 7315 Sheffield, South Yorkshire United Kingdom of Great Britain and Northern Ireland (e-mail: chrisjameswright@gmail.com); Computer Science, Allegheny College, 5368 Meadville, Pennsylvania United States (e-mail: mccurdyc@allegheny.edu); Computer Science, Allegheny College, Meadville, Pennsylvania United States 16335 (e-mail: gkapfham@allegheny.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Data is one of an organization&#x0027;s most valuable and strategic assets. Testing the relational database schema, which protects the integrity of this data, is of paramount importance. Mutation analysis is a means of estimating the fault-finding ""strength"" of a test suite. As with program mutation, however, relational database schema mutation results in many ""ineffective"" mutants that both degrade test suite quality estimates and make mutation analysis more time consuming. This paper presents a taxonomy of ineffective mutants for relational database schemas, summarizing the root causes of ineffectiveness with a series of key patterns evident in database schemas. On the basis of these, we introduce algorithms that automatically detect and remove ineffective mutants. In an experimental study involving the mutation analysis of 34 schemas used with three popular relational database management systems&#x2014;HyperSQL, PostgreSQL, and SQLite&#x2014;the results show that our algorithms can identify and discard large numbers of ineffective mutants that can account for up to 24% of mutants, leading to a change in mutation score for 33 out of 34 schemas. The tests for seven schemas were found to achieve 100% scores, indicating that they were capable of detecting and killing all non-equivalent mutants. The results also reveal that the execution cost of mutation analysis may be significantly reduced, especially with ""heavyweight"" DBMSs like PostgreSQL.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2786286","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8240964","","Relational databases;Algorithm design and analysis;Testing;Taxonomy;Google;Software","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A test generation strategy for pairwise testing","Kuo-Chung Tai; Yu Lei","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA","IEEE Transactions on Software Engineering","","2002","28","1","109","111","Pairwise testing is a specification-based testing criterion which requires that for each pair of input parameters of a system, every combination of valid values of these two parameters be covered by at least one test case. The authors propose a novel test generation strategy for pairwise testing.","0098-5589;1939-3520;2326-3881","","10.1109/32.979992","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979992","","Testing","program testing;formal specification","test generation strategy;pairwise testing;specification-based testing criterion;input parameters;valid values;test case;software testing","","149","","5","","","","","","IEEE","IEEE Journals & Magazines"
"On the relationship between partition and random testing","T. Y. Chen; Y. T. Yu","Dept. of Comput. Sci., Melbourne Univ., Parkville, Vic., Australia; Dept. of Comput. Sci., Melbourne Univ., Parkville, Vic., Australia","IEEE Transactions on Software Engineering","","1994","20","12","977","980","Weyuker and Jeng (ibid., vol. SE-17, pp. 703-711, July 1991) have investigated the conditions that affect the performance of partition testing and have compared analytically the fault-detecting ability of partition testing and random testing. This paper extends and generalizes some of their results. We give more general ways of characterizing the worst case for partition testing, along with a precise characterization of when this worst case is as good as random testing. We also find that partition testing is guaranteed to perform at least as well as random testing so long as the number of test cases selected is in proportion to the size of the subdomains.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.368132","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=368132","","Software testing;Performance evaluation;Costs;Performance analysis;Computer bugs;Computer science","program testing;random processes;software performance evaluation","partition testing;random testing;fault-detecting ability;worst case;subdomain size;test cases;software testing;performance","","66","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Foreword What is AI? And What Does It Have to Do with Software Engineering?","J. Mostow","NA","IEEE Transactions on Software Engineering","","1985","SE-11","11","1253","1256","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231876","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701944","","Artificial intelligence;Software engineering;Medical expert systems;Diagnostic expert systems;Medical diagnosis;Machine intelligence;Learning systems;Programming environments;Software design;History","","","","10","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Scaling step-wise refinement","D. Batory; J. N. Sarvela; A. Rauschmayer","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA","IEEE Transactions on Software Engineering","","2004","30","6","355","371","Step-wise refinement is a powerful paradigm for developing a complex program from a simple program by adding features incrementally. We present the AHEAD (algebraic hierarchical equations for application design) model that shows how step-wise refinement scales to synthesize multiple programs and multiple noncode representations. AHEAD shows that software can have an elegant, hierarchical mathematical structure that is expressible as nested sets of equations. We review a tool set that supports AHEAD. As a demonstration of its viability, we have bootstrapped AHEAD tools from equational specifications, refining Java and nonJava artifacts automatically; a task that was accomplished only by ad hoc means previously.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321059","Specification;design notations and documentation;representation;design concepts;methodologies;data abstraction;extensible languages;program synthesis;feature-oriented programming;refinement.","Equations;Packaging;Java;Refining;Collaboration;Unified modeling language;Jacobian matrices;Application software;Documentation;Design methodology","algebraic specification;data structures;Java;specification languages","step-wise refinement;algebraic hierarchical equation;application design;multiple noncode representation;hierarchical mathematical structure;Java;design notation;data abstraction;program synthesis;feature-oriented programming","","197","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Measurements of Ada overhead in OSI-style communications systems","N. R. Howes; A. C. Weaver","Dept. of Comput. Sci., George Mason Univ., Fairfax, VA, USA; NA","IEEE Transactions on Software Engineering","","1989","15","12","1507","1517","A discussion is given on whether the Ada model of concurrency is suitable for implementing the seven-layer OSI reference model. Using the communications model introduced by R.J.A. Buhr (1984), they determine the overhead introduced by Ada when the model is implemented on two single-processor machines, a VAX 11/785 and a Rational 1000. The authors then calculate a lower bound on expected message delay. A novel model using server tasks is proposed and shown to have better performance. The authors investigate performance on an eight-processor Sequent Model 821 and a 14-processor Encore Multimax 320 by implementing the Buhr model, the server task model and a third model which abandons the Ada rendezvous in favor of procedure calls. They determine the Ada overhead per message as a function of the number of processors and calculate lower bounds on expected message delay attributable to Ada overhead for all three communications models.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58763","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58763","","Concurrent computing;Proposals;Delay;Computer science;Mathematical model;Magnetic heads;Communication standards;Standards organizations;LAN interconnection;Open systems","Ada;computer networks;open systems;parallel programming;standards","OSI-style communications systems;Ada model;concurrency;seven-layer OSI reference model;communications model;single-processor machines;VAX 11/785;Rational 1000;expected message delay;novel model;server tasks;eight-processor Sequent Model 821;14-processor Encore Multimax 320;Buhr model;server task model;Ada rendezvous;procedure calls;Ada overhead per message","","8","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm","N. Mittas; L. Angelis","Aristotle University of Thessaloniki, Thessaloniki; Aristotle University of Thessaloniki, Thessaloniki","IEEE Transactions on Software Engineering","","2013","39","4","537","551","Software Cost Estimation can be described as the process of predicting the most realistic effort required to complete a software project. Due to the strong relationship of accurate effort estimations with many crucial project management activities, the research community has been focused on the development and application of a vast variety of methods and models trying to improve the estimation procedure. From the diversity of methods emerged the need for comparisons to determine the best model. However, the inconsistent results brought to light significant doubts and uncertainty about the appropriateness of the comparison process in experimental studies. Overall, there exist several potential sources of bias that have to be considered in order to reinforce the confidence of experiments. In this paper, we propose a statistical framework based on a multiple comparisons algorithm in order to rank several cost estimation models, identifying those which have significant differences in accuracy, and clustering them in nonoverlapping groups. The proposed framework is applied in a large-scale setup of comparing 11 prediction models over six datasets. The results illustrate the benefits and the significant information obtained through the systematic comparison of alternative methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6235961","Cost estimation;management;metrics/measurement;statistical methods","Predictive models;Estimation;Accuracy;Measurement uncertainty;Prediction algorithms;Clustering algorithms;Systematics","pattern clustering;software cost estimation;software development management;statistical analysis","software cost estimation model;multiple comparisons algorithm;software project;project management;statistical framework;software cost estimation ranking;software cost estimation clustering","","44","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Software fault interactions and implications for software testing","D. R. Kuhn; D. R. Wallace; A. M. Gallo","Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA","IEEE Transactions on Software Engineering","","2004","30","6","418","421","Exhaustive testing of computer software is intractable, but empirical studies of software failures suggest that testing can in some cases be effectively exhaustive. We show that software failures in a variety of domains were caused by combinations of relatively few conditions. These results have important implications for testing. If all faults in a system can be triggered by a combination of n or fewer parameters, then testing all n-tuples of parameters is effectively equivalent to exhaustive testing, if software behavior is not dependent on complex event sequences and variables have a small set of discrete values.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321063","Statistical methods;testing strategies;test design.","Software testing;System testing;Databases;Software quality;History;Microwave ovens;Fault detection;Drugs;Embedded system","software fault tolerance;program testing;failure analysis;statistical analysis","computer software testing;software failure;software behavior;event sequence;discrete value;statistical method;test design","","315","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Integer Linear Programming-Based Property Checking for Asynchronous Reactive Systems","S. Leue; W. Wei","University of Konstanz, Konstanz; SAP AG, Darmstadt","IEEE Transactions on Software Engineering","","2013","39","2","216","236","Asynchronous reactive systems form the basis of a wide range of software systems, for instance in the telecommunications domain. It is highly desirable to rigorously show that these systems are correctly designed. However, traditional formal approaches to the verification of these systems are often difficult because asynchronous reactive systems usually possess extremely large or even infinite state spaces. We propose an integer linear program (ILP) solving-based property checking framework that concentrates on the local analysis of the cyclic behavior of each individual component of a system. We apply our framework to the checking of the buffer boundedness and livelock freedom properties, both of which are undecidable for asynchronous reactive systems with an infinite state space. We illustrate the application of the proposed checking methods to Promela, the input language of the SPIN model checker. While the precision of our framework remains an issue, we propose a counterexample guided abstraction refinement procedure based on the discovery of dependences among control flow cycles. We have implemented prototype tools with which we obtained promising experimental results on real-life system models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680910","Software verification;formal methods;property checking;integer linear programming;static analysis;abstraction;refinement;counterexamples;asynchronous communication;buffer boundedness;livelock freedom;control flow cycles;cycle dependences;UML;Promela","Unified modeling language;Complexity theory;Analytical models;Message passing;Integer linear programming;Mathematical model;Cost accounting","data structures;formal languages;formal verification;integer programming;linear programming;program diagnostics;state-space methods","integer linear programming-based property checking;asynchronous reactive systems;software systems;telecommunications domain;formal approaches;infinite state spaces;ILP solving-based property checking framework;cyclic behavior;individual component;buffer boundedness;livelock freedom properties;Promela;SPIN model checker input language;counterexample guided abstraction refinement procedure;control flow cycles;real-life system models","","","","73","","","","","","IEEE","IEEE Journals & Magazines"
"Designing process replication and activation: a quantitative approach","M. Litoiu; J. Rolia; G. Serazzi","IBM Canada Ltd., Toronto, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2000","26","12","1168","1178","Distributed application systems are composed of classes of objects with instances that interact to accomplish common goals. Such systems can have many classes of users with many types of requests. Furthermore, the relative load of these classes can shift throughout the day, causing changes to system behavior and bottlenecks. When designing and deploying such systems, it is necessary to determine a process replication and threading policy for the server processes that contain the objects, as well as process activation policies. To avoid bottlenecks, the policy must support all possible workload conditions. Licensing, implementation or resource constraints can limit the number of permitted replicas or threads of a server process. Process activation policies determine whether a server is persistent or should be created and terminated with each call. This paper describes quantitative techniques for choosing process replication or threading levels and process activation policies. Inappropriate policies can lead to unnecessary queuing delays for callers or unnecessarily high consumption of memory resources. The algorithms presented consider all workload conditions, are iterative in nature and are hybrid mathematical programming and analytic performance evaluation methods. An example is given to demonstrate the technique and describe how the results can be applied during software design and deployment.","0098-5589;1939-3520;2326-3881","","10.1109/32.888630","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888630","","Process design;Licenses;Delay;Iterative algorithms;Iterative methods;Mathematical programming;Performance analysis;Algorithm design and analysis;Software design","multi-threading;distributed programming;queueing theory;mathematical programming;software performance evaluation;distributed object management;systems analysis","process replication design;process activation design;quantitative approach;distributed application systems;object classes;common goals;user classes;user requests;relative load;system behavior;bottlenecks;threading policy;server processes;workload conditions;licensing constraints;implementation constraints;resource constraints;queuing delays;memory resource consumption;iterative algorithms;mathematical programming;analytic performance evaluation methods;software design;software deployment;closed queuing networks;performance analysis;performance modeling;distributed design;nonlinear programming;linear programming","","21","","18","","","","","","IEEE","IEEE Journals & Magazines"
"The use of proof in diversity arguments","B. Littlewood","Centre for Software Reliability, City Univ., London, UK","IEEE Transactions on Software Engineering","","2000","26","10","1022","1023","The limits to the reliability that can be claimed for a design-diverse fault-tolerant system are mainly determined by the dependence that must be expected in the failure behaviours of the different versions: claims for independence between version failure processes are not believable. We examine a different approach, in which a simple secondary system is used as a back-up to a more complex primary. The secondary system is sufficiently simple that claims for its perfection (with respect to design faults) are possible, but there is not complete certainty about such perfection. It is shown that assessment of the reliability of the overall fault-tolerant system in this case may take advantage of claims for independence that are more plausible than those involved in design diversity.","0098-5589;1939-3520;2326-3881","","10.1109/32.879822","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879822","","Safety;Protection;Phase frequency detector;Air traffic control;Aerospace control;Fault tolerant systems;Fault tolerance;Aircraft;Cultural differences;Battery powered vehicles","software fault tolerance","software reliability;software fault tolerance;version failure processes;design diversity","","8","","7","","","","","","IEEE","IEEE Journals & Magazines"
"A simulation study of the model evaluation criterion MMRE","T. Foss; E. Stensrud; B. Kitchenham; I. Myrtveit","Norwegian Sch. of Manage., Sandvika, Norway; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","11","985","995","The mean magnitude of relative error, MMRE, is probably the most widely used evaluation criterion for assessing the performance of competing software prediction models. One purpose of MMRE is to assist us to select the best model. In this paper, we have performed a simulation study demonstrating that MMRE does not always select the best model. Our findings cast some doubt on the conclusions of any study of competing software prediction models that use MMRE as a basis of model comparison. We therefore recommend not using MMRE to evaluate and compare prediction models. At present, we do not have any universal replacement for MMRE. Meanwhile, we therefore recommend using a combination of theoretical justification of the models that are proposed together with other metrics proposed in this paper.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245300","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245300","","Predictive models;Software engineering;Costs;Regression tree analysis;Computational modeling;Software performance;Regression analysis;Accuracy;Classification tree analysis;Analysis of variance","software metrics;software cost estimation;digital simulation","simulation study;model evaluation criterion MMRE;mean magnitude of relative error;performance assessment;software prediction models;software metrics;software cost estimation;software engineering","","200","","52","","","","","","IEEE","IEEE Journals & Magazines"
"A coding scheme to support systematic analysis of software comprehension","A. von Mayrhauser; S. Lang","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; NA","IEEE Transactions on Software Engineering","","1999","25","4","526","540","Protocol analysis is a valuable tool for gaining qualitative data from observations of programmer behaviour during software maintenance. However, there are some major drawbacks with protocol analysis as it is currently practiced. Firstly, protocol analysis requires a daunting amount of effort at each stage of analysis. Secondly, the results from one protocol analysis are often difficult to compare with results from another. The paper describes a coding scheme, AFECS, designed to reduce the effort required to perform protocol analysis and to resolve the problem of noncomparable results. AFECS uses codes that consist of expandable and flexible segments. This allows AFECS to be tailored to the requirements of a variety of research studies, while maintaining a degree of consistency. Explicit segmentation also makes AFECS easy to use. An example shows AFECS' use and ability to adapt to diverse research questions.","0098-5589;1939-3520;2326-3881","","10.1109/32.799950","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=799950","","Software systems;Protocols;Software maintenance;Programming profession;Software tools;Software quality;Performance analysis;Reverse engineering;Guidelines;Documentation","reverse engineering;software maintenance;systems re-engineering;protocols","coding scheme;systematic analysis;software comprehension;protocol analysis;qualitative data;programmer behaviour;software maintenance;AFECS;flexible segments;research studies;explicit segmentation;diverse research questions","","15","","32","","","","","","IEEE","IEEE Journals & Magazines"
"A critique of software defect prediction models","N. E. Fenton; M. Neil","Centre for Software Reliability, London, UK; NA","IEEE Transactions on Software Engineering","","1999","25","5","675","689","Many organizations want to predict the number of defects (faults) in software systems, before they are deployed, to gauge the likely delivered quality and maintenance effort. To help in this numerous software metrics and statistical models have been developed, with a correspondingly large literature. We provide a critical review of this literature and the state-of-the-art. Most of the wide range of prediction models use size and complexity metrics to predict defects. Others are based on testing data, the ""quality"" of the development process, or take a multivariate approach. The authors of the models have often made heroic contributions to a subject otherwise bereft of empirical studies. However, there are a number of serious theoretical and practical problems in many studies. The models are weak because of their inability to cope with the, as yet, unknown relationship between defects and failures. There are fundamental statistical and data quality problems that undermine model validity. More significantly many prediction models tend to model only part of the underlying problem and seriously misspecify it. To illustrate these points the Goldilock's Conjecture, that there is an optimum module size, is used to show the considerable problems inherent in current defect prediction approaches. Careful and considered analysis of past and new results shows that the conjecture lacks support and that some models are misleading. We recommend holistic models for software defect prediction, using Bayesian belief networks, as alternative approaches to the single-issue models used at present. We also argue for research into a theory of ""software decomposition"" in order to test hypotheses about defect introduction and help construct a better science of software engineering.","0098-5589;1939-3520;2326-3881","","10.1109/32.815326","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=815326","","Predictive models;Bayesian methods;Software quality;Computer Society;System testing;Process design;Software maintenance;Software systems;Software metrics;Software testing","program testing;software quality;software maintenance;software reliability;software metrics;belief networks","software defect prediction models;software quality;software maintenance;software metrics;statistical models;literature review;multivariate approach;holistic models;Bayesian belief networks;software decomposition;software engineering","","450","","","","","","","","IEEE","IEEE Journals & Magazines"
"No special schemes are needed for solving software reliability optimization models","H. Sarper","Dept. of Eng., Southern Colorado Univ., Puebio, CO, USA","IEEE Transactions on Software Engineering","","1995","21","8","701","702","This paper resolves four previous software reliability optimization models published in this journal (Berman and Ashrafi, 1993). It is shown that a recent optimization software, LINGO, makes it unnecessary to develop special branch and bound or dynamic programming schemes to solve nonlinear reliability optimization models with binary decision variables.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.403793","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=403793","","Software reliability;Reliability engineering;Dynamic programming;Mathematical model;Design optimization;Software design;Redundancy;Operations research;Clocks;Software tools","software reliability;dynamic programming;optimisation;tree searching","software reliability optimization models;optimization software;LINGO;branch and bound;dynamic programming;nonlinear reliability optimization models;binary decision variables","","3","","6","","","","","","IEEE","IEEE Journals & Magazines"
"The case for electric design of real-time software","B. Sanden","Dept. of Inf. Syst. & Syst. Eng., George Mason Univ., Fairfax, VA, USA","IEEE Transactions on Software Engineering","","1989","15","3","360","362","G. Booch (see ibid., vol.SE-12, no.2, p.211-21, Feb. 1986) has analyzed a problem involving the software of a set of free-floating buoys. The correspondence points out that Booch's analysis fails to address one important system issue, namely the fact that the software must support two concurrent activities, and shows that an analysis according to the M.A. Jackson method will reveal this difficulty at an early design stage. On the other hand, the Jackson approach does not deal with some configuration issues, which are handled in Booch's analysis. This shows that one method is sometimes not enough to address all important, systemwide aspects of a problem. Rather than arguing about which one design method is best, the author recommends taking an electric view and using any combination of approaches that yields important results in a given situation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21764","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21764","","Computer aided software engineering;Software design;Temperature sensors;Switches;Ocean temperature;Broadcasting;Wind speed;Failure analysis;Radio transmitters;Receivers","real-time systems;software engineering;structured programming","electric design;real-time software;free-floating buoys;Jackson method;configuration issues","","11","","6","","","","","","IEEE","IEEE Journals & Magazines"
"Using spanning sets for coverage testing","M. Marre; A. Bertolino","Departamento de Computacion, Buenos Aires Univ., Argentina; NA","IEEE Transactions on Software Engineering","","2003","29","11","974","984","A test coverage criterion defines a set E/sub r/ of entities of the program flowgraph and requires that every entity in this set is covered under some test Case. Coverage criteria are also used to measure the adequacy of the executed test cases. In this paper, we introduce the notion of spanning sets of entities for coverage testing. A spanning set is a minimum subset of E/sub r/, such that a test suite covering the entities in this subset is guaranteed to cover every entity in E/sub r/. When the coverage of an entity always guarantees the coverage of another entity, the former is said to subsume the latter. Based on the subsumption relation between entities, we provide a generic algorithm to find spanning sets for control flow and data flow-based test coverage criteria. We suggest several useful applications of spanning sets: They help reduce and estimate the number of test cases needed to satisfy coverage criteria. We also empirically investigate how the use of spanning sets affects the fault detection effectiveness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245299","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245299","","Testing;Fault detection","program testing;data flow graphs","spanning sets;coverage testing;program flowgraph;subsumption relation;generic algorithm;data flow-based test coverage;fault detection","","52","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Programming with streams in a Pascal-like language","I. Nakata; M. Sassa","Inst. of Inf. Sci. & Electron., Tsukuba Univ., Ibaraki, Japan; Inst. of Inf. Sci. & Electron., Tsukuba Univ., Ibaraki, Japan","IEEE Transactions on Software Engineering","","1991","17","1","1","9","A description is given of features which were added to a conventional programming language that will manipulate streams of values. A stream is a sequence of values of a certain fixed type. The number of elements of a stream may be determined at execution time, and evaluation of each element can be postponed until its value is actually needed. Many programs can be expressed naturally and clearly as networks of processes communicating by means of streams. The network is called a composite function and consists of several component functions. Since component functions are connected solely by streams, they greatly increase the flexibility of combinations and the reusability of programs. Loop statements can be considered as iterative statements over streams. One general problem in these networks is the mechanism of terminating each process of the network. A practical solution for this problem is presented. Comparisons to other programming styles, such as coroutines, Lisp, functional programming, and dataflow languages, are described. Three modes of execution are considered for the implementation of composite functions: parallel mode, coroutine mode, and inline mode. In the inline mode, a composite function is expanded and transformed into a single function, realizing maximum run-time efficiency. Algorithms for this expansion are given.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67573","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67573","","Computer languages;Functional programming;Data structures;Joining processes;Runtime;Software engineering;Level control;Algorithms","programming","loop statements;streams;Pascal-like language;fixed type;composite function;component functions;combinations;reusability;iterative statements;programming styles;coroutines;Lisp;functional programming;dataflow languages;parallel mode;coroutine mode;inline mode","","2","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to 'On satisfying timing constraints in hard-real-time systems' by J. Xu and D.L. Parnas","J. Xu; D. L. Parnas","Dept. of Comput. Sci., York Univ., North York, Ont., Canada; NA","IEEE Transactions on Software Engineering","","1993","19","3","310","","Corrected versions are presented for tables I and III that appear in the above-titled paper (see ibid,. vol.19, no.1, p.70-84, 1993).<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.221141","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=221141","","Timing;Scheduling algorithm;Runtime;Processor scheduling;Production systems;Length measurement;Velocity measurement;Software engineering","real-time systems;scheduling","timing constraints;hard-real-time systems","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"On the complexity of generating optimal test sequences","S. C. Boyd; H. Ural","Dept. of Comput. Sci., Ottawa Univ., Ont., Canada; Dept. of Comput. Sci., Ottawa Univ., Ont., Canada","IEEE Transactions on Software Engineering","","1991","17","9","976","978","The authors investigate whether maximal overlapping of protocol test subsequences can be achieved in polynomial time. They review the concepts related to FSM (finite state machine)-based test sequence generation and then define the optimal test sequence generation (OTSG) problem. It is proved that the OTSG problem is NP-complete. Therefore an efficient solution to the problem should not be expected in the general case.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92918","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92918","","Testing;Protocols;Tail;Councils;Computer science;Polynomials","computational complexity;finite automata;program testing;protocols","protocol testing;communications protocols;maximal overlapping;test subsequences;polynomial time;FSM;finite state machine;optimal test sequence generation;OTSG problem;NP-complete","","20","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Performance analysis of periodic and concurrent data structure maintenance strategies for network servers","W. H. Bahaa-El-Din; F. B. Bastani; J. -. Teng","Digital Equipment Corp., Colorado Springs, CO, USA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","12","1526","1536","Three strategies for designing servers and maintaining their data structures are discussed: incremental maintenance, periodic maintenance, and concurrent maintenance. The authors study periodic and concurrent maintenance strategies analytically in order to gain more insight into the behavior of servers using these strategies and determine when and how the maintenance should be performed. For periodic maintenance, it is shown that there is a value of the period which minimizes the average response time, and a formula to compute this value analytically is derived. For concurrent maintenance, a formula for its average response time and the condition under which concurrent maintenance would be preferable to periodic maintenance is derived. The authors have conducted a series of experiments to compare the performance of different maintenance strategies. For the system considered in the experiment, periodic maintenance yields the best average response time, whereas concurrent maintenance gives the least standard deviation and the smallest maximum response time.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58765","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58765","","Performance analysis;Data structures;Network servers;Local area networks;Delay;Databases;Analytical models;Resource management;Information retrieval;Computer science","data structures;network servers;parallel programming;performance evaluation","performance analysis;concurrent data structure maintenance strategies;network servers;incremental maintenance;periodic maintenance;average response time;maximum response time","","3","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to “optimal load balancing in a multiple processor system with many job classes”","L. M. Ni; K. Hwang","Department of Computer Science, Michigan State University, East Lansing, MI 48824; Computer Research Institute and the Department of Electrical Engineering-Systems, University of Southern California, Los Angeles, CA 90089","IEEE Transactions on Software Engineering","","1986","SE-12","3","500","500","In the above paper, an error was made in the Load Balancing Algorithm. A more clear recursive way to present this algorithm is to modify steps S3 to S5 as follows.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312891","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312891","","Load management;Nickel;Educational institutions;Software algorithms;Computer science;Computers","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"A control-flow analysis for a calculus of concurrent objects","P. di Blasio; K. Fisher; C. Talcott","Arthur Andersen MBA, Rome, Italy; NA; NA","IEEE Transactions on Software Engineering","","2000","26","7","617","634","We present a set-based control flow analysis for an imperative, concurrent object calculus extending the Fisher-Honsell-Mitchell functional object-oriented calculus described in Fisher, Honsell and Mitchell, (1993). The analysis is shown to be sound with respect to a transition-system semantics.","0098-5589;1939-3520;2326-3881","","10.1109/32.859531","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859531","","Calculus;Performance analysis;Runtime;Object oriented programming;Information analysis;Software safety;Prototypes;Concurrent computing;Programming profession;Data flow computing","object-oriented programming;calculus;concurrency theory;program diagnostics","control-flow analysis;calculus of concurrent objects;concurrent object calculus;concurrency;object-oriented;soundness;prototype-based","","3","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Structuring communication software for quality-of-service guarantees","A. Mehra; A. Indiresan; K. G. Shin","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","10","616","634","We propose architectural mechanisms for structuring host communication software to provide QoS guarantees. We present and evaluate a QoS sensitive communication subsystem architecture for end hosts that provides real time communication support for generic network hardware. This architecture provides services for managing communication resources for guaranteed QoS (real time) connections, such as admission control, traffic enforcement, buffer management, and CPU and link scheduling. The architecture design is based on three key goals: maintenance of QoS guarantees on a per connection basis, overload protection between established connections, and fairness in delivered performance to best effort traffic. Using this architecture we implement real time channels, a paradigm for real time communication services in packet switched networks. The proposed architecture features a process per channel model that associates a channel handler with each established channel. The model employed for handler execution is one of ""cooperative"" preemption, where an executing handler yields the CPU to a waiting higher priority handler at well defined preemption points. The architecture provides several configurable policies for protocol processing and overload protection. We present extensions to the admission control procedure for real time channels to account for cooperative preemption and overlap between protocol processing and link transmission at a sending host. We evaluate the implementation to demonstrate the efficacy with which the architecture maintains QoS guarantees on outgoing traffic while adhering to the stated design goals.","0098-5589;1939-3520;2326-3881","","10.1109/32.637145","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637145","","Software quality;Quality of service;Computer architecture;Resource management;Admission control;Communication system traffic control;Protection;Protocols;Hardware;Traffic control","computer communications software;real-time systems;software quality;packet switching;telecommunication congestion control;resource allocation","communication software structuring;quality of service guarantees;architectural mechanisms;QoS guarantees;QoS sensitive communication subsystem architecture;real time communication support;generic network hardware;communication resource management;admission control;traffic enforcement;overload protection;real time channels;real time communication services;packet switched networks;channel model;channel handler;handler execution;cooperative preemption;higher priority handler;well defined preemption points;configurable policies","","16","","81","","","","","","IEEE","IEEE Journals & Magazines"
"Using abstraction and model checking to detect safety violations in requirements specifications","C. Heitmeyer; J. Kirby; B. Labaw; M. Archer; R. Bharadwaj","Centre for High Assurance Comput. Syst., Naval Res. Lab., Washington, DC, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","11","927","948","Exposing inconsistencies can uncover many defects in software specifications. One approach to exposing inconsistencies analyzes two redundant specifications, one operational and the other property-based, and reports discrepancies. This paper describes a ""practical"" formal method, based on this approach and the SCR (software cost reduction) tabular notation, that can expose inconsistencies in software requirements specifications. Because users of the method do not need advanced mathematical training or theorem-proving skills, most software developers should be able to apply the method without extraordinary effort. This paper also describes an application of the method which exposed a safety violation in the contractor-produced software requirements specification of a sizable, safety-critical control system. Because the enormous state space of specifications of practical software usually renders direct analysis impractical, a common approach is to apply abstraction to the specification. To reduce the state space of the control system specification, two ""pushbutton"" abstraction methods were applied, one which automatically removes irrelevant variables and a second which replaces the large, possibly infinite, type sets of certain variables with smaller type sets. Analyzing the reduced specification with the model checker Spin uncovered a possible safety violation. Simulation demonstrated that the safety violation was not spurious but an actual defect in the original specification.","0098-5589;1939-3520;2326-3881","","10.1109/32.730543","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=730543","","Software safety;Thyristors;Costs;Control systems;Automatic control;State-space methods;Aerospace electronics;Software tools;Application software;Analytical models","program verification;formal specification;software cost estimation;safety-critical software;computerised control","pushbutton abstraction methods;model checking;safety violation detection;software requirements specifications;inconsistencies;software specification defects;redundant specifications;operational specification;property-based specification;discrepancies;software cost reduction;SCR tabular notation;safety-critical control system;contractor-produced specification;state space reduction;irrelevant variable removal;infinite type sets;reduced specification;Spin model checker;simulation;formal verification;safety analysis;consistency checking","","83","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing partially-implemented real-time systems","G. S. Avrunin; J. C. Corbett; L. K. Dillon","Dept. of Math. & Stat., Massachusetts Univ., Amherst, MA, USA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","8","602","614","Most analysis methods for real-time systems assume that all the components of the system are at roughly the same stage of development and can be expressed in a single notation, such as a specification or programming language. There are, however, many situations in which developers would benefit from tools that could analyze partially-implemented systems: those for which some components are given only as high-level specifications while others are fully implemented in a programming language. In this paper, we propose a method for analyzing such partially-implemented real-time systems. We consider real-time concurrent systems for which some components are implemented in Ada and some are partially specified using regular expressions and graphical interval logic (GIL), a real-time temporal logic. We show how to construct models of the partially-implemented systems that account for such properties as run-time overhead and scheduling of processes, yet support tractable analysis of nontrivial programs. The approach can be fully automated, and we illustrate it by analyzing a small example.","0098-5589;1939-3520;2326-3881","","10.1109/32.707696","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=707696","","Real time systems;Computer languages;Timing;System testing;Computer Society;Logic programming;Processor scheduling;Concurrent computing;Sequential analysis;Error correction","real-time systems;temporal logic;program diagnostics;processor scheduling;multiprocessing programs;Ada;algebraic specification","partially-implemented real-time systems analysis;high-level specifications;programming language;real-time concurrent systems;Ada-implemented components;partially specified components;regular expressions;graphical interval logic;GIL;real-time temporal logic;run-time overhead;process scheduling;static analysis;hybrid systems","","1","","26","","","","","","IEEE","IEEE Journals & Magazines"
"BDL: a specialized language for per-object reactive control","F. Bertrand; M. Augeraud","Lab. d'Inf. et d'Imagerie Ind., La Rochelle Univ., France; NA","IEEE Transactions on Software Engineering","","1999","25","3","347","362","The problem of describing the concurrent behavior of objects in object oriented languages is addressed. The approach taken is to let methods be the behavior units whose synchronization is controlled separate from their specification. Our proposal is a domain-specific language called BDL for expressing constraints on this control and actually implementing its enforcement. We propose a model where each object includes a so-called ""execution controller"", programmed in BDL. This separates cleanly the concepts of what the methods do, the object processes, from the circumstances in which they are allowed to do it, the control. The object controller ensures that scheduling constraints between the object's methods are met. Aggregate objects can be controlled in terms of their components. This language has a convenient formal base. Thus, using BDL expressions, behavioral properties of objects or groups of interesting objects can be verified. Our approach allows, for example, deadlock detection or verification of safety properties, while maintaining a reasonable code size for the running controller. A compiler from BDL has been implemented, automatically generating controller code in an Esterel program, i.e., in a reactive programming language. From this code, the Esterel compiler, in turn, generates an automaton on which verifications are done. Then this automaton is translated into a C code to be executed. This multistage process typifies the method for successful use of a domain-specific language. This also allows high level concurrent programming.","0098-5589;1939-3520;2326-3881","","10.1109/32.798324","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=798324","","Domain specific languages;Automata;Proposals;Object oriented modeling;Aggregates;System recovery;Safety;Automatic control;Size control;Program processors","parallel languages;object-oriented languages;formal specification;program compilers;automata theory;synchronisation;scheduling","BDL;specialized language;per-object reactive control;concurrent behavior;object oriented languages;behavior units;domain-specific language;execution controller;object processes;object controller;scheduling constraints;aggregate objects;formal base;BDL expressions;behavioral properties;deadlock detection;safety properties;Esterel program;reactive programming language;Esterel compiler;automaton;C code;multistage process;high level concurrent programming","","3","","47","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study of Test Case Filtering Techniques Based on Exercising Information Flows","W. Masri; A. Podgurski; D. Leon","IEEE Computer Society; IEEE Computer Society; NA","IEEE Transactions on Software Engineering","","2007","33","7","454","477","Some software defects trigger failures only when certain local or nonlocal program interactions occur. Such interactions are modeled by the closely related concepts of information flows, program dependences, and program slices. The latter concepts underlie a 78 variety of proposed test data adequacy criteria, and they form a potentially important basis for filtering existing test cases. We report the results of an empirical study of several test case filtering techniques that are based on exercising information flows. Both coverage-based and profile-distribution-based filtering techniques are considered. They are compared to filtering techniques based on exercising simpler program elements, such as basic blocks, branches, function calls, and call pairs, with respect to their effectiveness for revealing defects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1020","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4227828","Information flow;dynamic slicing;test case filtering;test suite minimization;coverage-based filtering;distribution-based filtering;software testing;empirical studies.","Information filtering;Information filters;Automatic testing;Software testing;Computer Society;Instruments;Computer science;Joining processes;Timing","program slicing;program testing;software fault tolerance","test case filtering;information flows;software defects;program interactions;program dependences;program slices;coverage-based filtering;profile-distribution-based filtering","","45","","49","","","","","","IEEE","IEEE Journals & Magazines"
"An Integrated Approach for Effective Injection Vulnerability Analysis of Web Applications through Security Slicing and Hybrid Constraint Solving","J. Thom&#x00E9;; L. K. Shar; D. Bianculli; L. Briand","SnT, University of Luxembourg, Luxembourg, LU Luxembourg (e-mail: julian.thome@uni.lu); SnT Centre, Universite du Luxembourg, 81872 Luxembourg, Luxembourg Luxembourg (e-mail: shar@svv.lu); SnT, University of Luxembourg, Luxembourg, LU Luxembourg 2721 (e-mail: domenico.bianculli@uni.lu); SnT Centre, University of Luxembourg, Luxembourg, Luxembourg Luxembourg 2721 (e-mail: lionel.briand@uni.lu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Malicious users can attack Web applications by exploiting injection vulnerabilities in the source code. This work addresses the challenge of detecting injection vulnerabilities in the server-side code of Java Web applications in a scalable and effective way. We propose an integrated approach that seamlessly combines security slicing with hybrid constraint solving; the latter orchestrates automata-based solving with meta-heuristic search. We use static analysis to extract minimal program slices relevant to security from Web programs and to generate attack conditions. We then apply hybrid constraint solving to determine the satisfiability of attack conditions and thus detect vulnerabilities.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2844343","Fonds National de la Recherche Luxembourg; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8373739","Vulnerability detection;constraint solving;static analysis;search-based software engineering","Security;Benchmark testing;Tools;Explosions;Java;Static analysis;Reliability","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Representation inheritance: a safe form of ""White box"" code inheritance","S. H. Edwards","Dept. of Comput. Sci., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA","IEEE Transactions on Software Engineering","","1997","23","2","83","92","There are two approaches to using code inheritance for defining new component implementations in terms of existing implementations. Black box code inheritance allows subclasses to reuse superclass implementations as-is, without direct access to their internals. Alternatively, white box code inheritance allows subclasses to have direct access to superclass implementation details, which may be necessary for the efficiency of some subclass operations and to prevent unnecessary duplication of code. Unfortunately, white box code inheritance violates the protection that encapsulation affords superclasses, opening up the possibility of a subclass interfering with the correct operation of its superclass methods. Representation inheritance is proposed as a restricted form of white box code inheritance where subclasses have direct access to superclass implementation details, but are required to respect the representation invariant(s) and abstraction relation(s) of their ancestor(s). This preserves the protection that encapsulation provides, while allowing the freedom of access that component implementers sometimes desire.","0098-5589;1939-3520;2326-3881","","10.1109/32.585498","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585498","","Protection;Encapsulation;Computer Society;Object oriented modeling;Functional programming;Object oriented programming;Safety;Programming profession;Computer science","inheritance;encapsulation;object-oriented programming;software reusability","representation inheritance;black box code inheritance;superclass implementations;white box code inheritance;encapsulation;software reuse;abstraction relations;abstraction function;behavioral subtypes;model-based specification;object-oriented programming;representation invariance","","4","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Mutation Operators for Spreadsheets","R. Abraham; M. Erwig","Oregon State University, Corvallis; Oregon State University, Corvallis","IEEE Transactions on Software Engineering","","2009","35","1","94","108","Based on 1) research into mutation testing for general-purpose programming languages and 2) spreadsheet errors that have been reported in the literature, we have developed a suite of mutation operators for spreadsheets. We present an evaluation of the mutation adequacy of definition-use adequate test suites generated by a constraint-based automatic test-case generation system we have developed in previous work. The results of the evaluation suggest additional constraints that can be incorporated into the system to target mutation adequacy. In addition to being useful in mutation testing of spreadsheets, the operators can be used in the evaluation of error-detection tools and also for seeding spreadsheets with errors for empirical studies. We describe two case studies where the suite of mutation operators helped us carry out such empirical evaluations. The main contribution of this paper is a suite of mutation operators for spreadsheets that can be used for performing empirical evaluations of spreadsheet tools to indicate ways in which the tools can be improved.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4609389","Test coverage of code;Test design;Spreadsheets;Test coverage of code;Test design;Spreadsheets","Genetic mutations;System testing;Software testing;Computer languages;Automatic testing;Software engineering;Costs;Performance evaluation;Spreadsheet programs;Collaboration","program testing;spreadsheet programs","spreadsheet mutation operator;constraint-based automatic test-case generation system;error-detection tool;general purpose programming language;mutation testing","","31","","86","","","","","","IEEE","IEEE Journals & Magazines"
"Parametric fault tree for the dependability analysis of redundant systems and its high-level Petri net semantics","A. Bobbio; G. Franceschinis; R. Gaeta; L. Portinale","Dipt. di Informatica, Univ. del Piemonte Orientale, Alessandria, Italy; Dipt. di Informatica, Univ. del Piemonte Orientale, Alessandria, Italy; NA; NA","IEEE Transactions on Software Engineering","","2003","29","3","270","287","In order to cope efficiently with the dependability analysis of redundant systems with replicated units, a new, more compact fault-tree formalism, called Parametric Fault Tree (PFT), is defined. In a PFT formalism, replicated units are folded and indexed so that only one representative of the similar replicas is included in the model. From the PFT, a list of parametric cut sets can be derived, where only the relevant patterns leading to the system failure are evidenced regardless of the actual identity of the component in the cut set. The paper provides an algorithm to convert a PFT into a class of High-Level Petri Nets, called SWN. The purpose of this conversion is twofold: to exploit the modeling power and flexibility of the SWN formalism, allowing the analyst to include statistical dependencies that could not have been accommodated into the corresponding PFT and to exploit the capability of the SWN formalism to generate a lumped Markov chain, thus alleviating the state explosion problem. The search for the minimal cut sets (qualitative analysis) can be often performed by a structural T-invariant analysis on the generated SWN. The advantages that can be obtained from the translation of a PFT into a SWN are investigated considering a fault-tolerant multiprocessor system example.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183940","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183940","","Fault trees;Power system modeling;Independent component analysis;US Department of Transportation;Tellurium;Performance analysis;Failure analysis;Probability;Computer Society;Petri nets","Petri nets;redundancy;fault tolerant computing","Petri net semantics;dependability analysis;redundant systems;Parametric Fault Tree;SWN;lumped Markov chain;minimal cut sets;structural T-invariant analysis;fault-tolerant multiprocessor","","32","","31","","","","","","IEEE","IEEE Journals & Magazines"
"From UML to Petri Nets: The PCM-Based Methodology","S. Distefano; M. Scarpa; A. Puliafito","University of Messina, Sicily; University of Messina, Sicily; University of Messina, Sicily","IEEE Transactions on Software Engineering","","2011","37","1","65","79","In this paper, we present an evaluation methodology to validate the performance of a UML model, representing a software architecture. The proposed approach is based on open and well-known standards: UML for software modeling and the OMG Profile for Schedulability, Performance, and Time Specification for the performance annotations into UML models. Such specifications are collected in an intermediate model, called the Performance Context Model (PCM). The intermediate model is translated into a performance model which is subsequently evaluated. The paper is focused on the mapping from the PCM to the performance domain. More specifically, we adopt Petri nets as the performance domain, specifying a mapping process based on a compositional approach we have entirely implemented in the ArgoPerformance tool. All of the rules to derive a Petri net from a PCM and the performance measures assessable from the former are carefully detailed. To validate the proposed technique, we provide an in-depth analysis of a web application for music streaming.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5396344","Software engineering;performances evaluation;Petri nets;UML;software performance engineering.","Unified modeling language;Petri nets;Phase change materials;Software architecture;Software performance;Context modeling;Application software;Design engineering;Performance analysis;Stochastic processes","media streaming;Petri nets;software architecture;software metrics;software performance evaluation;Unified Modeling Language;Web services","UML;Petri nets;PCM;software architecture;software modeling;OMG profile;schedulability;time specification;performance context model;mapping process;ArgoPerformance tool;music streaming;Web application","","26","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on an optimal set of indices for a relational database","B. -. Falkowski","Datex Al, Gesellschaft fuer Angewandte Kunstliche Intelligenz mbH, Munchen, Germany","IEEE Transactions on Software Engineering","","1992","18","2","168","171","M. Y. L. Ip et al., (see ibid., vol.SE-9, p.135-43, 1983) solved the index selection problem for a relational database by reducing it to a classical knapsack problem and then applying an approximation algorithm. It is shown that this reduction process does not work in general by providing a counterexample, and its practical significance is discussed. It turns out that the main idea of Ip et al. need not be discarded. In particular, the approximation algorithm used can be adapted fairly easily to take care of the problems which were raised by the counterexample. In spite of its simplicity, this modification can lead to a reduced number of indices, which is rather attractive from a practical point of view.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.121758","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=121758","","Relational databases;Costs;Approximation algorithms;Artificial intelligence;Indexes;NP-complete problem;Organizing","approximation theory;database theory;optimisation;relational databases","index selection problem;relational database;classical knapsack problem;approximation algorithm;reduction process","","4","","6","","","","","","IEEE","IEEE Journals & Magazines"
"The model checker SPIN","G. J. Holzmann","Comput. Sci. Res. Center, AT&T Bell Labs., Murray Hill, NJ, USA","IEEE Transactions on Software Engineering","","1997","23","5","279","295","SPIN is an efficient verification system for models of distributed software systems. It has been used to detect design errors in applications ranging from high-level descriptions of distributed algorithms to detailed code for controlling telephone exchanges. The paper gives an overview of the design and structure of the verifier, reviews its theoretical foundation, and gives an overview of significant practical applications.","0098-5589;1939-3520;2326-3881","","10.1109/32.588521","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=588521","","Software systems;Application software;Distributed algorithms;Control system synthesis;Algorithm design and analysis;Error correction codes;Telephony;Design methodology;Concurrent computing;Message passing","formal verification;formal specification;distributed processing","SPIN model checker;efficient verification system;distributed software system models;design error detection;high-level distributed algorithm descriptions;detailed code;telephone exchange control;verifier design;verifier structure","","1557","","82","","","","","","IEEE","IEEE Journals & Magazines"
"Reference Model for Smooth Growth of Software Systems(003)5402022","","","IEEE Transactions on Software Engineering","","1996","22","8","","","","0098-5589;1939-3520;2326-3881","","10.1109/32.536959","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=536959","","Software systems;Size measurement;Software measurement;Reflection;Informatics;Electrical resistance measurement;Electric resistance","","","","1","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Environmental Testing Techniques for Software Certification","M. S. Karasik","OASAS Limited","IEEE Transactions on Software Engineering","","1985","SE-11","9","934","938","The problems of developing software requirements and quality assurance techniques have basically dealt with an environment where a single organization acts as the designer, developer, and user of the software product. Since the mid-1970' s, however, there has been a great increase in the use of ""packaged"" software products designed and developed by one organization for use in a variety of other organizations. The great profusion of products has resulted in many products being peddled for generic applications (accounting, manufacturing, etc.) which are of questionable quality and/or ""fit"" to a given organization's environment. This paper describes some techniques that are being used to certify software produced by third parties and how to determine if the ""fit"" is there. Current quality assurance techniques deal with the ""correctness"" of a program as compared to its specifications [2], [4], [7], [8], [12]. The real issue for a purchaser of software is whether the software is ""correct"" for its environment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232551","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702111","Environmental testing;logical pathway;quality assurance;system testing;test generator;test plans","Software testing;Certification;Software quality;Quality assurance;Software packages;Packaging;Software design;Product design;Application software;Manufacturing","","Environmental testing;logical pathway;quality assurance;system testing;test generator;test plans","","","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Extending statecharts with temporal logic","A. Sowmya; S. Ramesh","Sch. of Comput. Sci. & Eng., New South Wales Univ., Sydney, NSW, Australia; NA","IEEE Transactions on Software Engineering","","1998","24","3","216","231","The task of designing large real-time reactive systems, which interact continuously with their environment and exhibit concurrency properties, is a challenging one. The authors explore the utility of a combination of behavior and function specification languages in specifying such systems and verifying their properties. An existing specification language, statecharts, is used to specify the behavior of real-time reactive systems, while a new logic-based language called FNLOG (based on first-order predicate calculus and temporal logic) is designed to express the system functions over real time. Two types of system properties, intrinsic and structural, are proposed. It is shown that both types of system properties are expressible in FNLOG and may be verified by logical deduction, and also hold for the corresponding behavior specification.","0098-5589;1939-3520;2326-3881","","10.1109/32.667880","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=667880","","Real time systems;Concurrent computing;Specification languages;Formal specifications;Embedded computing;Embedded system;Calculus;Logic design;Robots;Hardware","specification languages;formal specification;real-time systems;logic programming languages","temporal logic;statecharts extension;large real-time reactive system design;concurrency properties;continuous environmental interaction;function specification languages;behavior specification languages;property verification;specification language;logic-based language;FNLOG;first-order predicate calculus;system functions;intrinsic system properties;structural system properties;logical deduction","","16","","64","","","","","","IEEE","IEEE Journals & Magazines"
"A theorem prover for verifying iterative programs over integers","D. Sarkar; S. C. De Sarkar","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Kharagpur, India; Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Kharagpur, India","IEEE Transactions on Software Engineering","","1989","15","12","1550","1566","An implementation of a rule-based theorem prover for verifying iterative programs over integers is presented. The authors emphasize the overall proof construction strategy of the prover which has been able to construct the correctness proofs of all iterative programs taken from the literature. Two performance measures for the prover are proposed, and its proof construction for an array-sorting program is evaluated using these measures.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58767","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58767","","Programmable logic arrays;Computer science;Sorting;Humans","expert systems;iterative methods;program verification;theorem proving","rule-based theorem prover;iterative programs;overall proof construction strategy;correctness proofs;performance measures;array-sorting program","","9","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Investigating reading techniques for object-oriented framework learning","F. Shull; F. Lanubile; V. R. Basili","Fraunhofer Center, Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","11","1101","1118","The empirical study described in the paper addresses software reading for construction: how application developers obtain an understanding of a software artifact for use in new system development. The study focuses on the processes that developers would engage in when learning and using object oriented frameworks. We analyzed 15 student software development projects using both qualitative and quantitative methods to gain insight into what processes occurred during framework usage. The contribution of the study is not to test predefined hypotheses but to generate well-supported hypotheses for further investigation. The main hypotheses produced are that example based techniques are well suited to use by beginning learners, while hierarchy based techniques are not, because of a larger learning curve. Other more specific hypotheses are proposed and discussed.","0098-5589;1939-3520;2326-3881","","10.1109/32.881720","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=881720","","Application software;Programming;Software engineering;Software libraries;Computer Society;Testing;Technical activities;User interfaces;Buildings;Skeleton","bibliographies;computer science education;teaching;object-oriented programming;reverse engineering","reading techniques;object oriented framework learning;software reading;application developers;software artifact;system development;object oriented frameworks;student software development projects;quantitative methods;framework usage;predefined hypotheses;example based techniques;beginning learners;hierarchy based techniques;learning curve","","33","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Conceptual modeling of coincident failures in multiversion software","B. Littlewood; D. R. Miller","Centre for Software Reliability, City Univ., London, UK; NA","IEEE Transactions on Software Engineering","","1989","15","12","1596","1614","Work by D.E. Eckhardt and L.D. Lee (1985), shows that independently developed program versions fail dependently. The authors show that there is a precise duality between input choice and program choice in this model and consider a generalization in which different versions can be developed using diverse methodologies. The use of diverse methodologies is shown to decrease the probability of the simultaneous failure of several versions. Indeed, it is theoretically possible to obtain versions which exhibit better than independent failure behavior. The authors formalize the notion of methodological diversity by considering the sequence of decision outcomes that constitute a methodology. They show that diversity of decision implies likely diversity of behavior for the different versions developed under such forced diversity. For certain one-out-of-n systems the authors obtain an optimal method for allocating diversity between versions. For two-out-of-three systems there seem to be no simple optimality results which do not depend on constraints which cannot be verified in practice.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58771","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58771","","Fault tolerance;Reliability engineering;Operations research;Battery powered vehicles;Diversity methods;Stochastic processes;Councils;Glands;Software reliability;Cities and towns","decision theory;fault tolerant computing;probability;software reliability","conceptual modeling;coincident failures;multiversion software;independently developed program versions;precise duality;input choice;program choice;diverse methodologies;simultaneous failure;independent failure behavior;methodological diversity;decision outcomes;optimal method;constraints","","128","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing software review meetings: results of a comparative analysis of two experimental studies","A. A. Porter; P. M. Johnson","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA","IEEE Transactions on Software Engineering","","1997","23","3","129","145","Software review is a fundamental tool for software quality assurance. Nevertheless, there are significant controversies as to the most efficient and effective review method. One of the most important questions currently being debated is the utility of meetings. Although almost all industrial review methods are centered around the inspection meeting, recent findings call their value into question. In prior research the authors separately and independently conducted controlled experimental studies to explore this issue. The paper presents new research to understand the broader implications of these two studies. To do this, they designed and carried out a process of ""reconciliation"" in which they established a common framework for the comparison of the two experimental studies, reanalyzed the experimental data with respect to this common framework, and compared the results. Through this process they found many striking similarities between the results of the two studies, strengthening their individual conclusions. It also revealed interesting differences between the two experiments, suggesting important avenues for future research.","0098-5589;1939-3520;2326-3881","","10.1109/32.585501","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585501","","Software reviews;Costs;Inspection;Software quality;Computer Society;Programming;Job shop scheduling;Collaborative work;Aggregates","software quality;software development management;inspection","software review meeting assessment;software quality assurance;inspection meeting;reconciliation","","56","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Problems with Precision: A Response to ""Comments on 'Data Mining Static Code Attributes to Learn Defect Predictors'""","T. Menzies; A. Dekhtyar; J. Distefano; J. Greenwald","Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV 26506; Department of Computer Science, California State Polytechnic University, San Luis Obispo, CA 93407; Integrated Software Metrics, 1000 Technology Drive, Suite 1215, Fairmont, WV 26554; Department of Computer Science, Portland State University, PO Box 751, Portland, OR 97207-0751","IEEE Transactions on Software Engineering","","2007","33","9","637","640","Zhang and Zhang argue that predictors are useless unless they have high precison&amp;recall. We have a different view, for two reasons. First, for SE data sets with large neg/pos ratios, it is often required to lower precision to achieve higher recall. Second, there are many domains where low precision detectors are useful.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70721","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288197","","Data mining;Detectors;Equations;Predictive models;Accuracy;Software engineering;NASA;Testing;Performance evaluation;Project management","data mining","data mining static code attributes;defect predictors","","97","","15","","","","","","IEEE","IEEE Journals & Magazines"
"A framework for quantitative modeling and analysis of highly (re)configurable systems","M. Ter Beek; A. Legay; A. Lluch Lafuente; A. Vandin","ISTI, CNR, Pisa, Pisa Italy 56124 (e-mail: maurice.terbeek@isti.cnr.it); computer science, Inria, Rennes, Britany France 35000 (e-mail: axel.legay@inria.fr); Department of Applied Mathematics and Computer Science, Technical University of Denmark, Kongens Lyngby, Hovedstaden Denmark (e-mail: albl@dtu.dk); Department of Applied Mathematics and Computer Science, Technical University of Denmark, Kongens Lyngby, Hovedstaden Denmark (e-mail: anvan@dtu.dk)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","This paper presents our approach to the quantitative modeling and analysis of highly (re)configurable systems, like software product lines. Different combinations of the optional features of such systems give rise to combinatorially many individual system variants. We use a formal modeling language that allows us to model systems with probabilistic behavior, possibly subject to quantitative feature constraints, and able to dynamically install, remove or replace features. Our models are defined in the probabilistic feature-oriented language QFLan, a rich domain specific language (DSL) for systems with variability defined in terms of features. QFLan specifications are automatically encoded in terms of a process algebra whose operational behavior interacts with a store of constraints and with a semantics based on discrete-time Markov chains. Our analysis is based on statistical model checking, which allow us to scale to larger models with respect to precise probabilistic techniques. The analyses we can conduct range from the likelihood of specific behavior to the expected average cost of specific system variants. Our approach is supported by a novel Eclipse-based tool including state-of-the-art DSL utilities for QFLan as well as analysis plug-ins. We provide a number of case studies that have driven and validated the development of our framework.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2853726","FP7 Information and Communication Technologies; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8405597","","Probabilistic logic;Model checking;Tools;Analytical models;Runtime;Computational modeling;DSL","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"Correction to 'The case for electric design of real-time software'","B. Sanden","Dept. of Inf. Syst. & Syst. Eng. George Mason Univ., Fairfax, VA, USA","IEEE Transactions on Software Engineering","","1989","15","7","926","","The title of this paper (see ibid., vol.15, p.360-2 (1989)) was printed incorrectly. It should have read: the case for the eclectic design of real-time software.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.29491","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=29491","","Computer aided software engineering;Software design","real-time systems;software engineering","real-time software;eclectic design","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Deriving a Slicing Algorithm via FermaT Transformations","M. P. Ward; H. Zedan","De Montfort University, Leicester; De Montfort University, Leicester","IEEE Transactions on Software Engineering","","2011","37","1","24","47","In this paper, we present a case study in deriving an algorithm from a formal specification via FermaT transformations. The general method (which is presented in a separate paper) is extended to a method for deriving an implementation of a program transformation from a specification of the program transformation. We use program slicing as an example transformation since this is of interest outside the program transformation community. We develop a formal specification for program slicing in the form of a WSL specification statement which is refined into a simple slicing algorithm by applying a sequence of general purpose program transformations and refinements. Finally, we show how the same methods can be used to derive an algorithm for semantic slicing. The main novel contributions of this paper are: 1) developing a formal specification for slicing, 2) expressing the definition of slicing in terms of a WSL specification statement, and 3) by applying correctness preserving transformations to the specification, we can derive a simple slicing algorithm.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401170","Program slicing;program transformations;formal methods;algorithm derivation.","Formal specifications;Logic;Software algorithms;Reverse engineering;Assembly;High level languages","formal specification;program slicing","slicing algorithm;FermaT transformations;program transformation;program slicing;WSL specification statement;semantic slicing","","5","","73","","","","","","IEEE","IEEE Journals & Magazines"
"Specifying and Validating Data-Aware Temporal Web Service Properties","S. Halle; R. Villemaire; O. Cherkaoui","University of California, Santa Barbara, Santa Barbara; Universit&#x0E9; du Qu&#x0E9;bec &#x0E0; Montr&#x0E9;al, Montr&#x0E9;al; Universit&#x0E9; du Qu&#x0E9;bec &#x0E0; Montr&#x0E9;al, Montr&#x0E9;al","IEEE Transactions on Software Engineering","","2009","35","5","669","683","Most works that extend workflow validation beyond syntactical checking consider constraints on the sequence of messages exchanged between services. These constraints are expressed only in terms of message names and abstract away their actual data content. We provide examples of real-world ""data-aware"" Web service constraints where the sequence of messages and their content are interdependent. To this end, we present CTL-FO<sup>+</sup>, an extension over computation tree logic that includes first-order quantification on message content in addition to temporal operators. We show how CTL-FO<sup>+</sup> is adequate for expressing data-aware constraints, give a sound and complete model checking algorithm for CTL-FO<sup>+</sup>, and establish its complexity to be PSPACE-complete. A ""naive"" translation of CTL-FO<sup>+</sup> into CTL leads to a serious exponential blowup of the problem that prevents existing validation tools to be used. We provide an alternate translation of CTL-FO<sup>+</sup> into CTL, where the construction of the workflow model depends on the property to validate. We show experimentally how this translation is significantly more efficient for complex formulas and makes model checking of data-aware temporal properties on real-world Web service workflows tractable using off-the-shelf tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4907003","Web services;software/program verification;model checking;temporal logic.","Web services;Logic;Formal languages;Computer Society;Web and internet services;Guidelines;Contracts;Terminology;Simple object access protocol","computational complexity;formal specification;program verification;temporal logic;trees (mathematics);Web services","data-aware temporal Web service property;workflow validation;syntactical checking;messages exchange;CTL-FO<sup>+</sup>;computation tree logic;first-order quantification;model checking algorithm complexity;PSPACE-complete;formal specification","","22","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Respectful type converters","J. M. Wing; J. Ockerbloom","Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","7","579","593","In converting an object of one type to another, we expect some of the original object's behavior to remain the same and some to change. How can we state the relationship between the original object and converted object to characterize what information is preserved and what is lost after the conversion takes place? We answer this question by introducing the new relation, respects, and say that a type converter function C:A/spl rarr/B respects a type T. We formally define respects in terms of the Liskov and Wing behavioral notion of subtyping; types A and B are subtypes of T. We explain in detail the applicability of respectful type converters in the context of the Typed Object Model (TOM) Conversion Service, built at Carnegie Mellon and used on a daily basis throughout the world. We also briefly discuss how our respects relation addresses a similar question in two other contexts: type evolution and interoperability.","0098-5589;1939-3520;2326-3881","","10.1109/32.859529","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859529","","Information retrieval;Image converters;Displays;HTML;Context-aware services;Object oriented modeling;Context modeling;Internet;Web sites","type theory;object-oriented programming","type converters;respects;type converter function;subtyping;respectful type converters;Typed Object Model;type evolution;interoperability","","6","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to 'Allocating programs containing branches and loops within a multiple processor system'","D. Towsley","Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA","IEEE Transactions on Software Engineering","","1990","16","4","472","","A number of typographical errors in the above-mentioned paper (see ibid., vol.12, no.10, p.1018-24 (1986)) are corrected.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.54299","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=54299","","Computer errors;Costs;Equations;Councils;Formal specifications","operating systems (computers);scheduling","branches;loops;multiple processor system;typographical errors","","6","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting fault incidence using software change history","T. L. Graves; A. F. Karr; J. S. Marron; H. Siy","Los Alamos Nat. Lab., NM, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","7","653","661","This paper is an attempt to understand the processes by which software ages. We define code to be aged or decayed if its structure makes it unnecessarily difficult to understand or change and we measure the extent of decay by counting the number of faults in code in a period of time. Using change management data from a very large, long-lived software system, we explore the extent to which measurements from the change history are successful in predicting the distribution over modules of these incidences of faults. In general, process measures based on the change history are more useful in predicting fault rates than product metrics of the code: For instance, the number of times code has been changed is a better indication of how many faults it will contain than is its length. We also compare the fault rates of code of various ages, finding that if a module is, on the average, a year older than an otherwise similar module, the older module will have roughly a third fewer faults. Our most successful model measures the fault potential of a module as the sum of contributions from all of the times the module has been changed, with large, recent changes receiving the most weight.","0098-5589;1939-3520;2326-3881","","10.1109/32.859533","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859533","","History;Predictive models;Software systems;Aging;Time measurement;Software measurement;Length measurement;Software development management;Statistical analysis;Degradation","software maintenance;software metrics;software fault tolerance;management of change","fault incidence;software change history;change management data;change history;fault potential;code decay;metrics;statistical analysis","","319","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to 'A practical view of software measurement and implementation experiences within Motorola'","M. K. Daskalantonakis","Motorola, Arlington Heights, IL, USA","IEEE Transactions on Software Engineering","","1993","19","2","199","200","A corrected reference list for the above-titled paper (see ibid., vol.18, no.11, p.998-1010, (1992)) is presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.214837","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=214837","","Software measurement;Software engineering;Application software;Software metrics;Conferences;Extraterrestrial measurements;Software prototyping;Quality control;Software quality","software metrics","software measurement;implementation;Motorola","","","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Clock trees: logical clocks for programs with nested parallelism","K. Audenaert","Univ. of Gent-ELIS, Belgium","IEEE Transactions on Software Engineering","","1997","23","10","646","658","A vector clock is a valuable tool for maintaining run time concurrency information in parallel programs. A novel method is presented for modifying vector clocks to make them suitable for programs with nested fork join parallelism (having a variable number of tasks). The resulting kind of clock is called a clock tree, due to its tree structure. The clock tree method compares favorably with other timestamping methods for variable parallelism: task identifier reuse and task recycling. The worst case space requirements of clock trees equals the best case for the latter two methods, and the average size of a clock tree is much smaller than the size of a vector with task recycling. Furthermore, the algorithm for maintaining clock trees does not require a shared data structure and thus avoids the serialization bottleneck that task recycling suffers from.","0098-5589;1939-3520;2326-3881","","10.1109/32.637147","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=637147","","Clocks;Recycling;Timing;Fault detection;Tree data structures;Runtime;Concurrent computing;Parallel processing;Labeling;Parallel programming","parallel programming;computational complexity;clocks;trees (mathematics);tree data structures","logical clocks;nested parallelism;vector clock;run time concurrency information;parallel programs;nested fork join parallelism;timestamping methods;tree structure;clock tree method;variable parallelism;task identifier reuse;task recycling;worst case space requirements;serialization bottleneck","","14","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Inconsistency management for multiple-view software development environments","J. Grundy; J. Hosking; W. B. Mugridge","Dept. of Comput. Sci., Waikato Univ., Hamilton, New Zealand; NA; NA","IEEE Transactions on Software Engineering","","1998","24","11","960","981","Developers need tool support to help manage the wide range of inconsistencies that occur during software development. Such tools need to provide developers with ways to define, detect, record, present, interact with, monitor and resolve complex inconsistencies between different views of software artifacts, different developers and different phases of software development. This paper describes our experience with building complex multiple-view software development tools that support diverse inconsistency management facilities. We describe software architectures that we have developed and user interface techniques that are used in our multiple-view development tools, and we discuss the effectiveness of our approaches compared to other architectural and HCI techniques.","0098-5589;1939-3520;2326-3881","","10.1109/32.730545","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=730545","","Programming;Software development management;Software systems;Documentation;Environmental management;Collaborative software;Computer Society;Phase detection;Computerized monitoring;Software tools","programming environments;software tools;software architecture;groupware;configuration management","inconsistency management facilities;multiple-view software development environments;software tool support;software artifacts;software developers;software development phases;software development tools;software architectures;integrated software development environments;user interface techniques;HCI techniques;collaborative software development","","57","","74","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient evaluation of multifactor dependent system performance using fractional factorial design","T. Berling; P. Runeson","Ericsson Microwave Syst. AB, Molndal, Sweden; NA","IEEE Transactions on Software Engineering","","2003","29","9","769","781","Performance of computer-based systems may depend on many different factors, internal and external. In order to design a system to have the desired performance or to validate that the system has the required performance, the effect of the influencing factors must be known. Common methods give no or little guidance on how to vary the factors during prototyping or validation. Varying the factors in all possible combinations would be too expensive and too time-consuming. This paper introduces a systematic approach to the prototyping and the validation of a system's performance, by treating the prototyping or validation as an experiment, in which the fractional factorial design methodology is commonly used. To show that this is possible, a case study evaluating the influencing factors of the false and real target rate of a radar system is described. Our findings show that prototyping and validation of system performance become structured and effective when using the fractional factorial design. The methodology enables planning, performance, structured analysis, and gives guidance for appropriate test cases. The methodology yields not only main factors, but also interacting factors. The effort is minimized for finding the results, due to the methodology. The case study shows that after 112 test cases, of 1024 possible, the knowledge gained was enough to draw conclusions on the effects and interactions of 10 factors. This is a reduction with a factor 5-9 compared to alternative methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232283","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232283","","System performance;Testing;Prototypes;Design methodology;Radar;Performance analysis;Time measurement;Microcomputers;Filters;Temperature dependence","radar computing;software prototyping;software performance evaluation;formal verification","multifactor dependent system performance;fractional factorial design;prototyping;validation;systematic approach;target rate;radar system;planning","","25","","20","","","","","","IEEE","IEEE Journals & Magazines"
"A domain-specific language for regular sets of strings and trees","N. Klarlund; M. I. Schwartzbach","AT&T Labs.-Res., Denmark; NA","IEEE Transactions on Software Engineering","","1999","25","3","378","386","We propose a novel high level programming notation, called FIDO, that we have designed to concisely express regular sets of strings or trees. In particular, it can be viewed as a domain-specific language for the expression of finite state automata on large alphabets (of sometimes astronomical size). FIDO is based on a combination of mathematical logic and programming language concepts. This combination shares no similarities with usual logic programming languages. FIDO compiles into finite state string or tree automata, so there is no concept of run-time. It has already been applied to a variety of problems of considerable complexity and practical interest. We motivate the need for a language like FIDO, and discuss our design and its implementation. Also, we briefly discuss design criteria for domain-specific languages that we have learned from the work with FIDO. We show how recursive data types, unification, implicit coercions, and subtyping can be merged with a variation of predicate logic, called the Monadic Second-order Logic (M2L) on trees. FIDO is translated first into pure M2L via suitable encodings, and finally into finite state automata through the MONA tool.","0098-5589;1939-3520;2326-3881","","10.1109/32.798326","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=798326","","Domain specific languages;Logic programming;Encoding;Application software;Computer languages;Runtime;Learning automata;Software systems;Embedded computing;Tree graphs","high level languages;set theory;trees (mathematics);string matching;finite state machines;data structures;type theory;formal logic","domain-specific language;regular sets;trees;high level programming notation;FIDO;finite state automata;large alphabets;mathematical logic;programming language concepts;logic programming languages;finite state string;tree automata;design criteria;recursive data types;unification;implicit coercions;subtyping;predicate logic;Monadic Second-order Logic;pure M2L;MONA tool","","3","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Finding Bugs in Web Applications Using Dynamic Test Generation and Explicit-State Model Checking","S. Artzi; A. Kiezun; J. Dolby; F. Tip; D. Dig; A. Paradkar; M. D. Ernst","Thomas J. Watson Research Center, Hawthorne; Women's Hospital/Harvard Medical School, Boston; Thomas J. Watson Research Center, Hawthorne; Thomas J. Watson Research Center, Hawthorne; University of Illinois at Urbana-Champaign, Urbana; Thomas J. Watson Research Center, Hawthorne; University of Washington, Seattle","IEEE Transactions on Software Engineering","","2010","36","4","474","494","Web script crashes and malformed dynamically generated webpages are common errors, and they seriously impact the usability of Web applications. Current tools for webpage validation cannot handle the dynamically generated pages that are ubiquitous on today's Internet. We present a dynamic test generation technique for the domain of dynamic Web applications. The technique utilizes both combined concrete and symbolic execution and explicit-state model checking. The technique generates tests automatically, runs the tests capturing logical constraints on inputs, and minimizes the conditions on the inputs to failing tests so that the resulting bug reports are small and useful in finding and fixing the underlying faults. Our tool Apollo implements the technique for the PHP programming language. Apollo generates test inputs for a Web application, monitors the application for crashes, and validates that the output conforms to the HTML specification. This paper presents Apollo's algorithms and implementation, and an experimental evaluation that revealed 673 faults in six PHP Web applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5416728","Software testing;Web applications;dynamic analysis;PHP;reliability;verification.","Computer bugs;Vehicle crash testing;Automatic testing;Logic testing;Computer crashes;Usability;Internet;Concrete;Computer languages;HTML","program debugging;program testing;program verification;software tools","dynamic test generation;explicit state model checking;Web script;Web pages;Apollo tool;PHP programming language;HTML specification;PHP Web applications;bugs;Internet","","41","","48","","","","","","IEEE","IEEE Journals & Magazines"
"An extended banker's algorithm for deadlock avoidance","S. -. Lang","Sch. of Comput. Sci., Univ. of Central Florida, Orlando, FL, USA","IEEE Transactions on Software Engineering","","1999","25","3","428","432","We describe a natural extension of the banker's algorithm (D.W. Dijkstra, 1968) for deadlock avoidance in operating systems. Representing the control flow of each process as a rooted tree of nodes corresponding to resource requests and releases, we propose a quadratic-time algorithm which decomposes each flow graph into a nested family of regions, such that all allocated resources are released before the control leaves a region. Also, information on the maximum resource claims for each of the regions can be extracted prior to process execution. By inserting operating system calls when entering a new region for each process at runtime, and applying the original banker's algorithm for deadlock avoidance, this method has the potential to achieve better resource utilization because information on the ""localized approximate maximum claims"" is used for testing system safety.","0098-5589;1939-3520;2326-3881","","10.1109/32.798330","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=798330","","System recovery;Resource management;Operating systems;Safety;System testing;Software algorithms;Tree graphs;Flow graphs;Data mining;Runtime","concurrency control;flow graphs;resource allocation;computational complexity","extended banker algorithm;deadlock avoidance;operating systems;control flow;rooted tree;resource requests;quadratic-time algorithm;flow graph;nested family of regions;allocated resources;maximum resource claims;process execution;operating system calls;resource utilization;localized approximate maximum claims;system safety testing","","11","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Statistical Estimation of Software Reliability","S. M. Ross","Department of Industrial Engineering and OPerations Research, University of California","IEEE Transactions on Software Engineering","","1985","SE-11","5","479","483","When a new computer software package is developed, a testing procedure is often put into effect to eliminate the faults, or bugs, in the package. One common procedure is to try the package on a set of well-known problems to try to see if any errors result. This goes for some fixed time with all detected errors being noted. Then the testing stops and the package is carefully checked to determine the specific bugs that were responsible for the observed errors, and the package is then altered to remove these bugs. A problem of great importance is the estimation of the error rate of this revised software package.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232487","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702038","Estimation;reliability;Poisson process","Software reliability;Packaging;Computer bugs;Software packages;Error analysis;Debugging;Software testing;Computer errors;Estimation error;Industrial engineering","","Estimation;reliability;Poisson process","","23","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Design-level performance prediction of component-based applications","Y. Liu; I. Gorton; A. Fekete","Nat. ICT Australia, NSW, Australia; Nat. ICT Australia, NSW, Australia; NA","IEEE Transactions on Software Engineering","","2005","31","11","928","941","Server-side component technologies such as Enterprise JavaBeans (EJBs), .NET, and CORBA are commonly used in enterprise applications that have requirements for high performance and scalability. When designing such applications, architects must select suitable component technology platform and application architecture to provide the required performance. This is challenging as no methods or tools exist to predict application performance without building a significant prototype version for subsequent benchmarking. In this paper, we present an approach to predict the performance of component-based server-side applications during the design phase of software development. The approach constructs a quantitative performance model for a proposed application. The model requires inputs from an application-independent performance profile of the underlying component technology platform, and a design description of the application. The results from the model allow the architect to make early decisions between alternative application architectures in terms of their performance and scalability. We demonstrate the method using an EJB application and validate predictions from the model by implementing two different application architectures and measuring their performance on two different implementations of the EJB platform.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.127","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556552","Index Terms- Quality analysis and evaluation;software architectures;performance measures.","Application software;Scalability;Computer architecture;Java;Buildings;Software prototyping;Prototypes;Software design;Programming;Predictive models","object-oriented programming;software metrics;software performance evaluation;Java","design-level performance prediction;enterprise JavaBeans;.NET;CORBA;component-based server-side application;software development;quantitative performance model;application-independent performance profile","","42","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional programming abstractions for mobile computing","P. J. McCann; G. -. Roman","Lucent Technol., Naperville, IL, USA; NA","IEEE Transactions on Software Engineering","","1998","24","2","97","110","Recent advances in wireless networking technology and the increasing demand for ubiquitous, mobile connectivity demonstrate the importance of providing reliable systems for managing the reconfiguration and disconnection of components. The design of such systems requires tools and techniques appropriate to the task. Many formal models of computation, including UNITY, are not adequate for expressing reconfiguration and disconnection and are, therefore, inappropriate vehicles for investigating the impact of mobility on the construction of modular and composable systems. Algebraic formalisms such as the /spl pi/-calculus have been proposed for modeling mobility. This paper addresses the question of whether UNITY, a state-based formalism with a foundation in temporal logic, can be extended to address concurrent, mobile systems. In the process, we examine some new abstractions for communication among mobile components that express reconfiguration and disconnection and which can be composed in a modular fashion.","0098-5589;1939-3520;2326-3881","","10.1109/32.666824","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=666824","","Mobile computing;Computer networks;Pervasive computing;Computer Society;Computational modeling;Computer network reliability;Computer network management;Technology management;Vehicles;Modular construction","wireless LAN;process algebra;temporal logic;reconfigurable architectures;programming theory;algebraic specification","compositional programming abstractions;mobile computing;wireless networking technology;mobile connectivity;reliable systems;component reconfiguration;component disconnection;formal models;computation;UNITY;modular systems;composable systems;algebraic formalisms;/spl pi/-calculus;state-based formalism;temporal logic;concurrent mobile systems;mobile component communications;weak consistency;shared variables;synchronization;transient interactions","","35","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Transformation-based diagnosis of student programs for programming tutoring systems","Songwen Xu; Yam San Chee","Peoplesoft Inc., Santa Clara, CA, USA; NA","IEEE Transactions on Software Engineering","","2003","29","4","360","384","A robust technology that automates the diagnosis of students' programs is essential for programming tutoring systems. Such technology should be able to determine whether programs coded by a student are correct. If a student's program is incorrect, the system should be able to pinpoint errors in the program as well as explain and correct the errors. Due to the difficulty of this problem, no existing system performs this task entirely satisfactorily, and this problem still hampers the development of programming tutoring systems. This paper describes a transformation-based approach to automate the diagnosis of students' programs for programming tutoring systems. Improved control-flow analysis and data-flow analysis are used in program analysis. Automatic diagnosis of student programs is achieved by comparing the student program with a specimen program at the semantic level after both are standardized. The approach was implemented and tested on 525 real student programs for nine different programming tasks. Test results show that the method satisfies the requirements stated above. Compared to other existing approaches to automatic diagnosis of student programs, the approach developed here is more rigorous and safer in identifying student programming errors. It is also simpler to make use of in practice. Only specimen programs are needed for the diagnosis of student programs. The techniques of program standardization and program comparison developed here may also be useful for research in the fields of program understanding and software maintenance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1191799","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1191799","","Automatic programming;Error correction;Program processors;Data analysis;Testing;Computer Society;Robustness;Computer errors;Automatic control;Standardization","computer science education;computer aided instruction;program diagnostics","transformation-based program diagnosis;student programs;programming tutoring systems;control-flow analysis;data-flow analysis;student programming error identification;program standardization;program comparison;program understanding;software maintenance","","20","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic verification of C++ generic algorithms","Changqing Wang; D. R. Musser","Dept. of Software Syst., GTE Labs. Inc., Waltham, MA, USA; NA","IEEE Transactions on Software Engineering","","1997","23","5","314","323","Dynamic verification is a new approach to formal verification, applicable to generic algorithms such as those found in the Standard Template Library (STL, part of the Draft ANSI/ISO C++ Standard Library). Using behavioral abstraction and symbolic execution techniques, verifications are carried out at an abstract level such that the results can be used in a variety of instances of the generic algorithms without repeating the proofs. This is achieved by substituting for type parameters of generic algorithms special data types that model generic concepts by accepting symbolic inputs and deducing outputs using inference methods. By itself, this symbolic execution technique supports testing of programs with symbolic values at an abstract level. For formal verification one also needs to generate multiple program execution paths and use assertions (to handle while loops, for example), but the authors show how this can be achieved via directives to a conventional debugger program and an analysis database. The assertions must still be supplied, but they can be packaged separately and evaluated as needed by appropriate transfers of control orchestrated via the debugger. Unlike all previous verification methods, the dynamic verification method thus works without having to transform source code or process it with special interpreters. They include an example of the formal verification of an STL generic algorithm.","0098-5589;1939-3520;2326-3881","","10.1109/32.588523","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=588523","","Formal verification;ISO standards;Debugging;ANSI standards;Libraries;Inference algorithms;Testing;Data analysis;Databases;Packaging","formal verification;program debugging;program interpreters;subroutines;software libraries;data structures;inference mechanisms;program testing","C++ generic algorithms;dynamic verification;formal verification;Standard Template Library;behavioral abstraction;symbolic execution techniques;data types;model generic concepts;symbolic inputs;output deduction;inference methods;program testing;multiple program execution paths;debugger program;analysis database;control transfer;STL generic algorithm","","7","","32","","","","","","IEEE","IEEE Journals & Magazines"
"System Testing Aided by Structured Analysis: A Practical Experience","T. J. Mc Cabe; G. G. Schulmeyer","McCabe &amp; Associates, Inc.; NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","917","921","This paper deals with the use of Structured Analysis just prior to system acceptance testing. Specifically, the drawing of data flow diagrams (DFD) was done after integration testing. The DFD's provided a picture of the logical flow through the integrated system for thorough system acceptance testing. System test sets, were derived from the flows in the DFD's. System test repeatability was enhanced by the matrix which flowed from the test sets.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232549","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702109","Data flow diagrams;structured analysis;system testing","System testing;Design for disassembly;Costs;Project management;Packaging;Documentation;Companies;Logic testing;Software testing;Software systems","","Data flow diagrams;structured analysis;system testing","","4","","10","","","","","","IEEE","IEEE Journals & Magazines"
"ADTEST: a test data generation suite for Ada software systems","M. J. Gallagher; V. Lakshmi Narasimhan","Sch. of Inf. Technol., Queensland Univ., Qld., Australia; NA","IEEE Transactions on Software Engineering","","1997","23","8","473","484","Presents the design of the software system ADTEST (ADa TESTing), for generating test data for programs developed in Ada83. The key feature of this system is that the problem of test data generation is treated entirely as a numerical optimization problem and, as a consequence, this method does not suffer from the difficulties commonly found in symbolic execution systems, such as those associated with input variable-dependent loops, array references and module calls. Instead, program instrumentation is used to solve a set of path constraints without explicitly knowing their form. The system supports not only the generation of integer and real data types, but also non-numerical discrete types such as characters and enumerated types. The system has been tested on large Ada programs (60,000 lines of code) and found to reduce the effort required to test programs as well as providing an increase in test coverage.","0098-5589;1939-3520;2326-3881","","10.1109/32.624304","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=624304","","Software testing;System testing;Software systems;Software design;Instruments;Optimization methods;Character generation;Inspection;Software tools;Production","program testing;Ada;software tools;optimisation;abstract data types","ADTEST;software test data generation suite;Ada83 software systems;numerical optimization;symbolic execution systems;program instrumentation;path constraints;integer data type;real data type;nonnumerical discrete types;characters;enumerated types;test coverage","","70","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Experimental evaluation of a real-time scheduler for a multiprocessor system","B. A. Blake; K. Schwan","Dept. of Comput. & Inf. Sci., Cleveland State Univ., OH, USA; NA","IEEE Transactions on Software Engineering","","1991","17","1","34","44","A description is given of the design, implementation, and experimental evaluation of a multiprocessor scheduler used with robotics applications and other real-time programs. The scheduler makes decisions concerning both the assignment of processes and the scheduling of these processes on each processor such that a near-optimal numer of processor deadlines is satisfied. It assumes that process execution times, deadlines, and earliest possible start times are known.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67577","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67577","","Real time systems;Multiprocessing systems;Dynamic scheduling;Processor scheduling;Scheduling algorithm;Robots;Operating systems;Application software;Costs;Helium","multiprogramming;real-time systems;scheduling","real-time scheduler;multiprocessor system;multiprocessor scheduler;robotics applications;assignment;processor deadlines;process execution times;earliest possible start times","","19","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Optimization models for reliability of modular software systems","O. Berman; N. Ashrafi","Fac. of Manage., Toronto Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","1993","19","11","1119","1123","The authors present optimization models for software systems that are developed using a modular design technique. Four different software structures are considered: one program, no redundancy; one program, with redundancy; multiple programs, no redundancy; and multiple programs, with redundancy. The optimization problems are solved by using the authors' version of established optimization methods. The practical usefulness of this study is to draw the attention of software practitioners to an existing methodology that may be used to make an optimal selection out of an available pool of modules with known reliability and cost. All four models maximize software reliability while ensuring that expenditures remain within available resources. The software manager is allowed to select the appropriate model for a given situation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.256858","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=256858","","Software systems;Redundancy;Software reliability;Cost function;Fault tolerance;Application software;Fault tolerant systems;Dynamic programming;Resource management;Software development management","dynamic programming;fault tolerant computing;redundancy;software reliability","optimization models;modular software systems;modular design technique;redundancy;software practitioners;software reliability;dynamic programming;fault tolerance;integer programming;modularization","","67","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Performance measurement and modeling to evaluate various effects on a shared memory multiprocessor","X. Zhang","Div. of Math. & Comput. Sci., Texas Univ., San Antonio, TX, USA","IEEE Transactions on Software Engineering","","1991","17","1","87","93","Shared-memory multiprocessor performance is strongly affected by factors such as sequential code, barriers, cache coherence, virtual memory paging, and the multiprocessor system itself with resource scheduling and multiprogramming. Several timing models and analysis for these effects are presented. A modified Ware model based on these timing models is given to evaluate comprehensive performance of a shared-memory multiprocessor. Performance measurement has been done on the Encore Multimax, a shared-memory multiprocessor. The evaluation models are the analyses based on a general shared-memory multiprocessor system and architecture and can be applied to other types of shared-memory multiprocessors. Analytical and experimental results give a clear understanding of the various effects and a correct measure of the performance, which are important for the effective use of a shared-memory multiprocessor.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67581","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67581","","Measurement;Timing;Performance analysis;Parallel processing;Coherence;Multiprocessing systems;Algorithm design and analysis;Parallel algorithms;Runtime;Parallel architectures","multiprocessing systems;performance evaluation","performance measurement;performance modelling;shared memory multiprocessor;sequential code;barriers;cache coherence;virtual memory paging;resource scheduling;multiprogramming;timing models;modified Ware model;Encore Multimax;architecture","","9","","20","","","","","","IEEE","IEEE Journals & Magazines"
"A method for design and performance modeling of client/server systems","D. A. Menasce; H. Gomaa","Dept. of Comput. Sci., George Mason Univ., Fairfax, VA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","11","1066","1085","Designing complex distributed client/server applications that meet performance requirements may prove extremely difficult in practice if software developers are not willing or do not have the time to help software performance analysts. The paper advocates the need to integrate both design and performance modeling activities so that one can help the other. We present a method developed and used by the authors in the design of a fairly large and complex client/server application. The method is based on a software performance engineering language developed by one of the authors. Use cases were developed and mapped to a performance modeling specification using the language. A compiler for the language generates an analytic performance model for the system. Service demand parameters at servers, storage boxes, and networks are derived by the compiler from the system specification. A detailed model of DBMS query optimizers allows the compiler to estimate the number of I/Os and CPU time for SQL statements. The paper concludes with some results of the application that prompted the development of the method and language.","0098-5589;1939-3520;2326-3881","","10.1109/32.881718","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=881718","","Design methodology;Application software;Software performance;Network servers;Performance analysis;Computer Society;Databases;Unified modeling language;Mission critical systems;Costs","client-server systems;software performance evaluation;formal specification;program compilers;query processing","performance modeling;client/server systems design;distributed client/server applications;performance requirements;software developers;software performance analysts;performance modeling activities;software performance engineering language;use cases;performance modeling specification;compiler;analytic performance model;service demand parameters;storage boxes;system specification;DBMS query optimizers;CPU time;SQL statements","","37","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Predictability of process resource usage: a measurement-based study on UNIX","M. V. Devarakonda; R. K. Iyer","Coord. Sci. Lab., Illinois Univ., Urbana, IL, USA; Coord. Sci. Lab., Illinois Univ., Urbana, IL, USA","IEEE Transactions on Software Engineering","","1989","15","12","1579","1586","A statistical approach is developed for predicting the CPU time, the file I/O, and the memory requirements of a program at the beginning of its life, given the identity of the program. Initially, statistical clustering is used to identify high-density regions of process resource usage. The identified regions form the states for building a state-transition model to characterize the resource usage of each program in its past executions. The prediction scheme uses the knowledge of the program's resource usage in its last execution together with its state-transition model to predict the resource usage in its next execution. The prediction scheme is shown to work using process resource-usage data collected from a VAX 11/780 running 4.3 BSD Unix. The results show that the predicted values correlate strongly with the actual; the coefficient of correlation between the predicted and actual values for CPU time is 0.84. The errors in prediction are mostly small and are heavily skewed toward small values.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58769","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58769","","Predictive models;Time measurement;Accuracy;Load management;Costs;Distributed computing;Aerodynamics;NASA;High performance computing","performance evaluation;program testing;scheduling;statistical analysis;Unix","measurement-based study;statistical approach;CPU time;file I/O;memory requirements;statistical clustering;high-density regions;process resource usage;state-transition model;prediction scheme;VAX 11/780;BSD Unix;correlation","","52","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Validating Halstead's theory for Pascal programs","L. Felician; G. Zalateu","Dipartimento di Ingegneria Elettronica, Elettrotecnica, e Inf., Trieste Univ., Italy; NA","IEEE Transactions on Software Engineering","","1989","15","12","1630","1632","M.H. Halstead's theory (1977) has been validated for different languages, but Pascal programs seem to fit only partially with the theory. D.B. Johnston and A.M. Lister (1981) first recognized the lack of operators due to the structure of this language and proposed a modification of Halstead's formula. The article confirms their results but suggests a correction to their formula, which is particularly necessary for large programs. Experimental results, obtained by examining about 550 Pascal programs, represent the widest test to date of Halstead theory with regard to Pascal programs.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58773","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58773","","Data processing;Testing;Software algorithms","Pascal;programming theory","Pascal programs;operators;large programs;Halstead theory","","4","","17","","","","","","IEEE","IEEE Journals & Magazines"
"On the practical need for abstraction relations to verify abstract data type representations","M. Sitaraman; B. W. Weide; W. F. Ogden","Dept. of Stat. & Comput. Sci., West Virginia Univ., Morgantown, WV, USA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","3","157","170","The typical correspondence between a concrete representation and an abstract conceptual value of an abstract data type (ADT) variable (object) is a many-to-one function. For example, many different pointer aggregates give rise to exactly the same binary tree. The theoretical possibility that this correspondence generally should be relational has long been recognized. By using a nontrivial ADT for handling an optimization problem, the authors show why the need for generalizing from functions to relations arises naturally in practice. Making this generalization is among the steps essential for enhancing the practical applicability of formal reasoning methods to industrial-strength software systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.585503","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585503","","Concrete;Computer Society;Aggregates;Binary trees;Computer industry;Industrial relations;Software systems;Formal specifications;Greedy algorithms","abstract data types;data structures;formal specification;program verification;optimisation;tree data structures","abstraction relations;abstract data type representation verification;concrete representation;abstract conceptual value;abstract data type variable;pointer aggregates;binary tree;optimization problem;formal reasoning methods;industrial-strength software systems","","6","","26","","","","","","IEEE","IEEE Journals & Magazines"
"A note on inconsistent axioms in Rushby's ""systematic formal verification for fault-tolerant time-triggered algorithms""","L. Pike","Galois Connections, Beaverton, OR, USA","IEEE Transactions on Software Engineering","","2006","32","5","347","348","We describe some inconsistencies in John Rushby's axiomatization of time-triggered algorithms that he presented in these transactions and that he formally specifies and verifies in the mechanical theorem-prover PVS. We present corrections for these inconsistencies that have been checked for consistency in PVS","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642681","Formal methods;formal verification;time-triggered algorithms;synchronous systems;PVS.","Formal verification;Fault tolerant systems;Clocks;Formal specifications;Real time systems;Conferences;Error correction;Floors","formal specification;formal verification;software fault tolerance;theorem proving","systematic formal verification;John Rushby axiomatization;fault-tolerant computing;formal specification;time-triggered algorithms;PVS mechanical theorem-prover","","7","","4","","","","","","IEEE","IEEE Journals & Magazines"
"An Extension of ""Representative Instances and γ-Acyclic Relational Schemes""","S. Jajodia","Computer Science and Systems Branch, Naval Research Laboratory","IEEE Transactions on Software Engineering","","1987","SE-13","9","1047","1048","Let R be a γ-acyclic relational scheme, and let F be the set of functional dependencies (FD's) embodied in R. Given an existence constrained database r over R, it was shown in [1] that it is possible to connect tuples from different relations in r and construct a universal instance L, possibly containing null values δ, such that the total projection of L onto R yields exactly the set r. Moreover, conditions were given which guarantee that this L would satisfy the functional dependency with nulls (NFD) counterparts of FD's, in F. The purpose of this note is to generalize the latter result and show that under the same conditions, L actually satisfies NFD counterparts of FD's in the closure F+ of F.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233792","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702327","Functional dependency with nulls;γ-acyclic relational scheme;representative instance;universal instance","Relational databases;Constraint theory;Terminology;Computer science;Transaction databases","","Functional dependency with nulls;γ-acyclic relational scheme;representative instance;universal instance","","","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Critical races in Ada programs","C. M. McNamee; R. A. Olsson","Dept. of Electr. Eng. & Comput. Sci., California Univ., Davis, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Davis, CA, USA","IEEE Transactions on Software Engineering","","1990","16","12","1439","","Comments are made on the above named work by G.M. Karam, C.M. Stanczyk, and G.W. Bond (see ibid., vol.15, no.11, p.1471-80, 1989), in which the semantics of the Ada rendezvous mechanism are discussed in terms of the critical race problem and a method is proposed for designing critical race-free programs. It is noted that this problem has been well described and numerous solutions have been presented in the literature during the past ten years (1980-90).<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.62452","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=62452","","Bonding;Design methodology;Computer science;Strontium;Computer languages;Discrete event simulation;Processor scheduling;Prototypes;Handicapped aids;Operating systems","Ada;programming","semantics;Ada rendezvous mechanism;critical race problem;critical race-free programs","","","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Secure broadcasting using the secure lock","Guang-Huei Chiou; Wen-Tsuen Chen","Inst. of Comput. Sci., Nat. Tsing-Hua Univ., Hsinchu, Taiwan; Inst. of Comput. Sci., Nat. Tsing-Hua Univ., Hsinchu, Taiwan","IEEE Transactions on Software Engineering","","1989","15","8","929","934","The authors discuss secure broadcasting, effected by means of a secure lock, on broadcast channels, such as satellite, radio, etc. This lock is implemented by using the Chinese Remainder theorem (CRT). The secure lock offers the following advantages: only one copy of the ciphertext is sent; the deciphering operation is efficient; and the number of secret keys held by each user is minimized. Protocols for secure broadcasting using the secure lock, based on the public-key cryptosystem as well as the private-key cryptosystem, are presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31350","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31350","","Satellite broadcasting;Radio broadcasting;Public key cryptography;Casting;Cathode ray tubes;Telecommunication traffic;Protocols;Local area networks;Packet radio networks","broadcasting;cryptography;protocols;telecommunication channels","secure broadcasting protocols;secret key minimization;session key;secure broadcasting;secure lock;broadcast channels;satellite;radio;Chinese Remainder theorem;ciphertext;deciphering operation;secret keys;public-key cryptosystem;private-key cryptosystem","","145","","5","","","","","","IEEE","IEEE Journals & Magazines"
"On the Asymptotic Optimality of First-Fit Storage Allocation","E. G. Coffman; T. T. Kadota; L. A. Shepp","AT& T Bell Laboratories; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","2","235","239","Suppose requests to store files arrive at a storage facility in a Poisson stream at rate 1. Each file is allocated storage space on arrival and each remains independently for an exponential time with mean l/p. The lengths of the files are assumed to be independent with common distribution F. Each file is placed in the lowest addressed contiguous sequence of locations large enough to accommodate the fre at its arrival time. This is the so-called first-fit storage discipline. We conjecture that first-fit is asymptotically optimal in the sense that the ratio of expected empty space to expected occupied space tends to zero as p → 0, i.e., as the occupied space tends to ∞. This conjecture seems very hard to prove, but it has been proved for constant file lengths [1], i.e., when F degenerates. We are unable to prove the conjecture but give a graphic display of the results of a Monte Carlo simulation which makes it very convincing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232200","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701993","Analysis of algorithms;data structures;dynamic storage allocation;first-fit allocation","Graphics;Displays;Heuristic algorithms;Data structures;Markov processes;State-space methods;Random variables;Length measurement;Distribution functions;Application software","","Analysis of algorithms;data structures;dynamic storage allocation;first-fit allocation","","1","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""On object systems and behavioral inheritance""","J. C. Chen; H. C. Jiau","Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","IEEE Transactions on Software Engineering","","2003","29","6","576","","After reviewing the regular paper ""On Object Systems and Behavioral Inheritance"" in IEEE Transactions on Software Engineering, vol. 28, no. 9, Sept. 2002, and discussing it with Professor H.C. Jiau, several errors were found and are stated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205185","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205185","","Computer science;Humans;Error correction;Mathematics","inheritance;object-oriented programming","object systems;behavioral inheritance","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing the Effectiveness of Sequence Diagrams in the Comprehension of Functional Requirements: Results from a Family of Five Experiments","S. Abrahão; C. Gravino; E. Insfran; G. Scanniello; G. Tortora","Universitat Politècnica de València, València; University of Salerno via Ponte Don Melillo, Salerno; Universitat Politècnica de València, València; University of Basilicata Viale DellAteneo, Macchia Romana, Potenza; University of Salerno via Ponte Don Melillo, Salerno","IEEE Transactions on Software Engineering","","2013","39","3","327","342","Modeling is a fundamental activity within the requirements engineering process and concerns the construction of abstract descriptions of requirements that are amenable to interpretation and validation. The choice of a modeling technique is critical whenever it is necessary to discuss the interpretation and validation of requirements. This is particularly true in the case of functional requirements and stakeholders with divergent goals and different backgrounds and experience. This paper presents the results of a family of experiments conducted with students and professionals to investigate whether the comprehension of functional requirements is influenced by the use of dynamic models that are represented by means of the UML sequence diagrams. The family contains five experiments performed in different locations and with 112 participants of different abilities and levels of experience with UML. The results show that sequence diagrams improve the comprehension of the modeled functional requirements in the case of high ability and more experienced participants.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6193111","Documentation;software engineering;requirements specifications","Unified modeling language;Object oriented modeling;Analytical models;Computational modeling;Software systems;Materials","formal specification;Unified Modeling Language","effectiveness assessment;UML sequence diagrams;functional requirements;family-of-five experiments;requirements engineering process;abstract descriptions;requirement interpretation;requirement validation;functional stakeholders;software engineering;requirements specifications;unified modeling language","","23","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Alpha: an extension of relational algebra to express a class of recursive queries","R. Agrawal","AT&T Bell Labs., Murray Hill, NJ, USA","IEEE Transactions on Software Engineering","","1988","14","7","879","885","An extension of E.F. Codd's relational algebra (1970) with an alpha ( alpha ) operator is presented that allows a large class of natural and useful recursive queries to be expressed, and yet has the property of being efficiently implementable. Formally, this class is a superset of linear recursive queries. Intuitively, this class comprises queries that examine transitive relationships between various instances of an entity. It is believed that this class covers many natural and interesting recursive queries. Examples of such queries include determining parts requirements for manufacturing a product, finding the critical path in a project management network, finding the shortest path between two cities, verifying connectivity between two points of a circuit, etc.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42731","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42731","","Algebra;Relational databases;Database languages;Calculus;Database systems;Deductive databases;Aggregates;Proposals;Manufacturing;Project management","database theory;query languages;recursive functions;relational databases","Codd;alpha operator;deductive databases;relational algebra;recursive queries;transitive relationships;critical path;project management network","","57","","32","","","","","","IEEE","IEEE Journals & Magazines"
"StakeRare: Using Social Networks and Collaborative Filtering for Large-Scale Requirements Elicitation","S. L. Lim; A. Finkelstein","University College London, London; University College London, London","IEEE Transactions on Software Engineering","","2012","38","3","707","735","Requirements elicitation is the software engineering activity in which stakeholder needs are understood. It involves identifying and prioritizing requirements-a process difficult to scale to large software projects with many stakeholders. This paper proposes StakeRare, a novel method that uses social networks and collaborative filtering to identify and prioritize requirements in large software projects. StakeRare identifies stakeholders and asks them to recommend other stakeholders and stakeholder roles, builds a social network with stakeholders as nodes and their recommendations as links, and prioritizes stakeholders using a variety of social network measures to determine their project influence. It then asks the stakeholders to rate an initial list of requirements, recommends other relevant requirements to them using collaborative filtering, and prioritizes their requirements using their ratings weighted by their project influence. StakeRare was evaluated by applying it to a software project for a 30,000-user system, and a substantial empirical study of requirements elicitation was conducted. Using the data collected from surveying and interviewing 87 stakeholders, the study demonstrated that StakeRare predicts stakeholder needs accurately and arrives at a more complete and accurately prioritized list of requirements compared to the existing method used in the project, taking only a fraction of the time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5740931","Requirements/specifications;elicitation methods;requirements prioritization;experimentation;human factors;recommender systems;social network analysis;stakeholder analysis.","Social network services;Collaboration;Filtering;Software;Size measurement;Software engineering;Business","collaborative filtering;data acquisition;project management;recommender systems;social networking (online);software management","social network;collaborative filtering;requirement elicitation;software engineering;stakeholder;StakeRare;recommender system;software project;data collection","","51","","113","","","","","","IEEE","IEEE Journals & Magazines"
"Software cost reduction methods in practice","J. A. Hager","State College, PA, USA","IEEE Transactions on Software Engineering","","1989","15","12","1638","1644","In 1978, a software cost reduction program was initiated with the goal of applying modern design and documentation principles to the development of large systems. The results of this effort have been documented in several research papers published by D. Parnas et al. (1983). The author extends that approach by updating the methodology based on lessons learned during the application of the concepts to the development of a computer-based training system. An engineering life-cycle which provides more visibility to maintenance concerns is described, and the lessons learned during its implementation are discussed. A summary provides the author's impressions of the methodology and its potential to reduce system maintenance costs.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58775","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58775","","Costs;Software maintenance;Software design;Computer networks;Application software;Documentation;Distributed computing;Equations;Delay;Probability","computer aided instruction;software engineering","software cost reduction program;modern design;documentation principles;computer-based training system;engineering life-cycle;maintenance concerns;system maintenance costs","","5","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Explicit communication revisited: two new attacks on authentication protocols","M. Abadi","Syst. Res. Center, Digital Equipment Corp., Palo Alto, CA, USA","IEEE Transactions on Software Engineering","","1997","23","3","185","186","SSH and AKA are recent, practical protocols for secure connections over an otherwise unprotected network. The paper shows that, despite the use of public-key cryptography, SSH and AKA do not provide authentication as intended. The flaws of SSH and AKA can be viewed as the result of their disregarding a basic principle for the design of sound authentication protocols: the principle that messages should be explicit.","0098-5589;1939-3520;2326-3881","","10.1109/32.585505","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585505","","Authentication;Cryptographic protocols;Public key cryptography;Public key;Knowledge based systems;Internet;Web server;Terminology;Protection;Displays","computer networks;public key cryptography;message authentication;protocols;software engineering","explicit communication;AKA protocol;SSH protocol;authentication protocols;secure connections;unprotected network;public key cryptography;sound authentication protocol design;explicit messages","","10","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Guiding goal modeling using scenarios","C. Rolland; C. Souveyet; C. B. Achour","Centre de Recherche en Inf., Paris I Univ., France; NA; NA","IEEE Transactions on Software Engineering","","1998","24","12","1055","1071","Even though goal modeling is an effective approach to requirements engineering, it is known to present a number of difficulties in practice. The paper discusses these difficulties and proposes to couple goal modeling and scenario authoring to overcome them. Whereas existing techniques use scenarios to concretize goals, we use them to discover goals. Our proposal is to define enactable rules which form the basis of a software environment called L'Ecritoire to guide the requirements elicitation process through interleaved goal modeling and scenario authoring. The focus of the paper is on the discovery of goals from scenarios. The discovery process is centered around the notion of a requirement chunk (RC) which is a pair <Goal, Scenario>. The paper presents the notion of RC, the rules to support the discovery of RCs and illustrates the application of the approach within L'Ecritoire using the ATM example. It also evaluates the potential practical benefits expected from the use of the approach.","0098-5589;1939-3520;2326-3881","","10.1109/32.738339","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=738339","","Proposals;Air traffic control;Human resource management;Documentation;Joining processes","formal specification;systems analysis;programming environments;authoring systems","goal modeling;enactable rules;requirements engineering;scenario authoring;software environment;L'Ecritoire;requirements elicitation process;interleaved goal modeling;goal discovery;discovery process;requirement chunk;ATM example","","211","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Use case maps as architectural entities for complex systems","R. J. A. Buhr","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada","IEEE Transactions on Software Engineering","","1998","24","12","1131","1155","The paper presents a novel, scenario based notation called Use Case Maps (UCMs) for describing, in a high level way, how the organizational structure of a complex system and the emergent behavior of the system are intertwined. The notation is not a behavior specification technique in the ordinary sense, but a notation for helping a person to visualize, think about, and explain the big picture. UCMs are presented as ""architectural entities"" that help a person stand back from the details during all phases of system development. The notation has been thoroughly exercised on systems of industrial scale and complexity and the distilled essence of what has been found to work in practice is summarized. Examples are presented that confront difficult complex system issues directly: decentralized control, concurrency, failure, diversity, elusiveness and fluidity of runtime views of software, self modification of system makeup, difficulty of seeing large scale units of emergent behavior cutting across systems as coherent entities (and of seeing how such entities arise from the collective efforts of components), and large scale.","0098-5589;1939-3520;2326-3881","","10.1109/32.738343","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=738343","","Computer aided software engineering;Runtime;Software systems;Large-scale systems;Visualization;Computer languages;Electrical equipment industry;Distributed control;Concurrent computing","program visualisation;formal specification;systems analysis;software architecture","use case maps;architectural entities;complex systems;scenario based notation;UCMs;organizational structure;emergent behavior;behavior specification technique;decentralized control;concurrency;runtime views;software failure;self modification;system makeup;large scale units;software architecture;system behavior;requirements","","118","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Controversies about the black and white diamonds","F. Barbier; B. Henderson-Sellers","Pau Univ., France; NA","IEEE Transactions on Software Engineering","","2003","29","11","1056","","F. Barbier et al. (2003) offered a formal definition for the semantics of the whole-part relationship in the Unified Modeling Language or UML. H.B.K. Tan et al. (2003) raised problems within some parts of the formalization. We discuss these problems and their remedies developed by H.B.K. Tan et al. (2003).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245308","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245308","","Unified modeling language;Navigation;Australia","specification languages","semantics;whole-part relationship;Unified Modeling Language;UML","","1","","2","","","","","","IEEE","IEEE Journals & Magazines"
"Bayesian analysis of empirical software engineering cost models","S. Chulani; B. Boehm; B. Steece","Centre for Software Eng., IBM Res., San Jose, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","1999","25","4","573","583","Many parametric software estimation models have evolved in the last two decades (L.H. Putnam and W. Myers, 1992; C. Jones, 1997; R.M. Park et al., 1992). Almost all of these parametric models have been empirically calibrated to actual data from completed software projects. The most commonly used technique for empirical calibration has been the popular classical multiple regression approach. As discussed in the paper, the multiple regression approach imposes a few assumptions frequently violated by software engineering datasets. The paper illustrates the problems faced by the multiple regression approach during the calibration of one of the popular software engineering cost models, COCOMO II. It describes the use of a pragmatic 10 percent weighted average approach that was used for the first publicly available calibrated version (S. Chulani et al., 1998). It then moves on to show how a more sophisticated Bayesian approach can be used to alleviate some of the problems faced by multiple regression. It compares and contrasts the two empirical approaches, and concludes that the Bayesian approach was better and more robust than the multiple regression approach.","0098-5589;1939-3520;2326-3881","","10.1109/32.799958","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=799958","","Bayesian methods;Software engineering;Costs;Predictive models;Programming;Calibration;Accuracy;Scheduling;Software quality;Parametric statistics","software cost estimation;Bayes methods;statistical analysis;calibration;project management","Bayesian analysis;empirical software engineering cost models;parametric software estimation models;empirical calibration;software projects;classical multiple regression approach;software engineering datasets;multiple regression approach;software engineering cost models;COCOMO II;weighted average approach;publicly available calibrated version;Bayesian approach;empirical approaches","","124","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Hypercharts: extended statecharts to support hypermedia specification","F. Borges Paulo; P. C. Masiero; M. C. Ferreira de Oliveira","Anderson Consulting, Sao Paulo, Brazil; NA; NA","IEEE Transactions on Software Engineering","","1999","25","1","33","49","Introduces hypercharts, a novel and effective notation that extends the well-known statechart formalism to make it suitable for the specification of temporal and information synchronization requirements of hypermedia applications. Three new definitions are added: timed history, timed transitions, and a set of synchronization mechanisms. The proposed extensions are based on the major characteristics of some Petri net-based multimedia models, and have their semantics described in terms of conventional statechart models. Therefore, any hyperchart construction can be transformed into a statechart that exhibits the desired behavior, giving hyperchart models the same semantic behavior as statecharts. The new constructs are illustrated using a case study based on a hypermedia-modeling example.","0098-5589;1939-3520;2326-3881","","10.1109/32.748917","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=748917","","Multimedia databases;History;Information retrieval;Navigation;Timing;Writing;Computer science;Web sites;Distributed databases;Internet","hypermedia;synchronisation;Petri nets;formal specification;finite state machines","hypercharts;statecharts;hypermedia specification;temporal synchronization;information synchronization;timed history;timed transitions;synchronization mechanisms;Petri net-based multimedia models;semantic behavior;case study;requirements specification","","10","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Reply to comments on ""An Interval Logic for Real-Time System Specification""","P. Bellini; P. Nesi; D. Rogai","Dept. of Syst. & Informatics, Florence Univ., Italy; Dept. of Syst. & Informatics, Florence Univ., Italy; Dept. of Syst. & Informatics, Florence Univ., Italy","IEEE Transactions on Software Engineering","","2006","32","6","428","431","The paper on Comments on ""An Interval Logic for Real-Time System Specification"" presents some remarks on the comparison examples from TILCO and other logics and some slips on the related examples. This paper gives evidence that such issues have no impact on the validity of the TILCO theory of paper and provides some further clarifications about some aspects of the comparison","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650217","Formal language;temporal logic;interval temporal logic;TILCO;conciseness.","Logic;Real time systems;Usability;Solids;Production;Timing","formal specification;real-time systems;specification languages;temporal logic","interval logic;real-time system specification;TILCO theory","","2","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Change Distilling:Tree Differencing for Fine-Grained Source Code Change Extraction","B. Fluri; M. Wuersch; M. PInzger; H. Gall","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","11","725","743","A key issue in software evolution analysis is the identification of particular changes that occur across several versions of a program. We present change distilling, a tree differencing algorithm for fine-grained source code change extraction. For that, we have improved the existing algorithm by Chawathe et al. for extracting changes in hierarchically structured data. Our algorithm extracts changes by finding both a match between the nodes of the compared two abstract syntax trees and a minimum edit script that can transform one tree into the other given the computed matching. As a result, we can identify fine-grained change types between program versions according to our taxonomy of source code changes. We evaluated our change distilling algorithm with a benchmark that we developed, which consists of 1,064 manually classified changes in 219 revisions of eight methods from three different open source projects. We achieved significant improvements in extracting types of source code changes: Our algorithm approximates the minimum edit script 45 percent better than the original change extraction approach by Chawathe et al. We are able to find all occurring changes and almost reach the minimum conforming edit script, that is, we reach a mean absolute percentage error of 34 percent, compared to the 79 percent reached by the original algorithm. The paper describes both our change distilling algorithm and the results of our evolution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70731","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339230","Source code change extraction;tree differencing algorithms;software repositories;software evolution analysis","Data mining;Taxonomy;Software maintenance;Programming profession;Software algorithms;Algorithm design and analysis;Software tools;Maintenance engineering;Software systems;History","software maintenance;software prototyping;tree data structures","minimum edit script;abstract syntax trees;software evolution analysis;fine-grained source code change extraction;change distilling tree differencing algorithm","","190","","47","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient distributed online algorithm to detect strong conjunctive predicates","Loon-Been Chen; I-Chen Wu","Dept. of Inf. Manage., Chin-Min Coll., Miao-Li, Taiwan; NA","IEEE Transactions on Software Engineering","","2002","28","11","1077","1084","Detecting strong conjunctive predicates is a fundamental problem in debugging and testing distributed programs. A strong conjunctive predicate is a logical statement to represent the desired event of the system. Therefore, if the predicate is not true, an error may occur because the desired event does not happen. Recently, several reported detection algorithms reveal the problem of unbounded state queue growth since the system may generate a huge number of execution states in a very short time. In order to solve this problem, this paper introduces the notion of removable states which can be disregarded in the sense that detection results still remain correct. A fully distributed algorithm is developed in this paper to perform the detection in an online manner. Based on the notion of removable states, the time complexity of the detection algorithm is improved as the number of states to be evaluated is reduced.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049405","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049405","","Debugging;Detection algorithms;Testing;Distributed algorithms;Size control;Distributed computing;Event detection;Performance evaluation;Algorithm design and analysis","computational complexity;program debugging;program testing;distributed programming;distributed algorithms","strong conjunctive predicate detection;debugging;testing;distributed online algorithm;distributed programs;logical statement;desired system event;error;unbounded state queue growth;execution states;removable states;time complexity","","2","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Impact of Budget and Schedule Pressure on Software Development Cycle Time and Effort","N. Nan; D. E. Harter","University of Oklahoma, Norman; Syracuse University, Syracuse","IEEE Transactions on Software Engineering","","2009","35","5","624","637","As excessive budget and schedule compression becomes the norm in today's software industry, an understanding of its impact on software development performance is crucial for effective management strategies. Previous software engineering research has implied a nonlinear impact of schedule pressure on software development outcomes. Borrowing insights from organizational studies, we formalize the effects of budget and schedule pressure on software cycle time and effort as U-shaped functions. The research models were empirically tested with data from a 25 billion/year international technology firm, where estimation bias is consciously minimized and potential confounding variables are properly tracked. We found that controlling for software process, size, complexity, and conformance quality, budget pressure, a less researched construct, has significant U-shaped relationships with development cycle time and development effort. On the other hand, contrary to our prediction, schedule pressure did not display significant nonlinear impact on development outcomes. A further exploration of the sampled projects revealed that the involvement of clients in the software development might have ldquoerodedrdquo the potential benefits of schedule pressure. This study indicates the importance of budget pressure in software development. Meanwhile, it implies that achieving the potential positive effect of schedule pressure requires cooperation between clients and software development teams.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815275","Cost estimation;time estimation;schedule and organizational issues;systems development.","Programming;Job shop scheduling;Software performance;Computer industry;Financial management;Software development management;Software engineering;Testing;Pressure control;Size control","budgeting;DP industry;project management;sampling methods;scheduling;software cost estimation;software development management;software metrics;software quality;statistical testing","budget pressure;schedule pressure;software development cycle time estimation;software development effort estimation;software industry;software development performance;software engineering research;nonlinear impact;organizational study;U-shaped function;empirical testing;international technology firm;potential confounding variable;software process control;software size control;software complexity control;software conformance quality control;sampled project management;potential positive effect;software development team management strategy;software cost estimation","","31","","74","","","","","","IEEE","IEEE Journals & Magazines"
"Reliability of systems with Markov transfer of control, II","K. Siegrist","Dept. of Math. & Stat., Alabama Univ., Huntsville, AL, USA","IEEE Transactions on Software Engineering","","1988","14","10","1478","1480","A software/hardware system is considered that can be decomposed into a finite number of modules. It is assumed that control of the system is transferred among the modules according to a Markov process. Each module has an associated reliability that gives the probability that the module will operate correctly when called and will transfer control successfully when finished. The measure of reliability considered is the mean number of transitions until failure, starting in a designated initial state. This measure of system reliability is studied in terms of the module reliability and the transition probabilities. Methods of predicting system reliability are obtained and special branching and sequential systems are studied.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6192","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6192","","Control systems;State-space methods;Probability;Markov processes;Software reliability;Software systems;Hardware;Mathematics;Statistics","Markov processes;probability;software reliability","software reliability;Markov process;probability;system reliability","","16","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Message logging: pessimistic, optimistic, causal, and optimal","L. Alvisi; K. Marzullo","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA","IEEE Transactions on Software Engineering","","1998","24","2","149","159","Message-logging protocols are an integral part of a popular technique for implementing processes that can recover from crash failures. All message-logging protocols require that, when recovery is complete, there be no orphan processes, which are surviving processes whose states are inconsistent with the recovered state of a crashed process. We give a precise specification of the consistency property ""no orphan processes"". From this specification, we describe how different existing classes of message-logging protocols (namely optimistic, pessimistic, and a class that we call causal) implement this property. We then propose a set of metrics to evaluate the performance of message-logging protocols, and characterize the protocols that are optimal with respect to these metrics. Finally, starting from a protocol that relies on causal delivery order, we show how to derive optimal causal protocols that tolerate f overlapping failures and recoveries for a parameter f (1/spl les/f/spl les/n).","0098-5589;1939-3520;2326-3881","","10.1109/32.666828","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=666828","","Computer crashes;Multicast protocols;Fault tolerant systems;Measurement;Checkpointing;Context","protocols;electronic messaging;system recovery;fault tolerant computing;formal specification;performance evaluation","message-logging protocols;crash failure recovery;orphan processes;inconsistent states;recovered state;consistency property specification;optimistic protocols;pessimistic protocols;causal protocols;performance evaluation metrics;optimal protocols;causal delivery order;overlapping failures;checkpoint-restart protocols;resilient processes;fault-tolerance techniques","","98","","22","","","","","","IEEE","IEEE Journals & Magazines"
"The POSTGRES rule manager","M. Stonebraker; E. N. Hanson; S. Potamianos","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA","IEEE Transactions on Software Engineering","","1988","14","7","897","907","The rule subsystem that is being implemented in the POSTGRES DBMS is explained. It is novel in several ways. First, it gives users the capability of defining rules as well as data. Moreover, depending on the scope of each rule defined, optimization is handled differently. This leads to good performance both when there are many rules each of small scope and when there are a few rules each of large scope. In addition, rules provide either a forward-chaining or a backward-chaining control flow, and the system chooses the control mechanism that optimizes performance whenever possible. Priority rules can be defined, allowing a user to specify rule systems that have conflicts. This use of exceptions seems necessary in many applications. Database services such as views, protection, integrity constraints, and referential integrity can be obtained simply by applying the rules system in the appropriate way. Consequently, no special-purpose code need be included in POSTGRES to handle these tasks.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42733","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42733","","Databases;Control systems;Expert systems;Application software;Birds;Protection;Software systems;Memory management;Data security;Laboratories","expert systems;knowledge engineering;relational databases","priority rules;relational databases;inferencing;knowledge engineering;expert systems;query languages;POSTGRES rule manager;rule subsystem;DBMS;optimization;forward-chaining;backward-chaining;exceptions;integrity constraints;referential integrity","","74","","28","","","","","","IEEE","IEEE Journals & Magazines"
"On the statistical analysis of the number of errors remaining in a software design document after inspection","N. B. Ebrahimi","Div. of Stat., Northern Illinois Univ., DeKalb, IL, USA","IEEE Transactions on Software Engineering","","1997","23","8","529","532","Sometimes, complex software systems fail because of faults introduced in the requirements and design stages of the development process. Reviewing documents related to requirements and design by several reviewers can remove some of these faults, but often a few remain undetected until the software is developed. In this paper, we propose a procedure leading to the estimation of the number of faults which are not discovered. The main advantage of our procedure is that we do not need the standard assumption of independence among reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/32.624308","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=624308","","Statistical analysis;Software design;Software systems;Smoothing methods;Software engineering;Programming;Fault detection;Inspection;Parameter estimation;Phase estimation","system documentation;systems analysis;statistical analysis;coding errors;inspection","statistical analysis;remaining errors;software design document;inspection;complex software systems;system failure;software fault estimation;requirements stage;software design stage;software development process;document review;reviewer independence;likelihood function;confidence interval;Chi-squared distribution;nonparametric estimation;smoothing parameter;undiscovered faults","","27","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Hierarchical structuring of superposed GSPNs","P. Buchholz","Dept. of Comput. Sci., Dortmund Univ., Germany","IEEE Transactions on Software Engineering","","1999","25","2","166","181","Superposed generalized stochastic Petri nets (SGSPNs) and stochastic automata networks (SANs) are formalisms to describe Markovian models as a collection of synchronously communicating components. Both formalisms allow a compact representation of the generator matrix of the Markov chain, which can be exploited for very space efficient analysis techniques. The main drawback of the approaches is that for many models the compositional description introduces a large number of unreachable states, such that the gain due to the compact representation of the generator matrix is completely lost. This paper proposes a new approach to avoid unreachable states without losing the possibility to represent the generator matrix in a compact form. The central idea is to introduce a preprocessing step to generate a hierarchical structure which defines a block structure of the generator matrix, where every block can be represented in a compact form similar to the representation of generator matrices originally proposed for SGSPNs or SANs. The resulting structure includes no unreachable states, needs only slightly more space than the compact representation developed for SANs and can still be exploited in efficient numerical solution techniques. Furthermore, the approach is a very efficient method to generate and represent huge reachability sets and graphs.","0098-5589;1939-3520;2326-3881","","10.1109/32.761443","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=761443","","State-space methods;Sparse matrices;Stochastic processes;Numerical analysis;Reachability analysis;Performance analysis;Algebra;Tensile stress;Petri nets;Automata","Petri nets;stochastic automata;reachability analysis;software performance evaluation;Markov processes;matrix algebra;numerical analysis","superposed generalized stochastic Petri nets;stochastic automata networks;hierarchical structuring;Markovian models;synchronously communicating components;space efficient analysis techniques;compositional description;unreachable states;compact generator matrix representation;preprocessing step;block structure;reachability sets;reachability graphs;numerical solution techniques","","32","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Software support for multiprocessor latency measurement and evaluation","Yong Yan; Xiaodong Zhang; Qian Ma","High Performance Comput. & Software Lab., Texas Univ., San Antonio, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","1","4","16","Parallel computing scalability evaluates the extent to which parallel programs and architectures can effectively utilize increasing numbers of processors. In this paper, we compare a group of existing scalability metrics and evaluation models with an experimental metric which uses network latency to measure and evaluate the scalability of parallel programs and architectures. To provide insight into dynamic system performance, we have developed an integrated software environment prototype for measuring and evaluating multiprocessor scalability performance, called Scale-Graph. Scale-Graph uses a graphical instrumentation monitor to collect, measure and analyze latency-related data, and to display scalability performance based on various program execution patterns. The graphical software tool is X-Windows based and is currently implemented on standard workstations to analyze performance data of the KSR-1, a hierarchical ring-based shared-memory architecture.","0098-5589;1939-3520;2326-3881","","10.1109/32.581326","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=581326","","Delay;Software measurement;Scalability;Computer architecture;Data analysis;Performance analysis;Parallel processing;System performance;Software performance;Software prototyping","shared memory systems;parallel architectures;parallel programming;performance evaluation;graphical user interfaces;system monitoring;software metrics;software tools;software performance evaluation;engineering graphics","software support;multiprocessor latency measurement;multiprocessor latency evaluation models;parallel computing scalability;parallel programs;parallel architectures;processor number;scalability metrics;network latency;dynamic system performance;integrated software environment prototype;Scale-Graph;graphical instrumentation monitor;program execution patterns;X-Windows based software tool;workstations;KSR-1 performance data analysis;hierarchical ring-based shared-memory architecture;performance graphical presentation","","3","","16","","","","","","IEEE","IEEE Journals & Magazines"
"An experiment to assess the cost-benefits of code inspections in large scale software development","A. A. Porter; H. P. Siy; C. A. Toman; L. G. Votta","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","6","329","346","We conducted a long term experiment to compare the costs and benefits of several different software inspection methods. These methods were applied by professional developers to a commercial software product they were creating. Because the laboratory for this experiment was a live development effort, we took special care to minimize cost and risk to the project, while maximizing our ability to gather useful data. The article has several goals: (1) to describe the experiment's design and show how we used simulation techniques to optimize it; (2) to present our results and discuss their implications for both software practitioners and researchers; and (3) to discuss several new questions raised by our findings. For each inspection, we randomly assigned three independent variables: (1) the number of reviewers on each inspection team (1, 2, or 4); (2) the number of teams inspecting the code unit (1 or 2); and (3) the requirement that defects be repaired between the first and second team's inspections. The reviewers for each inspection were randomly selected without replacement from a pool of 11 experienced software developers. The dependent variables for each inspection included inspection interval (elapsed time), total effort, and the defect detection rate. Our results showed that these treatments did not significantly influence the defect detection effectiveness, but that certain combinations of changes dramatically increased the inspection interval.","0098-5589;1939-3520;2326-3881","","10.1109/32.601071","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=601071","","Inspection;Large-scale systems;Programming;Costs;Analysis of variance;Software quality;Switches;Computer Society;Laboratories;Design optimization","software cost estimation;cost-benefit analysis;inspection;software quality;professional aspects","code inspection cost benefits;large scale software development;long term experiment;software inspection methods;professional developers;commercial software product;live development effort;experiment design;simulation techniques;software practitioners;independent variables;reviewers;inspection team;code unit;experienced software developers;inspection interval;defect detection rate;defect detection effectiveness","","74","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Reply To: Comments On ""towards A Framework Of Software Measurement Validation""","B. Kitchenham; S. L. Pfleeger; N. Fenton","Keele University; NA; NA","IEEE Transactions on Software Engineering","","1997","23","3","189","189","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1997.585507","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585507","","Software measurement;Size measurement;Logic;Software engineering;Impedance;Time measurement;Size control;Performance analysis;Volume measurement","","","","8","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Special Section - On the International Conference on the Foundations of Software Engineering","","","IEEE Transactions on Software Engineering","","2003","29","10","0_1","0_2","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237167","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237167","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Centroidal Voronoi Tessellations- A New Approach to Random Testing","A. Shahbazi; A. F. Tappenden; J. Miller","University of Alberta, Edmonton; The King's University College, Edmonton; University of Alberta, Edmonton","IEEE Transactions on Software Engineering","","2013","39","2","163","183","Although Random Testing (RT) is low cost and straightforward, its effectiveness is not satisfactory. To increase the effectiveness of RT, researchers have developed Adaptive Random Testing (ART) and Quasi-Random Testing (QRT) methods which attempt to maximize the test case coverage of the input domain. This paper proposes the use of Centroidal Voronoi Tessellations (CVT) to address this problem. Accordingly, a test case generation method, namely, Random Border CVT (RBCVT), is proposed which can enhance the previous RT methods to improve their coverage of the input space. The generated test cases by the other methods act as the input to the RBCVT algorithm and the output is an improved set of test cases. Therefore, RBCVT is not an independent method and is considered as an add-on to the previous methods. An extensive simulation study and a mutant-based software testing investigation have been performed to demonstrate the effectiveness of RBCVT against the ART and QRT methods. Results from the experimental frameworks demonstrate that RBCVT outperforms previous methods. In addition, a novel search algorithm has been incorporated into RBCVT reducing the order of computational complexity of the new approach. To further analyze the RBCVT method, randomness analysis was undertaken demonstrating that RBCVT has the same characteristics as ART methods in this regard.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6165316","Adaptive random testing;centroidal Voronoi tessellation;P-measure;random testing;software testing;test case generation;test strategies","Subspace constraints;Software testing;Generators;Software algorithms;Power capacitors;Runtime","computational complexity;computational geometry;program testing","centroidal Voronoi tessellations;adaptive random testing method;ART method;quasi-random testing method;QRT method;test case generation method;random border CVT;RBCVT algorithm;mutant-based software testing;search algorithm;computational complexity;randomness analysis;software defects","","23","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Temporal logic query checking: a tool for model exploration","A. Gurfinkel; M. Chechik; B. Devereux","Dept. of Comput. Sci., Toronto Univ., Ont., Canada; Dept. of Comput. Sci., Toronto Univ., Ont., Canada; Dept. of Comput. Sci., Toronto Univ., Ont., Canada","IEEE Transactions on Software Engineering","","2003","29","10","898","914","Temporal logic query checking was first introduced by W. Chan in order to speed up design understanding by discovering properties not known a priori. A query is a temporal logic formula containing a special symbol ?/sub 1/, known as a placeholder. Given a Kripke structure and a propositional formula /spl phi/, we say that /spl phi/ satisfies the query if replacing the placeholder by /spl phi/ results in a temporal logic formula satisfied by the Kripke structure. A solution to a temporal logic query on a Kripke structure is the set of all propositional formulas that satisfy the query. Query checking helps discover temporal properties of a system and, as such, is a useful tool for model exploration. In this paper, we show that query checking is applicable to a variety of model exploration tasks, ranging from invariant computation to test case generation. We illustrate these using a Cruise Control System. Additionally, we show that query checking is an instance of a multi-valued model checking of Chechik et al. This approach enables us to build an implementation of a temporal logic query checker, TLQSolver, on top of our existing multi-valued model checker /sub /spl chi//Chek. It also allows us to decide a large class of queries and introduce witnesses for temporal logic queries-an essential notion for effective model exploration.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237171","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237171","","Multivalued logic;Logic design;Computer Society;Testing;Control systems;Buildings","temporal logic;multivalued logic;program diagnostics","temporal logic;model understanding;multi-valued logic;query;Kripke structure;propositional formula;Cruise Control System","","21","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to 'A4 logic-based approach to reverse engineering tools production (Dec 92 1053-1064)","G. Canfora; A. Cimitile; U. de Carlini","Dipartimento di Inf. e Sistemistica, Naples Univ., Italy; Dipartimento di Inf. e Sistemistica, Naples Univ., Italy; Dipartimento di Inf. e Sistemistica, Naples Univ., Italy","IEEE Transactions on Software Engineering","","1993","19","6","640","","A typographic error in rule 10 in the above-titled paper (see ibid., vol.18, no.12, p.1053-64, Dec. 1992), is corrected.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232029","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232029","","Reverse engineering;Production","software engineering","logic-based approach;reverse engineering tools production","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Sloan research project","T. Bergin","NA","IEEE Transactions on Software Engineering","","2000","26","5","478","478","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.846303","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846303","","History;Computer languages;Software engineering;Computer science;Software tools;Collaborative tools;Writing;Permission;Programming profession","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Experiences using lightweight formal methods for requirements modeling","S. Easterbrook; R. Lutz; R. Covington; J. Kelly; Y. Ampo; D. Hamilton","NASA IV&V Fac., Fairmont, WV, USA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","1","4","14","The paper describes three case studies in the lightweight application of formal methods to requirements modeling for spacecraft fault protection systems. The case studies differ from previously reported applications of formal methods in that formal methods were applied very early in the requirements engineering process to validate the evolving requirements. The results were fed back into the projects to improve the informal specifications. For each case study, we describe what methods were applied, how they were applied, how much effort was involved, and what the findings were. In all three cases, formal methods enhanced the existing verification and validation processes by testing key properties of the evolving requirements and helping to identify weaknesses. We conclude that the benefits gained from early modeling of unstable requirements more than outweigh the effort needed to maintain multiple representations.","0098-5589;1939-3520;2326-3881","","10.1109/32.663994","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=663994","","Application software;Protection;Computer Society;NASA;Space vehicles;Embedded software;Feedback;Software safety;Aerospace engineering;Testing","formal specification;systems analysis;program verification;space vehicles;fault tolerant computing;aerospace computing","lightweight formal methods;requirements modeling;case studies;lightweight application;spacecraft fault protection systems;requirements engineering process;evolving requirements validation;informal specifications;validation processes;unstable requirements;multiple representations","","69","","27","","","","","","IEEE","IEEE Journals & Magazines"
"ConTesa: Directed Test Suite Augmentation for Concurrent Software","T. Yu; Z. Huang; C. Wang","Computer Science, University of Kentucky, Lexington, Kentucky United States 40506 (e-mail: tyu@cs.uky.edu); Computer Science, New York University, New York, New York United States (e-mail: zh946@nyu.edu); Department of Computer Science, University of Southern California, Los Angeles, California United States (e-mail: wang626@usc.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","As software evolves, test suite augmentation techniques may be used to identify which part of the program needs to be tested due to code changes and how to generate these new test cases for regression testing. However, existing techniques focus exclusively on sequential software, without considering concurrent software in which multiple threads may interleave with each other during the execution and thus lead to a combinatorial explosion. To fill the gap, we propose ConTesa, the first test suite augmentation tool for concurrent software. The goal is to generate new test cases capable of exercising both code changes and the thread interleavings affected by these code changes. At the center of ConTesa is a two-pronged approach. First, it judiciously reuses the current test inputs while amplifying their interleaving coverage using random thread schedules. Then, it leverages an incremental symbolic execution technique to generate more test inputs and interleavings, to cover the new concurrency-related program behaviors. We have implemented ConTesa and evaluated it on a set of real-world multithreaded Linux applications. Our results show that it can achieve a significantly high interleaving coverage and reveal more bugs than state-of-the-art testing techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2861392","Division of Computing and Communication Foundations; Division of Computer and Network Systems; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8423082","","Testing;Schedules;Instruction sets;Concurrent computing;Tools;Context","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"SYGRAF: implementing logic programs in a database style","M. Kifer; E. L. Lozinskii","Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA; NA","IEEE Transactions on Software Engineering","","1988","14","7","922","935","It is shown how Horn logic programs can be implemented using database techniques, namely, mostly bottom-up in combination with certain top-down elements (as opposed to the top-down implementations of logic programs prevailing so far). The proposed method is sound and complete. It easily lends itself to a parallel implementation and is free of nonlogical features like backtracking. As an extension to the common approach to deductive databases, function symbols are allowed to appear in programs, and it is shown that much of database query optimization can be applied to optimize logic programs. An important advantage of present approach is its ability to evaluate successfully many programs that terminate under neither pure top-down nor bottom-up evaluation strategies.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42735","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42735","","Logic programming;Spatial databases;Deductive databases;Query processing;Database systems;Computer science;Termination of employment;Safety;Computational efficiency;Councils","database theory;formal logic;logic programming;programming theory","logic programming;parallel programming;SYGRAF;Horn logic programs;deductive databases;function symbols;query optimization;bottom-up evaluation","","11","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Finding Faster Configurations using FLASH","V. Nair; Z. Yu; T. Menzies; N. Siegmund; S. Apel","Computer science, North Carolina State University, 6798 Raleigh, North Carolina United States 27606 (e-mail: vivekaxl@gmail.com); Computer science, North Carolina State University, 6798 Raleigh, North Carolina United States (e-mail: zyu9@ncsu.edu); Computer science, North Carolina State University, 6798 Raleigh, North Carolina United States (e-mail: tim.menzies@gmail.com); Media, Bauhaus-University Weimar, Weimar, Thuringia Germany 99423 (e-mail: Norbert.Siegmund@uni-weimar.de); Department of Informatics and Mathematics, University of Passau, Passau, Bavaria Germany 94032 (e-mail: apel@uni-passau.de)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Finding good configurations of a software system is often challenging since the number of configuration options can be large. Software engineers often make poor choices about configuration or, even worse, they usually use a sub-optimal configuration in production, which leads to inadequate performance. To assist engineers in finding the better configuration, this article introduces FLASH, a sequential model-based method that sequentially explores the configuration space by reflecting on the configurations evaluated so far to determine the next best configuration to explore. FLASH scales up to software systems that defeat the prior state-of-the-art model-based methods in this area. FLASH runs much faster than existing methods and can solve both single-objective and multi-objective optimization problems. The central insight of this article is to use the prior knowledge of the configuration space (gained from prior runs) to choose the next promising configuration. This strategy reduces the effort (i.e., number of measurements) required to find the better configuration. We evaluate FLASH using 30 scenarios based on 7 software systems to demonstrate that FLASH saves effort in 100% and 80% of cases in single-objective and multi-objective problems respectively by up to several orders of magnitude compared to state-of-the-art techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2870895","Deutsche Forschungsgemeinschaft; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8469102","Performance prediction;Search-based SE;Configuration;Multi-objective optimization;Sequential Model-based Methods","Software systems;Optimization;Throughput;Storms;Task analysis;Cloud computing","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Corrections to ""Avoiding packaging mismatch with flexible packaging""","R. DeLine","Carnegie Mellon University","IEEE Transactions on Software Engineering","","2001","27","6","577","577","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.926178","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=926178","","Packaging;Connectors","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Classifying Software Changes: Clean or Buggy?","S. Kim; E. J. Whitehead, Jr.; Y. Zhang","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","181","196","This paper introduces a new technique for predicting latent software bugs, called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent buggy change recall on average. Change classification has several desirable qualities: 1) The prediction granularity is small (a change to a single file), 2) predictions do not require semantic information about the source code, 3) the technique works for a broad array of project types and programming languages, and 4) predictions can be made immediately upon the completion of a change. Contributions of this paper include a description of the change classification approach, techniques for extracting features from the source code and change histories, a characterization of the performance of change classification across 12 open source projects, and an evaluation of the predictive power of different groups of features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70773","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4408585","Software maintenance;Metrics/Measurement;Clustering;classification;and association rules;Configuration Management;Data mining;Software maintenance;Metrics/Measurement;Clustering;classification;and association rules;Configuration Management;Data mining","Computer bugs;Machine learning;Open source software;Classification algorithms;History;Data mining;Feature extraction;Software debugging;Project management;Computer languages","data mining;feature extraction;learning (artificial intelligence);program debugging;software maintenance;software metrics","change classification;machine learning classifier;software change;software project;software configuration management repository;programming languages;feature extraction;open source projects;software metrics;software maintenance;association rule","","191","","56","","","","","","IEEE","IEEE Journals & Magazines"
"A fourth-order algorithm with automatic stepsize control for the transient analysis of DSPNs","A. Heindl; R. German","Inst. fur Tech. Inf., Tech. Univ. Berlin, Germany; NA","IEEE Transactions on Software Engineering","","1999","25","2","194","206","This paper presents an efficient and numerically reliable method for the transient analysis of deterministic and stochastic Petri nets. The transient behavior is described by state equations derived by the method of supplementary variables. Significant features of the proposed solution algorithm of fourth order are an automatic stepsize control and a two-stage relative error control. Furthermore, a formal way of dealing with discontinuities in the transient state equations is developed. This resolves the problems posed by initially enabled deterministic transitions and also improves the accuracy of numerical results. Experiments with a queueing system with failure and repair illustrate the efficiency (with respect to both CPU-time and memory space) and the numerical quality of the new algorithm.","0098-5589;1939-3520;2326-3881","","10.1109/32.761445","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=761445","","Automatic control;Transient analysis;Equations;Stochastic processes;Petri nets;Algorithm design and analysis;Error correction;Programmable control;Adaptive control;Software tools","Petri nets;numerical analysis;software performance evaluation","fourth-order algorithm;automatic stepsize control;transient analysis;deterministic stochastic Petri nets;numerically reliable method;efficient method;state equations;supplementary variable;two-stage relative error control;discontinuities;transient state equations;initially enabled deterministic transitions;queueing system;failure;repair;numerical quality","","6","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Achieving strong consistency in a distributed file system","P. Triantafillou; C. Neilson","Dept. of Comput. Eng., Crete Tech. Univ., Greece; NA","IEEE Transactions on Software Engineering","","1997","23","1","35","55","Distributed file systems need to provide for fault tolerance. This is typically achieved with the replication of files. Existing approaches to the construction of replicated file systems sacrifice strong semantics (i.e. the guarantees the systems make to running computations when failures occur and/or files are accessed concurrently). This is done mainly for efficiency reasons. This paper puts forward a replicated file system protocol that enforces strong consistency semantics. Enforcing strong semantics allows for distributed systems to behave more like their centralized counterparts-an essential feature in order to provide the transparency that is so strived for in distributed computing systems. One characteristic of our protocol is its distributed nature. Because of it, the extra cost needed to ensure the stronger consistency is kept low (since the bottleneck problem noticed in primary-copy systems is avoided), load balancing is facilitated, clients can choose physically close servers, and the work required during failure handling and recovery is reduced. Another characteristic is that, instead of optimizing each operation type on its own, file system activity is viewed at the level of a file session and the costs of individual operations were able to be spread over the life of a file session. We have developed a prototype and compared its performance to both NFS and a nonreplicated version of the prototype that also achieves strong consistency semantics. Through these comparisons, the cost of replication and the cost of enforcing the strong consistency semantics are shown.","0098-5589;1939-3520;2326-3881","","10.1109/32.581328","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=581328","","File systems;File servers;Prototypes;Concurrent computing;Access protocols;Distributed computing;Fault tolerant systems;Load management;Cost function;Availability","replicated databases;concurrency control;distributed databases;system recovery;client-server systems;access protocols","strong consistency semantics enforcement;distributed file system;fault tolerance;replicated file system protocol;transparency;system failure recovery;concurrent access;efficiency;load balancing;physically close server selection;file system activity;operation costs;file session;prototype;performance;NFS;nonreplicated version;availability;caching","","8","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Generalization of queueing network product form solutions to stochastic Petri nets","G. Florin; S. Natkin","Centre d'Etudes et de Recherche en Inf. du CNAM, Paris, France; Centre d'Etudes et de Recherche en Inf. du CNAM, Paris, France","IEEE Transactions on Software Engineering","","1991","17","2","99","107","A new solution is given for the steady-state probability computation of closed synchronized queuing networks. A closed synchronized queuing network is a particular Markov stochastic Petri net (a bounded and monovaluated Petri net with a strongly connected reachability graph and constant firing rates independent of markings). It is shown that the steady-state probability distribution can be expressed using matrix products. The results generalize the Gordon-Newell theorem. The solution is similar to the Gordon-Newell product form using matrix and vectors instead of scalars. A prototype solver developed from this result is presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67591","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67591","","Stochastic processes;Petri nets;Steady-state;Probability distribution;Prototypes;Queueing analysis;Computer networks;Helium;Network theory (graphs);Algorithm design and analysis","performance evaluation;Petri nets;queueing theory","performance evaluation;queueing network product form solutions;stochastic Petri nets;steady-state probability;closed synchronized queuing networks;Markov stochastic Petri net;strongly connected reachability graph;constant firing rates;matrix products;Gordon-Newell theorem","","8","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Response To: Comments On ""property-based Software Engineering Measurement: Refining The Additivity Properties""","L. C. Briand; S. Morasca; V. R. Basili","Fraunlwfer Institute for Experimental Software Engineering (IESE); NA; NA","IEEE Transactions on Software Engineering","","1997","23","3","196","197","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1997.585509","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=585509","","Software engineering;Software measurement;Computer Society;Merging","","","","11","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Specification and verification using dependent types","F. K. Hanna; N. Daeche; M. Longley","Fac. of Inf. Technol., Kent Univ., Canterbury, UK; Fac. of Inf. Technol., Kent Univ., Canterbury, UK; Fac. of Inf. Technol., Kent Univ., Canterbury, UK","IEEE Transactions on Software Engineering","","1990","16","9","949","964","VERITAS/sup +/, a specification logic based on dependent types, is described. The overall aim is to demonstrate how the use of dependent types together with subtypes and datatypes allows the writing of specifications that are clear, concise, and generic. The development of theories of arithmetic, numerals, and iterative structures is described, and the proof of a theorem that greatly simplifies the formal verification of iterative arithmetic structures is outlined. The VERITAS/sup +/ logic is defined by modeling it as a partial algebra within a purely functional metalanguage. Aspects of the computational implementation of the logic and its associated toolset are briefly described.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58783","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58783","","Specification languages;Logic design;Formal verification;Logic programming;Arithmetic;Computer languages;Writing;H infinity control;Calculus;Councils","formal specification;iterative methods;modelling;theorem proving","theorem proving;dependent types;VERITAS/sup +/;specification logic;numerals;iterative structures;modeling;functional metalanguage;computational implementation","","15","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Template semantics for model-based notations","Jianwei Niu; J. M. Atlee; N. A. Day","Sch. of Comput. Sci., Waterloo Univ., Ont., Canada; Sch. of Comput. Sci., Waterloo Univ., Ont., Canada; Sch. of Comput. Sci., Waterloo Univ., Ont., Canada","IEEE Transactions on Software Engineering","","2003","29","10","866","882","We propose a template-based approach to structuring the semantics of model-based specification notations. The basic computation model is a nonconcurrent, hierarchical state-transition machine (HTS), whose execution semantics are parameterized. Semantics that are common among notations (e.g., the concept of an enabled transition) are captured in the template, and a notation's distinct semantics (e.g., which states can enable transitions) are specified as parameters. The template semantics of composition operators define how multiple HTSs execute concurrently and how they communicate and synchronize with each other by exchanging events and data. The definitions of these operators use the template parameters to preserve notation-specific behavior in composition. Our template is sufficient to capture the semantics of basic transition systems, CSP, CCS, basic LOTOS, a subset of SDL88, and a variety of statecharts notations. We believe that a description of a notation's semantics using our template can be used as input to a tool that automatically generates formal analysis tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237169","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237169","","Carbon capture and storage;Concurrent computing;Computer Society;Computational modeling;High temperature superconductors;Algebra;Logic;Data mining;Thyristors","programming language semantics;software engineering;parallel programming;communicating sequential processes;calculus of communicating systems","specification notations;hierarchical state-transition machine;HTS;execution semantics;concurrency;computation model;template semantics;software","","21","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Elements of Software Configuration Management""","E. H. Bersoff","BTG, Inc.","IEEE Transactions on Software Engineering","","1985","SE-11","8","822","822","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232531","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702091","","Programming;Joining processes;Books;Software maintenance;Software development management;Documentation;Hardware;Quality assurance;Application software;Engineering management","","","","","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""A formal semantics for object model diagrams""","R. J. Botting; R. H. Bourdeau; B. H. C. Cheng","Dept. of Comput. Sci., California State Univ., San Bernardino, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","1996","22","12","911","","The author indicates some things that need clarifying in the paper cited in the title. The paper does not make it clear that a class of objects is not equivalent to a set of tuples. In a set of tuples two different tuples cannot contain the same data. Two different objects in a given class can contain the same data. In most of the paper this does not matter. However, the simulation function between a subclass and a superclass must be injective, but the (overloaded) simulation function between the sets of tuples usually cannot be injective.","0098-5589;1939-3520;2326-3881","","10.1109/32.553639","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=553639","","Error correction;Object oriented programming","algebraic specification;formal specification;object-oriented methods;diagrams","formal semantics;object model diagrams;tuples;simulation function;subclass;superclass","","","","2","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on “Researcher Bias: The Use of Machine Learning in Software Defect Prediction”","C. Tantithamthavorn; S. McIntosh; A. E. Hassan; K. Matsumoto","Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; School of Computing, Queen's University, Kingston, ON, Canada; Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan","IEEE Transactions on Software Engineering","","2016","42","11","1092","1094","Shepperd et al. find that the reported performance of a defect prediction model shares a strong relationship with the group of researchers who construct the models. In this paper, we perform an alternative investigation of Shepperd et al.'s data. We observe that (a) research group shares a strong association with other explanatory variables (i.e., the dataset and metric families that are used to build a model); (b) the strong association among these explanatory variables makes it difficult to discern the impact of the research group on model performance; and (c) after mitigating the impact of this strong association, we find that the research group has a smaller impact than the metric family. These observations lead us to conclude that the relationship between the research group and the performance of a defect prediction model are more likely due to the tendency of researchers to reuse experimental components (e.g., datasets and metrics). We recommend that researchers experiment with a broader selection of datasets and metrics to combat any potential bias in their results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2553030","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7450669","Software quality assurance;defect prediction;researcher bias","Measurement;Interference;Analysis of variance;Predictive models;Analytical models;NASA;Data models","learning (artificial intelligence);software fault tolerance;software quality","researcher bias;machine learning;software defect prediction;software quality assurance","","10","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Reliability estimation for a software system with sequential independent reviews","N. E. Rallis; Z. F. Lansdowne","Math. Dept., Boston Coll., Chestnut Hill, MA, USA; NA","IEEE Transactions on Software Engineering","","2001","27","12","1057","1061","Suppose that several sequential test and correction cycles have been completed for the purpose of improving the reliability of a given software system. One way to quantify the success of these efforts is to estimate the probability that all faults are found by the end of the last cycle, We describe how to evaluate this probability both prior to and after observing the numbers of faults detected in each cycle and we show when these two evaluations would be the same.","0098-5589;1939-3520;2326-3881","","10.1109/32.988707","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988707","","Software systems;Fault detection;System testing;Software reliability;Fault diagnosis;Software testing;Sequential analysis;Bayesian methods;Performance evaluation;Software measurement","software reliability;program testing;program debugging","reliability estimation;software system;sequential independent reviews;fault probability;sequential test/correction cycles","","15","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Foreword","W. E. Howden","NA","IEEE Transactions on Software Engineering","","1985","SE-11","3","241","241","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232206","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701999","","Programming environments;Software engineering;User interfaces;Engineering management;Design engineering;Graphics;Software design;Conference management;Project management;Research and development management","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""A metrics suite for object oriented design","N. I. Churcher; M. J. Shepperd; S. Chidamber; C. F. Kemerer","Dept. of Appl. Comput. & Electron., Bournemouth Univ., UK; Dept. of Appl. Comput. & Electron., Bournemouth Univ., UK; NA; NA","IEEE Transactions on Software Engineering","","1995","21","3","263","265","A suite of object oriented software metrics has recently been proposed by S.R. Chidamber and C.F. Kemerer (see ibid., vol. 20, p. 476-94, 1994). While the authors have taken care to ensure their metrics have a sound measurement theoretical basis, we argue that is premature to begin applying such metrics while there remains uncertainty about the precise definitions of many of the quantities to be observed and their impact upon subsequent indirect metrics. In particular, we show some of the ambiguities associated with the seemingly simple concept of the number of methods per class. The usefulness of the proposed metrics, and others, would be greatly enhanced if clearer guidance concerning their application to specific languages were to be provided. Such empirical considerations are as important as the theoretical issues raised by the authors.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.372153","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=372153","","Software metrics;Engineering management;Software systems;Software engineering;Terminology;Application software;Information analysis;Algorithm design and analysis;Software algorithms;Software measurement","object-oriented programming;software metrics;software development management","metrics suite;object oriented design;object oriented software metrics;measurement theoretical basis;indirect metrics;number of methods per class;empirical considerations;complexity measures;software management;object orientation","","41","","7","","","","","","IEEE","IEEE Journals & Magazines"
"SOFL: a formal engineering methodology for industrial applications","Shaoying Liu; A. J. Offutt; C. Ho-Stuart; Y. Sun; M. Ohba","Fac. of Inf. Sci., Hiroshima City Univ., Japan; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","1","24","45","Formal methods have yet to achieve wide industrial acceptance for several reasons. They are not well integrated into established industrial software processes, their application requires significant abstraction and mathematical skills, and existing tools do not satisfactorily support the entire formal software development process. We have proposed a language called SOFL (Structured-Object-based-formal Language) and a SOFL methodology for system development that attempts to address these problems using an integration of formal methods, structured methods and object oriented methodology. Construction of a system uses structured methods in requirements analysis and specifications, and an object based methodology during design and implementation stages, with formal methods applied throughout the development in a manner that best suits their capabilities. The paper describes the SOFL methodology, which introduces some substantial changes from current formal methods practice. A comprehensive, practical case study of an actual industrial Residential Suites Management System illustrates how SOFL is used.","0098-5589;1939-3520;2326-3881","","10.1109/32.663996","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=663996","","Computer industry;Application software;Costs;Computer Society;Programming;Formal specifications;Sun;Software tools;Design methodology;Formal languages","formal specification;structured programming;object-oriented programming;formal languages","system development;formal engineering methodology;industrial applications;formal methods;industrial acceptance;industrial software processes;formal software development process;Structured-Object-based-formal Language;SOFL methodology;structured methods;object oriented methodology;requirements analysis;object based methodology;industrial Residential Suites Management System","","37","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Requirements-based monitors for real-time systems","D. K. Peters; D. L. Parnas","Fac. of Eng. & Appl. Sci., Memorial Univ. of Newfoundland, St. John's, Nfld., Canada; NA","IEEE Transactions on Software Engineering","","2002","28","2","146","158","Before designing safety- or mission-critical real-time systems, a specification of the required behavior of the system should be produced and reviewed by domain experts. After the system has been implemented, it should be thoroughly tested to ensure that it behaves correctly. This is best done using a monitor, a system that observes the behavior of a target system and reports if that behavior is consistent with the requirements. Such a monitor can be used both as an oracle during testing and as a supervisor during operation. Monitors should be based on the documented requirements of the system. If the target system is required to monitor or control real-valued quantities, then the requirements, which are expressed in terms of the monitored and controlled quantities, will allow a range of behaviors to account for errors and imprecision in observation and control of these quantities. Even if the controlled variables are discrete valued, the requirements must specify the timing tolerance. Because of the limitations of the devices used by the monitor to observe the environmental quantities, there is unavoidable potential for false reports, both negative and positive, This paper discusses design of monitors for real-time systems, and examines the conditions under which a monitor will produce false reports. We describe the conclusions that can be drawn when using a monitor to observe system behavior.","0098-5589;1939-3520;2326-3881","","10.1109/32.988496","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988496","","Real time systems;System testing;Control systems;Mission critical systems;Monitoring;Timing;Terminology;Computer displays;Error correction;Safety devices","real-time systems;system monitoring;formal specification;safety-critical software;program testing","requirements-based monitors;mission-critical real-time systems;safety-critical real-time systems;specification;oracle;testing;supervisor;documented requirements;errors;imprecision;timing tolerance;environmental quantities;false reports","","26","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing Families of Experiments in SE: a Systematic Mapping Study","A. Santos; O. S. Gómez; N. Juristo","M-Group, University of Oulu, Oulu, Oulu Finland (e-mail: Adrian.Santos.Parrilla@oulu.fi); Facultad de Informática y Electrónica, Escuela Superior Politécnica de Chimborazo, Riobamba, Chimborazo Ecuador 060106 (e-mail: ogomez@espoch.edu.ec); Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politecnica de Madrid, Boadilla del Monte, Madrid Spain 28660 (e-mail: natalia@fi.upm.es)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Context: Families of experiments (i.e., groups of experiments with the same goal) are on the rise in Software Engineering (SE). Selecting unsuitable aggregation techniques to analyze families may undermine their potential to provide in-depth insights from experiments' results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2864633","Ministerio de Ciencia e Innovacion; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8430603","Family of experiments;Meta-Analysis;Narrative Synthesis;IPD;AD","Systematics;Aggregates;Reliability;Data analysis;Logic gates;Bibliographies;Electronic mail","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Automatic mapping of system of N-dimensional affine recurrence equations (SARE) onto distributed memory parallel systems","A. Marongiu; P. Palazzari","Dept. of Electron. Eng., Rome Univ., Italy; NA","IEEE Transactions on Software Engineering","","2000","26","3","262","275","The automatic extraction of parallelism from algorithms, and the consequent parallel code generation, is a challenging problem. We present a procedure for automatic parallel code generation in the case of algorithms described through a SARE (Set of Affine Recurrence Equations). Starting from the original SARE description in an N-dimensional iteration space, the algorithm is converted into a parallel code for an (eventually virtual) m-dimensional distributed memory parallel machine (m<N). We demonstrate some theorems which are the mathematical basis for the proposed parallel generation tool. The projection technique used in the tool is based on the polytope model. Some affine transformations are introduced to project the polytope from the original iteration space onto another polytope, preserving the SARE semantic, in the time-processor (t,p) space. Points in (t,p) are individuated through the m-dimensional p coordinate and the n-dimensional t coordinate, resulting in N=n+m. Along with polytope transformation, a methodology to generate the code within processors is given. Finally, a cost function, used to guide the heuristic search for the polytope transformation and derived from the actual implementation of the method on an MPP SIMD machine, is introduced.","0098-5589;1939-3520;2326-3881","","10.1109/32.842951","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842951","","Difference equations;Signal processing algorithms;Cost function;Parallel machines;Linear algebra;Terminology;Parallel architectures;Sufficient conditions;Solid modeling;Timing","equations;mathematics computing;parallelising compilers;distributed memory systems;heuristic programming;programming theory;parallel programming;iterative methods;search problems","automatic mapping;N-dimensional affine recurrence equations;distributed memory parallel machine;automatic parallelism extraction;automatic parallel code generation;SARE;N-dimensional iteration space;projection technique;polytope transformation;processor-time space;cost function;heuristic search;SIMD machine;massively parallel processor;automatic parallelization;affine functions","","6","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Automatic analysis of consistency between requirements and designs""","Hewijin Christine Jiau; Dung-Feng Yu","Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","IEEE Transactions on Software Engineering","","2006","32","4","279","280","This article comments on ""automatic analysis of consistency between requirements and designs"".","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628973","","Error correction;Algorithm design and analysis","formal specification;formal verification","requirements engineering;software design;consistency analysis","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Detecting Memory Leaks Statically with Full-Sparse Value-Flow Analysis","Y. Sui; D. Ye; J. Xue","Programming Language and Compilers Group, School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; Programming Language and Compilers Group, School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; Programming Language and Compilers Group, School of Computer Science and Engineering, University of New South Wales, Sydney, Australia","IEEE Transactions on Software Engineering","","2014","40","2","107","122","We introduce a static detector, Saber, for detecting memory leaks in C programs. Leveraging recent advances on sparse pointer analysis, Saber is the first to use a full-sparse value-flow analysis for detecting memory leaks statically. Saber tracks the flow of values from allocation to free sites using a sparse value-flow graph (SVFG) that captures def-use chains and value flows via assignments for all memory locations represented by both top-level and address-taken pointers. By exploiting field-, flow- and context-sensitivity during different phases of the analysis, Saber detects memory leaks in a program by solving a graph reachability problem on its SVFG. Saber, which is fully implemented in Open64, is effective at detecting 254 leaks in the 15 SPEC2000 C programs and seven applications, while keeping the false positive rate at 18.3 percent. Saber compares favorably with several static leak detectors in terms of accuracy (leaks and false alarms reported) and scalability (LOC analyzed per second). In particular, compared with Fastcheck (which analyzes allocated objects flowing only into top-level pointers) using the 15 SPEC2000 C programs, Saber detects 44.1 percent more leaks at a slightly higher false positive rate but is only a few times slower.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2302311","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6720116","Memory Leaks;sparse value-flow analysis;static analysis;pointer analysis","Detectors;Resource management;Accuracy;Scalability;Abstracts;Standards;Sensitivity","C language;program diagnostics;reachability analysis;storage management","memory leaks detection;full-sparse value-flow analysis;Saber static detector;sparse pointer analysis;sparse value-flow graph;SVFG;def-use chains;value flows;memory locations;top-level pointers;address-taken pointers;field-sensitivity;flow-sensitivity;context-sensitivity;graph reachability problem;Open64;SPEC2000 C programs;false positive rate;static leak detectors;Fastcheck","","13","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Towards a better understanding of data models through the multilingual database system","S. A. Demurjian; D. K. Hsiao","Dept. of Comput. Sci. & Eng., Connecticut Univ., Storrs, CT, USA; NA","IEEE Transactions on Software Engineering","","1988","14","7","946","958","An approach to the design of a database system, the multilingual database system (MLDS), has been proposed and implemented. MLDS is a single database system that can execute may transactions written respectively in different data languages and support many databases structured correspondingly in various data models, i.e. DL/I transactions on hierarchical databases, CODASYL-DML transactions on network databases, SQL transactions on relational databases, and Daplex transactions on functional databases. The authors describe MLDS, focusing on its motivation and structure. It is shown how MLDS, by providing an integrated environment for experimenting with data models and data languages, also serves as a testbed that provides insight to data models and data-model semantics, using qualitative and quantitative techniques. Related work on data-language comparison and analysis is indicated.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42737","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42737","","Data models;Database systems;Relational databases;Transaction databases;Natural languages;Computer science;Taxonomy;Marine vehicles;Testing;Information management","database management systems;query languages","query languages;multilingual database system;data languages;data models;integrated environment;data-model semantics","","20","","40","","","","","","IEEE","IEEE Journals & Magazines"
"A physical database design evaluation system for CODASYL databases","H. Lam; S. Y. W. Su; N. R. Koganti","Database Syst. Res. & Dev. Center, Florida Univ., Gainesville, FL, USA; Database Syst. Res. & Dev. Center, Florida Univ., Gainesville, FL, USA; Database Syst. Res. & Dev. Center, Florida Univ., Gainesville, FL, USA","IEEE Transactions on Software Engineering","","1988","14","7","1010","1022","An interactive design tool for designing CODASYL databases is described. The system is composed of three main modules: a user interface, a transaction analyzer, and a core module. The user interface allows a designer to enter interactively information concerning a database design which is to be evaluated. The transaction analyzer allows the designer to specify the processing requirements in terms of typical logical transactions to be executed against the database and translates these logical transaction into physical transaction which access and manipulate the physical databases. The core module is the implementation of a set of analytical models and cost formulas developed for the manipulation of indexed sequential and hash-based files and CODASYL sets. These models and formulas account for the situation in which occurrences of multiple record types are stored in the same area. Also presented are the results of a series of experiments in which key design parameters are varied. The system is implemented in UCSD Pascal running on IBM PCs.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.42741","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=42741","","Transaction databases;Analytical models;Costs;Spatial databases;Process design;User interfaces;Marine vehicles;Database systems;Research and development;Terminology","database management systems;file organisation;IBM computers;program testing;software tools;user interfaces","IBM PC;software tools;file organisation;DBMS;network data model;physical database design evaluation system;CODASYL databases;interactive design tool;user interface;transaction analyzer;core module;processing requirements;logical transactions;physical transaction;analytical models;cost formulas;multiple record types;UCSD Pascal","","2","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal semijoins for distributed database systems","J. K. Mullin","Dept. of Comput. Sci., Western Ont. Univ., London, Ont., Canada","IEEE Transactions on Software Engineering","","1990","16","5","558","560","A Bloom-filter-based semijoin algorithm for distributed database systems is presented. This algorithm reduces communications costs to process a distributed natural join as much as possible with a filter approach. An optimal filter is developed in pieces. Filter information is used both to recognize when the semijoin will cease to be effective and to optimally process the semijoin. An ineffective semijoin will be quickly and cheaply recognized. An effective semijoin will use all of the transmitted bits optimally.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.52778","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=52778","","Database systems;Costs;Filters;Relational databases;Distributed databases;Remuneration;Computer science","database theory;distributed databases","optimal semijoins;filter information;distributed database systems;Bloom-filter-based semijoin algorithm;communications costs;distributed natural join;optimal filter;recognize;transmitted bits","","71","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Estimation methods for nonregenerative stochastic Petri nets","P. J. Haas","IBM Res. Div., Almaden Res. Center, San Jose, CA, USA","IEEE Transactions on Software Engineering","","1999","25","2","218","236","When a computer, manufacturing, telecommunication, or transportation system is modeled as a stochastic Petri net (SPN), many long-run performance characteristics of interest can be expressed as time-average limits of the associated marking process. For nets with generally-distributed firing times, such limits often cannot be computed analytically or numerically, but must be estimated using simulation. Previous work on estimation methods for SPNs has focused on the case in which there exists a sequence of regeneration points for the marking process of the net, so that point estimates and confidence intervals for time-average limits can be obtained using the regenerative method for analysis of simulation output. This paper is concerned with SPNs for which the regenerative method is not applicable. We provide conditions on the clock-setting distributions and new-marking probabilities of an SPN under which time-average limits are well defined and the output process of the simulation obeys a multivariate functional central limit theorem. It then follows from results of Glynn and Iglehart (1990) that methods based on standardized time series can be used to obtain strongly consistent point estimates and asymptotic confidence intervals for time-average limits. In particular, the method of batch means is applicable. Moreover, the methods of Munoz and Glynn can be used to obtain point estimates and confidence intervals for ratios of time-average limits. We illustrate our results using an SPN model of an interactive video-on-demand system.","0098-5589;1939-3520;2326-3881","","10.1109/32.761447","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=761447","","Stochastic processes;Petri nets;Computer aided manufacturing;Telecommunication computing;Analytical models;Manufacturing processes;Virtual manufacturing;Transportation;Stochastic systems;Computational modeling","Petri nets;simulation;performance evaluation;discrete event systems;stochastic systems;time series;Markov processes","estimation methods;nonregenerative stochastic Petri nets;long-run performance characteristics;time-average limits;marking process;generally-distributed firing times;simulation;regeneration point sequence;point estimates;confidence intervals;clock-setting distributions;new-marking probabilities;multivariate functional central limit theorem;standardized time series;interactive video-on-demand system","","1","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Foreword Special Issue on Distributed Systems","F. Cristian; D. Skeen","NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","1","1","2","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232558","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702125","","Computer networks;Delay;Prototypes;Electronic mail;Distributed databases;Software prototyping;Manufacturing automation;Process control;Application software","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Foreword","S. Jajodia; S. K. Tripathy","NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","8","869","870","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233504","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702304","","Local area networks;Operating systems;Protocols;Resource management;Application software;Broadcasting;LAN interconnection;Microcomputers;Network servers;Prototypes","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Ergodicity and throughput bounds of Petri nets with unique consistent firing count vector","J. Campos; G. Chiola; M. Silva","Dept. de Ingenieria Electr. e Inf., Zaragoza Univ., Spain; NA; NA","IEEE Transactions on Software Engineering","","1991","17","2","117","125","Ergodicity and throughput bound characterization are addressed for a subclass of timed and stochastic Petri nets, interleaving qualitative and quantitative theories. The nets considered represent an extension of the well-known subclass of marked graphs, defined as having a unique consistent firing count vector, independently of the stochastic interpretation of the net model. In particular, persistent and mono-T-semiflow net subclasses are considered. Upper and lower throughput bounds are computed using linear programming problems defined on the incidence matrix of the underlying net. The bounds proposed depend on the initial marking and the mean values of the delays but not on the probability distributions (thus including both the deterministic and the stochastic cases). From a different perspective, the considered subclasses of synchronized queuing networks; thus, the proposed bounds can be applied to these networks.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.67593","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=67593","","Throughput;Petri nets;Stochastic processes;Probability distribution;Linear programming;Timing;Delay estimation;Interleaved codes;Vectors;Queueing analysis","linear programming;Petri nets","persistent nets;ergodicity;throughput bounds;Petri nets;unique consistent firing count vector;marked graphs;mono-T-semiflow net subclasses;linear programming;incidence matrix;synchronized queuing networks","","68","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Constructive protocol specification using Cicero","Y. -. Huang; C. V. Ravishankar","IBM Corp., Research Triangle Park, NC, USA; NA","IEEE Transactions on Software Engineering","","1998","24","4","252","267","This paper describes Cicero, a set of language constructs to allow constructive protocol specifications. Unlike other protocol specification languages, Cicero gives programmers explicit control over protocol execution, and facilitates both sequential and parallel implementations, especially for protocols above the transport-layer. It is intended to be used in conjunction with domain-specific libraries, and is quite different in philosophy and mode of use from existing protocol specification languages. A feature of Cicero is the use of event patterns to control synchrony, asynchrony, and concurrency in protocol execution, which helps programmers build robust protocol implementations. Event-pattern driven execution also enables implementers to exploit parallelism of varying grains in protocol execution. Event patterns can also be translated into other formal models, so that existing verification techniques may be used.","0098-5589;1939-3520;2326-3881","","10.1109/32.677183","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=677183","","Transport protocols;Specification languages;Programming profession;Fault tolerance;Throughput;Network synthesis;Testing;Debugging;Communication system control;Libraries","protocols;specification languages;distributed processing","protocol specification;Cicero;protocol specification languages;language constructs;protocol execution;event patterns;synchrony;asynchrony;concurrency","","","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on: evaluating alternative software production functions","L. Pickard; B. Kitchenham; P. Jones; Qing Hu","Dept. of Comput. Sci., Keele Univ., UK; NA; NA; NA","IEEE Transactions on Software Engineering","","1999","25","2","282","285","Software development projects are notorious for cost overruns and schedule delays. While dozens of software cost models have been proposed, few of them seem to have any degree of consistent accuracy. One major factor contributing to this persistent and widespread problem is an inadequate understanding of the real behavior of software development processes. We believe that software development could be studied as an economic production process and that established economic theories and methods could be used to develop and validate software production and cost models. We present the results of evaluating four alternative software production models using the P-test, a statistical procedure developed specifically for testing the truth of a hypothesis in the presence of alternatives in econometric studies. We found that the truth of the widely used Cobb-Douglas type of software production and cost models (e.g., COCOMO) cannot be maintained in the presence of quadratic or translog models. Overall, the quadratic software production function is shown to be the most plausible model for representing software production processes. Limitations of this study and future directions are also discussed.","0098-5589;1939-3520;2326-3881","","10.1109/32.761451","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=761451","","Production;Econometrics;Parameter estimation;Programming;Cost function;Software testing;Delay;Software maintenance;Maintenance engineering;Data engineering","software cost estimation;project management;economic cybernetics;statistical analysis;economics","alternative software production function evaluation;software development projects;cost overruns;schedule delays;software cost models;software development process behavior;economic production process;economic theories;software production models;P-test;statistical procedure;hypothesis testing;econometric studies;Cobb-Douglas type model;COCOMO;translog models;quadratic software production function;software production processes","","8","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Approximate reasoning about the semantic effects of program changes","M. Moriconi; T. C. Winkler","SRI Int., Menlo Park, CA, USA; SRI Int., Menlo Park, CA, USA","IEEE Transactions on Software Engineering","","1990","16","9","980","992","It is pointed out that the incremental cost of a change to a program is often disproportionately high because of inadequate means of determining the semantic effects of the change. A practical logical technique for finding the semantic effects of changes through a direct analysis of the program is presented. The programming language features considered include parametrized modules, procedures, and global variables. The logic described is approximate in that weak (conservative) results sometimes are inferred. Isolating the exact effects of a change is undecidable in general. The basis for an approximation is a structural interpretation of the information-flow relationships among program objects. The approximate inference system is concise, abstract, extensible, and decidable, giving it significant advantages over the main alternative formalizations. The authors' implementation of the logic records the justification for each dependency to facilitate the interpretation of results.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.58785","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=58785","","Costs;Computer languages;Logic programming;Marine vehicles;Information analysis;Specification languages;Computer science;Functional programming","formal specification;inference mechanisms;program verification","approximate reasoning;semantic effects;program changes;logical technique;direct analysis;parametrized modules;procedures;global variables;structural interpretation;inference system","","19","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Software Reliability: The Stopping Rule Problem","S. M. Ross","Department of Industrial Engineering and Operations Research, University of California","IEEE Transactions on Software Engineering","","1985","SE-11","12","1472","1476","When a new computer software package is developed and all obvious erros removed, a testing procedure is often put into effect to eliminate the remaining errors in the package. One common procedure is to try the package on a set of randomly chosen problems. We suppose that whenever a program encounters an error, a system failure results. At this point the software is inspected to determine and remove the error responsible for the failure. This goes on for some time and two problems of interest are 1) to estimate the error rate of the software at a given time t, and 2) to develop a stopping rule for determining when to discontinue the testing and declare that the software is ready for use. In this paper, a model for the above is proposed as an estimation and stopping rule procedure.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231891","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701970","Failure rates;software reliability;stopping times","Software reliability;Computer errors;Software packages;Software testing;Packaging;Error analysis;Random variables;Industrial engineering;Operations research;Reactive power","","Failure rates;software reliability;stopping times","","15","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Interaction protocols as design abstractions for business processes","N. Desai; A. U. Mallya; A. K. Chopra; M. P. Singh","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","12","1015","1027","Business process modeling and enactment are notoriously complex, especially in open settings, where business partners are autonomous, requirements must be continually finessed, and exceptions frequently arise because of real-world or organizational problems. Traditional approaches, which attempt to capture processes as monolithic flows, have proven inadequate in addressing these challenges. We propose (business) protocols as components for developing business processes. A protocol is an abstract, modular, publishable specification of an interaction among different roles to be played by different participants. When instantiated with the participants' internal policies, protocols yield concrete business processes. Protocols are reusable and refinable, thus simplifying business process design. We show how protocols and their composition are theoretically founded in the phi;-calculus.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.140","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566604","Index Terms- Multiagent systems;software reuse;interaction-based modeling;software design methodologies;rule-based processing;\pi-calculus.","Protocols;Skeleton;OWL;Concrete;Process design;Software systems;Software design;Multiagent systems;Mirrors;Business communication","business data processing;calculus;multi-agent systems;organisational aspects;software reusability;systems analysis","rule-based processing;software design methodology;interaction-based modeling;software reuse;multiagent system;φ-calculus;publishable specification;business protocols;organizational problem;business process modeling;design abstraction;interaction protocols","","60","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Current trends in exception handling","D. E. Perry; A. Romanovsky; A. Tripathi","University of Texas at Austin; NA; NA","IEEE Transactions on Software Engineering","","2000","26","9","817","819","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.877843","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877843","","Computer languages;Operating systems;Object oriented modeling;Robustness;Computer science;Computer Society;Software design;Software systems;Reliability engineering","","","","3","","2","","","","","","IEEE","IEEE Journals & Magazines"
"The AETG system: an approach to testing based on combinatorial design","D. M. Cohen; S. R. Dalal; M. L. Fredman; G. C. Patton","IDA Center for Comput. Sci., Bowie, MD, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1997","23","7","437","444","This paper describes a new approach to testing that uses combinatorial designs to generate tests that cover the pairwise, triple, or n-way combinations of a system's test parameters. These are the parameters that determine the system's test scenarios. Examples are system configuration parameters, user inputs and other external events. We implemented this new method in the AETG system. The AETG system uses new combinatorial algorithms to generate test sets that cover all valid n-way parameter combinations. The size of an AETG test set grows logarithmically in the number of test parameters. This allows testers to define test models with dozens of parameters. The AETG system is used in a variety of applications for unit, system, and interoperability testing. It has generated both high-level test plans and detailed test cases. In several applications, it greatly reduced the cost of test plan development.","0098-5589;1939-3520;2326-3881","","10.1109/32.605761","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=605761","","System testing;Application software;Telephony;Costs;Computer Society;Design for experiments;Programming;Software testing;Asynchronous transfer mode;Monitoring","program testing;software tools;open systems;software cost estimation","AETG system;software testing;combinatorial design;system test parameters;test scenarios;system configuration parameters;user inputs;test set generation;n-way parameter combination;pairwise parameter combination;triple parameter combination;interoperability testing;unit testing;system testing;high-level test plans","","470","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Expert systems and optimization","A. Kusiak","Dept. of Ind. & Manage. Eng., Iowa Univ., Iowa City, IA, USA","IEEE Transactions on Software Engineering","","1989","15","8","1017","1020","A problem-solving approach involving the integration of expert systems and optimization techniques is presented. A class of expert systems called tandem expert systems is introduced. Three variants of the tandem expert system, the data-reducing, model-based, and model-modifying expert systems, are explained with examples. It is emphasized that optimization techniques can be used more frequently in future expert systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31358","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31358","","Expert systems;Operations research;Heuristic algorithms;Mathematical model;Manufacturing;Problem-solving;Large-scale systems;Industrial plants;Humans;Optimization methods","expert systems;optimisation;problem solving","data-reducing expert system;model-based expert systems;model-modifying expert systems;problem-solving;optimization techniques;tandem expert systems;future expert systems","","9","","6","","","","","","IEEE","IEEE Journals & Magazines"
"Validating Halstead's theory with system 3 data","M. Trachtenberg","RCA Corporation, Moorestown, NJ 08057","IEEE Transactions on Software Engineering","","1986","SE-12","4","584","584","Gaffney (ibid., vol.SE-10, no.7, pp.459-463, 1984) concludes that the number of faults per line of code is independent of whether Assembly language or a high order language is used. This is contrary to the results published by the commenter (ibid. vol.SE-8, pp.437-439, July 1982). It is contended that the paper by Gaffney contains some technical errors and uses erroneous data. An explanation of the source of the erroneous data is given. Also, previously published information by the commenter on the average number of operators plus operands per line of Assembly language code is corrected.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312906","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312906","","Software;Assembly;Software reliability;Programming;Psychology;Predictive models;System testing","programming theory","faults per line of code;Assembly language;high order language;average number of operators;operands per line","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Foreword Computers Come and Go But Data Go On Forever","P. B. Berra","NA","IEEE Transactions on Software Engineering","","1985","SE-11","7","573","573","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232500","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702060","","Data engineering;Hardware;Knowledge management;Engineering management;Computer aided manufacturing;Relational databases;Manufacturing processes;Optimal control;Management information systems;Power engineering computing","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Adaptive Multi-Objective Evolutionary Algorithms for Overtime Planning in Software Projects","F. Sarro; F. Ferrucci; M. Harman; A. Manna; J. Ren","University College London, CREST Centre, London, United Kingdom; University of Salerno, Fisciano, SA, Italy; University College London, CREST Centre, London, United Kingdom; University of Salerno, Fisciano, SA, Italy; Beihang University, Beijing, China","IEEE Transactions on Software Engineering","","2017","43","10","898","917","Software engineering and development is well-known to suffer from unplanned overtime, which causes stress and illness in engineers and can lead to poor quality software with higher defects. Recently, we introduced a multi-objective decision support approach to help balance project risks and duration against overtime, so that software engineers can better plan overtime. This approach was empirically evaluated on six real world software projects and compared against state-of-the-art evolutionary approaches and currently used overtime strategies. The results showed that our proposal comfortably outperformed all the benchmarks considered. This paper extends our previous work by investigating adaptive multi-objective approaches to meta-heuristic operator selection, thereby extending and (as the results show) improving algorithmic performance. We also extended our empirical study to include two new real world software projects, thereby enhancing the scientific evidence for the technical performance claims made in the paper. Our new results, over all eight projects studied, showed that our adaptive algorithm outperforms the considered state of the art multi-objective approaches in 93 percent of the experiments (with large effect size). The results also confirm that our approach significantly outperforms current overtime planning practices in 100 percent of the experiments (with large effect size).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2650914","EPSRC; Microsoft Azure Research; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7814340","Software engineering;management;planning;search-based software engineering;project scheduling;overtime;hyperheuristic;multi-objective evolutionary algorithms;NSGAII","Software;Planning;Software engineering;Search problems;Adaptive algorithms;Project management;Standards","decision support systems;DP management;evolutionary computation;project management;software development management;software quality","adaptive multiobjective evolutionary algorithms;multiobjective decision support approach;project risks;software engineers;evolutionary approaches;overtime strategies;adaptive multiobjective approaches;meta-heuristic operator selection;software projects;overtime planning practices","","5","","96","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic Software Project Scheduling through a Proactive-Rescheduling Method","X. Shen; L. L. Minku; R. Bahsoon; X. Yao","B-DAT & CICAEET, School of Information and Control, Nanjing University of Information Science and Technology, No.219, Ning-Liu Road, Pu-Kou District, Nanjing, P.R. China; Department of Computer Science, University of Leicester, University Road, Leicester, United Kingdom; CERCIA, University of Birmingham, Edgbaston, Birmingham, United Kingdom; CERCIA, University of Birmingham, Edgbaston, Birmingham, United Kingdom","IEEE Transactions on Software Engineering","","2016","42","7","658","686","Software project scheduling in dynamic and uncertain environments is of significant importance to real-world software development. Yet most studies schedule software projects by considering static and deterministic scenarios only, which may cause performance deterioration or even infeasibility when facing disruptions. In order to capture more dynamic features of software project scheduling than the previous work, this paper formulates the project scheduling problem by considering uncertainties and dynamic events that often occur during software project development, and constructs a mathematical model for the resulting multi-objective dynamic project scheduling problem (MODPSP), where the four objectives of project cost, duration, robustness and stability are considered simultaneously under a variety of practical constraints. In order to solve MODPSP appropriately, a multi-objective evolutionary algorithm based proactive-rescheduling method is proposed, which generates a robust schedule predictively and adapts the previous schedule in response to critical dynamic events during the project execution. Extensive experimental results on 21 problem instances, including three instances derived from real-world software projects, show that our novel method is very effective. By introducing the robustness and stability objectives, and incorporating the dynamic optimization strategies specifically designed for MODPSP, our proactive-rescheduling method achieves a very good overall performance in a dynamic environment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2512266","National Natural Science Foundation of China (NSFC); Natural Science Foundation of Jiangsu Province of China; EPSRC; DAASE: Dynamic Adaptive Automated Software Engineering; EPSRC; Evolutionary Computation for Dynamic Optimization in Network Environments; CERCIA; School of Computer Science, University of Birmingham, United Kingdom; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7365465","Schedule and organizational issues;dynamic software project scheduling;search-based software engineering;multi-objective evolutionary algorithms;mathematical modeling","Dynamic scheduling;Software;Schedules;Uncertainty;Robustness;Job shop scheduling","evolutionary computation;project management;scheduling;software development management","uncertain environments;dynamic environments;dynamic features;software project development;multiobjective dynamic software project scheduling problem;MODPSP;project cost;project duration;project robustness;project stability;multiobjective evolutionary algorithm based proactive-rescheduling method;critical dynamic events;project execution;dynamic optimization strategies","","7","","49","","","","","","IEEE","IEEE Journals & Magazines"
"The Oracle Problem in Software Testing: A Survey","E. T. Barr; M. Harman; P. McMinn; M. Shahbaz; S. Yoo","Department of Computer Science, University College London, Gower Street, London WC2R 2LS, London, United Kingdom; Department of Computer Science, University College London, Gower Street, London WC2R 2LS, London, United Kingdom; Department of Computer Science, University of Sheffield, Sheffield S1 4DP, South Yorkshire, United Kingdom; Department of Computer Science, University of Sheffield, Sheffield S1 4DP, South Yorkshire, United Kingdom; Department of Computer Science, University College London, Gower Street, London WC2R 2LS, London, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","5","507","525","Testing involves examining the behaviour of a system in order to discover potential faults. Given an input for a system, the challenge of distinguishing the corresponding desired, correct behaviour from potentially incorrect behavior is called the “test oracle problem”. Test oracle automation is important to remove a current bottleneck that inhibits greater overall test automation. Without test oracle automation, the human has to determine whether observed behaviour is correct. The literature on test oracles has introduced techniques for oracle automation, including modelling, specifications, contract-driven development and metamorphic testing. When none of these is completely adequate, the final source of test oracle information remains the human, who may be aware of informal specifications, expectations, norms and domain specific information that provide informal oracle guidance. All forms of test oracles, even the humble human, involve challenges of reducing cost and increasing benefit. This paper provides a comprehensive survey of current approaches to the test oracle problem and an analysis of trends in this important area of software testing research and practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2372785","Engineering and Physical Sciences Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6963470","Test oracle;Automatic testing;Testing formalism;Test oracle;automatic testing;testing formalism","Probabilistic logic;Licenses;Automation;Software testing;Market research;Reliability","formal specification;program testing","test oracle problem;test oracle information;informal specifications;domain specific information;informal oracle guidance;software testing research;software testing practice;oracle automation;contract-driven development;metamorphic testing;oracle automation","","129","","211","","","","","","IEEE","IEEE Journals & Magazines"
"Safer User Interfaces: A Case Study in Improving Number Entry","H. Thimbleby","Department of Computer Science, Swansea University, Swansea SA2 0SF, Wales, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","7","711","729","Numbers are used in critical applications, including finance, healthcare, aviation, and of course in every aspect of computing. User interfaces for number entry in many devices (calculators, spreadsheets, infusion pumps, mobile phones, etc.) have bugs and design defects that induce unnecessary use errors that compromise their dependability. Focusing on Arabic key interfaces, which use digit keys 0-9-· usually augmented with correction keys, this paper introduces a method for formalising and managing design problems. Since number entry and devices such as calculators have been the subject of extensive user interface research since at least the 1980s, the diverse design defects uncovered imply that user evaluation methodologies are insufficient for critical applications. Likewise, formal methods are not being applied effectively. User interfaces are not trivial and more attention should be paid to their correct design and implementation. The paper includes many recommendations for designing safer number entry user interfaces.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2383396","Engineering and Physical Sciences Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6991548","Error processing;Software/Software Engineering;User interfaces;Human Factors in Software Design;User Interfaces;Information Interfaces;Representation (HCI);Error processing;software/software engineering;user interfaces;human factors in software design;user interfaces;information interfaces and representation (HCI)","User interfaces;Calculators;Computer bugs;Safety;Pressing;Software;Debugging","user interfaces","safer user interfaces;number entry;Arabic key interfaces;correction keys;design problem formalization;design problem management","","9","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Who Will Stay in the FLOSS Community? Modeling Participant’s Initial Behavior","M. Zhou; A. Mockus","School of Electronics Engineering and Computer Science, Peking University and Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China; Department of Electrical Engineering and Computer Science, University of Tennessee, Min H. Kao Building, Room 613, 1520 Middle Drive, Knoxville, TN","IEEE Transactions on Software Engineering","","2015","41","1","82","99","Motivation: To survive and succeed, FLOSS projects need contributors able to accomplish critical project tasks. However, such tasks require extensive project experience of long term contributors (LTCs). Aim: We measure, understand, and predict how the newcomers' involvement and environment in the issue tracking system (ITS) affect their odds of becoming an LTC. Method: ITS data of Mozilla and Gnome, literature, interviews, and online documents were used to design measures of involvement and environment. A logistic regression model was used to explain and predict contributor's odds of becoming an LTC. We also reproduced the results on new data provided by Mozilla. Results: We constructed nine measures of involvement and environment based on events recorded in an ITS. Macro-climate is the overall project environment while micro-climate is person-specific and varies among the participants. Newcomers who are able to get at least one issue reported in the first month to be fixed, doubled their odds of becoming an LTC. The macro-climate with high project popularity and the micro-climate with low attention from peers reduced the odds. The precision of LTC prediction was 38 times higher than for a random predictor. We were able to reproduce the results with new Mozilla data without losing the significance or predictive power of the previously published model. We encountered unexpected changes in some attributes and suggest ways to make analysis of ITS data more reproducible. Conclusions: The findings suggest the importance of initial behaviors and experiences of new participants and outline empirically-based approaches to help the communities with the recruitment of contributors for long-term participation and to help the participants contribute more effectively. To facilitate the reproduction of the study and of the proposed measures in other contexts, we provide the data we retrieved and the scripts we wrote at https://www.passion-lab.org/projects/developerfluency.html.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2349496","National Basic Research Program of China; National Natural Science Foundation of China; National Hi-Tech Research and Development Program of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6880395","Long term contributor;open source software;issue tracking system;mining software repository;extent of involvement;interaction with environment;initial behavior","Communities;Atmospheric measurements;Particle measurements;Predictive models;Data models;Data mining;Electronic mail","behavioural sciences;project management;public domain software","Free-Libre and/or open source software projects;open source software;Mozilla data;microclimate;macroclimate;logistic regression model;Gnome;ITS data;issue tracking system;LTC;long term contributors;critical project;FLOSS community","","23","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Specialising Software for Different Downstream Applications Using Genetic Improvement and Code Transplantation","J. Petke; M. Harman; W. B. Langdon; W. Weimer","University College London, London, United Kingdom; University College London, London, United Kingdom; University College London, London, United Kingdom; University of Virginia, Charlottesville, VA","IEEE Transactions on Software Engineering","","2018","44","6","574","594","Genetic improvement uses automated search to find improved versions of existing software. Genetic improvement has previously been concerned with improving a system with respect to all possible usage scenarios. In this paper, we show how genetic improvement can also be used to achieve specialisation to a specific set of usage scenarios. We use genetic improvement to evolve faster versions of a C++ program, a Boolean satisfiability solver called MiniSAT, specialising it for three different applications, each with their own characteristics. Our specialised solvers achieve between 4 and 36 percent execution time improvement, which is commensurate with efficiency gains achievable using human expert optimisation for the general solver. We also use genetic improvement to evolve faster versions of an image processing tool called ImageMagick, utilising code from GraphicsMagick, another image processing tool which was forked from it. We specialise the format conversion functionality to greyscale images and colour images only. Our specialised versions achieve up to 3 percent execution time improvement.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2702606","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7962212","Genetic improvement;GI;code transplants;code specialisation;SAT;ImageMagick;GraphicsMagick","Software;Software engineering;Image processing;C++ languages;Genetic programming;Optimization","Boolean algebra;C++ language;computability;genetic algorithms;image colour analysis;source code (software)","genetic improvement;execution time improvement;code transplantation;C++ program;Boolean satisfiability solver;MiniSAT;software specialisation;downstream application;image processing tool;ImageMagick;GraphicsMagick;greyscale images;colour images","","","","97","","","","","","IEEE","IEEE Journals & Magazines"
"An interleaving approach to combinatorial testing and failure-inducing interaction identification","X. Niu; n. changhai; H. K. N. Leung; Y. Lei; X. Wang; J. Xu; Y. Wang","computer science and technology, Nanjing University, Nanjing, Jiangsu China 210023 (e-mail: niuxintao@gmail.com); computer science and technology, Nanjing University, nanjing, Jiangsu China 210096 (e-mail: changhainie@nju.edu.cn); Department of Computing, The Hong Kong Polytechnic University, Hong Kong, Hong Kong Hong Kong (e-mail: hareton.leung@polyu.edu.hk); Computer Science and Engineering, The University of Texas at Arlington, Arlington, Texas United States 76019 (e-mail: ylei@cse.uta.edu); Computer Science, University of Texas at San Antonio, San Antonio, Texas United States 78249 (e-mail: xiaoyin.wang@utsa.edu); School of Information Engineering, Nanjing Xiaozhuang University, Nanjing, Jiangsu China (e-mail: xujiaxi@njxzc.edu.cn); School of Information Engineering, Nanjing Xiaozhuang University, Nanjing, Jiangsu China (e-mail: wangyan@njxzc.edu.cn)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Combinatorial testing(CT) seeks to detect potential faults caused by various interactions of factors that can influence the software systems. When applying CT, it is a common practice to first generate a set of test cases to cover each possible interaction and then to identify the failure-inducing interaction after a failure is detected. Although this conventional procedure is simple and forthright, we conjecture that it is not the ideal choice in practice. This is because 1) testers desire to identify the root cause of failures before all the needed test cases are generated and executed 2) the early identified failure-inducing interactions can guide the remaining test case generation so that many unnecessary and invalid test cases can be avoided. For these reasons, we propose a novel CT framework that allows both generation and identification process to interact with each other. As a result, both generation and identification stages will be done more effectively and efficiently. We conducted a series of empirical studies on several open-source software, the results of which show that our framework can identify the failure-inducing interactions more quickly than traditional approaches while requiring fewer test cases.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2865772","Science and Technology Directorate; US National Institute of Standards and Technologies Award; Division of Computer and Network Systems; National Key Research and Development Plan; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8438906","Software Testing;Combinatorial Testing;Covering Array;Failure-inducing interactions","Testing;Software systems;Computer science;Fault diagnosis;Open source software;Indexes","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"The Role of Method Chains and Comments in Software Readability and Comprehension—An Experiment","J. Börstler; B. Paech","Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Computer Science, Heidelberg University, Heidelberg, Germany","IEEE Transactions on Software Engineering","","2016","42","9","886","898","Software readability and comprehension are important factors in software maintenance. There is a large body of research on software measurement, but the actual factors that make software easier to read or easier to comprehend are not well understood. In the present study, we investigate the role of method chains and code comments in software readability and comprehension. Our analysis comprises data from 104 students with varying programming experience. Readability and comprehension were measured by perceived readability, reading time and performance on a simple cloze test. Regarding perceived readability, our results show statistically significant differences between comment variants, but not between method chain variants. Regarding comprehension, there are no significant differences between method chain or comment variants. Student groups with low and high experience, respectively, show significant differences in perceived readability and performance on the cloze tests. Our results do not show any significant relationships between perceived readability and the other measures taken in the present study. Perceived readability might therefore be insufficient as the sole measure of software readability or comprehension. We also did not find any statistically significant relationships between size and perceived readability, reading time and comprehension.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2527791","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7404062","Software readability;software comprehension;software measurement;comments;method chains;experiment","Software;Guidelines;Software measurement;Software engineering;Programming;Complexity theory;Object oriented modeling","software maintenance;software metrics","software readability;software comprehension;software maintenance;software measurement;method chains;code comments;cloze tests","","3","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Self-Adaptive and Online QoS Modeling for Cloud-Based Software Services","T. Chen; R. Bahsoon","CERCIA, School of Computer Science, University of Birmingham, Birmingham, United Kingdom; CERCIA, School of Computer Science, University of Birmingham, Birmingham, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","5","453","475","In the presence of scale, dynamism, uncertainty and elasticity, cloud software engineers faces several challenges when modeling Quality of Service (QoS) for cloud-based software services. These challenges can be best managed through self-adaptivity because engineers' intervention is difficult, if not impossible, given the dynamic and uncertain QoS sensitivity to the environment and control knobs in the cloud. This is especially true for the shared infrastructure of cloud, where unexpected interference can be caused by co-located software services running on the same virtual machine; and co-hosted virtual machines within the same physical machine. In this paper, we describe the related challenges and present a fully dynamic, self-adaptive and online QoS modeling approach, which grounds on sound information theory and machine learning algorithms, to create QoS model that is capable to predict the QoS value as output over time by using the information on environmental conditions, control knobs and interference as inputs. In particular, we report on in-depth analysis on the correlations of selected inputs to the accuracy of QoS model in cloud. To dynamically selects inputs to the models at runtime and tune accuracy, we design self-adaptive hybrid dual-learners that partition the possible inputs space into two sub-spaces, each of which applies different symmetric uncertainty based selection techniques; the results of sub-spaces are then combined. Subsequently, we propose the use of adaptive multi-learners for building the model. These learners simultaneously allow several learning algorithms to model the QoS function, permitting the capability for dynamically selecting the best model for prediction on the fly. We experimentally evaluate our models in the cloud environment using RUBiS benchmark and realistic FIFA 98 workload. The results show that our approach is more accurate and effective than state-of-the-art modelings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2608826","EPSRC Grant; DAASE: Dynamic Adaptive Automated Software Engineering; The PhD scholarship from the School of Computer Science; University of Birmingham; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7572219","Software quality;search-based software engineering;self-adaptive systems;machine learning;cloud computing;performance modeling","Quality of service;Cloud computing;Interference;Adaptation models;Sensitivity;Uncertainty","cloud computing;learning (artificial intelligence);quality of service;virtual machines","cloud-based software services;quality of service;self-adaptivity;uncertain QoS sensitivity;cloud infrastructure;colocated software service;virtual machine;fully dynamic self-adaptive online QoS modeling;information theory;machine learning algorithm;environmental conditions;control knobs;self-adaptive hybrid dual-learners;input space partitioning;symmetric uncertainty based selection technique;QoS function;cloud environment;RUBiS benchmark;FIFA 98 workload","","7","","54","","","","","","IEEE","IEEE Journals & Magazines"
"An I/O Efficient Approach for Detecting All Accepting Cycles","L. Wu; K. Su; S. Cai; X. Zhang; C. Zhang; S. Wang","School of Computer Science and Engineering, University of Electronic Science and Technology, Chengdu, China; Institute for Integrated and Intelligent Systems, Griffith University, Brisbane, Australia; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Engineering, University of Electronic Science and Technology, Chengdu, China; School of Information Technology and Electrical Engineering, University of Queensland, Brisbane, Australia; School of Computer Science and Engineering, University of Electronic Science and Technology, Chengdu, China","IEEE Transactions on Software Engineering","","2015","41","8","730","744","Existing algorithms for I/O Linear Temporal Logic (LTL) model checking usually output a single counterexample for a system which violates the property. However, in real-world applications, such as diagnosis and debugging in software and hardware system designs, people often need to have a set of counterexamples or even all counterexamples. For this purpose, we propose an I/O efficient approach for detecting all accepting cycles, called Detecting All Accepting Cycles (DAAC), where the properties to be verified are in LTL. Different from other algorithms for finding all cycles, DAAC first searches for the accepting strongly connected components (ASCCs), and then finds all accepting cycles of every ASCC, which can avoid searching for a great many paths that are impossible to be extended to accepting cycles. In order to further lower DAAC's I/O complexity and improve its performance, we propose an intersection computation technique and a dynamic path management technique, and exploit a minimal perfect hash function (MPHF). We carry out both complexity and experimental comparisons with the state-of-the-art algorithms including Detect Accepting Cycle (DAC), Maximal Accepting Predecessors (MAP) and Iterative-Deepening Depth-First Search (IDDFS). The comparative results show that our approach is better on the whole in terms of I/O complexity and practical performance, despite the fact that it finds all counterexamples.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2411284","National Natural Science Foundation of China; China National; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7056483","Model Checking;Detection of All Accepting Cycles;Breath-First Search;Model checking;detection of all accepting cycles;state space explosion;accepting strongly connected component;breath-first search","Complexity theory;Model checking;Algorithm design and analysis;Educational institutions;Heuristic algorithms;Software algorithms;Software","formal verification;temporal logic","input-output efficient approach;I/O linear temporal logic;LTL model checking;DAAC approach;detecting all accepting cycles approach;accepting strongly connected components;ASCC;I/O complexity;intersection computation technique;dynamic path management technique;minimal perfect hash function;MPHF;detect accepting cycle algorithm;DAC algorithm;maximal accepting predecessors algorithm;iterative-deepening depth-first search algorithm;IDDFS algorithm","","","","36","","","","","","IEEE","IEEE Journals & Magazines"
"A Survey of App Store Analysis for Software Engineering","W. Martin; F. Sarro; Y. Jia; Y. Zhang; M. Harman","Department of Computer Science, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","9","817","847","App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2630689","EPRSC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7765038","App store;analysis;mining;API;feature;release planning;requirements engineering;reviews;security;ecosystem","Software;Security;Software engineering;Market research;Ecosystems;Mobile communication;Google","program testing;software engineering","app store analysis;software engineering;software deployment;software repositories;software development;requirements engineering;release planning;software design;software security;software testing","","15","","262","","","","","","IEEE","IEEE Journals & Magazines"
"The Value of Exact Analysis in Requirements Selection","L. Li; M. Harman; F. Wu; Y. Zhang","Department of Computer Science, CREST, University College London, Gower Street, London, United Kingdom; Department of Computer Science, CREST, University College London, Gower Street, London, United Kingdom; Department of Computer Science, CREST, University College London, Gower Street, London, United Kingdom; Department of Computer Science, CREST, University College London, Gower Street, London, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","6","580","596","Uncertainty is characterised by incomplete understanding. It is inevitable in the early phase of requirements engineering, and can lead to unsound requirement decisions. Inappropriate requirement choices may result in products that fail to satisfy stakeholders' needs, and might cause loss of revenue. To overcome uncertainty, requirements engineering decision support needs uncertainty management. In this research, we develop a decision support framework METRO for the Next Release Problem (NRP) to manage algorithmic uncertainty and requirements uncertainty. An exact NRP solver (NSGDP) lies at the heart of METRO. NSGDP's exactness eliminates interference caused by approximate existing NRP solvers. We apply NSGDP to three NRP instances, derived from a real world NRP instance, RALIC, and compare with NSGA-II, a widely-used approximate (inexact) technique. We find the randomness of NSGA-II results in decision makers missing up to 99.95 percent of the optimal solutions and obtaining up to 36.48 percent inexact requirement selection decisions. The chance of getting an inexact decision using existing approximate approaches is negatively correlated with the implementation cost of a requirement (Spearman r up to -0.72). Compared to the inexact existing approach, NSGDP saves 15.21 percent lost revenue, on average, for the RALIC dataset.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2615100","China Scholarship Council (CSC); EPSRC; DAASE; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7582553","Software engineering;exact multi-objective optimisation;simulation optimisation;next release problem","Uncertainty;Stakeholders;Robustness;Optimization;Software;Software engineering;Software algorithms","decision support systems;formal specification","requirements engineering decision support;METRO;next-release problem;algorithmic uncertainty management;requirements uncertainty management;exact NRP solver;NSGDP;NRP solvers;optimal solutions;inexact requirement selection decisions;RALIC dataset","","1","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Parallel Performance Problems on Shared-Memory Multicore Systems: Taxonomy and Observation","R. Atachiants; G. Doherty; D. Gregg","Trinity College Dublin, Ireland; Trinity College Dublin, Ireland; Trinity College Dublin, Ireland","IEEE Transactions on Software Engineering","","2016","42","8","764","785","The shift towards multicore processing has led to a much wider population of developers being faced with the challenge of exploiting parallel cores to improve software performance. Debugging and optimizing parallel programs is a complex and demanding task. Tools which support development of parallel programs should provide salient information to allow programmers of multicore systems to diagnose and distinguish performance problems. Appropriate design of such tools requires a systematic analysis of the problems which might be identified, and the information used to diagnose them. Building on the literature, we put forward a potential taxonomy of parallel performance problems, and an observational model which links measurable performance data to these problems. We present a validation of this model carried out with parallel programming experts, identifying areas of agreement and disagreement. This is accompanied with a survey of the prevalence of these problems in software development. From this we can identify contentious areas worthy of further exploration, as well as those with high prevalence and strong agreement, which are natural candidates for initial moves towards better tool support.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2519346","Science Foundation Ireland; Irish Software Research Centre; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7386691","Parallel programming, multicore, multi-threaded, optimization, performance problems, performance analysis, diagnosis, debugging, taxonomy.","Multicore processing;Software;Taxonomy;Computers;Context;Parallel programming;Hardware","parallel programming;performance evaluation;program debugging;shared memory systems;software tools","shared-memory multicore systems;parallel performance problems;multicore processing;software performance;parallel program debugging;parallel program optimization;tool support","","1","","97","","","","","","IEEE","IEEE Journals & Magazines"
"<sc>SymbexNet</sc>: Testing Network Protocol Implementations with Symbolic Execution and Rule-Based Specifications","J. Song; C. Cadar; P. Pietzuch","Department of Computer and Information Security, Sejong University, Seoul, Republic of Korea; Department of Computing, Imperial College London, London, SW7 2AZ, U.K.; Department of Computer and Information Security, Sejong University, Seoul, Republic of Korea","IEEE Transactions on Software Engineering","","2014","40","7","695","709","Implementations of network protocols, such as DNS, DHCP and Zeroconf, are prone to flaws, security vulnerabilities and interoperability issues caused by developer mistakes and ambiguous requirements in protocol specifications. Detecting such problems is not easy because (i) many bugs manifest themselves only after prolonged operation; (ii) reasoning about semantic errors requires a machine-readable specification; and (iii) the state space of complex protocol implementations is large. This article presents a novel approach that combines symbolic execution and rule-based specifications to detect various types of flaws in network protocol implementations. The core idea behind our approach is to (1) automatically generate high-coverage test input packets for a network protocol implementation using single- and multi-packet exchange symbolic execution (targeting stateless and stateful protocols, respectively) and then (2) use these packets to detect potential violations of manual rules derived from the protocol specification, and check the interoperability of different implementations of the same network protocol. We present a system based on these techniques, SymbexNet, and evaluate it on multiple implementations of two network protocols: Zeroconf, a service discovery protocol, and DHCP, a network configuration protocol. SymbexNet is able to discover non-trivial bugs as well as interoperability problems, most of which have been confirmed by the developers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2323977","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6815719","Symbolic execution;network security;testing;interoperability testing","Protocols;IP networks;Interoperability;Servers;Concrete;Testing;Computer bugs","formal specification;open systems;program debugging;program testing","SymbexNet;testing network protocol implementations;symbolic execution;rule based specifications;DNS;DHCP;Zeroconf;interoperability issues;security vulnerabilities;protocol specifications;semantic errors;machine readable specification;protocol implementations;protocol specification;network protocol;interoperability problems","","5","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Understanding Diverse Usage Patterns from Large-Scale Appstore-Service Profiles","X. Liu; H. Li; X. Lu; T. Xie; Q. Mei; F. Feng; H. Mei","Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, Beijing, China; Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, Beijing, China; Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, Beijing, China; University of Illinois Urbana-Champaign, Champaign, IL; University of Michigan, Ann Arbor, MI; Wandoujia, Beijing, China; Beijing Institute of Technology","IEEE Transactions on Software Engineering","","2018","44","4","384","411","The prevalence of smart mobile devices has promoted the popularity of mobile applications (a.k.a. apps). Supporting mobility has become a promising trend in software engineering research. This article presents an empirical study of behavioral service profiles collected from millions of users whose devices are deployed with Wandoujia, a leading Android app-store service in China. The dataset of Wandoujia service profiles consists of two kinds of user behavioral data from using 0.28 million free Android apps, including (1) app management activities (i.e., downloading, updating, and uninstalling apps) from over 17 million unique users and (2) app network usage from over 6 million unique users. We explore multiple aspects of such behavioral data and present patterns of app usage. Based on the findings as well as derived knowledge, we also suggest some new open opportunities and challenges that can be explored by the research community, including app development, deployment, delivery, revenue, etc.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2685387","National Basic Research Program (973) of China; Natural Science Foundation of China; National Science Foundation; National Science Foundation; MCubed; University of Michigan; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7883939","Mobile apps;app store;user behavior analysis","Androids;Humanoid robots;Software;Biological system modeling;Mobile communication;Electronic mail;Software engineering","Android (operating system);mobile computing;public domain software;software engineering","Wandoujia service profiles;user behavioral data;app usage;app development;diverse usage patterns;large-scale appstore-service profiles;smart mobile devices;mobile applications;software engineering research;behavioral service profiles;leading Android app-store service;free Android apps;app management activities;app network usage","","1","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Uncovering the Periphery: A Qualitative Survey of Episodic Volunteering in Free/Libre and Open Source Software Communities","A. Barcomb; A. Kaufmann; D. Riehle; K. Stol; B. Fitzgerald","Lero, University of Limerick, 8808 Limerick, Co Limerick Ireland (e-mail: ann@barcomb.org); Department of Computer Science, Friedrich-Alexander University Erlangen-Nürnberg, Erlangen, Bavaria Germany (e-mail: andreas.kaufmann@fau.de); Computer Science, Friedrich-Alexander-Universitat Erlangen-Nurnberg, 9171 Erlangen, Bavaria Germany 91058 (e-mail: dirk@riehle.org); Computer Science, University College Cork, Cork, Cork Ireland (e-mail: kjstol@gmail.com); Lero - the Irish Software Research centre, University of Limerick, Limerick, Limerick Ireland (e-mail: bf@ul.ie)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Free/Libre and Open Source Software (FLOSS) communities are composed, in part, of volunteers, many of whom contribute infrequently. However, these infrequent volunteers contribute to the sustainability of FLOSS projects, and should ideally be encouraged to continue participating, even if they cannot be persuaded to contribute regularly. Infrequent contributions are part of a trend which has been widely observed in other sectors of volunteering, where it has been termed ""episodic volunteering"" (EV). Previous FLOSS research has focused on the Onion model, differentiating core and peripheral developers, with the latter considered as a homogeneous group. We argue this is too simplistic, given the size of the periphery group and the myriad of valuable activities they perform beyond coding. Our exploratory qualitative survey of 13 FLOSS communities investigated what episodic volunteering looks like in a FLOSS context. EV is widespread in FLOSS communities, although not specifically managed. We suggest several recommendations for managing EV based on a framework drawn from the volunteering literature. Also, episodic volunteers make a wide range of value-added contributions other than code, and they should neither be expected nor coerced into becoming habitual volunteers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2872713","Science Foundation Ireland; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8477174","community management;episodic volunteering;free software;open source software;peripheral developer","Companies;Lenses;Open source software;Sustainable development;Task analysis","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"Detecting Trivial Mutant Equivalences via Compiler Optimisations","M. Kintis; M. Papadakis; Y. Jia; N. Malevris; Y. Le Traon; M. Harman","Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Esch-sur-Alzette 4365, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Esch-sur-Alzette 4365, Luxembourg; CREST Centre, University College London, London, United Kingdom; Athens University of Economics and Business, Athens, Greece; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Esch-sur-Alzette 4365, Luxembourg; CREST Centre, University College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","4","308","333","Mutation testing realises the idea of fault-based testing, i.e., using artificial defects to guide the testing process. It is used to evaluate the adequacy of test suites and to guide test case generation. It is a potentially powerful form of testing, but it is well-known that its effectiveness is inhibited by the presence of equivalent mutants. We recently studied Trivial Compiler Equivalence (TCE) as a simple, fast and readily applicable technique for identifying equivalent mutants for C programs. In the present work, we augment our findings with further results for the Java programming language. TCE can remove a large portion of all mutants because they are determined to be either equivalent or duplicates of other mutants. In particular, TCE equivalent mutants account for 7.4 and 5.7 percent of all C and Java mutants, while duplicated mutants account for a further 21 percent of all C mutants and 5.4 percent Java mutants, on average. With respect to a benchmark ground truth suite (of known equivalent mutants), approximately 30 percent (for C) and 54 percent (for Java) are TCE equivalent. It is unsurprising that results differ between languages, since mutation characteristics are language-dependent. In the case of Java, our new results suggest that TCE may be particularly effective, finding almost half of all equivalent mutants.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2684805","Research Centre of Athens University of Economics and Business (RC/AUEB); National Research Fund, Luxembourg; Microsoft Azure; UK EPSRC projects; Centre for Research on Evolution Search and Testing (CREST); UCL; EPSRC project; Microsoft Azure; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7882714","Mutation testing;equivalent mutants;duplicated mutants;compiler optimisation","Java;Testing;Optimization;Syntactics;Program processors;Electronic mail","Java;program compilers;program testing","mutation testing;test case generation;Java programming language;TCE equivalent mutants;trivial mutant equivalences detection;fault-based testing;TCE duplicated mutants;Java mutants;compiler equivalence","","","","75","","","","","","IEEE","IEEE Journals & Magazines"
"Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering","S. L. Lim; P. J. Bentley; N. Kanakam; F. Ishikawa; S. Honiden","Department of Computer Science, University College, London; Department of Computer Science, University College London; Department of Clinical, Education and Health Psychology, University College, London; Digital Content and Media Sciences Research Division, National Institute of Informatics, Japan; National Institute of Informatics, Japan","IEEE Transactions on Software Engineering","","2015","41","1","40","64","Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2360674","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6913003","Requirements/specifications;market-driven software engineering;mobile application development;user requirements;survey research;app user behavior;software product lines;software ecosystems;Requirements/specifications;market-driven software engineering;mobile application development;user requirements;survey research;app user behavior;software product lines;software ecosystems","Mobile communication;Software;Smart phones;Software engineering;Data mining;Educational institutions","consumer behaviour;mobile computing;smart phones;software engineering","market-driven software engineering;medical applications;data analysis;South Korea;South;Mexico;Australia;Spain;Canada;India;Russia;Italy;United Kingdom;Brazil;France;Germany;Japan;China;USA;applications stores;mobile devices;user behavior;mobile application","","37","","65","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Comparison of Combinatorial Testing, Random Testing and Adaptive Random Testing","H. Wu; n. changhai; J. Petke; Y. Jia; M. Harman","Computer Science and Technology, Nanjing University, Nanjing, Jiangsu China (e-mail: hywu@smail.nju.edu.cn); Computer Science and Technology, Nanjing University, Nanjing, Jiangsu China (e-mail: changhainie@nju.edu.cn); Computer Science, University College London, London, London United Kingdom of Great Britain and Northern Ireland WC1E 6BT (e-mail: j.petke@ucl.ac.uk); Facebook, Facebook Inc., London, London United Kingdom of Great Britain and Northern Ireland (e-mail: yue.jia@ucl.ac.uk); Facebook, Facebook Inc., London, London United Kingdom of Great Britain and Northern Ireland (e-mail: mark.harman@ucl.ac.uk)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","We present an empirical comparison of three test generation techniques, namely, Combinatorial Testing (CT), Random Testing (RT) and Adaptive Random Testing (ART), under different test scenarios. This is the first study in the literature to account for the (more realistic) testing setting in which the tester may not have complete information about the parameters and constraints that pertain to the system, and to account for the challenge posed by faults (in terms of failure rate). Our study was conducted on nine real-world programs under a total of 1683 test scenarios (combinations of available parameter and constraint information and failure rate). The results show significant differences in the techniques&#x0027; fault detection ability when faults are hard to detect (failure rates are relatively low). CT performs best overall; no worse than any other in 98% of scenarios studied. ART enhances RT, and is comparable to CT in 96% of scenarios, but its computational cost can be up to 3.5 times higher than CT when the program is highly constrained. Additionally, when constraint information is unavailable for a highly-constrained program, a large random test suite is as effective as CT or ART, yet its computational cost of test generation is significantly lower than that of other techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2852744","National Key Research and Development Plan; Program B for Outstanding PhD Candidate of Nanjing University; DAASE EPSRC Grant; EPSRC Fellowship; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8405609","combinatorial testing;random testing;adaptive random testing","Testing;Subspace constraints;Computational efficiency;Fault detection;Analytical models;Software systems","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"PPChecker: Towards Accessing the Trustworthiness of Android Apps' Privacy Policies","L. Yu; X. Luo; J. Chen; H. Zhou; T. Zhang; H. Chang; H. K. N. Leung","Department of Computing, The Hong Kong Polytechnic University, Hong Kong, Hong Kong Hong Kong (e-mail: yulele08@gmail.com); Department of Computing, The Hong Kong Polytechnic University, Hong Kong, Hong Kong Hong Kong (e-mail: csxluo@comp.polyu.edu.hk); Computing, Hong Kong Polytechnic University, 26680 Kowloon, Hong Kong Hong Kong (e-mail: chenjiachi317@gmail.com); Department of Computing, The Hong Kong Polytechnic University, Hong Kong, Hong Kong Hong Kong (e-mail: sunmoonsky0001@gmail.com); College of Computer Science and Technology, Harbin Engineering University, 12428 Harbin, Heilongjiang China (e-mail: cstzhang@hrbeu.edu.cn); Law and Technology Centre, University of Hong Kong, Hong Kong, Hong Kong Hong Kong (e-mail: hcychang@hku.hk); Department of Computing, The Hong Kong Polytechnic University, Hong Kong, Hong Kong Hong Kong (e-mail: hareton.leung@polyu.edu.hk)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Recent years have witnessed a sharp increase of malicious apps that steal users' personal information. To address users' concerns about privacy risks and to comply with data protection laws, more and more apps are supplied with privacy policies written in natural language to help users to understand an app's privacy practices. However, little is known whether these privacy policies are trustworthy or not. Questionable privacy policies may be prepared by careless app developers or someone with malicious intention. In this paper, we carry out a systematic study on privacy policy by proposing a novel approach to automatically identify five kinds of problems in privacy policy. After tackling several challenging issues, we implement the approach in a system, named PPChecker, and evaluate it with real apps and their privacy policies. The experimental results show that PPChecker can effectively identify questionable privacy policies with high precision. Applying PPChecker to 2,044 popular apps, we find that 1,429 apps (i.e., 69.9\%) have at least one kind of problems. This study sheds light on the research of improving and regulating apps' privacy policies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2886875","HKPolyU Research Grant; China Postdoctoral Science Foundation; National Natural Science Foundation of China; Research Grants Council RGC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8576579","","Privacy;Google;Natural languages;Mobile handsets;Data protection;Force","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Competition-Based Crowdsourcing Software Development: A Multi-Method Study from a Customer Perspective","K. Stol; B. Caglayan; B. Fitzgerald","Lero, University of Limerick, Limerick, Limerick Ireland (e-mail: kjstol@gmail.com); Innovation Exchange, IBM Ireland Limited, 42426 Dublin, Dublin Ireland (e-mail: bora.caglayan@ibm.com); Lero - the Irish Software Research centre, University of Limerick, Limerick, Limerick Ireland (e-mail: bf@ul.ie)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Crowdsourcing is emerging as an alternative outsourcing strategy which is gaining increasing attention in the software engineering community. However, crowdsourcing software development involves complex tasks which differ significantly from the micro-tasks that can be found on crowdsourcing platforms such as Amazon Mechanical Turk which are much shorter in duration, are typically very simple, and do not involve any task interdependencies. To achieve the potential benefits of crowdsourcing in the software development context, companies need to understand how this strategy works, and what factors might affect crowd participation. We present a multi-method qualitative and quantitative theory-building research study. Firstly, we derive a set of key concerns from the crowdsourcing literature as an initial analytical framework for an exploratory case study in a Fortune 500 company. We complement the case study findings with an analysis of 13,602 crowdsourcing competitions over a ten-year period on the very popular Topcoder crowdsourcing platform. Drawing from our empirical findings and the crowdsourcing literature, we propose a theoretical model of crowd interest and actual participation in crowdsourcing competitions. We evaluate this model using Structural Equation Modeling. Among the findings are that the level of prize and duration of competitions do not significantly increase crowd interest in competitions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2774297","Science Foundation Ireland; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8119867","Crowdsourcing;software engineering;multi-method study;case study;sample study","Crowdsourcing;Software;Outsourcing;Mathematical model;Companies","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"Improved Evolutionary Algorithm Design for the Project Scheduling Problem Based on Runtime Analysis","L. L. Minku; D. Sudholt; X. Yao","CERCIA, School of Computer Science , The University of Birmingham, Birmingham B15 2TT, United Kingdom; Department of Computer Science , University of Sheffield, Sheffield S1 4DP, United Kingdom; CERCIA, School of Computer Science , The University of Birmingham, Birmingham B15 2TT, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","1","83","102","Several variants of evolutionary algorithms (EAs) have been applied to solve the project scheduling problem (PSP), yet their performance highly depends on design choices for the EA. It is still unclear how and why different EAs perform differently. We present the first runtime analysis for the PSP, gaining insights into the performance of EAs on the PSP in general, and on specific instance classes that are easy or hard. Our theoretical analysis has practical implications-based on it, we derive an improved EA design. This includes normalizing employees' dedication for different tasks to ensure they are not working overtime; a fitness function that requires fewer pre-defined parameters and provides a clear gradient towards feasible solutions; and an improved representation and mutation operator. Both our theoretical and empirical results show that our design is very effective. Combining the use of normalization to a population gave the best results in our experiments, and normalization was a key component for the practical effectiveness of the new design. Not only does our paper offer a new and effective algorithm for the PSP, it also provides a rigorous theoretical analysis to explain the efficiency of the algorithm, especially for increasingly large projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6648326","Schedule and organizational issues;evolutionary algorithms;software project scheduling;software project management;search-based software engineering;runtime analysis","Software;Schedules;Scheduling;Algorithm design and analysis;Software algorithms;Resource management;Software engineering","evolutionary computation;project management;scheduling;software development management","improved evolutionary algorithm;project scheduling problem;runtime analysis;PSP;improved EA design;employee dedication;fitness function;representation operator;mutation operator;population normalization;software project scheduling","","16","","45","","","","","","IEEE","IEEE Journals & Magazines"
"Practical Combinatorial Interaction Testing: Empirical Findings on Efficiency and Early Fault Detection","J. Petke; M. B. Cohen; M. Harman; S. Yoo","Computer Science Department, University College London, London, United Kingdom; Computer Science & Engineering Department, University of Nebraska-Lincoln, Lincoln, Nebraska, United States; Computer Science Department, University College London, London, United Kingdom; Computer Science Department, University College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","9","901","924","Combinatorial interaction testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. Simulated Annealing (SA) and Greedy Algorithms have been widely used to find CIT test suites. From the literature, there is a widely-held belief that SA is slower, but produces more effective tests suites than Greedy and that SA cannot scale to higher strength coverage. We evaluated both algorithms on seven real-world subjects for the well-studied two-way up to the rarely-studied six-way interaction strengths. Our findings present evidence to challenge this current orthodoxy: real-world constraints allow SA to achieve higher strengths. Furthermore, there was no evidence that Greedy was less effective (in terms of time to fault revelation) compared to SA; the results for the greedy algorithm are actually slightly superior. However, the results are critically dependent on the approach adopted to constraint handling. Moreover, we have also evaluated a genetic algorithm for constrained CIT test suite generation. This is the first time strengths higher than 3 and constraint handling have been used to evaluate GA. Our results show that GA is competitive only for pairwise testing for subjects with a small number of constraints.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2421279","National Science Foundation; Air Force Office of Scientific Research; UK Engineering and Physical Sciences Research Council (EPSRC); DAASE: Dynamic Adaptive Automated Software Engineering; GISMO: Genetic Improvement of Software for Multiple Objectives; CREST: Centre for Research on Evolution, Search and Testing; DAASE; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7081752","Combinatorial Interaction Testing;Prioritisation;Empirical Studies;Software Testing;Combinatorial interaction testing;prioritisation;empirical studies;software testing","Testing;Simulated annealing;Genetic algorithms;Fault detection;Greedy algorithms;Turning;Flexible printed circuits","genetic algorithms;greedy algorithms;program testing;simulated annealing;software fault tolerance","combinatorial interaction testing;early fault detection;software system configuration space;simulated annealing;SA;greedy algorithm;CIT test suite generation;constraint handling;pairwise testing;genetic algorithm","","29","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Symbolic Crosschecking of Data-Parallel Floating-Point Code","P. Collingbourne; C. Cadar; P. H. J. Kelly","Google Inc,; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","7","710","737","We present a symbolic execution-based technique for cross-checking programs accelerated using SIMD or OpenCL against an unaccelerated version, as well as a technique for detecting data races in OpenCL programs. Our techniques are implemented in KLEE-CL, a tool based on the symbolic execution engine KLEE that supports symbolic reasoning on the equivalence between expressions involving both integer and floating-point operations. While the current generation of constraint solvers provide effective support for integer arithmetic, the situation is different for floating-point arithmetic, due to the complexity inherent in such computations. The key insight behind our approach is that floating-point values are only reliably equal if they are essentially built by the same operations. This allows us to use an algorithm based on symbolic expression matching augmented with canonicalisation rules to determine path equivalence. Under symbolic execution, we have to verify equivalence along every feasible control-flow path. We reduce the branching factor of this process by aggressively merging conditionals, if-converting branches into select operations via an aggressive phi-node folding transformation. To support the Intel Streaming SIMD Extension (SSE) instruction set, we lower SSE instructions to equivalent generic vector operations, which in turn are interpreted in terms of primitive integer and floating-point operations. To support OpenCL programs, we symbolically model the OpenCL environment using an OpenCL runtime library targeted to symbolic execution. We detect data races by keeping track of all memory accesses using a memory log, and reporting a race whenever we detect that two accesses conflict. By representing the memory log symbolically, we are also able to detect races associated with symbolically-indexed accesses of memory objects. We used KLEE-CL to prove the bounded equivalence between scalar and data-parallel versions of floating-point programs and find a number of issues in a variety of open source projects that use SSE and OpenCL, including mismatches between implementations, memory errors, race conditions and a compiler bug.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.2297120","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6698391","Data-parallel code;floating point;symbolic execution;SIMD;OpenCL;KLEE-CL","Vectors;Kernel;Computational modeling;Computer architecture;Semantics;Programming;Parallel processing","data handling;floating point arithmetic;parallel processing;program debugging","symbolic crosschecking;data parallel floating point code;symbolic execution based technique;crosschecking programs;SIMD;OpenCL programs;KLEE-CL;symbolic execution engine;symbolic reasoning;floating-point operations;integer operations;integer arithmetic;floating point arithmetic;symbolic expression matching;phi node folding transformation;intel streaming SIMD extension;SSE instruction set;equivalent generic vector operations;floating point operations;primitive integer operations;OpenCL environment;memory accesses;memory log;open source projects;floating-point programs;compiler bug;memory errors;race conditions","","9","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Software Repair: A Survey","L. Gazzola; D. Micucci; L. Mariani","Department of Informatics, Systems and Communication (DISCo), University of Milano Bicocca, Milano, Italy; Department of Informatics, Systems and Communication (DISCo), University of Milano Bicocca, Milano, Italy; Department of Informatics, Systems and Communication (DISCo), University of Milano Bicocca, Milano, Italy","IEEE Transactions on Software Engineering","","2019","45","1","34","67","Despite their growing complexity and increasing size, modern software applications must satisfy strict release requirements that impose short bug fixing and maintenance cycles, putting significant pressure on developers who are responsible for timely producing high-quality software. To reduce developers workload, repairing and healing techniques have been extensively investigated as solutions for efficiently repairing and maintaining software in the last few years. In particular, repairing solutions have been able to automatically produce useful fixes for several classes of bugs that might be present in software programs. A range of algorithms, techniques, and heuristics have been integrated, experimented, and studied, producing a heterogeneous and articulated research framework where automatic repair techniques are proliferating. This paper organizes the knowledge in the area by surveying a body of 108 papers about automatic software repair techniques, illustrating the algorithms and the approaches, comparing them on representative examples, and discussing the open challenges and the empirical evidence reported so far.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2755013","EU H2020; ERC Consolidator; MIUR; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8089448","Automatic program repair;generate and validate;search-based;semantics-driven repair;correct by construction;program synthesis;self-repairing","Software;Maintenance engineering;Debugging;Computer bugs;Software algorithms;Fault diagnosis;Conferences","program debugging;software maintenance;software quality","modern software applications;maintenance cycles;software programs;heterogeneous research framework;articulated research framework;automatic software repair techniques;high-quality software","","1","","176","","","","","","IEEE","IEEE Journals & Magazines"
"A Templating System to Generate Provenance","L. Moreau; B. V. Batlajery; T. D. Huynh; D. Michaelides; H. Packer","Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","2","103","121","PROV-TEMPLATEIS a declarative approach that enables designers and programmers to design and generate provenance compatible with the PROV standard of the World Wide Web Consortium. Designers specify the topology of the provenance to be generated by composing templates, which are provenance graphs containing variables, acting as placeholders for values. Programmers write programs that log values and package them up in sets of bindings, a data structure associating variables and values. An expansion algorithm generates instantiated provenance from templates and sets of bindings in any of the serialisation formats supported by PROV. A quantitative evaluation shows that sets of bindings have a size that is typically 40 percent of that of expanded provenance templates and that the expansion algorithm is suitably tractable, operating in fractions of milliseconds for the type of templates surveyed in the article. Furthermore, the approach shows four significant software engineering benefits: separation of responsibilities, provenance maintenance, potential runtime checks and static analysis, and provenance consumption. The article gathers quantitative data and qualitative benefits descriptions from four different applications making use of PROV-TEMPLATE. The system is implemented and released in the open-source library ProvToolbox for provenance processing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2659745","EPSRC SOCIAM; ORCHID; FP7 SmartSociety; ESRC eBook; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7909036","Provenance;prov;provenance generation;template","Electronic publishing;Instruments;Standards;Maintenance engineering;Runtime;Libraries;Automobiles","data structures;graph theory;Internet;software engineering;software maintenance","data structure;PROV-TEMPLATE;provenance generation;open-source library ProvToolbox;software engineering;templating system;provenance processing;quantitative data;provenance consumption;provenance maintenance;expanded provenance templates;expansion algorithm;provenance graphs;World Wide Web Consortium;PROV standard","","2","","64","","","","","","IEEE","IEEE Journals & Magazines"
"The Good, the Bad and the Ugly: A Study of Security Decisions in a Cyber-Physical Systems Game","S. Frey; A. Rashid; P. Anthonysamy; M. Pinto-Albuquerque; S. A. Naqvi","Electronics and Computer Science, University of Southampton Faculty of Physical Sciences and Engineering, 243732 Southampton, Southampton United Kingdom of Great Britain and Northern Ireland (e-mail: s.a.f.frey@soton.ac.uk); Department of Computer Science, University of Bristol Department of Computer Science, 152321 Bristol, Bristol United Kingdom of Great Britain and Northern Ireland (e-mail: a.rashid@lancaster.ac.uk); Google, Google Inc, 93176 Mountain View, California United States (e-mail: p.anthonysamy@lancaster.ac.uk); Departamento de Ci&#x00EA;ncias e Tecnologias da Informa&#x00E7;&#x00E3;o, ISCTE-Instituto Universitario de Lisboa, 56061 Lisboa, Lisboa Portugal (e-mail: maria.albuquerque@iscte.pt); Infolab21, Lancaster University, 4396 Lancaster, Lancashire United Kingdom of Great Britain and Northern Ireland (e-mail: s.naqvi@lancaster.ac.uk)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Stakeholders&#x0027; security decisions play a fundamental role in determining security requirements, yet, little is currently understood about how different stakeholder groups within an organisation approach security and the drivers and tacit biases underpinning their decisions. We studied and contrasted the security decisions of three demographics -- security experts, computer scientists and managers -- when playing a tabletop game that we designed and developed. The game tasks players with managing the security of a cyber-physical environment while facing various threats. Analysis of 12 groups of players (4 groups in each of our demographics) reveals strategies that repeat in particular demographics, e.g., managers and security experts generally favoring technological solutions over personnel training, which computer scientists preferred. Surprisingly, security experts were not ipso facto better players -- in some cases, they made very questionable decisions -- yet they showed a higher level of confidence in themselves. We classified players&#x0027; decision-making processes, i.e., procedure-, experience-, scenario- or intuition-driven. We identified decision patterns, both good practices and typical errors and pitfalls. Our game provides a requirements sandbox in which players can experiment with security risks, learn about decision-making and its consequences, and reflect on their own perception of security.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2782813","Engineering and Physical Sciences Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8194898","Security decisions;security requirements;game;decision patterns","Games;Computer security;Electronic mail;Companies","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Overcoming the Equivalent Mutant Problem: A Systematic Literature Review and a Comparative Experiment of Second Order Mutation","L. Madeyski; W. Orzeszyna; R. Torkar; M. Józala","Institute of Informatics, Wroclaw University of Technology, Wyb. Wyspianskiego 27, Poland; Institute of Informatics, Wroclaw University of Technology, Poland; Division of Software Engineering , Department of Computer Science and Engineering, Chalmers University of Technology , SE-41296, Sweden; Institute of Informatics, Wroclaw University of Technology, Wyb. Wyspianskiego 27, Poland","IEEE Transactions on Software Engineering","","2014","40","1","23","42","Context. The equivalent mutant problem (EMP) is one of the crucial problems in mutation testing widely studied over decades. Objectives. The objectives are: to present a systematic literature review (SLR) in the field of EMP; to identify, classify and improve the existing, or implement new, methods which try to overcome EMP and evaluate them. Method. We performed SLR based on the search of digital libraries. We implemented four second order mutation (SOM) strategies, in addition to first order mutation (FOM), and compared them from different perspectives. Results. Our SLR identified 17 relevant techniques (in 22 articles) and three categories of techniques: detecting (DEM); suggesting (SEM); and avoiding equivalent mutant generation (AEMG). The experiment indicated that SOM in general and JudyDiffOp strategy in particular provide the best results in the following areas: total number of mutants generated; the association between the type of mutation strategy and whether the generated mutants were equivalent or not; the number of not killed mutants; mutation testing time; time needed for manual classification. Conclusions . The results in the DEM category are still far from perfect. Thus, the SEM and AEMG categories have been developed. The JudyDiffOp algorithm achieved good results in many areas.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6613487","Mutation testing;equivalent mutant problem;higher order mutation;second order mutation","Testing;Systematics;Educational institutions;Databases;Libraries;Java;Informatics","digital libraries;program testing","JudyDiffOp strategy;SEM;DEM;AEMG;avoiding equivalent mutant generation;suggesting;detecting;FOM;first order mutation;SOM strategies;digital libraries;SLR;mutation testing;EMP;second order mutation;comparative experiment;systematic literature review;equivalent mutant problem","","42","","79","","","","","","IEEE","IEEE Journals & Magazines"
"Mapping Bug Reports to Relevant Files: A Ranking Model, a Fine-Grained Benchmark, and Feature Evaluation","X. Ye; R. Bunescu; C. Liu","School of Electrical Engineering and Computer Science, Ohio University, Athens, OH; School of Electrical Engineering and Computer Science, Ohio University, Athens, OH; School of Electrical Engineering and Computer Science, Ohio University, Athens, OH","IEEE Transactions on Software Engineering","","2016","42","4","379","402","When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and improve productivity. This paper introduces an adaptive ranking approach that leverages project knowledge through functional decomposition of source code, API descriptions of library components, the bug-fixing history, the code change history, and the file dependency graph. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluate the ranking system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the learning-to-rank approach outperforms three recent state-of-the-art methods. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70 percent of the bug reports in the Eclipse Platform and Tomcat projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2479232","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7270328","Bug reports;software maintenance;learning to rank;Bug reports;software maintenance;learning to rank","Software;History;Computational modeling;Computer bugs;Collaboration;Benchmark testing;Standards","","","","8","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Effectively Incorporating Expert Knowledge in Automated Software Remodularisation","M. Hall; N. Walkinshaw; P. McMinn","Department of Computer Science, University of Sheffield, Sheffield, United Kingdom; Department of Informatics, University of Leicester, Leicester, United Kingdom; Department of Computer Science, University of Sheffield, Sheffield, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","7","613","630","Remodularising the components of a software system is challenging: sound design principles (e.g., coupling and cohesion) need to be balanced against developer intuition of which entities conceptually belong together. Despite this, automated approaches to remodularisation tend to ignore domain knowledge, leading to results that can be nonsensical to developers. Nevertheless, suppling such knowledge is a potentially burdensome task to perform manually. A lot information may need to be specified, particularly for large systems. Addressing these concerns, we propose the SUpervised reMOdularisation (SUMO) approach. SUMO is a technique that aims to leverage a small subset of domain knowledge about a system to produce a remodularisation that will be acceptable to a developer. With SUMO, developers refine a modularisation by iteratively supplying corrections. These corrections constrain the type of remodularisation eventually required, enabling SUMO to dramatically reduce the solution space. This in turn reduces the amount of feedback the developer needs to supply. We perform a comprehensive systematic evaluation using 100 real world subject systems. Our results show that SUMO guarantees convergence on a target remodularisation with a tractable amount of user interaction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2786222","EPSRC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8259332","Software remodularisation;domain knowledge;set partitioning","Clustering algorithms;Tools;Software algorithms;Software systems;Algorithm design and analysis","knowledge based systems;learning (artificial intelligence);reverse engineering;software maintenance","automated software remodularisation;domain knowledge;SUMO;target remodularisation;expert knowledge;supervised remodularisation approach","","","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Refactoring of OCL Constraints with Search","H. Lu; S. Wang; T. Yue; s. Ali; J. F. Nygård","Simula Research Laboratory, Fornebu, Norway; Simula Research Laboratory, Fornebu, Norway; Simula Research Laboratory, Fornebu, Norway; Simula Research Laboratory, Fornebu, Norway; Cancer Registry of Norway, Oslo, Norway","IEEE Transactions on Software Engineering","","2019","45","2","148","170","Object Constraint Language (OCL) constraints are typically used to provide precise semantics to models developed with the Unified Modeling Language (UML). When OCL constraints evolve regularly, it is essential that they are easy to understand and maintain. For instance, in cancer registries, to ensure the quality of cancer data, more than one thousand medical rules are defined and evolve regularly. Such rules can be specified with OCL. It is, therefore, important to ensure the understandability and maintainability of medical rules specified with OCL. To tackle such a challenge, we propose an automated <underline>s</underline>earch-<underline>b</underline>ased <underline>O</underline>CL constraint <underline>r</underline>efactoring <underline>a</underline>pproach (SBORA) by defining and applying four semantics-preserving refactoring operators (i.e., <italic>Context Change</italic>, <italic>Swap</italic>, <italic>Split</italic> and <italic>Merge</italic>) and three OCL quality metrics (<italic>Complexity</italic>, <italic>Coupling,</italic> and <italic>Cohesion</italic>) to measure the understandability and maintainability of OCL constraints. We evaluate SBORA along with six commonly used multi-objective search algorithms (e.g., Indicator-Based Evolutionary Algorithm (IBEA)) by employing four case studies from different domains: healthcare (i.e., cancer registry system from Cancer Registry of Norway (CRN)), Oil&Gas (i.e., subsea production systems), warehouse (i.e., handling systems), and an open source case study named SEPA. Results show: 1) IBEA achieves the best performance among all the search algorithms and 2) the refactoring approach along with IBEA can manage to reduce on average 29.25 percent <italic>Complexity</italic> and 39 percent <italic>Coupling</italic> and improve 47.75 percent <italic>Cohesion</italic>, as compared to the original OCL constraint set from CRN. To further test the performance of SBORA, we also applied it to refactor an OCL constraint set specified on the UML 2.3 metamodel and we obtained positive results. Furthermore, we conducted a controlled experiment with 96 subjects and results show that the understandability and maintainability of the original constraint set can be improved significantly from the perspectives of the 96 participants of the controlled experiment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2774829","RFF Hovedstaden; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8114267","Constraints;metrics/measurement;methodologies;CASE","Cancer;Unified modeling language;Measurement;Couplings;Complexity theory;Semantics;Computational modeling","","","","","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Developer Micro Interaction Metrics for Software Defect Prediction","T. Lee; J. Nam; D. Han; S. Kim; H. Peter In","Korea University, Seoul, South Korea; University of Waterloo, ON, Canada; University Colleage London, London, United Kingdom; Hong Kong University of Science and Technology, Hong Kong, China; Korea University, Seoul, South Korea","IEEE Transactions on Software Engineering","","2016","42","11","1015","1035","To facilitate software quality assurance, defect prediction metrics, such as source code metrics, change churns, and the number of previous defects, have been actively studied. Despite the common understanding that developer behavioral interaction patterns can affect software quality, these widely used defect prediction metrics do not consider developer behavior. We therefore propose micro interaction metrics (MIMs), which are metrics that leverage developer interaction information. The developer interactions, such as file editing and browsing events in task sessions, are captured and stored as information by Mylyn, an Eclipse plug-in. Our experimental evaluation demonstrates that MIMs significantly improve overall defect prediction accuracy when combined with existing software measures, perform well in a cost-effective manner, and provide intuitive feedback that enables developers to recognize their own inefficient behaviors during software development.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2550458","Next-Generation Information Computing Development Program; National Research Foundation of Korea; Ministry of Education, Science and Technology; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7447797","Defect prediction;software quality;software metrics;developer interaction;Mylyn","Software quality;Software metrics;Quality assurance;Complexity theory","software maintenance;software metrics;software quality","developer microinteraction metrics;software quality assurance;software defect prediction;defect prediction metrics;developer behavioral interaction patterns;MIM metric;developer interaction information;Mylyn plug-in;Eclipse plug-in;software development","","10","","66","","","","","","IEEE","IEEE Journals & Magazines"
"A taxonomy and catalog of runtime software-fault monitoring tools","N. Delgado; A. Q. Gates; S. Roach","Microsoft, Bellevue, WA, USA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","12","859","872","A goal of runtime software-fault monitoring is to observe software behavior to determine whether it complies with its intended behavior. Monitoring allows one to analyze and recover from detected faults, providing additional defense against catastrophic failure. Although runtime monitoring has been in use for over 30 years, there is renewed interest in its application to fault detection and recovery, largely because of the increasing complexity and ubiquitous nature of software systems. We present taxonomy that developers and researchers can use to analyze and differentiate recent developments in runtime software fault-monitoring approaches. The taxonomy categorizes the various runtime monitoring research by classifying the elements that are considered essential for building a monitoring system, i.e., the specification language used to define properties; the monitoring mechanism that oversees the program's execution; and the event handler that captures and communicates monitoring results. After describing the taxonomy, the paper presents the classification of the software-fault monitoring systems described in the literature.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377185","Index Terms- Assertion checkers;runtime monitors;specification;specification language;survey;software/program verification.","Taxonomy;Runtime;Condition monitoring;Computerized monitoring;Fault detection;Application software;Software systems;Specification languages;Software testing;Computer Society","software fault tolerance;program verification;specification languages;system monitoring;system recovery","runtime software-fault monitoring tools;software behavior;catastrophic failure;runtime monitoring;software system;specification language;program verification","","178","","76","","","","","","IEEE","IEEE Journals & Magazines"
"Quality, productivity, and learning in framework-based development: an exploratory case study","M. Morisio; D. Romano; I. Stamelos","Dipt. di Autom. e Inf., Politecnico di Torino, Italy; NA; NA","IEEE Transactions on Software Engineering","","2002","28","9","876","888","This paper presents an empirical study in an industrial context on the production of software using a framework. Frameworks are semicomplete applications, usually implemented as a hierarchy of classes. The framework is developed first, then several applications are derived from it. Frameworks are a reuse technique that supports the engineering of product lines. In the study, we compare quality (in the sense of rework effort) and productivity in traditional and framework-based software production. We observe that the latter is characterized by better productivity and quality, as well as a massive increase in productivity over time, that we attribute to the effect of learning the framework. Although we cannot extrapolate the results outside the local environment, enough evidence has been accumulated to stimulate future research work.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033227","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033227","","Productivity;Computer aided software engineering;Application software;Object oriented modeling;Software quality;Production;Software design;Investments;Computer Society;Computer industry","software quality;software reusability;software development management","software production;framework-based development;semicomplete applications;reuse technique;product line engineering;quality;productivity;learning","","22","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Discovering Services during Service-Based System Design Using UML","G. Spanoudakis; A. Zisman","City University, London; City University, London","IEEE Transactions on Software Engineering","","2010","36","3","371","389","Recently, there has been a proliferation of service-based systems, i.e., software systems that are composed of autonomous services but can also use software code. In order to support the development of these systems, it is necessary to have new methods, processes, and tools. In this paper, we describe a UML-based framework to assist with the development of service-based systems. The framework adopts an iterative process in which software services that can provide functional and nonfunctional characteristics of a system being developed are discovered, and the identified services are used to reformulate the design models of the system. The framework uses a query language to represent structural, behavioral, and quality characteristics of services to be identified, and a query processor to match the queries against service registries. The matching process is based on distance measurements between the queries and service specifications. A prototype tool has been implemented. The work has been evaluated in terms of recall, precision, and performance measurements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374424","Design notations and documentation;software process models;search discovery language;service discovery engine.","Unified modeling language;Software systems;Database languages;Quality of service;Computer Society;Distance measurement;Software prototyping;Prototypes;Documentation;Engines","pattern matching;query languages;query processing;software prototyping;Unified Modeling Language;Web services","service-based system design;software systems;software code;UML-based framework;iterative process;software services;query processor;matching process;distance measurements;query language","","23","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Is proof more cost-effective than testing?","S. King; J. Hammond; R. Chapman; A. Pryor","Dept. of Comput. Sci., York Univ., UK; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","8","675","686","The paper describes the use of formal development methods on an industrial safety-critical application. The Z notation was used for documenting the system specification and part of the design, and the SPARK subset of Ada was used for coding. However, perhaps the most distinctive nature of the project lies in the amount of proof that was carried out: proofs were carried out both at the Z level (approximately 150 proofs in 500 pages) and at the SPARK code level (approximately 9000 verification conditions generated and discharged). The project was carried out under UK Interim Defence Standards 00-55 and 00-56, which require the use of formal methods on safety-critical applications. It is believed to be the first to be completed against the rigorous demands of the 1991 version of these standards. The paper includes comparisons of proof with the various types of testing employed, in terms of their efficiency at finding faults. The most striking result is that the Z proof appears to be substantially more efficient at finding faults than the most efficient testing phase. Given the importance of early fault detection, we believe this helps to show the significant benefit and practicality of large-scale proof on projects of this kind.","0098-5589;1939-3520;2326-3881","","10.1109/32.879807","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879807","","Sparks;Helicopters;Formal specifications;Marine vehicles;Information systems;Fault detection;Large-scale systems;Software testing;Computer industry;Code standards","safety-critical software;formal specification;theorem proving;Ada;program testing;military computing","formal development methods;industrial safety-critical application;Z notation;system specification;SPARK subset;Ada;Z level;SPARK code level;verification conditions;UK Interim Defence Standards;00-55;00-56;formal methods;safety-critical applications;rigorous demands;Z proof;testing phase;fault detection;large-scale proof","","35","","","","","","","","IEEE","IEEE Journals & Magazines"
"Repository evaluation of software reuse","R. D. Banker; R. J. Kauffman; D. Zweig","Dept. of Accounting & Inf. Syst., Minnesota Univ., Minneapolis, MN, USA; NA; NA","IEEE Transactions on Software Engineering","","1993","19","4","379","389","The use and benefits of repository evaluation of software reuse are illustrated through an analysis of the evolving repositories of two large firms that recently implemented integrated CASE development tools. The analysis shows that these tools have supported high levels of software reuse, but it also suggests that there remains considerable unexploited reuse potential. The findings indicate that organizational changes will be required before the full potential of the new technology can be realized.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.223805","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=223805","","Computer aided software engineering;Software tools;Software quality;Project management;Application software;Productivity;Computerized monitoring;Costs;Software maintenance;Control system analysis","programming environments;software reusability","software reuse;repository evaluation;integrated CASE development tools;organizational changes","","40","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Validation of an approach for improving existing measurement frameworks","M. G. Mendonca; V. R. Basili","Comput. Networks Res. Group, Salvador Univ., Brazil; NA","IEEE Transactions on Software Engineering","","2000","26","6","484","499","Software organizations are in need of methods to understand, structure, and improve the data their are collecting. We have developed an approach for use when a large number of diverse metrics are already being collected by a software organization (M.G. Mendonca et al., 1998; M.G. Mendonca, 1997). The approach combines two methods. One looks at an organization's measurement framework in a top-down goal-oriented fashion and the other looks at it in a bottom-up data-driven fashion. The top-down method is based on a measurement paradigm called Goal-Question-Metric (GQM). The bottom-up method is based on a data mining technique called Attribute Focusing (AF). A case study was executed to validate this approach and to assess its usefulness in an industrial environment. The top-down and bottom-up methods were applied in the customer satisfaction measurement framework at the IBM Toronto Laboratory. The top-down method was applied to improve the customer satisfaction (CUSTSAT) measurement from the point of view of three data user groups. It identified several new metrics for the interviewed groups, and also contributed to better understanding of the data user needs. The bottom-up method was used to gain new insights into the existing CUSTSAT data. Unexpected associations between key variables prompted new business insights, and revealed problems with the process used to collect and analyze the CUSTSAT data. The paper uses the case study and its results to qualitatively compare our approach against current ad hoc practices used to improve existing measurement frameworks.","0098-5589;1939-3520;2326-3881","","10.1109/32.852739","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=852739","","Software measurement;Customer satisfaction;Data mining;Laboratories;Software maintenance;Data analysis;Current measurement;Knowledge management;Software development management;Time measurement","data mining;DP industry;software metrics;software quality","business insights;data user needs;CUSTSAT measurement;customer satisfaction measurement framework;industrial environment;case study;Attribute Focusing;data mining technique;Goal-Question-Metric;measurement paradigm;bottom-up data-driven method;top-down goal-oriented method;diverse metrics;software organizations;measurement frameworks","","34","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Investigating the defect detection effectiveness and cost benefit of nominal inspection teams","S. Biffl; M. Halling","Inst. for Software Technol., Vienna Univ. of Technol., Austria; NA","IEEE Transactions on Software Engineering","","2003","29","5","385","397","Inspection is an effective but also expensive quality assurance activity to find defects early during software development. The defect detection process, team size, and staff hours invested can have a considerable impact on the defect detection effectiveness and cost-benefit of an inspection. In this paper, we use empirical data and a probabilistic model to estimate this impact for nominal (noncommunicating) inspection teams in an experiment context. Further, the analysis investigates how cutting off the inspection after a certain time frame would influence inspection performance. Main findings of the investigation are: 1) Using combinations of different reading techniques in a team is considerably more effective than using the best single technique only (regardless of the observed level of effort). 2) For optimizing the inspection performance, determining the optimal process mix in a team is more important than adding an inspector (above a certain team size) in our model. 3) A high level of defect detection effectiveness is much more costly to achieve than a moderate level since the average cost for the defects found by the inspector last added to a team increases more than linearly with growing effort investment. The work provides an initial baseline of inspection performance with regard to process diversity and effort in inspection teams. We encourage further studies on the topic of time usage with defect detection techniques and its effect on inspection effectiveness in a variety of inspection contexts to support inspection planning with limited resources.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199069","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199069","","Inspection;Programming;Quality assurance;Cost function;Investments;Context modeling;Performance analysis;Software engineering;Systems engineering and theory;Humans","software development management;program debugging;inspection;optimisation","defect detection effectiveness;cost-benefit analysis;nominal inspection teams;quality assurance;software development;noncommunicating inspection teams;reading technique combinations;optimal process mix","","32","","28","","","","","","IEEE","IEEE Journals & Magazines"
"When to stop testing for large software systems with changing code","S. R. Dalal; A. A. McIntosh","Inf. Sci. & Technol. Lab., Bellcore, Morristown, NJ, USA; Inf. Sci. & Technol. Lab., Bellcore, Morristown, NJ, USA","IEEE Transactions on Software Engineering","","1994","20","4","318","323","Developers of large software systems must decide how long software should be tested before releasing it. A common and usually unwarranted assumption is that the code remains frozen during testing. We present a stochastic and economic framework to deal with systems that change as they are tested. The changes can occur because of the delivery of software as it is developed, the way software is tested, the addition of fixes, and so on. Specifically, we report the details of a real time trial of a large software system that had a substantial amount of code added during testing. We describe the methodology, give all of the relevant details, and discuss the results obtained. We pay particular attention to graphical methods that are easy to understand, and that provide effective summaries of the testing process. Some of the plots found useful by the software testers include: the Net Benefit Plot, which gives a running chart of the benefit; the Stopping Plot, which estimates the amount of additional time needed for testing; and diagnostic plots. To encourage other researchers to try out different models, all of the relevant data are provided.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.277579","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=277579","","Software testing;System testing;Software systems;Costs;Fault detection;Stochastic systems;Real time systems;Software metrics;Software reliability;Lead","program testing;software metrics;software reliability;computer graphics","large software systems;changing code;economic framework;real time trial;software testers;Net Benefit Plot;Stopping Plot;diagnostic plots;optimal stopping rule;graphical methods;software metrics;statistical inference;software reliability model;software fault detection","","35","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Mining of Opinions Expressed About APIs in Stack Overflow","G. Uddin; F. Khomh","School of Computer Science, McGill University, 5620 Montreal, Quebec Canada H3A 0G4 (e-mail: giasu@cs.mcgill.ca); Electrical and Computer Engineering, Queen?s University, Kingston, Ontario Canada K7L3N6 (e-mail: foutse.khomh@polymtl.ca)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","With the proliferation of online developer forums, developers share their opinions about the APIs they use. The plethora of such information can present challenges to the developers to get quick but informed insights about the APIs. To understand the potential benefits of such API reviews, we conducted a case study of opinions in Stack Overflow using a benchmark dataset of 4522 sentences. We observed that opinions about diverse API aspects (e.g., usability) are prevalent and offer insights that can shape developers' perception and decisions related to software development. Motivated by the finding, we built a suite of techniques to automatically mine and categorize opinions about APIs from forum posts. First, we detect opinionated sentences in the forum posts. Second, we associate the opinionated sentences to the API mentions. Third, we detect API aspects (e.g., performance, usability) in the reviews. We developed and deployed a tool called Opiner, supporting the above techniques. Opiner is available online as a search engine, where developers can search for APIs by their names to see all the aggregated opinions about the APIs that are automatically mined and summarized from developer forums.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2900245","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8643972","API;Opinion;Categorization;Review;API Aspect;API Review Mining","Benchmark testing;Usability;Search engines;Java;Data mining;Tools","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Connectors for mobile programs","M. Wermelinger; J. L. Fiadeiro","Dept. de Inf., Lisbon Univ., Portugal; NA","IEEE Transactions on Software Engineering","","1998","24","5","331","341","Software architecture has put forward the concept of connector to express complex relationships between system components, thus facilitating the separation of coordination from computation. This separation is especially important in mobile computing due to the dynamic nature of the interactions among participating processes. We present connector patterns, inspired in Mobile UNITY, that describe three basic kinds of transient interactions: action inhibition, action synchronization, and message passing. The connectors are given in COMMUNITY, a UNITY-like program design language which has a semantics in category theory. We show how the categorical framework can be used for applying the proposed connectors to specific components and how the resulting architecture can be visualized by a diagram showing the components and the connectors.","0098-5589;1939-3520;2326-3881","","10.1109/32.685257","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=685257","","Connectors;Computer architecture;Mobile computing;Message passing;Software architecture;Visualization;Software systems;Proposals;Quantum computing;Computational modeling","software engineering;diagrams;portable computers;message passing;synchronisation;category theory;software portability;high level languages","mobile programs;software architecture;connectors;system components;coordination;computation;mobile computing;connector patterns;Mobile UNITY;action inhibition;action synchronization;message passing;COMMUNITY;category theory;diagram","","27","","14","","","","","","IEEE","IEEE Journals & Magazines"
"An intelligent tutoring system for the Dijkstra-Gries methodology","F. Ng; G. Butler; J. Kay","ISSC Australia, Sydney, NSW, Australia; NA; NA","IEEE Transactions on Software Engineering","","1995","21","5","415","428","The paper describes the design and implementation of an intelligent tutoring system for the Dijkstra-Gries programming methodology as defined by Gries (1981) in ""The Science of Programming"". The first part of the paper identifies the requirements of intelligent tutoring systems in general and those of the methodology in particular. It shows the suitability of the Smalltalk environment for developing expandable intelligent systems and the compatibility of Smalltalk's object-oriented paradigm with the Gries methodology's goal/plan approach to programming. We then describe how these requirements are met: an overview of the system's support of the methodology and the modules that enable the system to respond intelligently. As an example, a reusable tutorial part is presented, first from a student's perspective, then from an author's perspective. Finally the results of an evaluation of the system drawn from actual student experience are presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.387471","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=387471","","Intelligent systems;Object oriented programming;Logic programming;Libraries;Mathematical programming;Algebra;Education;Parallel programming;Computer science;Object oriented modeling","intelligent tutoring systems;Smalltalk;knowledge engineering;software reusability;object-oriented programming;object-oriented methods;computer science education","intelligent tutoring system;Dijkstra-Gries programming methodology;requirements;Smalltalk environment;expandable intelligent systems development;object-oriented paradigm;system support;modules;reusable tutorial part;student's perspective;author's perspective;actual student experience;system evaluation","","3","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Towards a Formal Basis for the Formal Development Method and the Ina Jo Specification Language","D. M. Berry","SDC, A Burroughs Company, Santa Monica, CA. 90405, and the Department of Computer Science, University of California","IEEE Transactions on Software Engineering","","1987","SE-13","2","184","201","In carrying out SDC's Formal Development Method, one writes a specification of a system under design in the Ina Jo&#8482; specification language and proves that the specification meets the requirements of the system. This paper develops an abstract machine model of what is specified by a level specification in an Ina Jo specification. It describes the state as defined by the front matter, computations as defined by initial states and transforms, and invariants, criteria, and constraints as properties of computations. The paper then describes a number of formal design methods and the kinds of abstractions that they require. For each of these kinds of abstractions, there is a characteristic relationship between refinements that should be proved as one is carrying out the method.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232891","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702199","Abstract machine;correctness;formal specification;invariants;refinement methods;simulation;verification","Specification languages;Formal specifications;Design methodology;Computer security;Computer science;Trademarks;Humans;Contracts","","Abstract machine;correctness;formal specification;invariants;refinement methods;simulation;verification","","2","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Performance of MAP in the remote operation of a CNC","B. Mortazavi","Lab. d'Inf. Tech., Ecole Polytech. Federale de Lausanne, Switzerland","IEEE Transactions on Software Engineering","","1990","16","2","231","237","A major issue for the users of the Manufacturing Automation Protocol (MAP) is its performance in servicing time-critical applications. The author describes an experiment in which a computerized numerical controller (CNC) is remotely operated using MAP. A set of critical timing parameters is defined, a performance analysis of the system from the user's point of view is carried out, and a comparison with other similar systems is made. The transfer time between two front-end processors, implementing the seven MAP layers, for a 100-byte-long message is found to be about 18 ms, and the transfer time for the same message between two user processes on the hosts is about 40 ms.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44386","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44386","","Computer numerical control;Performance analysis;Protocols;Application software;Open systems;Software performance;EMP radiation effects;Manufacturing;Timing","computerised numerical control;program testing;protocols","MAP performance analysis;remote operation;Manufacturing Automation Protocol;servicing time-critical applications;computerized numerical controller;CNC;critical timing parameters;transfer time;front-end processors","","3","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Key establishment in large dynamic groups using one-way function trees","A. T. Sherman; D. A. McGrew","Departynent of Comput. Sci. & Electr. Eng., Maryland Univ., Baltimore, MD, USA; NA","IEEE Transactions on Software Engineering","","2003","29","5","444","458","We present, implement, and analyze a new scalable centralized algorithm, called OFT, for establishing shared cryptographic keys in large, dynamically changing groups. Our algorithm is based on a novel application of one-way function trees. In comparison with the top-down logical key hierarchy (LKH) method of Wallner et al., our bottom-up algorithm approximately halves the number of bits that need to be broadcast to members in order to rekey after a member is added or evicted. The number of keys stored by group members, the number of keys broadcast to the group when new members are added or evicted, and the computational efforts of group members, are logarithmic in the number of group members. Among the hierarchical methods, OFT is the first to achieve an approximate halving in broadcast length, an idea on which subsequent algorithms have built. Our algorithm provides complete forward and backward security: Newly admitted group members cannot read previous messages, and evicted members cannot read future messages, even with collusion by arbitrarily many evicted members. In addition, and unlike LKH, our algorithm has the option of being member contributory in that members can be allowed to contribute entropy to the group key. Running on a Pentium II, our prototype has handled groups with up to 10 million members. This algorithm offers a new scalable method for establishing group session keys for secure large-group applications such as broadcast encryption, electronic conferences, multicast sessions, and military command and control.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199073","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199073","","Satellite broadcasting;Cryptography;TV broadcasting;Security;Multicast algorithms;Conference management;Project management;Radio broadcasting;Algorithm design and analysis;Entropy","cryptography;protocols","shared cryptographic keys;encryption;conference keying;cryptography;cryptographic protocols;Dynamic Cryptographic Context Management;group keying;key agreement;key establishment;key management;logical key hierarchy;one-way functions;one-way function tree;secure conferences;secure group applications","","275","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Approximate analysis of load dependent general queueing networks","I. F. Akyildiz; A. Sieber","Dept. of Comput. Sci., Louisiana State Univ., Baton Rouge, LA, USA; Dept. of Comput. Sci., Louisiana State Univ., Baton Rouge, LA, USA","IEEE Transactions on Software Engineering","","1988","14","11","1537","1545","A method for obtaining approximate solutions to load-dependent closed queueing networks containing general service-time distributions and first-come-first-served scheduling disciplines is presented. The technique demonstrated is an extension of the well-known method of R. Marie (1979). A formula for the conditional throughputs is derived. After each iteration a check is performed to guarantee that the results obtained are within a tolerance level epsilon . These iterations are repeated whenever invalid results are detected. On the average, the solutions obtained vary by less than 5% from their respective exact and simulation results.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9042","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9042","","Queueing analysis;Computer science;Throughput;Computational modeling;Performance analysis;Computer networks;Communication networks;Computational efficiency;Predictive models;Computer architecture","performance evaluation;queueing theory;scheduling","performance evaluation;queueing networks;load-dependent;service-time distributions;scheduling;conditional throughputs;tolerance level","","12","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Recomputing Coverage Information to Assist Regression Testing","P. K. Chittimalli; M. J. Harrold","Tata Research Development &amp; Design Centre, India; Georgia Institute of Technology, Atlanta","IEEE Transactions on Software Engineering","","2009","35","4","452","469","This paper presents a technique that leverages an existing regression test selection algorithm to compute accurate, updated coverage data on a version of the software, P<sub>i+1</sub>, without rerunning any test cases that do not execute the changes from the previous version of the software, P<sub>i</sub> to P<sub>i+1</sub>. The technique also reduces the cost of running those test cases that are selected by the regression test selection algorithm by performing a selective instrumentation that reduces the number of probes required to monitor the coverage data. Users of our technique can avoid the expense of rerunning the entire test suite on P<sub>i+1</sub> or the inaccuracy produced by previous approaches that estimate coverage data for P<sub>i+1</sub> or that reuse outdated coverage data from Pi. This paper also presents a tool, RECOVER, that implements our technique, along with a set of empirical studies on a set of subjects that includes several industrial programs, versions, and test cases. The studies show the inaccuracies that can exist when an application-regression test selection-uses estimated or outdated coverage data. The studies also show that the overhead incurred by selective instrumentation used in our technique is negligible and overall our technique provides savings over earlier techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4760153","Regression testing;regression test selection;testing;maintenance.","Software testing;Costs;Performance evaluation;Instruments;Software quality;Software algorithms;Probes;Monitoring;Software performance;Error correction","program testing;regression analysis","coverage information;regression testing;software testing;industrial programs","","21","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Understanding Exception Handling: Viewpoints of Novices and Experts","H. Shah; C. Gorg; M. J. Harrold","Georgia Institute of Technology, Atlanta; Georgia Institute of Technology, Atlanta; Georgia Institute of Technology, Atlanta","IEEE Transactions on Software Engineering","","2010","36","2","150","161","Several recent studies indicate that many industrial applications exhibit poor quality in the design of exception-handling. To improve the quality of error-handling, we need to understand the problems and obstacles that developers face when designing and implementing exception-handling. In this paper, we present our research on understanding the viewpoint of developers-novices and experts-toward exception-handling. First, we conducted a study with novice developers in industry. The study results reveal that novices tend to ignore exceptions because of the complex nature of exception-handling. Then, we conducted a second study with experts in industry to understand their perspective on exception-handling. The study results show that, for experts, exception-handling is a crucial part in the development process. Experts also confirm the novices' approach of ignoring exception-handling and provide insights as to why novices do so. After analyzing the study data, we identified factors that influence experts' strategy selection process for handling exceptions and then built a model that represents a strategy selection process experts use to handle exceptions. Our model is based on interacting modules and fault scope. We conclude with some recommendations to help novices improve their understanding of exception-handling.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383375","Exception handling;user study;software developers.","Software performance;Application software;Debugging;Data analysis;Software tools;Functional programming;Programming profession;Performance evaluation;Visualization","error handling;software engineering","exception handling design;error handling quality;software development process;expert strategy selection process","","24","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Tool support for verifying UML activity diagrams","R. Eshuis; R. Wieringa","Dept. of Technol. Manage., Eindhoven Univ. of Technol., Netherlands; NA","IEEE Transactions on Software Engineering","","2004","30","7","437","447","We describe a tool that supports verification of workflow models specified in UML activity diagrams. The tool translates an activity diagram into an input format for a model checker according to a mathematical semantics. With the model checker, arbitrary propositional requirements can be checked against the input model. If a requirement fails to hold, an error trace is returned by the model checker, which our tool presents by highlighting a corresponding path in the activity diagram. We summarize our formal semantics, discuss the techniques used to reduce an infinite state space to a finite one, and motivate the need for strong fairness constraints to obtain realistic results. We define requirement-preserving rules for state space reduction. Finally, we illustrate the whole approach with a few example verifications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318605","Analysis;tools;software/program verification;model checking;state diagrams;workflow management.","Unified modeling language;Logic;Mathematical model;State-space methods;Software tools;Computer industry;Software standards;Software design;Workflow management software;Software systems","formal specification;specification languages;workflow management software;program verification","workflow model verification;UML activity diagrams;mathematical semantics;formal semantics;requirement-preserving rules;program verification;model checking;state diagrams;workflow management","","51","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis of the effects of software reuse on customer satisfaction in an RPG environment","G. Succi; L. Benedicenti; T. Vernazza","Dept. of Electr. & Comput. Eng., Alberta Univ., Edmonton, Alta., Canada; NA; NA","IEEE Transactions on Software Engineering","","2001","27","5","473","479","This paper reports on empirical research based on two software products. The research goal is to ascertain the impact of the adoption of a reuse policy on customer satisfaction. The results show that when a systematic reuse policy is implemented, such as the adoption of a domain specific library: reuse is significantly positively correlated with customer satisfaction; and there is a significant increase in customer satisfaction. The results have been extended to the underlying populations, supposed normal.","0098-5589;1939-3520;2326-3881","","10.1109/32.922717","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=922717","","Customer satisfaction;Software libraries;Software measurement;Productivity;Density measurement;Frequency measurement;Size measurement;Computer Society;Software metrics;Software engineering","software reusability;software metrics;software libraries","software reuse;customer satisfaction;software products;domain specific library;software metrics;software engineering","","18","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Rule-based design methodology for solving control problems","F. A. Etessami; G. S. Hura","Dept. of Comput. Sci. & Eng., Wright State Univ., Dayton, OH, USA; Dept. of Comput. Sci. & Eng., Wright State Univ., Dayton, OH, USA","IEEE Transactions on Software Engineering","","1991","17","3","274","282","A rule-based design methodology for solving control problems is presented. For the representation of various constraints, activities, and other dependency properties of the control problem, abstract Petri nets (APNs) which are an extended form of Petri net modeling are used as a specification and formalism tool which can be analyzed using the analysis techniques of Petri-net-based models. The APN provides a compact, consistent, and verifiable description of the dynamic behaviour of the system under consideration in a structured mode. The proposed design methodology supports specification, validation, and analysis through high-level interaction with the modeled system. The various steps which were taken towards the development of such a design paradigm are explained. An example which shows the APN modeling of an elevator system is given.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.75416","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=75416","","Design methodology;Petri nets;Power system modeling;Computer science;Student members;Problem-solving;Performance evaluation;Protocols;Information systems;Joining processes","formal specification;knowledge engineering;Petri nets;software tools","control problems;rule-based design methodology;abstract Petri nets;specification;formalism tool;dynamic behaviour;validation;high-level interaction;elevator system","","12","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Deadline analysis of interrupt-driven software","D. Brylow; J. Palsberg","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; NA","IEEE Transactions on Software Engineering","","2004","30","10","634","655","Real-time, reactive, and embedded systems are increasingly used throughout society (e.g., flight control, railway signaling, vehicle management, medical devices, and many others). For real-time, interrupt-driven software, timely interrupt handling is part of correctness. It is vital for software verification in such systems to check that all specified deadlines for interrupt handling are met. Such verification is a daunting task because of the large number of different possible interrupt arrival scenarios. For example, for a Z86-based microcontroller, there can be up to six interrupt sources and each interrupt can arrive during any clock cycle. Verification of such systems has traditionally relied upon lengthy and tedious testing; even under the best of circumstances, testing is likely to cover only a fraction of the state space in interrupt-driven systems. This paper presents the Zilog architecture resource bounding infrastructure (ZARBI), a tool for deadline analysis of interrupt-driven Z86-based software. The main idea is to use static analysis to significantly decrease the required testing effort by automatically identifying and isolating the segments of code that need the most testing. Our tool combines multiresolution static analysis and testing oracles in such a way that only the oracles need to be verified by testing. Each oracle specifies the worst-case execution time from one program point to another, which is then used by the static analysis to improve precision. For six commercial microcontroller systems, our experiments show that a moderate number of testing oracles are sufficient to do precise deadline analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339276","Index Terms- Real time;multiresolution static analysis;testing oracles.","System testing;Microcontrollers;Automatic testing;Real time systems;Embedded system;Aerospace control;Rail transportation;Vehicles;Clocks;State-space methods","embedded systems;interrupts;program diagnostics;microcontrollers;program testing;formal verification;program compilers","embedded systems;interrupt handling;software verification;Z86-based microcontroller;Zilog architecture resource bounding infrastructure;deadline analysis;interrupt-driven Z86-based software;multiresolution static analysis;testing oracles;worst-case execution time","","10","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Fast, centralized detection and resolution of distributed deadlocks in the generalized model","Soojung Lee","Dept. of Comput. Educ., GyeongIn Nat. Univ. of Educ., Inchon, South Korea","IEEE Transactions on Software Engineering","","2004","30","9","561","573","In the literature, only a few studies have been performed on the distributed deadlock detection and resolution problem in the generalized request model. Most of the studies are based on the diffusing computation technique where propagation of probes and backward propagation of replies are required to detect deadlock. The replies carry the dependency information between processes for the initiator of the algorithm to determine deadlock. Since fast detection of deadlock is critical, we take a centralized approach that removes the need of backward propagation of replies, but sends the dependency information directly to the initiator of the algorithm. This enables reduction of time cost for deadlock detection to half of that of the existing distributed algorithms. The algorithm is extended to handle concurrent executions in order to further improve deadlock detection time, whereas the current algorithms focus only on a single execution. Simulation experiments are performed to see the effectiveness of this centralized approach as compared to previous distributed algorithms. It is found that our algorithm shows better results in several performance metrics especially in deadlock latency and algorithm execution time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324644","Index Terms- Deadlock detection;deadlock resolution;distributed algorithms;distributed systems;wait-for graph.","System recovery;Probes;Distributed algorithms;Computer Society;Costs;Measurement;Delay;Resumes;Resource management;Operating systems","distributed algorithms;system recovery;operating systems (computers);directed graphs;computational complexity;message passing","distributed deadlock detection;distributed deadlock resolution;backward propagation;dependency information;centralized approach;algorithm initiator;distributed algorithm;concurrent execution;performance metrics;deadlock latency;algorithm execution time;distributed systems;wait-for graph","","19","","21","","","","","","IEEE","IEEE Journals & Magazines"
"A decomposition of a formal specification: an improved constraint-oriented method","Kentaro Go; N. Shiratori","Dept. of Comput. Sci., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA; NA","IEEE Transactions on Software Engineering","","1999","25","2","258","273","In this paper, the authors propose a decomposition method for a formal specification that divides the specification into two subspecifications composed by a parallel operator. To make these specification behaviors equivalent before and after decomposition, the method automatically synthesizes an additional control specification, which contains the synchronization information of the decomposed subspecifications. The authors prove that a parallel composition of the decomposed subspecifications synchronized with the control specification is strongly equivalent with the original (monolithic) specification. The authors also write formal specifications of the OSI application layer's association-control service and decompose it using their method as an example of decomposition of a practical specification. Their decomposition method can be applied to top-down system development based on stepwise refinement.","0098-5589;1939-3520;2326-3881","","10.1109/32.761449","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=761449","","Formal specifications;Automatic control;Open systems;Protocols;Computer Society;System analysis and design;Costs;Collaborative software;Collaborative work;Productivity","formal specification","formal specification decomposition;improved constraint-oriented method;subspecifications;parallel operator;equivalent specification behavior;control specification;parallel composition;OSI application layer association-control service;top-down system development;stepwise refinement","","3","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Executable logic specifications for protocol service interfaces","D. P. Sidhu; C. S. Crall","Dept. of Comput. Sci., Iowa State Univ., Ames, IA, USA; Dept. of Comput. Sci., Iowa State Univ., Ames, IA, USA","IEEE Transactions on Software Engineering","","1988","14","1","98","112","A general, formal modeling technique for protocol service interfaces is discussed. An executable description of the model using a logic-programming-based language, Prolog, is presented. The specification of protocol layers consists of two parts, the specification of the protocol interfaces and the specification of entities within the protocol layer. The specification of protocol interfaces forms the standard against which protocols are verified. When a protocol has been implemented, the correctness of its implementation can be tested using the sequences of events generated at the service interface. If the behavior of the protocol implementation is consistent with the behavior at the service interface, the implementation conforms to its standard. To illustrate how it works, the model is applied to the service interfaces of protocol standards developed for the transport layer of the ISO/OSI architecture. The results indicate that Prolog is a very useful formal language for specifying protocol interfaces.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4626","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4626","","Logic programming;ISO standards;Open systems;Computer science;Prototypes;Testing;Transport protocols;Standards development;NIST;Data communication","formal languages;PROLOG;protocols;specification languages","executable logic specifications;protocol service interfaces;formal modeling technique;logic-programming-based language;Prolog;protocol layers;correctness;protocol standards;transport layer;ISO/OSI;formal language","","14","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Local Distributed Deadlock Detection by Cycle Detection and Clusterng","I. Cidon; J. M. Jaffe; M. Sidi","IBM Thomas J. Watson Research Center; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","1","3","14","A distributed algorithm for the detection of deadlocks in store-and-forward communication networks is presented. At first, we focus on a static environment and develop an efficient knot detection algorithm for general graphs. The knot detection algorithm uses at most O(n<sup>2</sup>+ m) messages and O(log (n)) bits of memory to detect all deadlocked nodes in the static network. Using the knot detection algorithm as a building block, a deadlock detection algorithm in a dynamic environment is developed. This algorithm has the following properties: It detects all the nodes which cause the deadlock. The algorithm is triggered only when there is a potential for deadlock and only those nodes which are potentially deadlocked perform the algorithm. The algorithm does not affect other processes at the nodes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232560","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702127","Clustering;computer networks;cycle detection;deadlock detection;distributed algorithms","System recovery;Detection algorithms;Distributed algorithms;Computer networks;Buffer storage;Software algorithms;Communication networks;Distributed computing;Cities and towns","","Clustering;computer networks;cycle detection;deadlock detection;distributed algorithms","","8","","17","","","","","","IEEE","IEEE Journals & Magazines"
"A systematic literature review on bad smells — 5 W's: which, when, what, who, where","E. V. d. P. Sobrinho; A. De Lucia; M. d. A. Maia","Department of Electrical Engineering, Federal University of Triangulo Mineiro, Uberaba, Minas Gerias Brazil 38064-200 (e-mail: eldereng@hotmail.com); Dipartimento di Matematica e Informatica, University di Salerno, Fisciano, Salerno Italy 84084 (e-mail: adelucia@unisa.it); Faculty of Computing, Federal University of Uberlandia, Uberlandia, Minas Gerais Brazil (e-mail: marcelo.maia@ufu.br)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Bad smells are sub-optimal code structures that may represent problems that need attention. We conduct an extensive literature review on a huge body of knowledge from 1990 to 2017. We show that some smells are much more studied in the literature than others and some of them are intrinsically inter-related (which). We give a perspective on how the research has been driven across time (when). In particular, while the interest in duplicated code emerged before the reference publications by Fowler and Beck and by Brown et al., other types of bad smells started to be studied only after these seminal publications, with an increasing trend in the last decade. We analyzed aims, findings, and respective experimental settings and observed that the variability on these elements may be responsible for some apparently contradictory findings on bad smells (what). Moreover, while in general bad smells of different types are studied together, only a small percentage of these studies actually investigate the relations between them (co-studies). In addition, only few relations between some types of bad smells were investigated, while there are other possible relations for further investigation. We also noted that authors have different levels of interest in the subject, some of them publishing sporadically and others continuously (who). We observed that scientific connections are ruled by a large “small world” connected graph among researchers and several small disconnected graphs. We also found that the communities studying duplicated code and other types of bad smells are largely separated. Finally, we observed that some venues are more likely to disseminate knowledge on Duplicate Code (which often is listed as a conference topic on its own), while others have a more balanced distribution among other smells (where). Finally, we provide a discussion on future directions for bad smell research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2880977","Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8532309","Software maintenance;reengineering;bad smell","Systematics;Bibliographies;Software;Measurement;Organizations;Tools;Cloning","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Local Area Networks: Software and Related Issues","S. K. Tripathi; Yennun Huang; S. Jajodia","UMIACS and the Department of Computer Science, University of Maryland; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","8","872","879","In this paper, we present a review of the issues that affect the software requirements for a local area network. We introduce protocols for the local area networks and characterize their software needs. Two approaches to operating systems are outlined and examples of each approach are presented. Various applications which use local area networks and performance issues are also discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233506","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702306","LAN software;local area networks;network operating systems;protocols;remote procedure calls","Local area networks;Protocols;Operating systems;Application software;Software performance;Computer networks;System software;Delay;Computer architecture;Communication system software","","LAN software;local area networks;network operating systems;protocols;remote procedure calls","","","","75","","","","","","IEEE","IEEE Journals & Magazines"
"Architectural tradeoffs for a meaning-preserving program restructuring tool","W. G. Griswold; D. Notkin","Dept. of Comput. Sci. & Eng., California Univ., San Diego, La Jolla, CA, USA; NA","IEEE Transactions on Software Engineering","","1995","21","4","275","287","Maintaining the consistency of multiple program representations in a program manipulation tool is difficult. I describe a hybrid software architecture for a meaning-preserving program restructuring tool. Layering is the primary architectural paradigm, which successively provides increasingly integrated and unified abstract machines to implement the tool. However, layering does not provide adequate control over extensibility or the independence of components, so I also adopt the paradigm of keeping the key program abstractions separate throughout the layering, providing independent columns of abstract data types. A pair of columns is integrated by a mapping column that translates elements in one column's data type into related elements in the other column's data type. Thus, integration of function and separation of representation can be achieved simultaneously. This hybrid architecture was crucial in overcoming severe performance problems that became apparent once the basic tool was completed. By taking advantage of the independence of the columns and the special characteristics of meaning-preserving restructuring, it was possible to extend one representation column of the architecture to the uppermost layer to provide the required access for efficient updating without compromising independence. The cost of the extended architecture is that the upper layers are no longer as simple because they expose operations that only guarantee consistency under careful usage. However, the structural constraints of the hybrid architecture and the models for building the more complicated layers minimizes the negative impact of this tradeoff.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.385967","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=385967","","Computer architecture;Operating systems;Tree graphs;Computer science;Software architecture;Costs;Buildings;Software design;Software systems;Protocols","software tools;computer aided software engineering;abstract data types;program compilers","architectural tradeoffs;meaning-preserving program restructuring tool;consistency maintenance;multiple program representations;program manipulation tool;hybrid software architecture;layering;performance problems;abstract machines;extensibility;component independence;key program abstractions;abstract data type columns;mapping column;updating;structural constraints;software design;modularization;system evolution;layered systems","","15","","36","","","","","","IEEE","IEEE Journals & Magazines"
"A procedure for analyzing unbalanced datasets","B. Kitchenham","Dept. of Comput. Sci., Keele Univ., UK","IEEE Transactions on Software Engineering","","1998","24","4","278","301","This paper describes a procedure for analyzing unbalanced datasets that include many nominal- and ordinal-scale factors. Such datasets are often found in company datasets used for benchmarking and productivity assessment. The two major problems caused by lack of balance are that the impact of factors can be concealed and that spurious impacts can be observed. These effects are examined with the help of two small artificial datasets. The paper proposes a method of forward pass residual analysis to analyze such datasets. The analysis procedure is demonstrated on the artificial datasets and then applied to the COCOMO dataset. The paper ends with a discussion of the advantages and limitations of the analysis procedure.","0098-5589;1939-3520;2326-3881","","10.1109/32.677185","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=677185","","Data analysis;Productivity;Cause effect analysis;Analysis of variance;Statistical analysis;Software quality;Computer Society;Computer industry;Software measurement;Assembly","software metrics","unbalanced datasets;benchmarking;productivity assessment;COCOMO dataset;forward pass residual analysis;software metrics;statistical analysis;analysis of variance;residual analysis","","66","","21","","","","","","IEEE","IEEE Journals & Magazines"
"The Effects of Test-Driven Development on External Quality and Productivity: A Meta-Analysis","Y. Rafique; V. B. Mišić","Ryerson University, Toronto; Ryerson University, Toronto","IEEE Transactions on Software Engineering","","2013","39","6","835","856","This paper provides a systematic meta-analysis of 27 studies that investigate the impact of Test-Driven Development (TDD) on external code quality and productivity. The results indicate that, in general, TDD has a small positive effect on quality but little to no discernible effect on productivity. However, subgroup analysis has found both the quality improvement and the productivity drop to be much larger in industrial studies in comparison with academic studies. A larger drop of productivity was found in studies where the difference in test effort between the TDD and the control group's process was significant. A larger improvement in quality was also found in the academic studies when the difference in test effort is substantial; however, no conclusion could be derived regarding the industrial studies due to the lack of data. Finally, the influence of developer experience and task size as moderator variables was investigated, and a statistically significant positive correlation was found between task size and the magnitude of the improvement in quality.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6197200","Test-driven development;meta-analysis;code quality;programmer productivity;agile software development","Productivity;Computational modeling;Testing;Process control;Programming;Size measurement","program testing;software development management;software quality","test driven development;systematic meta analysis;code quality;code productivity;TDD;subgroup analysis;quality improvement","","31","","71","","","","","","IEEE","IEEE Journals & Magazines"
"Discarding Obsolete Information in a Replicated Database System","S. K. Sarin; N. A. Lynch","Computer Corporation of America; NA","IEEE Transactions on Software Engineering","","1987","SE-13","1","39","47","A replicated database architecture is described in which updates processed at a site must be saved to allow reconcilliation of newly arriving updates in a way that preserves mutual consistency. The storage space occupied by the saved updates increases indefinitely, and periodic discarding of old updates is needed to avoid running out of storage. A protocol is described which allows sites in the system to agree that updates older than a given timestamp are no longer needed and can be discarded. This protocol uses a ""distributed snapshot"" algorithm of Chandy and Lamport and represents a practical application of that algorithm. A protocol for permanent removal of sites is also described, which will allow the discarding of updates to continue when one or more sites crash and are expected not to recover.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232564","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702131","Distributed databases;distributed snapshots;mutual consistency;network partitions;replicated data;timestamps","Database systems;Protocols;Pipelines;Partitioning algorithms;Computer crashes;Distributed databases;Availability;Government;Computer science;Protection","","Distributed databases;distributed snapshots;mutual consistency;network partitions;replicated data;timestamps","","19","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Key Concepts of the INCAS Multicomputer Project","J. Nehmer; D. Haban; F. Mattern; D. Wybranietz; H. D. Rombach","Department of Computer Science, University of Kaiserslautern; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","8","913","923","This paper gives an overview of the INCAS (INCremental Architecture for distributed Systems) multicomputer project, which aims at the development of a comprehensive methodology for the design and implementation of locally distributed systems. A structuring concept for distributed operating systems has been developed and integrated into the system implementation language LADY. The concurrent high-level programming language CSSA, based on the actor model, has been designed for the implementation of distributed applications. A substantial effort in the INCAS project is directed towards the development of a distributed test methodology. An experimental system has been implemented on a network of ten MC68000 microcomputers. Preliminary experience with the methodology has been gained from a small number of prototype applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233510","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702310","Distributed operating systems;distributed programming languages;distributed systems;distributed testing;message passing;multicast communication;multicomputer","Operating systems;Application software;Design methodology;Computer languages;System testing;Computer science;Runtime;Microcomputers;Prototypes;Message passing","","Distributed operating systems;distributed programming languages;distributed systems;distributed testing;message passing;multicast communication;multicomputer","","15","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Specification and analysis of system architecture using Rapide","D. C. Luckham; J. J. Kenney; L. M. Augustin; J. Vera; D. Bryan; W. Mann","Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA","IEEE Transactions on Software Engineering","","1995","21","4","336","354","Rapide is an event-based, concurrent, object-oriented language specifically designed for prototyping system architectures. Two principle design goals are: (1) to provide constructs for defining executable prototypes of architectures and (2) to adopt an execution model in which the concurrency, synchronization, dataflow, and timing properties of a prototype are explicitly represented. This paper describes the partially ordered event set (poset) execution model and outlines with examples some of the event-based features for defining communication architectures and relationships between architectures. Various features of Rapide are illustrated by excerpts from a prototype of the X/Open distributed transaction processing reference architecture.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.385971","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=385971","","Object oriented modeling;Prototypes;Timing;Computer architecture;Concurrent computing;System testing;Desktop publishing;Virtual prototyping;Discrete event simulation;Analytical models","software prototyping;transaction processing;distributed processing;object-oriented languages;specification languages;synchronisation;open systems;parallel languages;formal specification","Rapide;system architecture prototyping;event-based concurrent object-oriented language;executable prototypes;execution model;specification language;synchronization;dataflow;timing properties;partially ordered event set;poset execution model;communication architectures;X/Open distributed transaction processing reference architecture;architecture definition language;simulation;formal constraints;constraint-based specification;event patterns;causality","","314","","43","","","","","","IEEE","IEEE Journals & Magazines"
"SARA (System ARchitects Apprentice): Modeling, analysis, and simulation support for design of concurrent systems","G. Estrin; R. S. Fenchel; R. R. Razouk; M. K. Vernon","Department of Computer Science, University of California, Los Angeles, CA 90024; Department of Computer Science, University of Wisconsin-Madison, Madison, WI 53706; Department of Information and Computer Science, University of California, Irvine, CA 92664; Department of Computer Science, University of Wisconsin-Madison, Madison, WI 53706","IEEE Transactions on Software Engineering","","1986","SE-12","2","293","311","An environment to support designers in the modeling, analysis, and simulation of concurrent systems is described. It is shown how a fully nested structure model supports multilevel design and focuses attention on the interfaces between the modules which serve to encapsulate behavior. Using simple examples, it is shown how a formal graph model can be used to model behavior in three domains: control flow, data flow, and interpretation. The effectiveness of the explicit environment model in SARA is discussed and the capability to analyze correctness and evaluate performance of a system model is demonstrated. A description of the integral help designed into SARA shows how the designer can be offered consistent use of any new tool introduced to support the design process.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312945","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312945","Concurrent systems;graph models;hierarchical design;integral help;interactive simulation;performance models;queueing models;reachability analysis","Analytical models;Data models;Manuals;Semantics;User interfaces;Grammar;Syntactics","parallel processing;programming environments","SARA;modeling;analysis;simulation support;concurrent systems;environment;fully nested structure model;multilevel design;interfaces;graph model;control flow;data flow;interpretation","","26","","","","","","","","IEEE","IEEE Journals & Magazines"
"Achieving service rate objectives with decay usage scheduling","J. L. Hellerstein","IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","1993","19","8","813","825","Decay usage scheduling is a priority- and usage-based approach to CPU allocation in which preference is given to processes that have consumed little CPU in the recent past. The author develops an analytic model for decay usage schedulers running compute-bound workloads, such as those found in many engineering and scientific environments; the model is validated from measurements of a Unix system. This model is used in two ways. First, ways to parameterize decay usage schedulers are studied to achieve a wide range of service rates. Doing so requires a fine granularity of control and a large range of control. The results show that, for a fixed representation of process priorities a larger range of control makes the granularity of control coarser, and a finer granularity of control decreases the range of control. A second use of the analytic model is to construct a low overhead algorithms for achieving service rate objectives. Existing approaches require adding a feedback loop to the scheduler. This overhead is avoided by exploiting the feedback already present in decay usage schedulers. Using both empirical and analytical techniques, it is shown that the algorithm is effective and that it provides fairness when the system is over- or under-loaded.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.238584","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=238584","","Processor scheduling;Central Processing Unit;Control systems;Algorithm design and analysis;Operating systems;Scheduling algorithm;Delay;Tellurium;Feedback loop;Throughput","resource allocation;scheduling;Unix","usage-based approach;CPU allocation;analytic model;decay usage schedulers;compute-bound workloads;scientific environments;Unix system;fine granularity;process priorities;low overhead algorithms;service rate objectives;feedback loop","","18","","22","","","","","","IEEE","IEEE Journals & Magazines"
"An executable language for modeling simple behavior","S. Lee; S. Sluizer","GTE Lab. Inc., Waltham, MA, USA; GTE Lab. Inc., Waltham, MA, USA","IEEE Transactions on Software Engineering","","1991","17","6","527","543","SXL, a modeling language that describes system behavior rather that software structure, is discussed. Using a conventional state-transition framework, model behavior is determined by rules that define pre- and postconditions for each transition. Behavior is also specified by constraints (logical invariants) that are automatically enforced during the execution of the model. Rules and constraints are expressed solely in terms of entity-relationship structure and declarative logic; the language lacks machine-oriented data or control structures, and has no facilities for specifying or implementing software. Application of SXL is demonstrated by its translation of a simple behavioral description (a scenario from an actual requirements document) into an executable model. Comparisons are made to software- and specification-oriented methods to illustrate the tradeoffs resulting from SXL's restriction to simple behavioral modeling. A brief account is given of one software development group's experience with SXL.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.87279","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=87279","","Laboratories;Intelligent systems;Logic;Application software;Programming;Engineering management;Software tools;Computer languages;Testing;Computer science","formal specification;logic programming;specification languages","state transition language;executable language;simple behavior;modeling language;system behavior;conventional state-transition framework;model behavior;logical invariants;entity-relationship structure;declarative logic;SXL;behavioral description;requirements document;executable model;specification-oriented methods;software development","","8","","29","","","","","","IEEE","IEEE Journals & Magazines"
"A specification and verification method for preventing denial of service","C. -. Yu; V. D. Gligor","Dept. of Electr. Eng., Maryland Univ., College Park, MD, USA; Dept. of Electr. Eng., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1990","16","6","581","592","A specification and verification method is presented for preventing denial of service in absence of failures and of integrity violations. The notion of user agreements is introduced, and it is argued that lack of specifications for these agreements and for simultaneity conditions makes it impossible to demonstrate denial-of-service prevention, in spite of demonstrably fair service access. The use of this method is illustrated with an example and it is explained why current methods for specification and verification of safety and liveness properties of concurrent programs do not handle this problem. The proposed specification and verification method is meant to augment current methods for secure system design.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.55087","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=55087","","Computer crime;Safety;Security;Logic;Laboratories;Delay;Fault tolerance;Access control","formal specification;security of data","specification method;failure absence;verification method;integrity violations;user agreements;simultaneity conditions;denial-of-service prevention;concurrent programs","","20","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing (Software) Reliability Growth Using a Random Coefficient Autoregressive Process and Its Ramifications","N. D. Singpurwalla; R. Soyer","Institute for Reliability and Risk Analysis, School of Engineering and Applied Science, George Washington University; NA","IEEE Transactions on Software Engineering","","1985","SE-11","12","1456","1464","In this paper we motivate a random coefficient autoregressive process of order 1 for describing reliability growth or decay. We introduce several ramifications of this process, some of which reduce it to a Kalman Filter model. We illustrate the usefulness of our approach by applying these processes to some real life data on software failures. Finally, we make a pairwise comparison of the models in terms of the ratio of likelihoods of their predictive distributions, and identify the ""best"" model.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231889","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701968","Dynamic linear and nonlinear models;Kalman Filtering;likelihood ratios;predictive distributions;prequential analysis;random coefficient autoregressive processes;reliability growth;software reliability","System testing;Autoregressive processes;Predictive models;Software reliability;Kalman filters;Nonlinear filters;Filtering;Software testing;Life testing;Risk analysis","","Dynamic linear and nonlinear models;Kalman Filtering;likelihood ratios;predictive distributions;prequential analysis;random coefficient autoregressive processes;reliability growth;software reliability","","59","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Service-Level Agreements for Electronic Services","J. Skene; F. Raimondi; W. Emmerich","The University of Auckland, Auckland; Middlesex University, London; University College London, London","IEEE Transactions on Software Engineering","","2010","36","2","288","304","The potential of communication networks and middleware to enable the composition of services across organizational boundaries remains incompletely realized. In this paper, we argue that this is in part due to outsourcing risks and describe the possible contribution of Service-Level Agreements (SLAs) to mitigating these risks. For SLAs to be effective, it should be difficult to disregard their original provisions in the event of a dispute between the parties. Properties of understandability, precision, and monitorability ensure that the original intent of an SLA can be recovered and compared to trustworthy accounts of service behavior to resolve disputes fairly and without ambiguity. We describe the design and evaluation of a domain-specific language for SLAs that tend to exhibit these properties and discuss the impact of monitorability requirements on service-provision practices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5210121","Service-level agreements;electronic services;contracts;domain-specific languages;model-driven engineering.","Outsourcing;Cloud computing;Consumer electronics;Middleware;Service oriented architecture;Domain specific languages;Distributed computing;Computer Society;Communication networks;Monitoring","client-server systems;high level languages;Internet;outsourcing","service level agreements;electronic services;communication networks;middleware;outsourcing risks;SLA;domain specific language","","18","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Why is software late? An empirical study of reasons for delay in software development","M. van Genuchten","Dept. of Manage. Inf. Syst. & Autom., Eindhoven Univ. of Technol., Netherlands","IEEE Transactions on Software Engineering","","1991","17","6","582","590","A study of the reasons for delay in software development is described. The aim of the study was to gain an insight into the reasons for differences between plans and reality in development activities in order to be able to take actions for improvement. A classification was used to determine the reasons. 160 activities, comprising over 15000 hours of work, have been analyzed. The results and interpretations of the results are presented. Insight into the predominant reasons for delay enabled actions for improvements to be taken in the department concerned. Because the distribution of reasons for delay varied widely from one department to another, it is recommended that every department should gain an insight into its reasons for delay in order to be able to take adequate actions for improvement.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.87283","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=87283","","Delay;Programming;Software engineering;Project management;Job shop scheduling;Software measurement;Software development management;Engineering management;Management information systems;Automation","project engineering;software engineering","software development;development activities;classification;delay enabled actions","","103","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Call-Stack Coverage for GUI Test Suite Reduction","S. McMaster; A. Memon","NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","99","115","Graphical user interfaces (GUIs) are used as front ends to most of today's software applications. The event-driven nature of GUIs presents new challenges for testing. One important challenge is test suite reduction. Conventional reduction techniques/tools based on static analysis are not easily applicable due to the increased use of multilanguage GUI implementations, callbacks for event handlers, virtual function calls, reflection, and multithreading. Moreover, many existing techniques ignore code in libraries and fail to consider the context in which event handlers execute. Consequently, they yield GUI test suites with seriously impaired fault-detection abilities. This paper presents a reduction technique based on the call-stack coverage criterion. Call stacks may be collected for any executing program with very little overhead. Empirical studies in this paper compare reduction based on call-stack coverage to reduction based on line, method, and event coverage, including variations that control for the size and optional consideration of library methods. These studies show that call-stack-based reduction provides unique trade-offs between the reduction in test suite size and the loss of fault detection effectiveness, which may be valuable in practice. Additionally, an analysis of the relationship between coverage requirements and fault-revealing test cases is presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70756","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4378345","Testing strategies;Test coverage of code;Test management;Testing tools;Testing strategies;Test coverage of code;Test management;Testing tools","Graphical user interfaces;Application software;Software testing;Reflection;Multithreading;Libraries;Fault detection;Computer Society;User interfaces;Size control","computer testing;graphical user interfaces","graphical user interfaces;test suite reduction;call-stack coverage criterion","","39","","32","","","","","","IEEE","IEEE Journals & Magazines"
"DEC: Service Demand Estimation with Confidence","A. Kalbasi; D. Krishnamurthy; J. Rolia; S. Dawson","University of Calgary, Calgary; University of Calgary, Calgary; Hewlett Packard Labs, Bristol; SAP Research Center Belfast, Belfast","IEEE Transactions on Software Engineering","","2012","38","3","561","578","We present a new technique for predicting the resource demand requirements of services implemented by multitier systems. Accurate demand estimates are essential to ensure the efficient provisioning of services in an increasingly service-oriented world. The demand estimation technique proposed in this paper has several advantages compared with regression-based demand estimation techniques, which many practitioners employ today. In contrast to regression, it does not suffer from the problem of multicollinearity, it provides more reliable aggregate resource demand and confidence interval predictions, and it offers a measurement-based validation test. The technique can be used to support system sizing and capacity planning exercises, costing and pricing exercises, and to predict the impact of changes to a service upon different service customers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728829","Benchmarking;resource demand prediction;statistical regression.","Benchmark testing;Equations;Software;Mathematical model;Estimation;Frequency modulation;Computers","multiprocessing systems;regression analysis;service-oriented architecture","DEC;service demand estimation technique;resource demand requirements;multitier systems;service-oriented world;regression-based demand estimation techniques;multicollinearity;system sizing;capacity planning","","20","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A language for specifying program transformations","D. Hildum; J. Cohen","Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA; NA","IEEE Transactions on Software Engineering","","1990","16","6","630","638","A language is described for specifying program transformations, from which programs can be generated to perform the transformations on sequences of code. The main objective of this work has been to develop a language that would allow the user to quickly and easily specify a wide range of transformations for a variety of programming languages. The rationale for the language constructs is given, as well as the details of an implementation which was prototyped using Prolog. Numerous examples of the language usage are provided.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.55091","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=55091","","Pattern matching;Computer languages;Specification languages;Formal specifications;Assembly;Data analysis;Performance analysis;Prototypes","specification languages","specification language;program transformations;language constructs;Prolog","","2","","17","","","","","","IEEE","IEEE Journals & Magazines"
"STAR: Stack Trace Based Automatic Crash Reproduction via Symbolic Execution","N. Chen; S. Kim","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","IEEE Transactions on Software Engineering","","2015","41","2","198","220","Software crash reproduction is the necessary first step for debugging. Unfortunately, crash reproduction is often labor intensive. To automate crash reproduction, many techniques have been proposed including record-replay and post-failure-process approaches. Record-replay approaches can reliably replay recorded crashes, but they incur substantial performance overhead to program executions. Alternatively, post-failure-process approaches analyse crashes only after they have occurred. Therefore they do not incur performance overhead. However, existing post-failure-process approaches still cannot reproduce many crashes in practice because of scalability issues and the object creation challenge. This paper proposes an automatic crash reproduction framework using collected crash stack traces. The proposed approach combines an efficient backward symbolic execution and a novel method sequence composition approach to generate unit test cases that can reproduce the original crashes without incurring additional runtime overhead. Our evaluation study shows that our approach successfully exploited 31 (59.6 percent) of 52 crashes in three open source projects. Among these exploitable crashes, 22 (42.3 percent) are useful reproductions of the original crashes that reveal the crash triggering bugs. A comparison study also demonstrates that our approach can effectively outperform existing crash reproduction approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2363469","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6926857","Crash reproduction;static analysis;symbolic execution;test case generation;optimization;Crash reproduction;static analysis;symbolic execution;test case generation;optimization","Computer crashes;Arrays;Indexes;Color;Optimization;Explosions;Software","program debugging;program testing;project management;public domain software;system recovery","STAR;stack trace based automatic crash reproduction;software crash reproduction;debugging;record-replay approach;post-failure-process approach;scalability issues;object creation challenge;crash stack traces;backward symbolic execution;method sequence composition approach;unit test case generation;open source projects","","12","","64","","","","","","IEEE","IEEE Journals & Magazines"
"The N-Version Approach to Fault-Tolerant Software","A. Avizienis","Department of Computer Science, University of California","IEEE Transactions on Software Engineering","","1985","SE-11","12","1491","1501","Evolution of the N-version software approach to the tolerance of design faults is reviewed. Principal requirements for the implementation of N-version software are summarized and the DEDIX distributed supervisor and testbed for the execution of N-version software is described. Goals of current research are presented and some potential benefits of the N-version approach are identified.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231893","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701972","Design diversity;fault tolerance;multiple computation;N-version programming;N-version software;software reliability;tolerance of design faults","Fault tolerance;Circuit faults;Fault tolerant systems;Hardware;Application software;Humans;Computer science;Computer errors;Software testing;Software reliability","","Design diversity;fault tolerance;multiple computation;N-version programming;N-version software;software reliability;tolerance of design faults","","501","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Information-theoretic software clustering","P. Andritsos; V. Tzerpos","Dept. of Comput. Sci., Toronto Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","2005","31","2","150","165","The majority of the algorithms in the software clustering literature utilize structural information to decompose large software systems. Approaches using other attributes, such as file names or ownership information, have also demonstrated merit. At the same time, existing algorithms commonly deem all attributes of the software artifacts being clustered as equally important, a rather simplistic assumption. Moreover, no method that can assess the usefulness of a particular attribute for clustering purposes has been presented in the literature. In this paper, we present an approach that applies information theoretic techniques in the context of software clustering. Our approach allows for weighting schemes that reflect the importance of various attributes to be applied. We introduce LIMBO, a scalable hierarchical clustering algorithm based on the minimization of information loss when clustering a software system. We also present a method that can assess the usefulness of any nonstructural attribute in a software clustering context. We applied LIMBO to three large software systems in a number of experiments. The results indicate that this approach produces clusterings that come close to decompositions prepared by system experts. Experimental results were also used to validate our usefulness assessment method. Finally, we experimented with well-established weighting schemes from information retrieval, Web search, and data clustering. We report results as to which weighting schemes show merit in the decomposition of software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401930","Index Terms- Reverse engineering;reengineering;architecture reconstruction;clustering;information theory.","Software systems;Clustering algorithms;Software algorithms;Computer Society;Computer architecture;Reverse engineering;Software engineering;Minimization methods;Information retrieval;Web search","reverse engineering;software metrics;software architecture;software maintenance;systems re-engineering;pattern clustering;information retrieval","software clustering;software system;information retrieval;Web search;data clustering","","115","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Simulation-verification: biting at the state explosion problem","D. A. Stuart; M. Brockmeyer; A. K. Mok; F. Jahanian","Boeing Co., St. Louis, MO, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","7","599","617","Simulation and verification are two conventional techniques for the analysis of specifications of real-time systems. While simulation is relatively inexpensive in terms of execution time, it only validates the behavior of a system for one particular computation path. On the other hand, verification provides guarantees over the entire set of computation paths of a system, but is, in general, very expensive due to the state-space explosion problem. We introduce a new technique: simulation-verification combines the best of both worlds by synthesizing an intermediate analysis method. This method uses simulation to limit the generation of a computation graph to that set of computations consistent with the simulation. This limited computation graph, called a simulation-verification graph, can be one or more orders of magnitude smaller than the full computation graph. A tool, XSVT, is described which implements simulation-verification graphs. Three paradigms for using the new technique are proposed. The paper illustrates the application of the proposed technique via an example of a robot controller for a manufacturing assembly line.","0098-5589;1939-3520;2326-3881","","10.1109/32.935853","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=935853","","Explosions;Computational modeling;Analytical models;Context modeling;Costs;Real time systems;Robotic assembly;Specification languages;Robot control;Pulp manufacturing","real-time systems;virtual machines;formal verification;formal specification","state explosion problem;specifications;real-time systems;execution time;computation paths;simulation-verification;intermediate analysis method;computation graph;simulation-verification graph;XSVT tool;robot controller;manufacturing assembly line","","17","","38","","","","","","IEEE","IEEE Journals & Magazines"
"State constraints and pathwise decomposition of programs","J. C. Huang","Dept. of Comput. Sci., Houston Univ., TX, USA","IEEE Transactions on Software Engineering","","1990","16","8","880","896","A state constraint is a programming construct designed to restrict a program's domain of definition. It can be used to decompose a program pathwise, i.e. dividing the program into subprograms along the control flow, as opposed to dividing the program across the control flow when the program is decomposed into functions and procedures. As a result, a program consisting of one or more execution paths of another program can be constructed and manipulated. The author describes the idea involved, examines the properties of state constraints, establishes a formal basis for pathwise decomposition and discusses their uses in program simplification, testing and verification.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.57625","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=57625","","Testing;Computer science;Aggregates;Capacitive sensors","computational complexity;program testing;software engineering","state constraint;programming construct;program pathwise;subprograms;control flow;execution paths;formal basis;pathwise decomposition;program simplification;testing;verification","","7","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient detection and resolution of generalized distributed deadlocks","A. D. Kshemkalyani; M. Singhal","IBM Corp., Research Triangle Park, NC, USA; NA","IEEE Transactions on Software Engineering","","1994","20","1","43","54","We present an efficient one-phase algorithm that consists of two concurrent sweeps of messages to detect generalized distributed deadlocks. In the outward sweep, the algorithm records a snapshot of a distributed wait-for-graph (WFG). In the inward sweep, the algorithm performs reduction of the recorded distributed WFG to check for a deadlock. The two sweeps can overlap in time at a process. We prove the correctness of the algorithm. The algorithm has a worst-case message complexity of 4e/spl minus/2n+2l and a time complexity of 2d hops, where e is the number of edges, n is the number of nodes, l is the number of leaf nodes, and d is the diameter of the WFG. This is a notable improvement over the existing algorithms to detect generalized deadlocks.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.263754","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=263754","","System recovery;Topology;Information science;Phase detection;Associate members;Software algorithms;Sufficient conditions;Detection algorithms;Algorithm design and analysis","concurrency control;computational complexity;directed graphs;operating systems (computers)","generalized distributed deadlock resolution;one-phase algorithm;concurrent sweeps;messages;generalized distributed deadlock detection;outward sweep;distributed wait-for-graph;inward sweep;distributed snapshot;algorithm correctness;worst-case message complexity;time complexity;leaf nodes;graph reduction;distributed system;directed graph","","46","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Testing the completeness of specifications","P. Jalote","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1989","15","5","526","531","A system is described that tests for the completeness of axiomatic specifications of abstract data types. For testing, the system generates a set of test cases and an implementation of the data type from the specifications. The generated implementation is such that if the specifications are not complete, the implementation is not complete, and the behavior of all of the sequences of valid operations on the data type is not defined. This implementation is tested with the generated test cases to detect the incompleteness of specifications. The system is implemented on a VAX system running Unix.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24701","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24701","","System testing;Formal specifications;Computer science;Software testing;Costs","conformance testing;data structures;program testing","completeness testing;axiomatic specifications;abstract data types;test cases;VAX system;Unix","","11","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Exploiting the Essential Assumptions of Analogy-Based Effort Estimation","E. Kocaguneli; T. Menzies; A. Bener; J. W. Keung","West Virginia University, Morgantown; West Virginia University, Morgantown; Ryerson University, Toronto; The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Software Engineering","","2012","38","2","425","438","Background: There are too many design options for software effort estimators. How can we best explore them all? Aim: We seek aspects on general principles of effort estimation that can guide the design of effort estimators. Method: We identified the essential assumption of analogy-based effort estimation, i.e., the immediate neighbors of a project offer stable conclusions about that project. We test that assumption by generating a binary tree of clusters of effort data and comparing the variance of supertrees versus smaller subtrees. Results: For 10 data sets (from Coc81, Nasa93, Desharnais, Albrecht, ISBSG, and data from Turkish companies), we found: 1) The estimation variance of cluster subtrees is usually larger than that of cluster supertrees; 2) if analogy is restricted to the cluster trees with lower variance, then effort estimates have a significantly lower error (measured using MRE, AR, and Pred(25) with a Wilcoxon test, 95 percent confidence, compared to nearest neighbor methods that use neighborhoods of a fixed size). Conclusion: Estimation by analogy can be significantly improved by a dynamic selection of nearest neighbors, using only the project data from regions with small variance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728833","Software cost estimation;analogy;k-NN.","Estimation;Training;Software;Training data;Linear regression;Euclidean distance;Humans","pattern clustering;program testing;project management;software cost estimation;trees (mathematics)","analogy-based effort estimation;software effort estimator design;essential assumption;supertree variance;subtree variance;Coc81 data set;Nasa93 data set;Desharnais data set;Albrecht data set;ISBSG data set;Turkish companies;estimation variance;binary cluster tree;cluster subtrees;dynamic selection;nearest neighbor selection;project data","","81","","69","","","","","","IEEE","IEEE Journals & Magazines"
"Event Logs for the Analysis of Software Failures: A Rule-Based Approach","M. Cinque; D. Cotroneo; A. Pecchia","Universitá degli Studi di Napoli Federico II, Naples; Universitá degli Studi di Napoli Federico II, Naples; Universitá degli Studi di Napoli Federico II, Naples","IEEE Transactions on Software Engineering","","2013","39","6","806","821","Event logs have been widely used over the last three decades to analyze the failure behavior of a variety of systems. Nevertheless, the implementation of the logging mechanism lacks a systematic approach and collected logs are often inaccurate at reporting software failures: This is a threat to the validity of log-based failure analysis. This paper analyzes the limitations of current logging mechanisms and proposes a rule-based approach to make logs effective to analyze software failures. The approach leverages artifacts produced at system design time and puts forth a set of rules to formalize the placement of the logging instructions within the source code. The validity of the approach, with respect to traditional logging mechanisms, is shown by means of around 12,500 software fault injection experiments into real-world systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6320555","Event log;logging mechanism;rule-based logging;error detection;software failures","Unified modeling language;Failure analysis;Analytical models;Systematics;Proposals;Software systems","software fault tolerance","event logs;software failures;rule-based approach;logging mechanism;log-based failure analysis;system design time","","17","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Synchronizability of conversations among Web services","X. Fu; T. Bultan; J. Su","Sch. of Comput. & Inf. Sci., Georgia Southwestern State Univ., Americus, GA, USA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","12","1042","1055","We present a framework for analyzing interactions among Web services that communicate with asynchronous messages. We model the interactions among the peers participating in a composite Web service as conversations, the global sequences of messages exchanged among the peers. This naturally leads to the following model checking problem: Given an LTL property and a composite Web service, do the conversations generated by the composite Web service satisfy the property? We show that asynchronous messaging leads to state space explosion for bounded message queues and undecidability of the model checking problem for unbounded message queues. We propose a technique called synchronizability analysis to tackle this problem. If a composite Web service is synchronizable, its conversation set remains the same when asynchronous communication is replaced with synchronous communication. We give a set of sufficient conditions that guarantee synchronizability and that can be checked statically. Based on our synchronizability results, we show that a large class of composite Web services with unbounded message queues can be verified completely using a finite state model checker such as SPIN. We also show that synchronizability analysis can be used to check the reliability of top-down conversation specifications and we contrast the conversation model with the Message Sequence Charts. We integrated synchronizability analysis to a tool we developed for analyzing composite Web services.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.141","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566606","Index Terms- Web services;asynchronous communication;conversations;model checking;verification;synchronizability;realizability.","Web services;XML;Asynchronous communication;Application software;Simple object access protocol;Communication standards;Data communication;Buffer storage;State-space methods;Explosions","message passing;formal specification;formal verification;synchronisation;finite state machines;Internet","Web service;messages exchange;model checking problem;synchronizability analysis;asynchronous communication;synchronous communication;SPIN finite state model checker;top-down conversation specification;message sequence charts","","59","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Protocol Validation in Argos: Assertion Proving and Scatter Searching","G. J. Holzmann","AT&amp; T Bell Laboratories","IEEE Transactions on Software Engineering","","1987","SE-13","6","683","696","Argos is a validation language for data communication protocols. To validate a protocol, a model in Argos is constructed consisting of a control flow specification and a formal description of the correctness requirements. This model can be compiled into a minimized lower level description that is based on a formal model of communicating finite state machines. An automated protocol validator trace uses these minimized descriptions to perform a partial symbolic execution of the protocol to establish its correctness for the given requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233206","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702274","Assertion proving;guarded command languages;protocol design;protocol validation;scatter searching;symbolic execution","Scattering;Performance analysis;Logic;Access protocols;Data communication;Automatic control;Communication system control;Automata;Command languages;Humans","","Assertion proving;guarded command languages;protocol design;protocol validation;scatter searching;symbolic execution","","19","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Reversible debugging using program instrumentation","Shyh-Kwei Chen; W. K. Fuchs; Jen-Yao Chung","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","8","715","727","Reversible execution has not been fully exploited in symbolic debuggers. Debuggers that can undo instructions usually incur a significant performance penalty during a debugging session. We describe an efficient reversible debugging mechanism based on program instrumentation. The approach enables repetitive debugging sessions with selectable reversible routines and recording modes. Experimental results indicate that the execution penalty can be significantly reduced with moderate code growth.","0098-5589;1939-3520;2326-3881","","10.1109/32.940726","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=940726","","Debugging;Instruments;History;Runtime;Computer languages;Checkpointing;Emulation;Databases;Program processors;Assembly","program debugging;reverse engineering;assembly language","reversible debugging;program instrumentation;reversible execution;symbolic debuggers;performance penalty;debugging session;repetitive debugging sessions;selectable reversible routines;recording modes;execution penalty;moderate code growth","","9","","35","","","","","","IEEE","IEEE Journals & Magazines"
"A Theoretical and Empirical Study of Diversity-Aware Mutation Adequacy Criterion","D. Shin; S. Yoo; D. Bae","KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea","IEEE Transactions on Software Engineering","","2018","44","10","914","931","Diversity has been widely studied in software testing as a guidance towards effective sampling of test inputs in the vast space of possible program behaviors. However, diversity has received relatively little attention in mutation testing. The traditional mutation adequacy criterion is a one-dimensional measure of the total number of killed mutants. We propose a novel, diversity-aware mutation adequacy criterion called distinguishing mutation adequacy criterion, which is fully satisfied when each of the considered mutants can be identified by the set of tests that kill it, thereby encouraging inclusion of more diverse range of tests. This paper presents the formal definition of the distinguishing mutation adequacy and its score. Subsequently, an empirical study investigates the relationship among distinguishing mutation score, fault detection capability, and test suite size. The results show that the distinguishing mutation adequacy criterion detects 1.33 times more unseen faults than the traditional mutation adequacy criterion, at the cost of a 1.56 times increase in test suite size, for adequate test suites that fully satisfies the criteria. The results show a better picture for inadequate test suites; on average, 8.63 times more unseen faults are detected at the cost of a 3.14 times increase in test suite size.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2732347","Information & communications Technology Promotion (IITP); Korea government (MSIP); Software R&D for Model-based Analysis and Verification of Higher-order Large Complex System; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7994647","Mutation testing;test adequacy criteria;diversity","Fault detection;Software engineering;Software testing;Correlation;Indexes;Subspace constraints","fault diagnosis;program testing;software engineering","empirical study;diversity-aware mutation adequacy criterion;software testing;test inputs;mutation testing;distinguishing mutation score;distinguishing mutation adequacy criterion;adequate test suites;inadequate test suites;theoretical study;program behaviors;killed mutants;fault detection capability","","2","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Selecting Software Test Data Using Data Flow Information","S. Rapps; E. J. Weyuker","Courant Institute of Mathematical Sciences, Department of Computer Science, New York University; NA","IEEE Transactions on Software Engineering","","1985","SE-11","4","367","375","This paper defines a family of program test data selection criteria derived from data flow analysis techniques similar to those used in compiler optimization. It is argued that currently used path selection criteria, which examine only the control flow of a program, are inadequate quate. Our procedure associates with each point in a program at which a variable is defined, those points at which the value is used. Several test data selection criteria, differing in the type and number of these associations, are defined and compared.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232226","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702019","Data flow;program testing;test data selection","Software testing;Error correction;Data analysis;Information analysis;Program processors;Optimizing compilers;Computer science;System testing;Intelligent systems","","Data flow;program testing;test data selection","","482","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Timing Constraints of Real-Time Systems: Constructs for Expressing Them, Methods of Validating Them","B. Dasarathy","GTE Laboratories Inc.","IEEE Transactions on Software Engineering","","1985","SE-11","1","80","86","This paper examines timing constraints as features of realtime systems. It investigates the various constructs required in requirements languages to express timing constraints and considers how automatic test systems can validate systems that include timing constraints. Specifically, features needed in test languages to validate timing constraints are discussed. One of the distinguishing aspects of three tools developed at GTE Laboratories for real-time systems specification and testing is in their extensive ability to handle timing constraints. Thus, the paper highlights the timing constraint features of these tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231845","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701900","Real-time systems;requirements specification;test generation;test language;timing constraints;validation","Timing;Real time systems;System testing;Automatic testing;Laboratories;Delay;Project management","","Real-time systems;requirements specification;test generation;test language;timing constraints;validation","","164","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Two controlled experiments assessing the usefulness of design pattern documentation in program maintenance","L. Prechelt; B. Unger-Lamprecht; M. Philippsen; W. F. Tichy","abaXX Technol., Stuttgart, Germany; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","6","595","606","Using design patterns is claimed to improve programmer productivity and software quality. Such improvements may manifest both at construction time (in faster and better program design) and at maintenance time (in faster and more accurate program comprehension). The paper focuses on the maintenance context and reports on experimental tests of the following question: does it help the maintainer if the design patterns in the program code are documented explicitly (using source code comments) compared to a well-commented program without explicit reference to design patterns? Subjects performed maintenance tasks on two programs ranging from 360 to 560 LOC including comments. The experiments tested whether pattern comment lines (PCL) help during maintenance if patterns are relevant and sufficient program comments are already present. This question is a challenge for the experimental methodology: A setup leading to relevant results is quite difficult to find. We discuss these issues in detail and suggest a general approach to such situations. A conservative analysis of the results supports the hypothesis that pattern-relevant maintenance tasks were completed faster or with fewer errors if redundant design pattern information was provided. The article provides the first controlled experiment results on design pattern usage and it presents a solution approach to an important class of experiment design problems for experiments regarding documentation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010061","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010061","","Documentation;Computer Society;Programming profession;Testing;Productivity;Software design;Software tools;Software quality;Lab-on-a-chip;Java","software maintenance;system documentation;object-oriented programming;human factors;user interfaces","controlled experiments;design pattern documentation;program maintenance;programmer productivity;software quality;program comprehension;experimental tests;source code comments;pattern comment lines;PCL;program comments;Java;German graduate students;C++;American undergraduate students;pattern-relevant maintenance tasks;design pattern information;design pattern usage;experiment design problems","","69","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Leveraging Historical Associations between Requirements and Source Code to Identify Impacted Classes","D. Falessi; J. Roll; J. L. C. Guo; J. Cleland-Huang","Computer Science, California Polytechnic State University, San Luis Obispo, California United States (e-mail: d.falessi@gmail.com); Computer Science and Software Engineering, California Polytechnic State University College of Engineering, 306564 San Luis Obispo, California United States (e-mail: jroll@calpoly.edu); Computer Science, McGill University, Montreal, Quebec Canada (e-mail: jguo@cs.mcgill.ca); Computer science, University of Notre Dame, South Bend, Illinois United States (e-mail: JaneClelandHuang@nd.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","As new requirements are introduced and implemented in a software system, developers must identify the set of source code classes which need to be changed. Therefore, past effort has focused on predicting the set of classes impacted by a requirement. In this paper, we introduce and evaluate a new type of information based on the intuition that the set of requirements which are associated with historical changes to a specific class are likely to exhibit semantic similarity to new requirements which impact that class. This new Requirements to Requirements Set (R2RS) family of metrics captures the semantic similarity between a new requirement and the set of existing requirements previously associated with a class. The aim of this paper is to present and evaluate the usefulness of R2RS metrics in predicting the set of classes impacted by a requirement. We consider 18 different R2RS metrics by combining six natural language processing techniques to measure the semantic similarity among texts (e.g., VSM) and three distribution scores to compute overall similarity (e.g., average among similarity scores). We evaluate if R2RS is useful for predicting impacted classes in combination and against four other families of metrics that are based upon temporal locality of changes, direct similarity to code, complexity metrics, and code smells. Our evaluation features five classifiers and 78 releases belonging to four large open-source projects, which result in over 700,000 candidate impacted classes. Experimental results show that leveraging R2RS information increases the accuracy of predicting impacted classes practically by an average of more than 60% across the various classifiers and projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2861735","California Polytechnic State University San Luis Obispo; Division of Computing and Communication Foundations; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8423658","Impact analysis;mining software repositories;traceability","Measurement;Semantics;Natural language processing;Complexity theory;Open source software;Task analysis","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A large-scale empirical study of just-in-time quality assurance","Y. Kamei; E. Shihab; B. Adams; A. E. Hassan; A. Mockus; A. Sinha; N. Ubayashi","Kyushu University, Fukuoka; Rochester Institute of Technology, Rochester; École Polytechnique de Montréal, Montréal; Queen's University, Kingston; Avaya Labs Research, Basking Ridge; Research In Motion, Waterloo; Kyushu University, Fukuoka","IEEE Transactions on Software Engineering","","2013","39","6","757","773","Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone (“risky”) software changes instead of files or packages. We refer to this type of quality assurance activity as “Just-In-Time Quality Assurance,” because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the characteristics of a software change, such as the number of added lines, and developer experience. A large-scale study of six open source and five commercial projects from multiple domains shows that our models can predict whether or not a change will lead to a defect with an average accuracy of 68 percent and an average recall of 64 percent. Furthermore, when considering the effort needed to review changes, we find that using only 20 percent of the effort it would take to inspect all changes, we can identify 35 percent of all defect-inducing changes. Our findings indicate that “Just-In-Time Quality Assurance” may provide an effort-reducing way to focus on the most risky changes and thus reduce the costs of developing high-quality software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6341763","Maintenance;software metrics;mining software repositories;defect prediction;just-in-time prediction","Measurement;Quality assurance;Predictive models;Software;Entropy;Object oriented modeling;Accuracy","program testing;software maintenance;software metrics;software quality","just-in-time quality assurance;defect prediction models;defect-prone file identification;defect-prone package identification;software systems;risk model;open source projects;commercial projects;risky changes;cost reduction;defect-prone software change identification;software metrics;software repository mining;software quality assurance activities;source code inspection;unit testing","","80","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Discrete Time Stochastic Petri Nets","M. K. Molloy","Department of Computer Science, University of Texas at Austin","IEEE Transactions on Software Engineering","","1985","SE-11","4","417","423","Basic graph models of processes, such as Petri nets, have usually omitted the concept of time as a parameter. Time has been added to the Petri net model in two ways. The timed Petri net (TPN) uses a fixed number of discrete time intervals. The stochastic Petri net (SPN) uses an exponentially distributed random variable. In this paper, a discrete time stochastic Petri model is described. These discrete time SPN's fill the gap between TPN and normal SPN. However, the use of discrete time complicates the SPN model in that more than one transition may fire at a time step. Finally, an example of a live and bounded Petri net which has nonempty, disjoint, recurrent subsets of markings is given.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232230","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702023","Markovian processes;performance;Petri nets;stochastic models","Stochastic processes;Petri nets;Delay;Performance analysis;Power system modeling;Random variables;Fires;Protocols;System recovery;Timing","","Markovian processes;performance;Petri nets;stochastic models","","89","","23","","","","","","IEEE","IEEE Journals & Magazines"
"The exception handling effectiveness of POSIX operating systems","P. Koopman; J. DeVale","Dept. of Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","9","837","848","Operating systems form a foundation for robust application software, making it important to understand how effective they are at handling exceptional conditions. The Ballista testing system was used to characterize the handling of exceptional input parameter values for up to 233 POSIX functions and system calls on each of 15 widely used operating system (OS) implementations. This identified ways to crash systems with a single call, ways to cause task hangs within OS code, ways to cause abnormal task termination within OS and library code, failures to implement defined POSIX functionality, and failures to report unsuccessful operations. Overall, only 55 percent to 76 percent of the exceptional tests performed generated error codes, depending on the operating system being tested. Approximately 6 percent to 19 percent of tests failed to generate any indication of error despite exceptional inputs. Approximately 1 percent to 3 percent of tests revealed failures to implement defined POSIX functionality for unusual, but specified, situations. Between 18 percent and 33 percent of exceptional tests caused the abnormal termination of an OS system call or library function, and five systems were completely crashed by individual system calls with exceptional parameter values. The most prevalent sources of these robustness failures were illegal pointer values, numeric overflows, and end-of-file overruns.","0098-5589;1939-3520;2326-3881","","10.1109/32.877845","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877845","","Operating systems;Robustness;Application software;System testing;Vehicle crash testing;Computer crashes;Libraries;Telecommunication computing;Programming profession;Performance evaluation","exception handling;operating systems (computers);program testing","exception handling effectiveness;POSIX operating systems;application software;Ballista testing system;system calls;system crash;task hangs;abnormal task termination;library code;error codes;illegal pointer values;numeric overflows;end-of-file overruns;robustness failures","","56","","35","","","","","","IEEE","IEEE Journals & Magazines"
"A comparison of some structural testing strategies","S. C. Ntafos","Comput. Sci. Program, Texas Univ., Richardson, TX, USA","IEEE Transactions on Software Engineering","","1988","14","6","868","874","Several structural testing strategies are compared in terms of their relative coverage of the program's structure and also in terms of the number of test cases needed to satisfy each strategy. Some of the deficiencies of such comparisons are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6165","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6165","","Costs;Data analysis;Error correction;Software testing;Software tools;Computer science;Linear code","program testing;programming theory;structured programming","program testing;structural testing","","136","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying failure-causing schemas in the presence of multiple faults","X. Niu; N. Changhai; Y. Lei; H. K. N. Leung; X. Wang","computer science and technology, Nanjing University, Nanjing, Jiangsu China 210023 (e-mail: niuxintao@gmail.com); computer science and technology, Nanjing University, nanjing, Jiangsu China 210096 (e-mail: changhainie@nju.edu.cn); Computer Science and Engineering, The University of Texas at Arlington, Arlington, Texas United States 76019 (e-mail: ylei@cse.uta.edu); Department of Computing, The Hong Kong Polytechnic University, Hong Kong, Hong Kong Hong Kong (e-mail: hareton.leung@polyu.edu.hk); Computer Science, University of Texas at San Antonio, San Antonio, Texas United States 78249 (e-mail: xiaoyin.wang@utsa.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Combinatorial testing (CT) has been proven effective in revealing the failures caused by the interaction of factors that affect the behavior of a system. The theory of Minimal Failure-Causing Schema (MFS) has been proposed to isolate the cause of a failure after CT. Most algorithms that aim to identify MFS focus on handling a single fault in the System Under Test (SUT). However, we argue that multiple faults are more common in practice, under which masking effects may be triggered so that some failures cannot be observed. The traditional MFS theory lacks a mechanism to handle such effects; hence, they may incorrectly isolate the MFS. To address this problem, we propose a new MFS model that takes into account multiple faults. We first formally analyze the impact of the multiple faults on existing MFS identifying algorithms, especially in situations where masking effects are triggered by multiple faults. We then develop an approach that can assist traditional algorithms to better handle multiple faults. Empirical studies were conducted using several kinds of open-source software, which showed that multiple faults with masking effects do negatively affect traditional MFS identifying approaches and that our approach can help to alleviate these effects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2844259","National Key Research and Development Plan; Division of Computer and Network Systems; US Department of Homeland Security; US National Institute of Standards and Technologies; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8372636","Software Testing;Combinatorial Testing;Failure-causing schemas;Masking effects","Testing;Bars;Fault diagnosis;Computer bugs;Software algorithms;Open source software","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"An Intrusion-Detection Model","D. E. Denning","SRI International","IEEE Transactions on Software Engineering","","1987","SE-13","2","222","232","A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system's audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232894","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702202","Abnormal behavior;auditing;intrusions;monitoring;profiles;security;statistical measures","Security;Real time systems;Expert systems;Environmental economics;Invasive software;Computerized monitoring;Object detection;Contracts;Joining processes;Operating systems","","Abnormal behavior;auditing;intrusions;monitoring;profiles;security;statistical measures","","809","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Measuring Program Comprehension: A Large-Scale Field Study with Professionals","X. Xia; L. Bao; D. Lo; Z. Xing; A. E. Hassan; S. Li","Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Singapore Management University, Singapore; Australian National University, Canberra, ACT, Australia; Queen’s University, Kingston, ON, Canada; Zhejiang University, Hangzhou, China","IEEE Transactions on Software Engineering","","2018","44","10","951","976","During software development and maintenance, developers spend a considerable amount of time on program comprehension activities. Previous studies show that program comprehension takes up as much as half of a developer's time. However, most of these studies are performed in a controlled setting, or with a small number of participants, and investigate the program comprehension activities only within the IDEs. However, developers' program comprehension activities go well beyond their IDE interactions. In this paper, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. We then measure the comprehension time by calculating the time that developers spend on program comprehension, e.g., inspecting console and breakpoints in IDE, or reading and understanding tutorials in web browsers. Using this approach, we can perform a more realistic investigation of program comprehension activities, through a field study of program comprehension in practice across a total of seven real projects, on 78 professional developers, and amounting to 3,148 working hours. Our study leverages interaction data that is collected across many applications by the developers. Our study finds that on average developers spend ~58 percent of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension, and we find senior developers spend significantly less percentages of time on program comprehension than junior developers. Our study also highlights the importance of several research directions needed to reduce program comprehension time, e.g., building automatic detection and improvement of low quality code and documentation, construction of software-engineering-specific search engines, designing better IDEs that help developers navigate code and browse information more efficiently, etc.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2734091","NSFC; National Key Technology R&D Program; Ministry of Science and Technology of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7997917","Program comprehension;field study;inference model","Navigation;Software;Time measurement;Browsers;Maintenance engineering;Programming;Debugging","human computer interaction;Internet;program compilers;reverse engineering;search engines;software maintenance","program comprehension activities;program comprehension time;developers time;software development;software maintenance;IDE interactions;ActivitySpace framework;human computer interaction;Web browsers;programming language;project phase;software-engineering;search engines","","1","","63","","","","","","IEEE","IEEE Journals & Magazines"
"The AdaPIC tool set: supporting interface control and analysis throughout the software development process","A. L. Wolf; L. A. Clarke; J. C. Wileden","Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA; Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA; Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA","IEEE Transactions on Software Engineering","","1989","15","3","250","263","The AdaPIC tool set, an important component of an Ada software development environment, is discussed. The AdaPIC tool set is one particular instantiation, specifically adapted for use with Ada, of the more general collection of language features and analysis capabilities that constitute the PIC approach to describing and analyzing relationships among software system components. This tool set is being tailored to support an incremental approach to the interface control aspects of the software development process. Following a discussion of the PIC interface control and incremental development concepts, the AdaPIC tool set is described, concentrating on its analysis tools and support for incremental development and demonstrating how it contributes to the technology for developing large Ada software systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21753","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21753","","Software systems;Control systems;Software tools;Control system analysis;Programming profession;Software maintenance;Laboratories;Information science;Marine vehicles;Software performance","Ada;programming environments;software tools","AdaPIC tool set;interface control;software development;software development environment;language features;software system components;analysis tools","","12","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Making changes to formal specifications: requirements and an example","D. W. Bustard; A. C. Winstanley","Dept. of Comput. Sci., Ulster Univ., Coleraine, UK; NA","IEEE Transactions on Software Engineering","","1994","20","8","562","568","Formal methods have had little impact on software engineering practice, despite the fact that most software engineering practitioners readily acknowledge the potential benefits to be gained from the mathematical modeling involved. One reason is that existing modeling techniques tend not to address basic software engineering concerns. In particular, while considerable attention has been paid to the construction of formal models, less attractive maintenance issues have largely been ignored. The purpose of this paper is to clarify those issues and examine the underlying requirements for change support. The discussion is illustrated with a description of a change technique and tool developed for the formal notation LOTOS. This work was undertaken as part of the SCAFFOLD project, which was concerned with providing broad support for the construction and analysis of formal specifications of concurrent systems. Most of the discussion is applicable to other process-oriented notations such as CCS and CSP.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310666","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310666","","Formal specifications;Software engineering;Mathematical model;Carbon capture and storage;Process control;Algebra;Programming;Software design;Instruction sets;Software tools","formal specification;specification languages;software maintenance;configuration management","formal specifications;software engineering;change technique;tool;formal notation LOTOS;SCAFFOLD project;process-oriented notations;concurrent systems;change control;formal specification;process algebra;LOTOS","","5","","26","","","","","","IEEE","IEEE Journals & Magazines"
"On the automatic modularization of software systems using the Bunch tool","B. S. Mitchell; S. Mancoridis","Dept. of Comput. Sci., Drexel Univ., Philadelphia, PA, USA; Dept. of Comput. Sci., Drexel Univ., Philadelphia, PA, USA","IEEE Transactions on Software Engineering","","2006","32","3","193","208","Since modern software systems are large and complex, appropriate abstractions of their structure are needed to make them more understandable and, thus, easier to maintain. Software clustering techniques are useful to support the creation of these abstractions by producing architectural-level views of a system's structure directly from its source code. This paper examines the Bunch clustering system which, unlike other software clustering tools, uses search techniques to perform clustering. Bunch produces a subsystem decomposition by partitioning a graph of the entities (e.g., classes) and relations (e.g., function calls) in the source code. Bunch uses a fitness function to evaluate the quality of graph partitions and uses search algorithms to find a satisfactory solution. This paper presents a case study to demonstrate how Bunch can be used to create views of the structure of significant software systems. This paper also outlines research to evaluate the software clustering results produced by Bunch.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610610","Clustering;reverse engineering;reengineering;program comprehension;optimization;maintainability.","Software systems;Software tools;Software maintenance;Clustering algorithms;Reverse engineering;Software quality;Search problems;Software performance;Partitioning algorithms;Chaos","software architecture;software maintenance;reverse engineering;systems re-engineering;software tools;search problems","automatic modularization;Bunch software clustering tool;software system abstraction;software system architectural-level view;graph partition;search algorithm;reverse engineering;software system reengineering;program comprehension;software maintainability","","207","","56","","","","","","IEEE","IEEE Journals & Magazines"
"How Software Designers Interact with Sketches at the Whiteboard","N. Mangano; T. D. LaToza; M. Petre; A. van der Hoek","Molimur, Mission Viejo, CA; Department of Informatics, Donald Bren School Information and Computer Sciences, University of California, Irvine, CA; Faculty of Mathematics and Computing, The Open University, Milton Keynes, United Kingdom; Department of Informatics, Donald Bren School Information and Computer Sciences, University of California, Irvine, CA","IEEE Transactions on Software Engineering","","2015","41","2","135","156","Whiteboard sketches play a crucial role in software development, helping to support groups of designers in reasoning about a software design problem at hand. However, little is known about these sketches and how they support design `in the moment', particularly in terms of the relationships among sketches, visual syntactic elements within sketches, and reasoning activities. To address this gap, we analyzed 14 hours of design activity by eight pairs of professional software designers, manually coding over 4000 events capturing the introduction of visual syntactic elements into sketches, focus transitions between sketches, and reasoning activities. Our findings indicate that sketches serve as a rich medium for supporting design conversations. Designers often use general-purpose notations. Designers introduce new syntactic elements to record aspects of the design, or re-purpose sketches as the design develops. Designers constantly shift focus between sketches, using groups of sketches together that contain complementary information. Finally, sketches play an important role in supporting several types of reasoning activities (mental simulation, review of progress, consideration of alternatives). But these activities often leave no trace and rarely lead to sketch creation. We discuss the implications of these and other findings for the practice of software design at the whiteboard and for the creation of new electronic software design sketching tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2362924","National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6922572","Interaction styles;systems analysis and design;user-centered design;Interaction styles;systems analysis and design;user-centered design","Encoding;Cognition;Software design;Visualization;Syntactics;Videos","software engineering","software design;whiteboard sketch;software development;visual syntactic elements;reasoning activity","","6","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Two-dimensional specification of universal quantification in a graphical database query language","K. -. Whang; A. Malhotra; G. H. Sockut; L. Burns; K. -. Choi","Dept. of Comput. Sci., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1992","18","3","216","224","A technique is proposed for specifying universal quantification and existential quantification (combined with negation) in a two-dimensional (graphical) database query language. Unlike other approaches that provide set operators to simulate universal quantification, this technique allows a direct representation of universal quantification. Syntactic constructs for specifying universal and existential quantifications, two-dimensional translation of universal quantification to existential quantification (with negation), and translation of existentially quantified two-dimensional queries to relational queries are presented. The resulting relational queries can be processed directly by many existing database systems. The authors claim that this technique renders universal quantifications easy to understand. To substantiate this claim, they provide a simple, easy-to-follow guideline for constructing universally quantified queries.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.126770","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=126770","","Database languages;Relational databases;Calculus;Database systems;Programming profession;Senior members;Guidelines;Computer science;Data models","computer graphics;database theory;formal specification;query languages;relational databases","graphical database query language;universal quantification;existential quantification;direct representation;two-dimensional translation;existentially quantified two-dimensional queries;relational queries","","14","","17","","","","","","IEEE","IEEE Journals & Magazines"
"A survey of controlled experiments in software engineering","D. I. K. Sjoeberg; J. E. Hannay; O. Hansen; V. B. Kampenes; A. Karahasanovic; N. -. Liborg; A. C. Rekdal","Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; NA; NA","IEEE Transactions on Software Engineering","","2005","31","9","733","753","The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.97","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514443","Index Terms- Controlled experiments;survey;research methodology;empirical software engineering.","Software engineering;Software maintenance;Application software;Conference proceedings;Computer Society;Recruitment;Computer industry;Programming;Software systems;Software metrics","software engineering","controlled experiment survey;software engineering;scientific articles","","266","","52","","","","","","IEEE","IEEE Journals & Magazines"
"CSDL: a language for cooperative systems design","F. De Paoli; F. Tisato","Dipartimento di Sci. dell'Inf., Milan Univ., Italy; Dipartimento di Sci. dell'Inf., Milan Univ., Italy","IEEE Transactions on Software Engineering","","1994","20","8","606","616","The aim of a cooperative system is to coordinate and support group activities. Cooperative Systems Design Language (CSDL) is an experimental language designed to support the development of cooperative systems from specification to implementation. In CSDL, a system is defined as a collection of reusable entities implementing floor control disciplines and shared workspaces. CSDL tries to address the difficulties of integrating different aspects of cooperative systems: cooperation control, communication, and system modularization. This paper presents CSDL as a specification language. Basic units are coordinators that can be combined hierarchically. A coordinator is composed of a specification, a body, and a context. The specification defines the cooperation policy; the body controls the underlying communication channels; and the context defines coordinators' interaction in modular systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310670","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310670","","Cooperative systems;Control systems;Communication system control;Specification languages;Context;Collaborative work;Application software;Communication channels;Collaborative software;Software design","cooperative systems;groupware;distributed processing;specification languages","cooperative systems design;CSDL;group activities;shared workspaces;floor control;specification language;cooperation policy;CSCW;conferencing systems;groupware;design language;software architecture;distributed systems","","7","","23","","","","","","IEEE","IEEE Journals & Magazines"
"A Synthetic Workload Generation Technique for Stress Testing Session-Based Systems","D. Krishnamurthy; J. A. Rolia; S. Majumdar","Department of Electrical and Computer Engineering, University of Calgary, 2500 University Drive NW, Calgary, AB, T2N 1N4, Canada; Enterprise Software and Systems Lab, Hewlett Packard Labs, 1501 Page Mill Road, Palo Alto, CA 94304; Department of Systems and Computer Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, ON, K1S 5B6, Canada","IEEE Transactions on Software Engineering","","2006","32","11","868","882","Enterprise applications are often business critical but lack effective synthetic workload generation techniques to evaluate performance. These workloads are characterized by sessions of interdependent requests that often cause and exploit dynamically generated responses. Interrequest dependencies must be reflected in synthetic workloads for these systems to exercise application functions correctly. This poses significant challenges for automating the construction of representative synthetic workloads and manipulating workload characteristics for sensitivity analyses. This paper presents a technique to overcome these problems. Given request logs for a system under study, the technique automatically creates a synthetic workload that has specified characteristics and maintains the correct interrequest dependencies. The technique is demonstrated through a case study involving a TPC-W e-commerce system. Results show that incorrect performance results can be obtained by neglecting interrequest dependencies, thereby highlighting the value of our technique. The study also exploits our technique to investigate the impact of several workload characteristics on system performance. Results establish that high variability in the distributions of session length, session idle times, and request service times can cause increased contention among sessions, leading to poor system responsiveness. To the best of our knowledge, these are the first results of this kind for a session-based system. We believe our technique is of value for studies where fine control over workload is essential","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.106","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015510","Performance of systems;measurement techniques;modeling techniques;software engineering;testing tools;Internet applications;electronic commerce;Web servers.","System testing;Application software;Sensitivity analysis;Delay;Web server;Stress control;Occupational stress;Computer Society;Character generation;System performance","electronic commerce;performance evaluation;program testing","synthetic workload generation technique;stress testing session-based system;enterprise application;e-commerce system;sensitivity analyses;TPC-W e-commerce system;performance evaluation","","49","","27","","","","","","IEEE","IEEE Journals & Magazines"
"App Store Effects on Software Engineering Practices","A. AlSubaihin; F. Sarro; S. Black; L. Capra; M. Harman","Computer Science, University College London, London, London United Kingdom of Great Britain and Northern Ireland WC1E 6BT (e-mail: afnan.alsubaihin.14@ucl.ac.uk); Computer Science, University College London, London, London United Kingdom of Great Britain and Northern Ireland WC1E 6B (e-mail: f.sarro@ucl.ac.uk); Computer Science, University College London, London, London United Kingdom of Great Britain and Northern Ireland (e-mail: sueblack@gmail.com); Computer Science, University College London, London, London United Kingdom of Great Britain and Northern Ireland (e-mail: l.capra@ucl.ac.uk); CS, UCL, London, London United Kingdom of Great Britain and Northern Ireland WC1E 6BT (e-mail: mark.harman@ucl.ac.uk)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","In this paper, we study the app store as a phenomenon from the developers perspective to investigate the extent to which app stores affect software engineering tasks. Through developer interviews and questionnaires, we uncover findings that highlight and quantify the effects of three high-level app store themes: bridging the gap between developers and users, increasing market transparency and affecting mobile release management. Our findings have implications for testing, requirements engineering and mining software repositories research fields. These findings can help guide future research in supporting mobile app developers through a deeper understanding of the app store-developer interaction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2891715","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8606261","Empirical Software Engineering;Mobile App Development;App Store Analysis","Software engineering;Software;Interviews;Data mining;Testing;Data collection;Business","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Better reliability assessment and prediction through data clustering","J. Tian","Dept. of Comput. Sci. & Eng., Southern Methodist Univ., Dallas, TX, USA","IEEE Transactions on Software Engineering","","2002","28","10","997","1007","This paper presents a new approach to software reliability modeling by grouping data into clusters of homogeneous failure intensities. This series of data clusters associated with different time segments can be directly used as a piecewise linear model for reliability assessment and problem identification, which can produce meaningful results early in the testing process. The dual model fits traditional software reliability growth models (SRGMs) to these grouped data to provide long-term reliability assessments and predictions. These models were evaluated in the testing of two large software systems from IBM. Compared with existing SRGMs fitted to raw data, our models are generally more stable over time and produce more consistent and accurate reliability assessments and predictions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041055","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041055","","Software systems;Predictive models;Software reliability;Software testing;System testing;Failure analysis;Piecewise linear techniques;Fluctuations;Data analysis","software reliability;statistical analysis;failure analysis;reliability theory","data clustering;reliability assessment;data grouping;input domain reliability models;data cluster based reliability models;piecewise linear model;identification;software reliability growth models","","11","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Design of multi-invariant data structures for robust shared accesses in multiprocessor systems","I-Ling Yen; F. B. Bastani; D. J. Taylor","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","193","207","Multiprocessor systems are widely used in many application programs to enhance system reliability and performance. However, reliability does not come naturally with multiple processors. We develop a multi-invariant data structure approach to ensure efficient and robust access to shared data structures in multiprocessor systems. Essentially, the data structure is designed to satisfy two invariants, a strong invariant, and a weak invariant. The system operates at its peak performance when the strong invariant is true. The system will operate correctly even when only the weak invariant is true, though perhaps at a lower performance level. The design ensures that the weak invariant will always be true in spite of fail-stop processor failures during the execution. By allowing the system to converge to a state satisfying only the weak invariant, the overhead for incorporating fault tolerance can be reduced. We present the basic idea of multi-invariant data structures. We also develop design rules that systematically convert fault-intolerant data abstractions into corresponding fault-tolerant versions. In this transformation, we augment the data structure and access algorithms to ensure that the system always converges to the weak invariant, even in the presence of fail-stop processor failures. We also design methods for the detection of integrity violations and for restoring the strong invariant. Two data structures, namely binary search tree and double-linked list, are used to illustrate the concept of multi-invariant data structures.","0098-5589;1939-3520;2326-3881","","10.1109/32.910857","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910857","","Data structures;Robustness;Multiprocessing systems;Fault tolerant systems;Protocols;Reliability;Tree data structures;Intelligent robots;Concurrent computing;System performance","shared memory systems;data structures;tree searching;fault tolerant computing;transaction processing;data integrity","multi-invariant data structure design;robust shared accesses;multiprocessor systems;application programs;system reliability;multiple processors;multi-invariant data structure approach;robust access;shared data structures;data structure;strong invariant;weak invariant;peak performance;performance level;fail-stop processor failures;fault tolerance;multi-invariant data structures;design rules;fault-intolerant data abstractions;fault-tolerant versions;access algorithms;integrity violations;binary search tree;double-linked list","","","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Explaining software developer acceptance of methodologies: a comparison of five theoretical models","C. K. Riemenschneider; B. C. Hardgrave; F. D. Davis","Sam M. Walton Coll. of Bus. Inf., Arkansas Univ., Fayetteville, AR, USA; Sam M. Walton Coll. of Bus. Inf., Arkansas Univ., Fayetteville, AR, USA; Sam M. Walton Coll. of Bus. Inf., Arkansas Univ., Fayetteville, AR, USA","IEEE Transactions on Software Engineering","","2002","28","12","1135","1145","Many organizations attempt to deploy methodologies intended to improve software development processes. However, resistance by individual software developers against using such methodologies often obstructs their successful deployment. To better explain why individual developers accept or resist methodologies, five theoretical models of individual intentions to accept information technology tools were examined. In a field study of 128 developers in a large organization that implemented a methodology, each model explained significant variance in developers' intentions to use the methodology. Similar to findings from the tool adoption context, we found that, if a methodology is not regarded as useful by developers, its prospects for successful deployment may be severely undermined. In contrast to the typical pattern of findings in a tool context, however, we found that methodology adoption intentions are driven by: 1) the presence of an organizational mandate to use the methodology, 2) the compatibility of the methodology with how developers perform their work, and 3) the opinions of developers' coworkers and supervisors toward using the methodology. Collectively, these results provide surprising new insights into why software developers accept or resist methodologies and suggest what software engineering managers might do to overcome developer resistance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158287","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158287","","Programming;Information technology;Software engineering;Productivity;Resists;Software development management;Computer Society;Engineering management;Technological innovation;Production","software engineering;information technology","software developer acceptance;software development processes;information technology tools;tool adoption context;software developers;software engineering managers","","91","","70","","","","","","IEEE","IEEE Journals & Magazines"
"Experience with a Software Engineering Project Course","W. M. Mc Keeman","Aiken Computation Laboratories, Harvard University","IEEE Transactions on Software Engineering","","1987","SE-13","11","1182","1192","This paper presents an approach to meeting the academic objectives of advanced software engineering project courses. The objectives are increased competence and confidence of the students in carrying out software development projects. The academic context includes a simulated industrial context. Part of the industrial context consists of industrial roles played for the student team by the instructor and others. The project itself is divided into tasks related to deliverables and collateral responsibilities. The software production model is a combination of the waterfall, iterative enhancement, and document-driven techniques. A software development environment is mentioned although the details are presented elsewhere. Further detail is given for five project courses conducted by the author.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232868","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702166","Design;documentation;project courses;requirements;role playing;software projects;specification;testing","Software engineering;Programming;Context modeling;Documentation;Software testing;Organizing;Education;Proposals;Production systems;Information technology","","Design;documentation;project courses;requirements;role playing;software projects;specification;testing","","2","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Process synchronization: design and performance evaluation of distributed algorithms","R. Bagrodia","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA","IEEE Transactions on Software Engineering","","1989","15","9","1053","1065","The author presents a simple solution for the committee coordination problem, which encompasses the synchronization and exclusion problems associated with implementing multiway rendezvous, and shows how it can be implemented to develop a family of algorithms. The algorithms use message counts to solve the synchronization problem, and they solve the exclusion problem by using a circulating token or by using auxiliary resources as in the solutions for the dining or drinking philosophers' problems. Results of a simulation study of the performance of the algorithms are presented. The experiments measured the response time and message complexity of each algorithm as a function of variations in the model parameters, including network topology and level of conflict in the system. The results show that the response time for algorithms proposed is significantly better than for existing algorithms, whereas the message complexity is considerably worse.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31364","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31364","","Process design;Algorithm design and analysis;Distributed algorithms;Context;Time measurement;Delay;Network topology;Anthropomorphism;Computer science;Protocols","computational complexity;message switching;network topology;synchronisation","process synchronization;performance evaluation;distributed algorithms;committee coordination problem;multiway rendezvous;message counts;synchronization problem;exclusion problem;circulating token;auxiliary resources;simulation study;response time;message complexity;model parameters;network topology;level of conflict","","48","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Integrating time domain and input domain analyses of software reliability using tree-based models","J. Tian","Dept. of Comput. Sci. & Eng., Methodist Univ., Dallas, TX, USA","IEEE Transactions on Software Engineering","","1995","21","12","945","958","The paper examines two existing approaches to software reliability analysis, time domain reliability growth modeling and input domain reliability analysis, and presents a new approach that combines some of their individual strengths. An analysis method called tree-based modeling is used to build models based on the combined measurement data. This new approach can be used to assess the reliability of software systems, to track reliability change over time, and to identify problematic subparts characterized by certain input states or time periods. The results can also be used to guide various remedial actions aimed at reliability improvement. This approach has been demonstrated to be applicable and effective in the testing of several large commercial software systems developed in the IBM Software Solutions Toronto Laboratory.","0098-5589;1939-3520;2326-3881","","10.1109/32.489071","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=489071","","Time domain analysis;Software reliability;Software testing;Software systems;Failure analysis;Laboratories;System testing;Programming;Data analysis;Sampling methods","software reliability;software reliability;testing;trees (mathematics);software metrics;system monitoring","time domain analyses;input domain analyses;software reliability analysis;tree-based models;time domain reliability growth modeling;input domain reliability analysis;measurement data;reliability change tracking;problematic subparts;time periods;input states;remedial actions;reliability improvement;large commercial software system testing;IBM Software Solutions Toronto Laboratory","","31","","29","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study using task assignment patterns to improve the accuracy of software effort estimation","R. K. Smith; J. E. Hale; A. S. Parrish","Math., Comput. & Inf. Sci. Dept., Jacksonville State Univ., AL, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","264","271","In most software development organizations, there is seldom a one-to-one mapping between software developers and development tasks. It is frequently necessary to concurrently assign individuals to multiple tasks and to assign more than one individual to work cooperatively on a single task. A principal goal in making such assignments should be to minimize the effort required to complete each task. But what impact does the manner in which developers are assigned to tasks have on the effort requirements? This paper identifies four task assignment factors: team size, concurrency, intensity, and fragmentation. These four factors are shown to improve the predictive ability of the well-known intermediate COCOMO cost estimation model. A parsimonious effort estimation model is also derived that utilizes a subset of the task assignment factors and unadjusted function points. For the data examined, this parsimonious model is shown to have goodness of fit and quality of estimation superior to that of the COCOMO model, while utilizing fewer cost factors.","0098-5589;1939-3520;2326-3881","","10.1109/32.910861","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910861","","Costs;Predictive models;Concurrent computing;Computer Society;Finishing;Project management;Productivity;Programming profession;Software engineering;Information science","software cost estimation;software development management;software metrics","task assignment patterns;software effort estimation;software development organizations;team size;COCOMO;cost estimation model;unadjusted function points","","30","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling security-relevant data semantics","G. W. Smith","Inf. Resources Manage. Coll., Nat. Defense Univ., Washington, DC, USA","IEEE Transactions on Software Engineering","","1991","17","11","1195","1203","The use of an extended data model which represents both integrity and secrecy aspects of data is demonstrated. This Semantic Data Model for Security (SDMS) provides a technique that assists domain experts, security officers, and database designers in first understanding their security requirements, and then translating them into a good database design. Identifying security requirements at this semantic level provides the basis for analyzing the security requirements and the database design for inference and signaling vulnerabilities. Another contribution is a comprehensive taxonomy of security-relevant data semantics that must be captured and understood to implement a multilevel secure automated information system.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.106974","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=106974","","Data security;Database systems;Data models;Information security;Operating systems;Application software;Information systems;Humans;Relational databases;Knowledge management","database management systems;security of data","inference vulnerability;extended data model;secrecy;Semantic Data Model for Security;security requirements;database design;signaling vulnerabilities;security-relevant data semantics;multilevel secure automated information system","","10","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Plat_Forms: A Web Development Platform Comparison by an Exploratory Experiment Searching for Emergent Platform Properties","L. Prechelt","Freie Universität Berlin, Berlin","IEEE Transactions on Software Engineering","","2011","37","1","95","108","Background: For developing Web-based applications, there exist several competing and widely used technological platforms (consisting of a programming language, framework(s), components, and tools), each with an accompanying development culture and style. Research question: Do Web development projects exhibit emergent process or product properties that are characteristic and consistent within a platform, but show relevant substantial differences across platforms or do team-to-team individual differences outweigh such differences, if any? Such a property could be positive (i.e., a platform advantage), negative, or neutral, and it might be unobvious which is which. Method: In a nonrandomized, controlled experiment, framed as a public contest called “Plat_Forms,” top-class teams of three professional programmers competed to implement the same requirements for a Web-based application within 30 hours. Three different platforms (Java EE, PHP, or Perl) were used by three teams each. We compare the resulting nine products and process records along many dimensions, both external (usability, functionality, reliability, security, etc.) and internal (size, structure, modifiability, etc.). Results: The various results obtained cover a wide spectrum: First, there are results that many people would have called “obvious” or “well known,” say, that Perl solutions tend to be more compact than Java solutions. Second, there are results that contradict conventional wisdom, say, that our PHP solutions appear in some (but not all) respects to be actually at least as secure as the others. Finally, one result makes a statement we have not seen discussed previously: Along several dimensions, the amount of within-platform variation between the teams tends to be smaller for PHP than for the other platforms. Conclusion: The results suggest that substantial characteristic platform differences do indeed exist in some dimensions, but possibly not in others.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5406528","Emergent properties;usability;functionality;reliability;security;product size;design structure;modifiability;Java;PHP;Perl.","Java;Computer languages;Usability;Security;Libraries;Programming profession;Product design;Buildings;Cascading style sheets;Ecosystems","emergent phenomena;Internet;Java;Perl;Web design","Web development platform;emergent platform properties;Web based applications;Java EE;Perl solutions;PHP solutions","","3","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Oracle Data Selection Support","G. Gay; M. Staats; M. Whalen; M. P. E. Heimdahl","Department of Computer Science & Engineering, University of South Carolina; Google, Inc.; Department of Computer Science and Engineering, University of Minnesota; Department of Computer Science and Engineering, University of Minnesota","IEEE Transactions on Software Engineering","","2015","41","11","1119","1137","The choice of test oracle-the artifact that determines whether an application under test executes correctly-can significantly impact the effectiveness of the testing process. However, despite the prevalence of tools that support test input selection, little work exists for supporting oracle creation. We propose a method of supporting test oracle creation that automatically selects the oracle data-the set of variables monitored during testing-for expected value test oracles. This approach is based on the use of mutation analysis to rank variables in terms of fault-finding effectiveness, thus automating the selection of the oracle data. Experimental results obtained by employing our method over six industrial systems (while varying test input types and the number of generated mutants) indicate that our method-when paired with test inputs generated either at random or to satisfy specific structural coverage criteria-may be a cost-effective approach for producing small, effective oracle data sets, with fault finding improvements over current industrial best practice of up to 1,435 percent observed (with typical improvements of up to 50 percent).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2436920","NASA; NSF; US National Science Foundation (NSF); Fonds National de la Recherche, Luxembourg; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7112189","Testing;Test Oracles;Oracle Data;Oracle Selection;Verification;Testing;test oracles;oracle data;oracle selection;verification","Testing;Monitoring;Software;Aerospace electronics;Training;Electronic mail;Computer crashes","program testing;program verification","automated oracle data selection support;mutation analysis;software testing;test oracle;oracle creation;specific structural coverage criteria","","5","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Size-Constrained Regression Test Case Selection Using Multicriteria Optimization","S. Mirarab; S. Akhlaghi; L. Tahvildari","University of Texas at Austin, Austin; Shahed University, Tehran; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2012","38","4","936","956","To ensure that a modified software system has not regressed, one approach is to rerun existing test cases. However, this is a potentially costly task. To mitigate the costs, the testing effort can be optimized by executing only a selected subset of the test cases that are believed to have a better chance of revealing faults. This paper proposes a novel approach for selecting and ordering a predetermined number of test cases from an existing test suite. Our approach forms an Integer Linear Programming problem using two different coverage-based criteria, and uses constraint relaxation to find many close-to-optimal solution points. These points are then combined to obtain a final solution using a voting mechanism. The selected subset of test cases is then prioritized using a greedy algorithm that maximizes minimum coverage in an iterative manner. The proposed approach has been empirically evaluated and the results show significant improvements over existing approaches for some cases and comparable results for the rest. Moreover, our approach provides more consistency compared to existing approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928351","Software regression testing;test case selection;integer programming;Pareto optimality","Testing;Software;Time factors;Fault detection;Optimization;Estimation;IP networks","greedy algorithms;integer programming;linear programming;program testing;regression analysis","size constrained regression test case selection;multicriteria optimization;modified software system;integer linear programming problem;voting mechanism;greedy algorithm;iterative manner","","28","","61","","","","","","IEEE","IEEE Journals & Magazines"
"Error recovery in asynchronous systems","R. H. Campbell; B. Randell","Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801; Computer Laboratory, University of Newcastle upon Tyne, Newcastle upon Tyne NEI 7RU, England","IEEE Transactions on Software Engineering","","1986","SE-12","8","811","826","A framework for the provision of fault tolerance in asynchronous systems is introduced. The proposal generalizes the form of simple recovery facilities supported by nested atomic actions in which the exception mechanisms only permit backward error recovery. It allows the construction of systems using both forward and backward error recovery and thus allows the exploitation of the complementary benefits of the two schemes. Backward recovery, forward recovery, and normal processing activities can occur concurrently within the organization proposed. Exception handling is generalized to provide a uniform basis for fault tolerance schemes with the atomic action structure. The generalization includes a resolution scheme for concurrently raised exceptions based on an exception tree and an abortion scheme that permits the termination of the internal atomic actions. An automatic resolution mechanism is outlined for exceptions in atomic actions which allows users to separate their recovery schemes from the details of the underlying algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312984","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312984","Asynchronous systems;atomic actions;error recovery;exception mechanism;programming techniques;software fault tolerance;software reliability","Fault tolerance;Fault tolerant systems;Context;Protocols;Atomic measurements;Software;Computers","fault tolerant computing;software reliability;system recovery","exception handling;software reliability;asynchronous systems;fault tolerance;nested atomic actions;error recovery;automatic resolution mechanism","","47","","","","","","","","IEEE","IEEE Journals & Magazines"
"A Lightweight System for Detecting and Tolerating Concurrency Bugs","M. Zhang; Y. Wu; S. Lu; S. Qi; J. Ren; W. Zheng","Tsinghua National Laboratory for Information Science and Technology (TNLIST), the Department of Computer Science and Technology, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology (TNLIST), the Department of Computer Science and Technology, Tsinghua University, Beijing, China; University of Chicago, Chicago, IL; UBER growth team, San Francisco, CA; Tsinghua National Laboratory for Information Science and Technology (TNLIST), the Department of Computer Science and Technology, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology (TNLIST), the Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Software Engineering","","2016","42","10","899","917","Along with the prevalence of multi-threaded programs, concurrency bugs have become one of the most important sources of software bugs. Even worse, due to the non-deterministic nature of concurrency bugs, these bugs are both difficult to detect and fix even after the detection. As a result, it is highly desired to develop an all-around approach that is able to not only detect them during the testing phase but also tolerate undetected bugs during production runs. However, existing bug-detecting and bug-tolerating tools are usually either<italic>1)</italic>constrained in types of bugs they can handle or<italic>2)</italic>requiring specific hardware supports for achieving an acceptable overhead. In this paper, we present a novel program invariant, name Anticipating Invariant (<sc>Ai</sc>), that can detect most types of concurrency bugs. More importantly,<sc>Ai</sc>can be used to anticipate many concurrency bugs before any irreversible changes have been made. Thus it enables us to develop a software-only system that is able to forestall failures with a simple thread stalling technique, which does not rely on execution roll-back and hence has good performance. Experiments with 35 real-world concurrency bugs demonstrate that<sc>Ai</sc>is capable of detecting and tolerating many important types of concurrency bugs, including both atomicity and order violations. It has also exposed two new bugs (confirmed by developers) that were never reported before in the literature. Performance evaluation with 6 representative parallel programs shows that<sc>Ai</sc>incurs negligible overhead (<inline-formula><tex-math notation=""LaTeX"">$ < 1\%$</tex-math><alternatives><inline-graphic xlink:href=""wu-ieq1-2531666.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>) for many nontrivial desktop and server applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2531666","Natural Science Foundation of China; National Basic Research (973) Program of China; National High-Tech R&D (863) Program of China; Chinese Special Project of Science and Technology; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7412768","Concurrency bugs;software reliability;bug tolerating","Computer bugs;Concurrent computing;Artificial intelligence;Turning;Testing;Hardware","","","","1","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Software Module Clustering as a Multi-Objective Search Problem","K. Praditwong; M. Harman; X. Yao","The University of Birmingham, Birmingham; University College London, London; The University of Birmingham, Birmingham","IEEE Transactions on Software Engineering","","2011","37","2","264","282","Software module clustering is the problem of automatically organizing software units into modules to improve program structure. There has been a great deal of recent interest in search-based formulations of this problem in which module boundaries are identified by automated search, guided by a fitness function that captures the twin objectives of high cohesion and low coupling in a single-objective fitness function. This paper introduces two novel multi-objective formulations of the software module clustering problem, in which several different objectives (including cohesion and coupling) are represented separately. In order to evaluate the effectiveness of the multi-objective approach, a set of experiments was performed on 17 real-world module clustering problems. The results of this empirical study provide strong evidence to support the claim that the multi-objective approach produces significantly better solutions than the existing single-objective approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5406532","SBSE;module clustering;multi-objective optimization;evolutionary computation.","Search problems;Computer science;Performance evaluation;Software engineering;Clustering algorithms;Computational intelligence;Testing;Educational institutions;Computer applications;Application software","optimisation;pattern clustering;search problems;software engineering","software module clustering;multi-objective search problem;program structure","","140","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Generation of Tests to Exploit XML Injection Vulnerabilities in Web Applications","S. Jan; A. Panichella; A. Arcuri; L. Briand","SnT, Universite du Luxembourg, 81872 Luxembourg, LU Luxembourg (e-mail: jan@svv.lu); Centre Interdisciplinary for Security, Reliability and Trust, Universite du Luxembourg, 81872 Luxembourg, Luxembourg Luxembourg L-2721 (e-mail: annibale.panichella@uni.lu); SnT, Universite du Luxembourg, 81872 Luxembourg, LU Luxembourg (e-mail: arcand@westerdals.no); SnT Centre, University of Luxembourg, Luxembourg, Luxembourg Luxembourg 2721 (e-mail: lionel.briand@uni.lu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Modern enterprise systems can be composed of many web services (e.g., SOAP and RESTful). Users of such systems might not have direct access to those services, and rather interact with them through a single entry point which provides a GUI (e.g., a web page or a mobile app). Although the interactions with such entry point might be secure, a hacker could trick such systems to send malicious inputs to those internal web services. A typical example is XML injection targeting SOAP communications. Previous work has shown that it is possible to automatically generate such kind of attacks using search-based techniques. In this paper, we improve upon previous results by providing more efficient techniques to generate such attacks. In particular, we investigate four different algorithms and two different fitness functions. A large empirical study, involving also two industrial systems, shows that our technique is effective at automatically generating XML injection attacks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2778711","H2020 European Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8125155","Evolutionary Testing;XML Injection;Security Testing","XML;Simple object access protocol;Testing;Service-oriented architecture","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Prioritizing test cases for regression testing","G. Rothermel; R. H. Untch; Chengyun Chu; M. J. Harrold","Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","10","929","948","Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detection, a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test and let software engineers begin correcting faults earlier than might otherwise be possible. One application of prioritization techniques involves regression testing, the retesting of software following modifications; in this context, prioritization techniques can take advantage of information gathered about the previous execution of test cases to obtain test case orderings. We describe several techniques for using test execution information to prioritize test cases for regression testing, including: 1) techniques that order test cases based on their total coverage of code components; 2) techniques that order test cases based on their coverage of code components not previously covered; and 3) techniques that order test cases based on their estimated ability to reveal faults in the code components that they cover. We report the results of several experiments in which we applied these techniques to various test suites for various programs and measured the rates of fault detection achieved by the prioritized test suites, comparing those rates to the rates achieved by untreated, randomly ordered, and optimally ordered suites.","0098-5589;1939-3520;2326-3881","","10.1109/32.962562","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=962562","","Computer aided software engineering;Software testing;Fault detection;Costs;Computer Society;System testing;Software maintenance;Processor scheduling;Feedback;Application software","program testing;program debugging","test case prioritization;regression testing;software testing;test case scheduling;software fault detection rate;software fault correction;code component coverage;experiments;cost-benefit analysis","","527","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Software measurement: a necessary scientific basis","N. Fenton","Centre for Software Reliability, City Univ., London, UK","IEEE Transactions on Software Engineering","","1994","20","3","199","206","Software measurement, like measurement in any other discipline, must adhere to the science of measurement if it is to gain widespread acceptance and validity. The observation of some very simple, but fundamental, principles of measurement can have an extremely beneficial effect on the subject. Measurement theory is used to highlight both weaknesses and strengths of software metrics work, including work on metrics validation. We identify a problem with the well-known Weyuker properties (E.J. Weyuker, 1988), but also show that a criticism of these properties by J.C. Cherniavsky and C.H. Smith (1991) is invalid. We show that the search for general software complexity measures is doomed to failure. However, the theory does help us to define and validate measures of specific complexity attributes. Above all, we are able to view software measurement in a very wide perspective, rationalising and relating its many diverse activities.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.268921","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=268921","","Software measurement;Software metrics;Humans;Gain measurement;Counting circuits;Software reliability;Software testing;Blood pressure;Cost function","software metrics;measurement theory;programming theory","software measurement;scientific basis;measurement theory;software metrics work;metrics validation;software complexity measures;complexity attributes","","212","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Language-Independent and Automated Software Composition: The FeatureHouse Experience","S. Apel; C. Kästner; C. Lengauer","University of Passau, Passau; Philipps University Marburg, Marburg; University of Passau, Passau","IEEE Transactions on Software Engineering","","2013","39","1","63","79","Superimposition is a composition technique that has been applied successfully in many areas of software development. Although superimposition is a general-purpose concept, it has been (re)invented and implemented individually for various kinds of software artifacts. We unify languages and tools that rely on superimposition by using the language-independent model of feature structure trees (FSTs). On the basis of the FST model, we propose a general approach to the composition of software artifacts written in different languages. Furthermore, we offer a supporting framework and tool chain, called FEATUREHOUSE. We use attribute grammars to automate the integration of additional languages. In particular, we have integrated Java, C#, C, Haskell, Alloy, and JavaCC. A substantial number of case studies demonstrate the practicality and scalability of our approach and reveal insights into the properties that a language must have in order to be ready for superimposition. We discuss perspectives of our approach and demonstrate how we extended FEATUREHOUSE with support for XML languages (in particular, XHTML, XMI/UML, and Ant) and alternative composition approaches (in particular, aspect weaving). Rounding off our previous work, we provide here a holistic view of the FEATUREHOUSE approach based on rich experience with numerous languages and case studies and reflections on several years of research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.120","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6095570","FeatureHouse;feature structure trees;software composition;superimposition;language independence","Software;Java;Grammar;Databases;Printers;Latches;Unified modeling language","attribute grammars;C++ language;Java;software tools;Unified Modeling Language;XML","language-independent software composition;automated software composition;FEATUREHOUSE;superimposition;software development;general-purpose concept;language-independent model;feature structure tree;software artifact composition;supporting framework;tool chain;attribute grammar;integrated Java;C#;Haskell;Alloy;JavaCC;XML language;XHTML;XMI/UML;Ant","","21","","67","","","","","","IEEE","IEEE Journals & Magazines"
"The location-based paradigm for replication: Achieving efficiency and availability in distributed systems","P. Triantafillou; D. J. Taylor","Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada; NA","IEEE Transactions on Software Engineering","","1995","21","1","1","18","Replication techniques for transaction-based distributed systems generally achieve increased availability but with a significant performance penalty. We present a new replication paradigm, the location-based paradigm, which addresses availability and other performance issues. It provides availability similar to quorum-based replication protocols but with transaction-execution delays similar to one-copy systems. The paradigm further exploits replication to improve performance in two instances. First, it takes advantage of local or nearby replicas to further improve the response time of transactions, achieving smaller execution delays than one-copy systems. Second, it takes advantage of replication to facilitate the independent crash recovery of replica sites-a goal which is unattainable in one-copy systems. In addition to the above the location-based paradigm avoids bottlenecks, facilitates load balancing, and minimizes the disruption of service when failures and recoveries occur. In this paper we present the paradigm, a formal proof of correctness, and a detailed simulation study comparing our paradigm to one-copy systems and to other approaches to replication control.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.341843","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=341843","","Availability;Protocols;Computer crashes;Costs;Concurrency control;Distributed computing;Collaboration;Delay systems;Delay effects;Load management","concurrency control;protocols;system recovery;software fault tolerance;transaction processing;distributed processing","location-based paradigm;replication;transaction-based distributed systems;performance penalty;availability;quorum-based replication protocols;transaction-execution delays;one-copy systems;independent crash recovery;replica sites;bottlenecks;load balancing","","19","","29","","","","","","IEEE","IEEE Journals & Magazines"
"CTDNet-a mechanism for the concurrent execution of lambda graphs","J. P. Gupta; S. C. Winter; D. R. Wilson","Dept. of Electron. & Comput. Eng., Roorkee Univ., India; NA; NA","IEEE Transactions on Software Engineering","","1989","15","11","1357","1367","The authors describe CTDNet, a data-driven reduction machine for the concurrent execution of applicative functional programs in the form of lambda calculus expressions. Such programs are stored as binary-tree-structured process graphs in which all processes maintain pointers to their immediate neighbors (i.e. ancestor and two children). Processes are of two basic types: master processes, which represent the original process graph, and slave processes, which carry out the actual executional work and are dynamically created and destroyed. CTDNet uses a distributed eager evaluation scheme with a modification to evaluate conditional expressions lazily, together with a form of distributed string reduction with some graphlike modifications.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41329","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41329","","Calculus;Concurrent computing;Computer architecture;Data flow computing;Pipelines;Hardware;Binary trees;Master-slave;Functional programming;Parallel processing","graph theory;parallel machines;parallel programming","CTDNet;concurrent execution;lambda graphs;data-driven reduction machine;applicative functional programs;lambda calculus expressions;binary-tree-structured process graphs;pointers;neighbors;ancestor;children;master processes;slave processes;distributed eager evaluation scheme;conditional expressions;distributed string reduction","","2","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Rapid software development through team collocation","S. D. Teasley; L. A. Covi; M. S. Krishnan; J. S. Olson","Sch. of Inf., Michigan Univ., Ann Arbor, MI, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","7","671","683","In a field study conducted at a leading Fortune 100 company, we examined how having development teams reside in their own large room (an arrangement called radical collocation) affected system development. The collocated projects had significantly higher productivity and shorter schedules than both the industry benchmarks and the performance of past similar projects within the firm. The teams reported high satisfaction about their process, and both customers and project sponsors were similarly highly satisfied. The analysis of questionnaire, interview and observational data from these teams showed that being ""at hand,"" i.e. both visible and available, helped them to coordinate their work better and learn from each other. Radical collocation seems to be one of the factors leading to high productivity in these teams.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019481","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019481","","Programming;Productivity;Job shop scheduling;Computer Society;Software quality;Costs;Processor scheduling;Software engineering;Application software;Quality management","software prototyping;software development management;project management;human resource management","rapid software development;team collocation;field study;Fortune 100 company;software development teams;team room;radical collocation;system development;collocated projects;project productivity;project schedules;industry benchmarks;project performance;satisfaction;questionnaire data;interview data;observational data;team visibility;team availability;work coordination;learning;war rooms;software metrics;software engineering","","62","","47","","","","","","IEEE","IEEE Journals & Magazines"
"On the Asymptotic Behavior of Adaptive Testing Strategy for Software Reliability Assessment","J. Lv; B. Yin; K. Cai","School of Automation Science and Electrical Engineering, Bejing, China; School of Automation Science and Electrical Engineering, Bejing, China; School of Automation Science and Electrical Engineering, Bejing, China","IEEE Transactions on Software Engineering","","2014","40","4","396","412","In software reliability assessment, one problem of interest is how to minimize the variance of reliability estimator, which is often considered as an optimization goal. The basic idea is that an estimator with lower variance makes the estimates more predictable and accurate. Adaptive Testing (AT) is an online testing strategy, which can be adopted to minimize the variance of software reliability estimator. In order to reduce the computational overhead of decision-making, the implemented AT strategy in practice deviates from its theoretical design that guarantees AT's local optimality. This work aims to investigate the asymptotic behavior of AT to improve its global performance without losing the local optimality. To this end, a new AT strategy named Adaptive Testing with Gradient Descent method (AT-GD) is proposed. Theoretical analysis indicates that AT-GD, a locally optimal testing strategy, converges to the globally optimal solution as the assessment process proceeds. Simulation and experiments are set up to validate AT-GD's effectiveness and efficiency. Besides, sensitivity analysis of AT-GD is also conducted in this study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2310194","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6762895","Adaptive testing;operational profile;software reliability;testing strategy","Testing;Software reliability;Software;Global Positioning System;Aircraft;Reliability theory","decision making;gradient methods;optimisation;program testing;sensitivity analysis;software reliability","asymptotic behavior;adaptive testing strategy;software reliability assessment;optimization goal;online testing strategy;software reliability estimator;computational overhead reduction;decision-making;global performance improvement;adaptive testing with gradient descent method;AT-GD;locally optimal testing strategy;sensitivity analysis","","16","","35","","","","","","IEEE","IEEE Journals & Magazines"
"An application of artificial intelligence to object-oriented performance design for real-time systems","S. Honiden; K. Nishimura; N. Uchihira; K. Itoh","Syst. & Software Eng. Lab., Toshiba Corp., Kawasaki, Japan; Syst. & Software Eng. Lab., Toshiba Corp., Kawasaki, Japan; Syst. & Software Eng. Lab., Toshiba Corp., Kawasaki, Japan; NA","IEEE Transactions on Software Engineering","","1994","20","11","849","867","The paper describes an application of artificial intelligence technology to the implementation of a rapid prototyping method in object-oriented performance design (OOPD) for real-time systems. OOPD consists of two prototyping phases for real-time systems. Each of these phases consists of three steps: prototype construction, prototype execution, and prototype evaluation. We present artificial intelligence based methods and tools to be applied to the individual steps. In the prototype construction step, a rapid construction mechanism using reusable software components is implemented based on planning. In the prototype execution step, a hybrid inference mechanism is used to execute the constructed prototype described in declarative knowledge representation. MENDEL, which is a Prolog based concurrent object-oriented language, can be used as a prototype construction tool and a prototype execution tool. In the prototype evaluation step, an expert system which is based on qualitative reasoning is implemented to detect and diagnose bottlenecks and generate an improvement plan for them.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.368123","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=368123","","Artificial intelligence;Real time systems;Software prototyping;Prototypes;Application software;Algorithm design and analysis;Software performance;Hardware;Productivity;Paper technology","software prototyping;real-time systems;software reusability;expert systems;inference mechanisms;knowledge representation;object-oriented programming;object-oriented languages;parallel languages","artificial intelligence;object-oriented performance design;real-time systems;rapid prototyping method;OOPD;prototyping phases;prototype construction;prototype execution;prototype evaluation;artificial intelligence based methods;rapid construction mechanism;reusable software components;hybrid inference mechanism;declarative knowledge representation;MENDEL;Prolog based concurrent object-oriented language;prototype construction tool;expert system;qualitative reasoning","","3","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Post-Failure Reconfiguration of CSP Programs","S. M. Shatz","Department of Electrical Engineering and Computer Science, University of Illinois","IEEE Transactions on Software Engineering","","1985","SE-11","10","1193","1202","In this paper a technique called process merging is introduced. This technique allows the merging of two communicating sequential processes into a new single process. Thus, this technique can be used to reconfigure a distributed program after a faulty processing element has been detected. The technique is most applicable to dedicated multiple microprocessor systems where the need for continuous operation is critical. A process merging algorithm which operates on distributed programs using the CSP notation is presented in detail and its operation is discussed. In order to illustrate the merging technique, the algorithm's behavior is demonstrated using two classical distributed programs: the Bounded Buffer, Producer, Consumer program and the Dining Philosophers program. Finally, the merging technique is examined with respect to its demands on overall system operation and overhead. This examinatiQn leads to suggestions for future research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231867","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701935","CSP programs;distributed computing;fault-tolerance;process merging;software reconfiguration","Merging;Fault detection;Microprocessors;Distributed computing;Fault tolerance;Hardware;Fault tolerant systems;Embedded system;Process design;Embedded software","","CSP programs;distributed computing;fault-tolerance;process merging;software reconfiguration","","1","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Software specialization via symbolic execution","A. Coen-Porisini; F. De Paoli; C. Ghezzi; D. Mandrioli","Dipartimento di Elettronica, Politecnico di Milano, Italy; Dipartimento di Elettronica, Politecnico di Milano, Italy; Dipartimento di Elettronica, Politecnico di Milano, Italy; Dipartimento di Elettronica, Politecnico di Milano, Italy","IEEE Transactions on Software Engineering","","1991","17","9","884","899","A technique and an environment-supporting specialization of generalized software components are described. The technique is based on symbolic execution. It allows one to transform a generalized software component into a more specific and more efficient component. Specialization is proposed as a technique that improves software reuse. The idea is that a library of generalized components exists and the environment supports a designer in customizing a generalized component when the need arises for reusing it under more restricted conditions. It is also justified as a reengineering technique that helps optimize a program during maintenance. Specialization is supported by an interactive environment that provides several transformation tools: a symbolic executor/simplifier, an optimizer, and a loop refolder. The conceptual basis for these transformation techniques is described, examples of their application are given, and how they cooperate in a prototype environment for the Ada programming language is outlined.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92907","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92907","","Application software;Software libraries;Costs;Software reusability;Software maintenance;Production;Software tools;Software prototyping;Prototypes;Computer languages","Ada;program compilers;software maintenance;software reusability;subroutines","environment-supporting specialization;generalized software components;symbolic execution;software reuse;reengineering technique;maintenance;interactive environment;transformation tools;symbolic executor/simplifier;optimizer;loop refolder;conceptual basis;Ada programming language","","24","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Multiparty interactions for interprocess communication and synchronization","M. Evangelist; N. Francez; S. Katz","MCC, Austin, TX, USA; MCC, Austin, TX, USA; MCC, Austin, TX, USA","IEEE Transactions on Software Engineering","","1989","15","11","1417","1426","The authors consider the essential properties of a multiparty interaction construct which serves as a primitive for interprocess communication and synchronization in distributed programs. It is claimed that more general constructs, which violate the suggested properties, are appropriate for abstraction but should not be seen as a communication primitive, and that both facilities are needed. Several acceptability criteria are posed for multiparty interactions, and various possibilities for constructs satisfying these criteria are presented. These include introducing a novel kind of nondeterminism within the assignments of an interaction, weakening the synchronization among the participants in an interaction, and varying the number of participants in order to provide a high-level treatment of fault tolerance.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41333","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41333","","Fault tolerance;Proposals;Veins;Assembly;High level languages;Computer science;Stress","fault tolerant computing;parallel programming","interprocess communication;synchronization;multiparty interaction construct;primitive;distributed programs;acceptability criteria;nondeterminism;weakening;participants;fault tolerance","","21","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Concept analysis for module restructuring","P. Tonella","Centro per la Ricerca Sci. e Tecnologica, Trento, Italy","IEEE Transactions on Software Engineering","","2001","27","4","351","363","Low coupling between modules and high cohesion inside each module are the key features of good software design. This paper proposes a new approach to using concept analysis for module restructuring, based on the computation of extended concept subpartitions. Alternative modularizations, characterized by high cohesion around the internal structures that are being manipulated, can be determined by such a method. To assess the quality of the restructured modules, the trade-off between encapsulation violations and decomposition is considered, and proper measures for both factors are defined. Furthermore, the cost of restructuring is evaluated through a measure of distance between the original and the new modularizations. Concept subpartitions were determined for a test suite of 20 programs of variable size: 10 public-domain and 10 industrial applications. The trade-off between encapsulation and decomposition was measured on the resulting module candidates, together with an estimate of the cost of restructuring. Moreover, the ability of concept analysis to determine meaningful modularizations was assessed in two ways. First, programs without encapsulation violations were used as oracles, assuming the absence of violations as an indicator of careful decomposition. Second, the suggested restructuring interventions were actually implemented in some case studies to evaluate the feasibility of restructuring and to deeply investigate the code organization before and after the intervention. Concept analysis was experienced to be a powerful tool supporting module restructuring.","0098-5589;1939-3520;2326-3881","","10.1109/32.917524","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917524","","Encapsulation;Data structures;Costs;Software design;Computer languages;Testing;Software engineering;Engineering management;Degradation;Preventive maintenance","subroutines;program control structures;systems re-engineering;data encapsulation;software engineering","concept analysis;module restructuring;module coupling;module cohesion;software design;extended concept subpartitions;modularization;internal structures;module quality;encapsulation violations;decomposition;cost evaluation;distance measure;concept subpartitions;program size;public-domain applications;industrial applications;oracles;restructuring interventions;case studies;code organization;abstract data types;legacy systems;reengineering","","77","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Input-Sensitive Profiling","E. Coppa; C. Demetrescu; I. Finocchi","Department of Computer Science, Sapienza University of Rome, Italy; Department of Computer, Control, and Management Engineering, Sapienza University of Rome, Italy; Department of Computer Science, Sapienza University of Rome, Italy","IEEE Transactions on Software Engineering","","2014","40","12","1185","1205","In this article we present a building block technique and a toolkit towards automatic discovery of workload-dependentperformance bottlenecks. From one or more runs of a program, our profiler automatically measures how the performance of individual routines scales as a function of the input size, yielding clues to their growth rate. The output of the profiler is, for each executed routine of the program, a set of tuples that aggregate performance costs by input size. The collected profiles can be used to produceperformance plots and derive trend functions by statistical curve fitting techniques. A key feature of our method is the ability toautomatically measure the size of the input given to a generic code fragment: to this aim, we propose an effective metric for estimating the input size of a routine and show how to compute it efficiently. We discuss several examples, showing that our approach can reveal asymptotic bottlenecks that other profilers may fail to detect and can provide useful characterizations of the workload and behavior of individual routines in the context of mainstream applications, yielding several code optimizations as well as algorithmic improvements. To prove the feasibility of our techniques, we implemented a Valgrind tool called aprof and performed an extensive experimentalevaluation on the SPEC CPU2006 benchmarks. Our experiments show that aprof delivers comparable performance to otherprominent Valgrind tools, and can generate informative plots even from single runs on typical workloads for mostalgorithmically-critical routines.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339825","Italian Ministry of Education, University, and Research (MIUR); “AMANDA-Algorithmics for MAssive and Networked DAta”; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6858059","Performance profiling;asymptotic analysis;dynamic program analysis;instrumentation","Context modeling;Algorithm design and analysis;Market research;Benchmark testing","curve fitting;program diagnostics;software performance evaluation;software tools;statistical analysis","input-sensitive profiling;building block technique;automatic workload-dependent performance bottleneck discovery;growth rate;program executed routine;tuples;performance plots;statistical curve fitting techniques;generic code fragment;code optimizations;aprof;experimental evaluation;SPEC CPU2006 benchmarks;Valgrind tools","","5","","60","","","","","","IEEE","IEEE Journals & Magazines"
"NLP-KAOS for Systems Goal Elicitation: Smart Metering System Case Study","E. Casagrande; S. Woldeamlak; W. L. Woon; H. H. Zeineldin; D. Svetinovic","Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE; Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE; Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE; Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE; Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE","IEEE Transactions on Software Engineering","","2014","40","10","941","956","This paper presents a computational method that employs Natural Language Processing (NLP) and text mining techniques to support requirements engineers in extracting and modeling goals from textual documents. We developed a NLP-based goal elicitation approach within the context of KAOS goal-oriented requirements engineering method. The hierarchical relationships among goals are inferred by automatically building taxonomies from extracted goals. We use smart metering system as a case study to investigate the proposed approach. Smart metering system is an important subsystem of the next generation of power systems (smart grids). Goals are extracted by semantically parsing the grammar of goal-related phrases in abstracts of research publications. The results of this case study show that the developed approach is an effective way to model goals for complex systems, and in particular, for the research-intensive complex systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339811","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6857327","Requirements engineering;goal elicitation;NLP;data mining;bibliometrics","Data mining;Taxonomy;Ontologies;Data models;Natural language processing;Abstracts;Data collection","data mining;formal specification;natural language processing;smart meters","NLP-KAOS;systems goal elicitation;smart metering system case study;computational method;natural language processing;text mining techniques;requirements engineers;textual documents;NLP-based goal elicitation approach;KAOS goal-oriented requirements engineering method;hierarchical relationships;power systems;smart grids;goal-related phrases;research publications;complex systems","","10","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Observation-Enhanced QoS Analysis of Component-Based Systems","C. A. Paterson; R. Calinescu","Computer Science, University of York, York, North Yorkshire United Kingdom of Great Britain and Northern Ireland YO10 5GH (e-mail: colin.paterson@york.ac.uk); Computer Science, University of York, York, North Yorkshire United Kingdom of Great Britain and Northern Ireland (e-mail: radu.calinescu@york.ac.uk)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","We present a new method for the accurate analysis of the quality-of-service (QoS) properties of component-based systems. Our method takes as input a QoS property of interest and a high-level continuous-time Markov chain (CTMC) model of the analysed system, and refines this CTMC based on observations of the execution times of the system components. The refined CTMC can then be analysed with existing probabilistic model checkers to accurately predict the value of the QoS property. The paper describes the theoretical foundation underlying this model refinement, the tool we developed to automate it, and two case studies that apply our QoS analysis method to a service-based system implemented using public web services and to an IT support system at a large university, respectively. Our experiments show that traditional CTMC-based QoS analysis can produce highly inaccurate results and may lead to invalid engineering and business decisions. In contrast, our new method reduced QoS analysis errors by 84.4-89.6% for the service-based system and by 94.7-97% for the IT support system, significantly lowering the risk of such invalid decisions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2864159","Defence Science and Technology Laboratory; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8428471","Quality of service;component-based systems;Markov models;probabilistic model checking","Quality of service;Unified modeling language;Analytical models;Markov processes;Probabilistic logic;Component architectures","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Adaptive programming","M. G. Gouda; T. Herman","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; Dept. of Comput. Sci., Texas Univ., Austin, TX, USA","IEEE Transactions on Software Engineering","","1991","17","9","911","921","An adaptive program is one that changes its behavior base on the current state of its environment. This notion of adaptivity is formalized, and a logic for reasoning about adaptive programs is presented. The logic includes several composition operators that can be used to define an adaptive program in terms of given constituent programs; programs resulting from these compositions retain the adaptive properties of their constituent programs. The authors begin by discussing adaptive sequential programs, then extend the discussion to adaptive distributed programs. The relationship between adaptivity and self-stabilization is discussed. A case study for constructing an adaptive distributed program where a token is circulated in a ring of processes is presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92911","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92911","","Adaptive systems;Logic;Parallel programming;Programming theory","adaptive systems;formal logic;parallel programming;programming theory","token ring networks;adaptivity;composition operators;constituent programs;adaptive sequential programs;adaptive distributed programs;self-stabilization","","39","","12","","","","","","IEEE","IEEE Journals & Magazines"
"An Application of Structural Modeling to Software Requirements Analysis and Design","K. Matsumura; H. Mizutani; M. Arai","Systems and Software Engineering Division, Toshiba Corporation; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","4","461","471","In software development, it has been pointed out that software engineers must pay attention to software requirements definition. One of the important problems in software engineering is to rationalize the processes from requirements definition to design. Computer tools are most useful and efficient for this purpose. This paper proposes a computer-aided software design system (CASDS), which supports software engineers with a series of structural modeling. As is well-known in systems planning, structural modeling helps to extract concepts from many fuzzy requirements. This system contains three structural modeling methods. They are used 1) to determine functional terms from fuzzy software requirements, 2) to obtain modules by structuring the functions with respect to the data flows, and 3) to make a program skeleton by imposing control flows on the functional elements obtained by breaking down the modules.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233182","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702237","Clustering;computer-aided tools;control flow;data flow;design methodology;graph theory;module;requirements analysis;structural modeling;system planning","Application software;Software design;Skeleton;Software engineering;Software tools;Programming;Design engineering;Data mining;Fuzzy systems;Fuzzy control","","Clustering;computer-aided tools;control flow;data flow;design methodology;graph theory;module;requirements analysis;structural modeling;system planning","","6","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Improving Source Code Lexicon via Traceability and Information Retrieval","A. De Lucia; M. Di Penta; R. Oliveto","University of Salerno, Fisciano; University of Sannio, Benevento; University of Molise, Pesche","IEEE Transactions on Software Engineering","","2011","37","2","205","227","The paper presents an approach helping developers to maintain source code identifiers and comments consistent with high-level artifacts. Specifically, the approach computes and shows the textual similarity between source code and related high-level artifacts. Our conjecture is that developers are induced to improve the source code lexicon, i.e., terms used in identifiers or comments, if the software development environment provides information about the textual similarity between the source code under development and the related high-level artifacts. The proposed approach also recommends candidate identifiers built from high-level artifacts related to the source code under development and has been implemented as an Eclipse plug-in, called COde Comprehension Nurturant Using Traceability (COCONUT). The paper also reports on two controlled experiments performed with master's and bachelor's students. The goal of the experiments is to evaluate the quality of identifiers and comments (in terms of their consistency with high-level artifacts) in the source code produced when using or not using COCONUT. The achieved results confirm our conjecture that providing the developers with similarity between code and high-level artifacts helps to improve the quality of source code lexicon. This indicates the potential usefulness of COCONUT as a feature for software development environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5601742","Software traceability;source code comprehensibility;source code identifier quality;information retrieval;software development environments;empirical software engineering.","Documentation;Programming;Large scale integration;Semantics;Software quality;Couplings","information retrieval;program diagnostics;software quality","source code lexicon;information retrieval;textual similarity;high level artifact;software development;candidate identifier;code comprehension nurturant using traceability;COCONUT;bachelor student;master student","","28","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Likely Analogical APIs across Third-Party Libraries via Large-Scale Unsupervised API Semantics Embedding","C. Chen; Z. Xing; Y. Liu; K. L. X. Ong","Faculty of Information Technology, Monash University, Melbourne, Victoria Australia (e-mail: chunyang.chen@monash.edu); Research School of Computer Sciecne, Australian National University, Canberra, Australian Capital Territory Australia (e-mail: zhenchang.xing@anu.edu.au); School of Computer Engineering, Nanyang Technological University, Singapore, Singapore Singapore 639798 (e-mail: yangliu@ntu.edu.sg); School of Computer Science and Engineering, Nanayng Technological University, Singapore, Singapore Singapore (e-mail: kent0002@e.ntu.edu.sg)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Establishing API mappings between third-party libraries is a prerequisite step for library migration tasks. Manually establishing API mappings is tedious due to the large number of APIs to be examined. Having an automatic technique to create a database of likely API mappings can significantly ease the task. Unfortunately, existing techniques either adopt supervised learning mechanism that requires already-ported or functionality similar applications across major programming languages or platforms, which are difficult to come by for an arbitrary pair of third-party libraries, or cannot deal with lexical gap in the API descriptions of different libraries. To overcome these limitations, we present an unsupervised deep learning based approach to embed both API usage semantics and API description (name and document) semantics into vector space for inferring likely analogical API mappings between libraries. Based on deep learning models trained using tens of millions of API call sequences, method names and comments of 2.8 millions of methods from 135,127 GitHub projects, our approach significantly outperforms other deep learning or traditional information retrieval (IR) methods for inferring likely analogical APIs. We implement a proof-of-concept website which can recommend analogical APIs for 583,501 APIs of 111 pairs of analogical Java libraries with diverse functionalities. This scale of third-party analogical-API database has never been achieved before.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2896123","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8630054","Analogical API;Word embedding;Skip thoughts","Libraries;Semantics;Databases;Task analysis;Recurrent neural networks;Deep learning;Java","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"The Semantic Database Constructor","D. B. Farmer; R. King; D. A. Myers","Interactive Systems Corporation; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","7","583","591","Sedaco (the semantic database constructor) is a tool which may be used to implement databases. It is based on a semantic data model. Sedaco provides primitives for implementing semantic schemas and buffers the database designer from most low-level data structuring issues. Sedaco also efficiently maintains consistency within complex semantic databases. The tool is written in C and runs under Unix.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232502","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702062","Database implementation;database physical design;semantic implementation;update propagation","Object oriented modeling;Relational databases;Transaction databases;Data models;Object oriented databases;Documentation;Indexes;Interactive systems;Computer science;Application software","","Database implementation;database physical design;semantic implementation;update propagation","","3","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Covering arrays for efficient fault characterization in complex configuration spaces","C. Yilmaz; M. B. Cohen; A. A. Porter","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","1","20","34","Many modern software systems are designed to be highly configurable so they can run on and be optimized for a wide variety of platforms and usage scenarios. Testing such systems is difficult because, in effect, you are testing a multitude of systems, not just one. Moreover, bugs can and do appear in some configurations, but not in others. Our research focuses on a subset of these bugs that are ""option-related""-those that manifest with high probability only when specific configuration options take on specific settings. Our goal is not only to detect these bugs, but also to automatically characterize the configuration subspaces (i.e., the options and their settings) in which they manifest. To improve efficiency, our process tests only a sample of the configuration space, which we obtain from mathematical objects called covering arrays. This paper compares two different kinds of covering arrays for this purpose and assesses the effect of sampling strategy on fault characterization accuracy. Our results strongly suggest that sampling via covering arrays allows us to characterize option-related failures nearly as well as if we had tested exhaustively, but at a much lower cost. We also provide guidelines for using our approach in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583600","Software testing;distributed continuous quality assurance;fault characterization;covering arrays.","System testing;Sampling methods;Computer bugs;Software systems;Costs;Quality assurance;Predictive models;Software design;Design optimization;Guidelines","program testing;program debugging;software fault tolerance;configuration management","covering arrays;fault characterization;complex configuration spaces;software system testing;software system bug detection;option-related failure characterization;distributed continuous quality assurance","","123","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Synthesis of Partial Behavior Models from Properties and Scenarios","S. Uchitel; G. Brunet; M. Chechik","Imperial College London and FCEN-University of Buenos Aires; University of Toronto, Toronto; University of Toronto, Toronto","IEEE Transactions on Software Engineering","","2009","35","3","384","406","Synthesis of behavior models from software development artifacts such as scenario-based descriptions or requirements specifications helps reduce the effort of model construction. However, the models favored by existing synthesis approaches are not sufficiently expressive to describe both universal constraints provided by requirements and existential statements provided by scenarios. In this paper, we propose a novel synthesis technique that constructs behavior models in the form of modal transition systems (MTS) from a combination of safety properties and scenarios. MTSs distinguish required, possible, and proscribed behavior, and their elaboration not only guarantees the preservation of the properties and scenarios used for synthesis but also supports further elicitation of new requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.107","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4721439","Modal transition systems;merge;synthesis;partial behavior models.","Computer Society;Analytical models;Programming;Safety;Process design;Buildings;Animation;Upper bound;Educational institutions;Computer science","formal specification","partial behavior model;software development;modal transition system;safety properties","","59","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Identification of Software System Differences","B. Steinholtz; K. Walden","ENEA DATA Svenska AB, Box 232, S-183 23 Taby, Sweden. While performing this work, they were also with SYSLAB, Department of Information Processing and Computer Science, University of Stockholm; NA","IEEE Transactions on Software Engineering","","1987","SE-13","4","493","497","A well-known method for software version control on the source file level is the incremental change technique, which isolates the differences, commonly called a delta, between two versions of a source module, and then stores only the delta. Any version of a source module can then be reconstructed from the original by successive application of deltas. The significant advantages of this approach include accuracy (because it is automatic), visibility (because it highlights differences), and economy (because it conserves storage space).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233186","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702241","Difference isolation;software configuration management;software control system;software database;version control","Software systems;Control systems;Computer languages;Flowcharts;Documentation;Shape;Algebra;Man machine systems;Functional programming;Automatic control","","Difference isolation;software configuration management;software control system;software database;version control","","","","13","","","","","","IEEE","IEEE Journals & Magazines"
"A logic-based approach to reverse engineering tools production","G. Canfora; A. Cimitile; U. de Carlini","Dipatimento di Inf. e Sistemistica, Naples Univ., Italy; Dipatimento di Inf. e Sistemistica, Naples Univ., Italy; Dipatimento di Inf. e Sistemistica, Naples Univ., Italy","IEEE Transactions on Software Engineering","","1992","18","12","1053","1064","Difficulties arising in the use of documents produced by reverse engineering tools are analyzed. With reference to intermodular data flow analysis for Pascal software systems, an interactive and evolutionary tool is proposed. The tool is based on the production of intermodular data flow information by static analysis of code, its representation in a Prolog program dictionary, and a Prolog abstractor that allows the specific queries to be answered.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.184760","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=184760","","Reverse engineering;Production;Data analysis;Software systems;Information analysis;Dictionaries","logic programming;software maintenance;software tools","interactive tool;query answering;reverse engineering tools;intermodular data flow analysis;Pascal software systems;evolutionary tool;static analysis;Prolog program dictionary;Prolog abstractor","","35","","44","","","","","","IEEE","IEEE Journals & Magazines"
"How effective developers investigate source code: an exploratory study","M. P. Robillard; W. Coelho; G. C. Murphy","Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada; NA; NA","IEEE Transactions on Software Engineering","","2004","30","12","889","903","Prior to performing a software change task, developers must discover and understand the subset of the system relevant to the task. Since the behavior exhibited by individual developers when investigating a software system is influenced by intuition, experience, and skill, there is often significant variability in developer effectiveness. To understand the factors that contribute to effective program investigation behavior, we conducted a study of five developers performing a change task on a medium-size open source system. We isolated the factors related to effective program investigation behavior by performing a detailed qualitative analysis of the program investigation behavior of successful and unsuccessful developers. We report on these factors as a set of detailed observations, such as evidence of the phenomenon of inattention blindness by developers skimming source code. In general, our results support the intuitive notion that a methodical and structured approach to program investigation is the most effective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.101","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377187","Index Terms- Software evolution;empirical software engineering;program investigation;program understanding.","Software systems;Performance analysis;Programming;Software tools;Computer Society;Software performance;Blindness;Software engineering;Scattering;Inspection","software prototyping;reverse engineering;public domain software;open systems;programming environments","source code;software system investigation;program investigation behavior;medium-size open source system;software evolution;empirical software engineering;program understanding","","95","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Probabilistic Interface Automata","E. Pavese; V. Braberman; S. Uchitel","Departamento de Computación, Universidad de Buenos Aires; Departamento de Computación, Universidad de Buenos Aires and CONICET; Departamento de Computación, Universidad de Buenos Aires; Imperial College London and CONICET","IEEE Transactions on Software Engineering","","2016","42","9","843","865","System specifications have long been expressed through automata-based languages, which allow for compositional construction of complex models and enable automated verification techniques such as model checking. Automata-based verification has been extensively used in the analysis of systems, where they are able to provide yes/no answers to queries regarding their temporal properties. Probabilistic modelling and checking aim at enriching this binary, qualitative information with quantitative information, more suitable to approaches such as reliability engineering. Compositional construction of software specifications reduces the specification effort, allowing the engineer to focus on specifying individual component behaviour to then analyse the composite system behaviour. Compositional construction also reduces the validation effort, since the validity of the composite specification should be dependent on the validity of the components. These component models are smaller and thus easier to validate. Compositional construction poses additional challenges in a probabilistic setting. Numerical annotations of probabilistically independent events must be contrasted against estimations or measurements, taking care of not compounding this quantification with exogenous factors, in particular the behaviour of other system components. Thus, the validity of compositionally constructed system specifications requires that the validated probabilistic behaviour of each component continues to be preserved in the composite system. However, existing probabilistic automata-based formalisms do not support specification of non-deterministic and probabilistic component behaviour which, when observed through logics such as pCTL, is preserved in the composite system. In this paper we present a probabilistic extension to Interface Automata which preserves pCTL properties under probabilistic fairness by ensuring a probabilistic branching simulation between component and composite automata. The extension not only supports probabilistic behaviour but also allows for weaker prerequisites to interfacing composition, that supports delayed synchronisation that may be required because of internal component behaviour. These results are equally applicable as an extension to non-probabilistic Interface Automata.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2527000","ANPCyT PICT; ANPCyT PICT; ANPCyT PICT; CONICET PIP; CONICET PIP; UBACYT; UBACYT; MEALS; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7401103","Behaviour models;probability;interface automata;model checking","Probabilistic logic;Automata;Interconnected systems;Computational modeling;Model checking;Semantics;Synchronization","formal specification;probabilistic automata;program verification","probabilistic interface automata;system specifications;automata-based languages;compositional construction;complex models;automated verification techniques;model checking;automata-based verification;temporal properties;probabilistic modelling;probabilistic checking;binary qualitative information;quantitative information;reliability engineering;software specifications;component behaviour;composite system behaviour;numerical annotations;probabilistically independent events;exogenous factors;system components;compositionally constructed system specifications;nondeterministic behaviour;probabilistic component behaviour;pCTL;probabilistic fairness;probabilistic branching simulation;composite automata;component automata;internal component behaviour;nonprobabilistic interface automata","","","","42","","","","","","IEEE","IEEE Journals & Magazines"
"A Probabilistic Analysis of the Efficiency of Automated Software Testing","M. Böhme; S. Paul","Software Engineering Chair, Germany; School of Computing, National University of Singapore, Singapore","IEEE Transactions on Software Engineering","","2016","42","4","345","360","We study the relative efficiencies of the random and systematic approaches to automated software testing. Using a simple but realistic set of assumptions, we propose a general model for software testing and define sampling strategies for random (R) and systematic (S<sub>0</sub>) testing, where each sampling is associated with a sampling cost: 1 and c units of time, respectively. The two most important goals of software testing are: (i) achieving in minimal time a given degree of confidence x in a program's correctness and (ii) discovering a maximal number of errors within a given time bound n̂. For both (i) and (ii), we show that there exists a bound on c beyond which R performs better than S<sub>0</sub>on the average. Moreover for (i), this bound depends asymptotically only on x. We also show that the efficiency of R can be fitted to the exponential curve. Using these results we design a hybrid strategy H that starts with R and switches to S<sub>0</sub>when S<sub>0</sub>is expected to discover more errors per unit time. In our experiments we find that H performs similarly or better than the most efficient of both and that S<sub>0</sub>may need to be significantly faster than our bounds suggest to retain efficiency over R.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2487274","Singapore's Ministry of Education; ERC; SPECMATE; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7289448","Partition Testing;Random Testing;Error-based Partitioning;Efficient Testing;Testing Theory;Partition testing;random testing;error-based partitioning;efficient testing;testing theory","Systematics;Software testing;Software engineering;Input variables;Random variables;Color","probability;program testing","probabilistic analysis;automated software testing;sampling strategies;random testing;systematic testing;exponential curve;hybrid strategy","","7","","37","","","","","","IEEE","IEEE Journals & Magazines"
"An enhanced neural network technique for software risk analysis","D. E. Neumann","Gen. Dynamics Land Syst., Warren, MI, USA","IEEE Transactions on Software Engineering","","2002","28","9","904","912","An enhanced technique for risk categorization is presented. This technique, PCA-ANN, provides an improved capability to discriminate high-risk software. The approach draws on the combined strengths of pattern recognition, multivariate statistics and neural networks. Principal component analysis is utilized to provide a means of normalizing and orthogonalizing the input data, thus eliminating the ill effects of multicollinearity. A neural network is used for risk determination/classification. A significant feature of this approach is a procedure, herein termed cross-normalization. This procedure provides the technique with capability to discriminate data sets that include disproportionately large numbers of high-risk software modules.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033229","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033229","","Neural networks;Risk analysis;Government;Costs;Principal component analysis;Mathematical model;Predictive models;Programming;Contracts;Pattern recognition","neural nets;risk management;principal component analysis;pattern recognition;statistics;software engineering","enhanced neural network technique;software risk analysis;risk categorization;pattern recognition;multivariate statistics;principal component analysis;input data normalization;multicollinearity;cross-normalization;high-risk software modules","","59","","36","","","","","","IEEE","IEEE Journals & Magazines"
"A Priority Based Distributed Deadlock Detection Algorithm","M. K. Sinha; N. Natarajan","National Centre for Software Development and Computing Techniques, Tata Institute of Fundamental Research; NA","IEEE Transactions on Software Engineering","","1985","SE-11","1","67","80","Deadlock handling is an important component of transaction management in a database system. In this paper, we contribute to the development of techniques for transaction management by presenting an algorithm for detecting deadlocks in a distributed database system. The algorithm uses priorities for transactions to minimize the number of messages initiated for detecting deadlocks. It does not construct any wait-for graph but detects cycles by an edge-chasing method. It does not detect any phantom deadlock (in the absence of failures), and for the resolution of deadlocks it does not need any extra computation. The algorithm also incorporates a post-resolution computation that leaves information characterizing dependence relations of remaining transactions of the deadlock cycle in the system, and this will help in detecting and resolving deadlocks which may arise in the future. An interesting aspect of this algorithm is that it is possible to compute the exact number of messages generated for a given deadlock configuration. The complexity is comparable to the best algorithm reported. We first present a basic algorithm and then extend it to take into account shared and exclusive lock modes, simultaneous acquisition of multiple locks, and nested transactions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231844","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701899","Deadlock;deadlock detection;distributed database;nested transaction;priority;timestamp;transaction","System recovery;Detection algorithms;Database systems;Transaction databases;Imaging phantoms;Laser mode locking;Distributed databases;Programming;Telecommunication network reliability","","Deadlock;deadlock detection;distributed database;nested transaction;priority;timestamp;transaction","","40","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Compiling real-time programs with timing constraint refinement and structural code motion","R. Gerber; Seongsoo Hong","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1995","21","5","389","404","We present a programming language called TCEL (Time-Constrained Event Language), whose semantics are based on time-constrained relationships between observable events. Such a semantics infers only those timing constraints necessary to achieve real-time correctness, without overconstraining the system. Moreover, an optimizing compiler can exploit this looser semantics to help tune the code, so that its worst-case execution time is consistent with its real-time requirements. In this paper we describe such a transformation system, which works in two phases. First, the TCEL source code is translated into an intermediate representation. Then an instruction-scheduling algorithm rearranges selected unobservable operations and synthesizes tasks guaranteed to respect the original event-based constraints.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.387469","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=387469","","Timing;Computer languages;Equations;Optimizing compilers;Program processors;Delay;Scheduling algorithm;Motion analysis;Real time systems;Time factors","real-time systems;high level languages;optimising compilers;scheduling;program interpreters;programming theory","real-time program compilation;timing constraint refinement;structural code motion;programming language;TCEL;Time-Constrained Event Language;time-constrained relationships;observable events;real-time correctness;optimizing compiler;worst-case execution time;real-time requirements;instruction-scheduling algorithm;event-based constraints","","14","","28","","","","","","IEEE","IEEE Journals & Magazines"
"On Event-Based Middleware for Location-Aware Mobile Applications","R. Meier; V. Cahill","Trinity College Dublin, Dublin and Lero&#x02014;The Irish Software Engineering Research Centre; Trinity College Dublin, Dublin and Lero&#x02014;The Irish Software Engineering Research Centre","IEEE Transactions on Software Engineering","","2010","36","3","409","430","As mobile applications become more widespread, programming paradigms and middleware architectures designed to support their development are becoming increasingly important. The event-based programming paradigm is a strong candidate for the development of mobile applications due to its inherent support for the loose coupling between components required by mobile applications. However, existing middleware that supports the event-based programming paradigm is not well suited to supporting location-aware mobile applications in which highly mobile components come together dynamically to collaborate at some location. This paper presents a number of techniques including location-independent announcement and subscription coupled with location-dependent filtering and event delivery that can be used by event-based middleware to support such collaboration. We describe how these techniques have been implemented in STEAM, an event-based middleware with a fully decentralized architecture, which is particularly well suited to deployment in ad hoc network environments. The cost of such location-based event dissemination and the benefits of distributed event filtering are evaluated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374426","Distributed systems;middleware;publish subscribe;event-based communication;mobile computing;collaborative and location-aware applications;wireless ad hoc networks.","Middleware;Collaboration;Mobile computing;Application software;Mobile communication;Unmanned aerial vehicles;Pervasive computing;Computer architecture;Filtering;Ad hoc networks","information dissemination;middleware;mobile computing;software architecture","event-based middleware;location-aware mobile applications;event-based programming paradigm;location-independent announcement;location-dependent filtering;STEAM;decentralized architecture;ad hoc network environments;location-based event dissemination;distributed event filtering","","15","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Two-dimensional program design","S. Rutenstreich; W. E. Howden","Department of Electrical Engineering and Computer Science, the George Washington University, Washington, DC 20052; Department of Electrical Engineering and Computer Science, University of California, San Diego, CA 92093","IEEE Transactions on Software Engineering","","1986","SE-12","3","377","384","One-dimensional design methods focus on one way of looking at data and control relationships in a design. Structured design concentrates on top-down, or vertical relationships. Data flow diagrams concentrate on horizontal relationships, in which the output from one computation flows to the next computation to be done. Two-dimensional design allows the simultaneous consideration of both the functional abstraction of vertical designs and the complex data sharing of horizontal designs through the use of functional and environmental models. The basic defects of one-dimensional approaches are described, followed by a description of a two-dimensional method. The method includes guidelines for transforming a one-dimensional design to a two-dimensional design.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312880","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312880","Data flow;design;modules;structured design","Data models;Abstracts;Object oriented modeling;Computational modeling;Patient monitoring;Analytical models;System analysis and design","software engineering","software engineering;structured design;program design;control relationships;vertical relationships;horizontal relationships;functional abstraction;vertical designs;complex data sharing;environmental models;two-dimensional design","","4","","","","","","","","IEEE","IEEE Journals & Magazines"
"L.0: a truly concurrent executable temporal logic language for protocols","L. Ness","Bellcore, Morristown, NJ, USA","IEEE Transactions on Software Engineering","","1993","19","4","410","423","The semantics L.0, a programming language designed for the specification and simulation of protocols that assumes a true concurrency model, is given in terms of predicate linear temporal logic, and the restricted universe of models assumed in L.0 programs is defined. The execution algorithm for L.0 constructs a model in this universe. The restricted subset of temporal logic exploited permits a nonbacktracking execution algorithm. Fundamental to the semantics of L.0 is a frame assumption, which generalizes the frame assumption of standard imperative programming, and which eases specification of protocols. The data domain assumed in L.0 programs is sets of trees with labeled edges, and the state predicates permitted include existence and nonexistence predicates, as well as the more traditional assignment and equality predicates. These choices for data domain and predicates permit convenient specification of the hierarchical message structure often assumed in telecommunications protocols, for in such message structures, the existence or nonexistence of parts of the message hierarchy is determined by logical properties of the rest of the message hierarchy. A small portion of the logical layer specification of Futurebus+ is taken as the main example in this study.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.223807","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=223807","","Concurrent computing;Access protocols;Logic programming;Interleaved codes;Computer languages;Logic design;Transport protocols;Multimedia communication;Broadcasting","protocols;simulation languages;specification languages;temporal logic","L.0;truly concurrent executable temporal logic language;protocols;specification;simulation;true concurrency model;execution algorithm;standard imperative programming;state predicates;equality predicates;hierarchical message structure;Futurebus+","","2","","46","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of software project bidding","M. Jorgensen; G. J. Carelius","Simula Res. Lab., L.Ysaker, Norway; Simula Res. Lab., L.Ysaker, Norway","IEEE Transactions on Software Engineering","","2004","30","12","953","969","The study described in this paper reports from a real-life bidding process in which 35 companies were bidding for the same contract. The bidding process consisted of two separate phases: a prestudy phase and a bidding phase. In the prestudy phase, 17 of the 35 bidding companies provided rough price indications based on a brief, incomplete description of user requirements. In the bidding phase, all 35 companies provided bids based on a more complete requirement specification that described a software system with substantially more functionality than the system indicated in the prestudy phase. The main result of the study is that the 17 companies involved in the prestudy phase presented bids that were, on average, about 70 percent higher than the bids of the other companies, although all companies based their bids on the same requirement specification. We propose an explanation for this difference that is consistent with the ""prospect theory"" and the ""precautionary bidding effect."" A possible implication of our findings is that software clients should not request early price indications based on limited and uncertain information when the final bids can be based on more complete and reliable information.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.92","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377191","Index Terms- Cost estimation;risk management;software psychology.","Costs;Contracts;Software systems;Reliability theory;Risk management;Psychology;Companies;Guidelines;Software quality;Scheduling","software management;project management;DP industry;contracts;formal specification;software cost estimation;risk management","software project bidding;prestudy phase;bidding phase;software system;requirement specification;software client","","24","","19","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of evaluating software development environment quality","T. Miyoshi; M. Azuma","Joint Syst. Dev. Corp., Tokyo, Japan; NA","IEEE Transactions on Software Engineering","","1993","19","5","425","435","A study that evaluates new-paradigm-oriented software development environments which have been developed in the five-year formal approach to software environment technology (FASET) project is reviewed. For this study, a software environment evaluation technology based on a software quality evaluation process model defined in ISO/IEC 9126 has been developed. The evaluation technology has been applied to the R&D project at the middle and final phase of development. The evaluation results provide useful information to develop a widely acceptable evaluation technology and to improve the new-paradigm-oriented software development environments that are based on various specification methods: the algebraic specification method, function-oriented specification method, declarative specification method, natural-language-oriented specification method, diagrammatic specification method, state-transition-oriented specification method, and model-based specification method.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232010","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232010","","Programming;Software quality;Software tools;Software engineering;IEC standards;ISO standards;Research and development;Formal specifications;International trade;Computer industry","formal specification;programming environments;software quality","software development environment quality;FASET;software quality evaluation process model;ISO/IEC 9126;algebraic specification method;function-oriented specification;declarative specification;natural-language-oriented specification;diagrammatic specification;state-transition-oriented;specification;model-based specification","","16","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Coordinating multiagent applications on the WWW: a reference architecture","P. Ciancarini; R. Tolksdorf; F. Vitali; D. Rossi; A. Knoche","Dept. of Comput. Sci., Bologna Univ., Italy; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","5","362","375","The original Web did not support multiuser, interactive applications. This shortcoming is being studied, and several approaches have been proposed to use the Web as a platform for programming Internet applications. However, most existing approaches are oriented to centralized applications at servers, or local programs within clients. To overcome this deficit, we introduce PageSpace, that is a reference architecture for designing interactive multiagent applications. We describe how we control agents in PageSpace, using variants of the coordination language Linda to guide their interactions. Coordination technology is integrated with the standard Web technology and the programming language Java. Several kinds of agents live in the PageSpace: user interface agents, personal homeagents, agents that implement applications, and agents which interoperate with legacy systems. Within our architecture, it is possible to support fault-tolerance and mobile agents as well.","0098-5589;1939-3520;2326-3881","","10.1109/32.685259","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=685259","","World Wide Web;Java;Application software;Service oriented architecture;Internet;Computer architecture;Electronic commerce;Computer Society;Web server;Computer languages","Internet;software agents;cooperative systems;parallel languages;object-oriented languages;open systems;client-server systems;interactive systems;user interfaces;software fault tolerance","multiagent application coordination;reference architecture;World Wide Web;multiuser interactive applications;programming;Internet applications;centralized applications;client server systems;PageSpace;coordination language;Linda;coordination technology;Java;user interface agents;personal homeagents;legacy systems;fault tolerance;mobile agents;open distributed systems","","52","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Some inference rules for integer arithmetic for verification of flowchart programs on integers","D. C. Sarkar; S. C. De Sarkar","Indian Inst. of Technol., Kharagpur, India; Indian Inst. of Technol., Kharagpur, India","IEEE Transactions on Software Engineering","","1989","15","1","1","9","Significant modifications of the first-order rules have been developed so that they can be applied directly to algebraic expressions. The importance and implication of normalization of formulas in any theorem prover are discussed. It is shown how the properties of the domain of discourse have been taken care of either by the normalizer or by the inference rules proposed. Using a nontrivial example, the following capabilities of the verifier that would use these inference rules are highlighted: (1) closeness of the proof construction process to the human thought process; and (2) efficient handling of user provided axioms. Such capabilities make interfacing with humans easy.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21720","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21720","","Arithmetic;Flowcharts;Humans;Logic;Heart;Calculus;Virtual colonoscopy;Computer science","inference mechanisms;program verification;theorem proving","inference rules;integer arithmetic;verification;flowchart programs;first-order rules;algebraic expressions;theorem prover;proof construction process;human thought process;user provided axioms","","9","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Static analysis of real-time distributed systems","L. Y. Liu; R. K. Shyamasundar","Dept. of Comput. Sci., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci., Pennsylvania State Univ., University Park, PA, USA","IEEE Transactions on Software Engineering","","1990","16","4","373","388","A static analysis for reasoning about the temporal behaviors of programs in real-time distributed programming languages is proposed. The analysis is based on the action set semantics using the pure maximal parallelism model. It is shown how to specify and verify various timing properties of real-time programs. The approach provides only an approximate timing behavior, because the state information is ignored. However, many interesting properties such as parallel actions, deadlocks, livelocks, terminations, temporal errors, and failures, can be identified. Furthermore, the approach is compositional and thus makes it possible to reason about the timing properties incrementally. The method not only leads to efficient algorithms for the static analysis of CSP programs but also applies to many other languages.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.54290","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=54290","","Real time systems;Merging;Timing;Algorithm design and analysis;Computer science;Concurrent computing;History;Low earth orbit satellites;System recovery;Automation","distributed processing;parallel programming;programming languages;real-time systems;software engineering","real-time distributed systems;static analysis;reasoning;temporal behaviors;programs;maximal parallelism model;timing properties;parallel actions;deadlocks;livelocks;terminations;temporal errors;failures;CSP programs","","21","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling and improving an industrial software process","S. Bandinelli; A. Fuggetta; L. Lavazza; M. Loi; G. P. Picco","CEFRIEL, Milan, Italy; CEFRIEL, Milan, Italy; CEFRIEL, Milan, Italy; NA; NA","IEEE Transactions on Software Engineering","","1995","21","5","440","454","The paper discusses the problems that a software development organization must address in order to assess and improve its software processes. In particular, the authors are involved in a project aiming at assessing and improving the current practice and the quality manual of the Business Unit Telecommunications for Defense (BUTD) of a large telecommunications company. The paper reports on the usage of formal process modeling languages to detect inconsistencies, ambiguities, incompleteness, and opportunities for improvement of both the software process and its documentation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.387473","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=387473","","Computer industry;Capability maturity model;Programming;Production;Project management;Continuous improvement;Coordinate measuring machines;Business communication;Telecommunications;Companies","telecommunication computing;programming environments;system documentation;DP industry;software quality","software development organization;industrial software process modelling;industrial software process improvement;current practice;quality manual;Business Unit Telecommunications for Defense;large telecommunications company;formal process modeling languages;inconsistency detection;ambiguity detection;incompleteness detection;documentation","","50","","33","","","","","","IEEE","IEEE Journals & Magazines"
"An effective approach to vertical partitioning for physical design of relational databases","D. W. Cornell; P. S. Yu","Digital Equipment Corp., Littleton, MA, USA; NA","IEEE Transactions on Software Engineering","","1990","16","2","248","258","Vertical partitioning can be used to enhance the performance of relational database systems by reducing the number of disk accesses. The authors identify the key parameters for capturing the behavior of an access plan and propose a two-step methodology consisting of a query analysis step to estimate the parameters and a binary partitioning step which can be applied recursively. The partitioning uses an integer linear programming technique to minimize the number of disk accesses. Significant performance benefit would be achieved for join if the partitioned (inner) relation could fit into the memory buffer under the inner-outer loop join method, or if the partitioned relation could fit into the sort buffer under the sort-merge join method, but not the original relation. For cases where a segment scan or a cluster index scan is used, vertical partitioning of the relation with the algorithm described is still often found to lead to substantial performance improvement.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44388","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44388","","Relational databases;Costs;Partitioning algorithms;Linear programming;Performance analysis;Transaction databases;Information retrieval;Algorithm design and analysis;Frequency;Delay","linear programming;relational databases;software engineering","vertical partitioning;physical design;relational databases;disk accesses;two-step methodology;query analysis;binary partitioning;integer linear programming;join;sort-merge;segment scan;cluster index scan","","36","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Description for a tool specifying and prototyping concurrent programs","N. De Francesco; G. Vaglini","Dept. of Inf., Pisa Univ., Italy; Dept. of Inf., Pisa Univ., Italy","IEEE Transactions on Software Engineering","","1988","14","11","1554","1564","A specification language is introduced, able to define the behavior of concurrent programs. The language is particularly devoted to describing distributed applications, mainly with respect to scheduling problems. For this purpose, the language allows visibility of the past history of a computation and such history may be explicitly used to derive the choices on the future behavior of the computation itself and to define the values exchanged at each communication. A behavior is a partial order on events (communications) accomplished by processes, while the values of the communications are specified by a functional language. The most noticeable characteristic of specifications written in this language is the capability to be easily translated into executable concurrent programs (written into a CSP-like concurrent language), so obtaining an early prototype for these programs. An algorithm is described to accomplish the translation. An environment is provided to support static semantics checks on specifications, while dynamic testing and debugging are accomplished using interactive tools of the concurrent language environment.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9044","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9044","","Prototypes;Specification languages;Processor scheduling;History;Testing;Debugging;Programming environments","automatic programming;parallel programming;program interpreters;programming environments;software tools;specification languages","automatic programming;programming environments;parallel programming;prototyping;specification language;distributed applications;scheduling;functional language;executable concurrent programs;concurrent language;translation;static semantics checks;dynamic testing;debugging;interactive tools","","3","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Software Component Models","K. Lau; Z. Wang","NA; NA","IEEE Transactions on Software Engineering","","2007","33","10","709","724","Component-based development (CBD) is an important emerging topic in software engineering, promising long-sought-after benefits like increased reuse, reduced time to market, and, hence, reduced software production cost. The cornerstone of a CBD technology is its underlying software component model, which defines components and their composition mechanisms. Current models use objects or architectural units as components. These are not ideal for component reuse or systematic composition. In this paper, we survey and analyze current component models and classify them into a taxonomy based on commonly accepted desiderata for CBD. For each category in the taxonomy, we describe its key characteristics and evaluate them with respect to these desiderata.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70726","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302781","software components;software component models;component life-cycle;component syntax;component semantics;component composition","Object oriented modeling;Costs;Taxonomy;Software engineering;Time to market;Software reusability;Application software;Software systems;Production systems;Computer industry","cost reduction;object-oriented programming;software cost estimation","software component models;component-based development;software engineering;software production cost","","145","","63","","","","","","IEEE","IEEE Journals & Magazines"
"High performance software testing on SIMD machines","E. W. Krauser; A. P. Mathur; V. J. Rego","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA","IEEE Transactions on Software Engineering","","1991","17","5","403","423","A method for high-performance, software testing, called mutant unification, is described. The method is designed to support program mutation on parallel machines based on the single instruction multiple data stream (SIMD) paradigm. Several parameters that affect the performance of unification have been identified and their effect on the time to completion of a mutation test cycle and speedup has been studied. Program mutation analysis provides an effective means for determining the reliability of large software systems and a systematic method for measuring the adequacy of test data. However, it is likely that testing large software systems using mutation is computation bound and prohibitive on traditional sequential machines. Current, implementations of mutation tools are unacceptably slow and are only suitable for testing relatively small programs. The proposed unification method provides a practical alternative to the current approaches. The method also opens up a new application domain for SIMD machines.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90444","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=90444","","Software testing;Genetic mutations;Software systems;System testing;Application software;Educational institutions;Software engineering;Parallel machines;Sequential analysis;Life testing","parallel machines;program testing;software reliability","software reliability;software testing;mutant unification;program mutation;parallel machines;single instruction multiple data stream;software systems;SIMD machines","","26","","38","","","","","","IEEE","IEEE Journals & Magazines"
"A Quantitative Investigation of the Acceptable Risk Levels of Object-Oriented Metrics in Open-Source Systems","R. Shatnawi","Jordan University of Science and Technology, Irbid","IEEE Transactions on Software Engineering","","2010","36","2","216","225","Object-oriented metrics have been validated empirically as measures of design complexity. These metrics can be used to mitigate potential problems in the software complexity. However, there are few studies that were conducted to formulate the guidelines, represented as threshold values, to interpret the complexity of the software design using metrics. Classes can be clustered into low and high risk levels using threshold values. In this paper, we use a statistical model, derived from the logistic regression, to identify threshold values for the Chidamber and Kemerer (CK) metrics. The methodology is validated empirically on a large open-source system-the Eclipse project. The empirical results indicate that the CK metrics have threshold effects at various risk levels. We have validated the use of these thresholds on the next release of the Eclipse project-Version 2.1-using decision trees. In addition, the selected threshold values were more accurate than those were selected based on either intuitive perspectives or on data distribution parameters. Furthermore, the proposed model can be exploited to find the risk level for an arbitrary threshold value. These findings suggest that there is a relationship between risk levels and object-oriented metrics and that risk levels can be used to identify threshold effects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383377","Object-oriented programming;product metrics;CK metrics;threshold values;open-source software.","Open source software;Object oriented modeling;Software metrics;Software quality;Software testing;Software design;Predictive models;Quality assurance;Probability;Fault diagnosis","decision trees;object-oriented programming;public domain software;software fault tolerance;software maintenance;software metrics;statistical analysis","object-oriented metrics;open source systems;software complexity;software design;software metrics;statistical model;logistic regression;Chidamber and Kemerer metrics;Eclipse project version 2.1;decision trees;threshold values;data distribution parameters","","44","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Abstract communication model for distributed systems","U. Glasser; Y. Gurevich; M. Veanes","Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada; NA; NA","IEEE Transactions on Software Engineering","","2004","30","7","458","472","In some distributed and mobile communication models, a message disappears in one place and miraculously appears in another. In reality, of course, there are no miracles. A message goes from one network to another; it can be lost or corrupted in the process. Here, we present a realistic but high-level communication model where abstract communicators represent various nets and subnets. The model was originally developed in the process of specifying a particular network architecture, namely, the Universal Plug and Play architecture. But, it is general. Our contention is that every message-based distributed system, properly abstracted, gives rise to a specialization of our abstract communication model. The purpose of the abstract communication model is not to design a new kind of network; rather, it is to discover the common part of all message-based communication networks. The generality of the model has been confirmed by its successful reuse for very different distributed architectures. The model is based on distributed abstract state machines. It is implemented in the specification language AsmL and is used for testing distributed systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318607","Abstract state machines;communication protocols;computer networks;distributed systems;requirement specification;system modeling;testing of distributed systems.","Protocols;Mobile communication;Plugs;Communication networks;Computer architecture;Specification languages;System testing;Programming;Documentation;Context modeling","protocols;computer networks;specification languages;formal specification;open systems;program compilers;message passing","mobile communication models;computer network architecture;Universal Plug and Play architecture;message-based distributed system;abstract communication model;message-based communication networks;distributed architectures;distributed abstract state machines;AsmL specification language;communication protocols;requirement specification;system modeling;distributed communication models","","20","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Communicating real-time state machines","A. C. Shaw","Dept. of Comput. Sci. & Eng., Washington Univ., Seattle, WA, USA","IEEE Transactions on Software Engineering","","1992","18","9","805","816","Communicating real-time state machines (CRSMs), a complete and executable notation for specifying concurrent real-time systems including the monitored and controlled physical environment, are introduced. They are essentially state machines that communicate synchronously in a manner much like the input-output in Hoare's CSP. In addition, CRSMs have a novel and small set of facilities for describing timing properties and accessing real time. The author defines the CRSM language, gives many examples of its use in requirements specification, outlines an algorithm for executing or simulating CRSMs, introduces some techniques for reasoning about the specifications, and discusses some open problems and issues.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.159840","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=159840","","Real time systems;Timing;Clocks;Condition monitoring;Concurrent computing;Turing machines;Automata;Computer science;Computational modeling","communicating sequential processes;finite state machines;formal specification;parallel machines;real-time systems","communicating real-time state machines;executable notation;concurrent real-time systems;controlled physical environment;state machines;CRSMs;timing properties;CRSM language;requirements specification","","81","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient algorithms for the instantiated transitive closure queries","G. Z. Qadah; L. J. Henschen; J. J. Kim","Dept. of Electr. Eng. & Comput. Sci., Northwestern Univ., Evanston, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Northwestern Univ., Evanston, IL, USA; NA","IEEE Transactions on Software Engineering","","1991","17","3","296","309","The performances of several algorithms suitable for processing an important class of recursive queries called the instantiated transitive closure (TC) queries are studied and compared. These algorithms are the wavefront, delta -wavefront, and a generic algorithm called super-TC. During the evaluation of a TC query, the first two algorithms may read a given disk page more than once, whereas super-TC reads the disk page at most once. A comprehensive performance evaluation of these three algorithms using rigorous analytical and simulation models is presented. The study reveals that the relative performance of the algorithms is a strong function of the parameters which characterize the processed TC query and the relation referenced by that query. The superiority of one of the super-TC variants over all of the other presented algorithms is shown.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.75418","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=75418","","Deductive databases;Analytical models;Logic;Virtual colonoscopy;Performance analysis;Algorithm design and analysis;Relational databases;Helium;Intelligent systems;Database systems","database theory;performance evaluation;query languages","database theory;instantiated transitive closure queries;recursive queries;wavefront;delta -wavefront;generic algorithm;super-TC;disk page;performance evaluation;processed TC query","","13","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Two-Phase Assessment Approach to Improve the Efficiency of Refactoring Identification","A. Han; S. Cha","Department of Computer Science and Engineering, Korea University, Sungbuk-gu, Seoul, South Korea; Department of Computer Science and Engineering, Korea University, Sungbuk-gu, Seoul, South Korea","IEEE Transactions on Software Engineering","","2018","44","10","1001","1023","To automate the refactoring identification process, a large number of candidates need to be compared. Such an overhead can make the refactoring approach impractical if the software size is large and the computational load of a fitness function is substantial. In this paper, we propose a two-phase assessment approach to improving the efficiency of the process. For each iteration of the refactoring process, refactoring candidates are preliminarily assessed using a lightweight, fast delta assessment method called the Delta Table. Using multiple Delta Tables, candidates to be evaluated with a fitness function are selected. A refactoring can be selected either interactively by the developer or automatically by choosing the best refactoring, and the refactorings are applied one after another in a stepwise fashion. The Delta Table is the key concept enabling a two-phase assessment approach because of its ability to quickly calculate the varying amounts of maintainability provided by each refactoring candidate. Our approach has been evaluated for three large-scale open-source projects. The results convincingly show that the proposed approach is efficient because it saves a considerable time while still achieving the same amount of fitness improvement as the approach examining all possible candidates.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2731853","Basic Science Research Program; National Research Foundation of Korea (NRF); Ministry of Education; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7990580","Refactoring assessment;refactoring identification;maintainability improvement","Measurement;Couplings;Symmetric matrices;Open source software;Computational efficiency;System analysis and design","iterative methods;public domain software;software maintenance","two-phase assessment approach;refactoring identification process;fitness function;computational load;delta assessment method;lightweight method;delta table;large-scale open-source projects","","","","60","","","","","","IEEE","IEEE Journals & Magazines"
"SubCM: a tool for improved visibility of software change in an industrial setting","H. Volzer; A. MacDonald; B. Atchison; A. Hanlon; P. Lindsay; P. Strooper","Lubeck Univ., Germany; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","10","675","693","Software configuration management is the discipline of managing large collections of software development artefacts from which software products are built. Software configuration management tools typically deal with artefacts at fine levels of granularity - such as individual source code files - and assist with coordination of changes to such artefacts. This paper describes a lightweight tool, designed to be used on top of a traditional file-based configuration management system. The add-on tool support enables users to flexibly define new hierarchical views of product structure, independent of the underlying artefact-repository structure. The tool extracts configuration and change data with respect to the user-defined hierarchy, leading to improved visibility of how individual subsystems have changed. The approach yields a range of new capabilities for build managers, and verification and validation teams. The paper includes a description of our experience using the tool in an organization that builds large embedded software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339278","Index Terms- Software configuration management;software maintenance;verification and validation.","Software tools;Computer industry;Software development management;Programming;Software maintenance;Project management;Testing;Computational Intelligence Society;Australia;Data mining","configuration management;software maintenance;formal verification;embedded systems;software tools","software change;software configuration management;software development artefacts;software products;source code files;file-based configuration management system;add-on tool support;artefact-repository structure;embedded software systems;software verification;software maintenance","","5","","36","","","","","","IEEE","IEEE Journals & Magazines"
"SPARTACAS: automating component reuse and adaptation","B. Morel; P. Alexander","Inf. & Telecommun. Technol., Kansas Univ., Lawrence, KS, USA; Inf. & Telecommun. Technol., Kansas Univ., Lawrence, KS, USA","IEEE Transactions on Software Engineering","","2004","30","9","587","600","A continuing challenge for software designers is to develop efficient and cost-effective software implementations. Many see software reuse as a potential solution; however, the cost of reuse tends to outweigh the potential benefits. The costs of software reuse include establishing and maintaining a library of reusable components, searching for applicable components to be reused in a design, as well as adapting components toward a proper implementation. We introduce SPARTACAS, a framework for automating specification-based component retrieval and adaptation that has been successfully applied to synthesis of software for embedded and digital signal processing systems. Using specifications to abstractly represent implementations allows automated theorem-provers to formally verify logical reusability relationships between specifications. These logical relationships are used to evaluate the feasibility of reusing the implementations of components to implement a problem. Retrieving a component that is a complete match to a problem is rare. It is more common to retrieve a component that partially satisfies the requirements of a problem. Such components have to be adapted. Rather than adapting components at the code level, SPARTACAS adapts the behavior of partial matches by imposing interactions with other components at the architecture level. A subproblem is synthesized that specifies the missing functionality required to complete the problem; the subproblem is used to query the library for components to adapt the partial match. The framework was implemented and evaluated empirically, the results suggest that automated adaptation using architectures successfully promotes software reuse, and hierarchically organizes a solution to a design problem.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324646","Index Terms- Reuse models;formal methods;programmer workbench;reuse library.","Software libraries;Computer architecture;Costs;Acoustical engineering;Design engineering;Software reusability;Programming;Software quality;Software design;Software maintenance","software reusability;software libraries;formal specification;program verification;software architecture;object-oriented programming","software design;cost-effective software implementation;software reuse;reusable component library;specification-based component retrieval;specification-based component adaptation;software synthesis;embedded system;digital signal processing system;automated theorem-provers;formal verification;formal methods;programmer workbench","","24","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Two Studies of Framework-Usage Templates Extracted from Dynamic Traces","A. Heydarnoori; K. Czarnecki; W. Binder; T. T. Bartolomei","University of Lugano, Lugano; University of Waterloo, Waterloo; University of Lugano, Lugano; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2012","38","6","1464","1487","Object-oriented frameworks are widely used to develop new applications. They provide reusable concepts that are instantiated in application code through potentially complex implementation steps such as subclassing, implementing interfaces, and calling framework operations. Unfortunately, many modern frameworks are difficult to use because of their large and complex APIs and frequently incomplete user documentation. To cope with these problems, developers often use existing framework applications as a guide. However, locating concept implementations in those sample applications is typically challenging due to code tangling and scattering. To address this challenge, we introduce the notion of concept-implementation templates, which summarize the necessary concept-implementation steps and identify them in the sample application code, and a technique, named FUDA, to automatically extract such templates from dynamic traces of sample applications. This paper further presents the results of two experiments conducted to evaluate the quality and usefulness of FUDA templates. The experimental evaluation of FUDA with 14 concepts in five widely used frameworks suggests that the technique is effective in producing templates with relatively few false positives and false negatives for realistic concepts by using two sample applications. Moreover, we observed in a user study with 28 programmers that the use of templates reduced the concept-implementation time compared to when documentation was used.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975174","Object-oriented application frameworks;framework comprehension;framework documentation;concept-implementation templates;application programming interface (API);dynamic analysis;concept location;feature identification","Dynamic programming;Feature extraction;Documentation;Java;Application programming interfaces;Runtime","application program interfaces;object-oriented methods","framework-usage template extraction;dynamic traces;object-oriented frameworks;application code;subclassing operation;interface implementation;calling framework operations;user documentation;code tangling;code scattering;concept-implementation templates;concept-implementation steps;FUDA templates;framework API understanding through dynamic analysis","","5","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Checkpointing and Rollback-Recovery for Distributed Systems","R. Koo; S. Toueg","Department of Computer Science, Cornell University; NA","IEEE Transactions on Software Engineering","","1987","SE-13","1","23","31","We consider the problem of bringing a distributed system to a consistent state after transient failures. We address the two components of this problem by describing a distributed algorithm to create consistent checkpoints, as well as a rollback-recovery algorithm to recover the system to a consistent state. In contrast to previous algorithms, they tolerate failures that occur during their executions. Furthermore, when a process takes a checkpoint, a minimal number of additional processes are forced to take checkpoints. Similarly, when a process rolls back and restarts after a failure, a minimal number of additional processes are forced to roll back with it. Our algorithms require each process to store at most two checkpoints in stable storage. This storage requirement is shown to be minimal under general assumptions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232562","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702129","Checkpoint;consistent state;distributed systems;fault-tolerance;rollback-recovery","Checkpointing;Distributed algorithms;Fault tolerant systems;Hardware;Resumes;Fault tolerance;Computer science;Distributed computing","","Checkpoint;consistent state;distributed systems;fault-tolerance;rollback-recovery","","313","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A Resource Sharing System for Personal Computers in a LAN: Concepts, Design, and Experience","R. C. Summers","IBM Los Angeles Scientific Center","IEEE Transactions on Software Engineering","","1987","SE-13","8","895","904","RM is an experimental prototype that supports the use of distributed services by personal computers in a LAN. Using a service request model, RM allows any PC on the LAN to offer and use services, which can be user-written or off-the-shelf applications. A user can start several activities that proceed concurrently and that use services offered by different machines. Program interfaces are provided for the development of distributed applications. Remote execution is supported within the service-request framework. The paper considers issues in resource sharing and discusses the choices that were made for RM. It provides an overview of RM concepts, design, and implementation, and reviews experience using the system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233508","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702308","Distributed services;personal-computer LAN's;remote execution;resource sharing;service-request models","Resource management;Microcomputers;Local area networks;Application software;Prototypes;Hardware;Operating systems;Software prototyping;US Department of Transportation;Peer to peer computing","","Distributed services;personal-computer LAN's;remote execution;resource sharing;service-request models","","4","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A syntactic theory of software architecture","T. R. Dean; J. R. Cordy","Dept. of Comput. & Inf. Sci., Queen's Univ., Kingston, Ont., Canada; Dept. of Comput. & Inf. Sci., Queen's Univ., Kingston, Ont., Canada","IEEE Transactions on Software Engineering","","1995","21","4","302","313","Introduces a general, extensible diagrammatic syntax for expressing software architectures based on typed nodes and connections and formalized using set theory. The syntax provides a notion of abstraction corresponding to the concept of a subsystem, and exploits this notion in a general mechanism for pattern matching over architectures. We demonstrate these ideas using a small example architecture language with a limited number of types of nodes and connectors, and a small taxonomy of architectures characterized as sets of patterns in the language.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.385969","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=385969","","Software architecture;Taxonomy;Computer architecture;Pattern matching;Set theory;Connectors;Councils;Information technology;Information science","software engineering;set theory;pattern matching;type theory;diagrams;formal languages","syntactic theory;software architecture;extensible diagrammatic syntax;typed nodes;typed connections;set theory;abstraction;subsystem;pattern matching;architecture language;connectors;taxonomy;software structure","","43","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Reasoning About Identifier Spaces: How to Make Chord Correct","P. Zave","AT&T Laboratories—Research, Bedminster, NJ","IEEE Transactions on Software Engineering","","2017","43","12","1144","1156","The Chord distributed hash table (DHT) is well-known and often used to implement peer-to-peer systems. Chord peers find other peers, and access their data, through a ring-shaped pointer structure in a large identifier space. Despite claims of proven correctness, i.e., eventual reachability, previous work has shown that the Chord ring-maintenance protocol is not correct under its original operating assumptions. Previous work has not, however, discovered whether Chord could be made correct under the same assumptions. The contribution of this paper is to provide the first specification of correct operations and initialization for Chord, an inductive invariant that is necessary and sufficient to support a proof of correctness, and two independent proofs of correctness. One proof is informal and intuitive, and applies to networks of any size. The other proof is based on a formal model in Alloy, and uses fully automated analysis to prove the assertions for networks of bounded size. The two proofs complement each other in several important ways.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2655056","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7823003","Computers and information processing;distributed computing;peer-to-peer computing;software engineering;formal verification","Peer-to-peer computing;Formal verification;Information processing;Analytical models;Structural rings;Distributed processing","peer-to-peer computing;protocols;telecommunication network topology","hash table;peer-to-peer systems;Chord peers;ring-shaped pointer structure;identifier space;Chord ring-maintenance protocol","","1","","33","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Analyzing the Effect of Gain Time on Soft-Task Scheduling Policies in Real-Time Systems","L. Búrdalo; A. Terrasa; A. Espinosa; A. García-Fornes","Universitat Politèecnica de València, Valencia; Universitat Politèecnica de València, Valencia; Universitat Politèecnica de València, Valencia; Universitat Politèecnica de València, Valencia","IEEE Transactions on Software Engineering","","2012","38","6","1305","1318","In hard real-time systems, gain time is defined as the difference between the Worst Case Execution Time (WCET) of a hard task and its actual processor consumption at runtime. This paper presents the results of an empirical study about how the presence of a significant amount of gain time in a hard real-time system questions the advantages of using the most representative scheduling algorithms or policies for aperiodic or soft tasks in fixed-priority preemptive systems. The work presented here refines and complements many other studies in this research area in which such policies have been introduced and compared. This work has been performed by using the authors' testing framework for soft scheduling policies, which produces actual, synthetic, randomly generated applications, executes them in an instrumented Real-Time Operating System (RTOS), and finally processes this information to obtain several statistical outcomes. The results show that, in general, the presence of a significant amount of gain time reduces the performance benefit of the scheduling policies under study when compared to serving the soft tasks in background, which is considered the theoretical worst case. In some cases, this performance benefit is so small that the use of a specific scheduling policy for soft tasks is questionable.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.95","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6025357","Real-time systems;RT-Linux;scheduling policies","Real time systems;Servers;Time factors;Generators;Scheduling;Heuristic algorithms;Decision support systems","operating systems (computers);real-time systems;scheduling","gain time;soft-task scheduling policies;hard real-time systems;worst case execution time;WCET;processor consumption;representative scheduling algorithms;aperiodic tasks;fixed-priority preemptive systems;author testing framework;instrumented real-time operating system;RTOS;statistical outcomes","","2","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Self-Organizing Roles on Agile Software Development Teams","R. Hoda; J. Noble; S. Marshall","The University of Auckland, Auckland; Victoria University of Wellington, Wellington; Victoria University of Wellington, Wellington","IEEE Transactions on Software Engineering","","2013","39","3","422","444","Self-organizing teams have been recognized and studied in various forms-as autonomous groups in socio-technical systems, enablers of organizational theories, agents of knowledge management, and as examples of complex-adaptive systems. Over the last decade, self-organizing teams have taken center stage in software engineering when they were incorporated as a hallmark of Agile methods. Despite the long and rich history of self-organizing teams and their recent popularity with Agile methods, there has been little research on the topic within software wngineering. Particularly, there is a dearth of research on how Agile teams organize themselves in practice. Through a Grounded Theory research involving 58 Agile practitioners from 23 software organizations in New Zealand and India over a period of four years, we identified informal, implicit, transient, and spontaneous roles that make Agile teams self-organizing. These roles-Mentor, Coordinator, Translator, Champion, Promoter, and Terminator-are focused toward providing initial guidance and encouraging continued adherence to Agile methods, effectively managing customer expectations and coordinating customer collaboration, securing and sustaining senior management support, and identifying and removing team members threatening the self-organizing ability of the team. Understanding these roles will help software development teams and their managers better comprehend and execute their roles and responsibilities as a self-organizing team.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6197202","Self-organizing;team roles;software engineering;Agile software development;grounded theory","Programming;Organizations;Collaboration;Software;Organizing;Software engineering","knowledge management;software management;software prototyping;team working","self-organizing roles;agile software development teams;self-organizing teams;autonomous groups;socio-technical systems;organizational theories enablers;knowledge management agents;complex-adaptive system examples;software engineering;grounded theory research;New Zealand;India;mentor role;coordinator role;translator role;champion role;promoter role;terminator role;customer expectation management;customer collaboration coordination;senior management support security;senior management support sustainability","","46","","115","","","","","","IEEE","IEEE Journals & Magazines"
"Exploiting Model Morphology for Event-Based Testing","F. Belli; M. Beyazıt","Department of Electrical Engineering and Information Technology, University of Paderborn, Paderborn, Germany; Department of Computer Engineering, Yaşar University, İzmir, Turkey","IEEE Transactions on Software Engineering","","2015","41","2","113","134","Model-based testing employs models for testing. Model-based mutation testing (MBMT) additionally involves fault models, called mutants, by applying mutation operators to the original model. A problem encountered with MBMT is the elimination of equivalent mutants and multiple mutants modeling the same faults. Another problem is the need to compare a mutant to the original model for test generation. This paper proposes an event-based approach to MBMT that is not fixed on single events and a single model but rather operates on sequences of events of length k ≥ 1 and invokes a sequence of models that are derived from the original one by varying its morphology based on k. The approach employs formal grammars, related mutation operators, and algorithms to generate test cases, enabling the following: (1) the exclusion of equivalent mutants and multiple mutants; (2) the generation of a test case in linear time to kill a selected mutant without comparing it to the original model; (3) the analysis of morphologically different models enabling the systematic generation of mutants, thereby extending the set of fault models studied in related literature. Three case studies validate the approach and analyze its characteristics in comparison to random testing and another MBMT approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2360690","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6915728","Model-based mutation testing;grammar-based testing;mutant selection;Model-based mutation testing;grammar-based testing;(model) morphology;mutant selection;test generation","Grammar;Testing;Unified modeling language;Production;Context;Morphology;Analytical models","computational complexity;grammars;program testing","model morphology;event-based testing;model-based mutation testing;MBMT;fault models;mutants;mutation operators;test generation;event-based approach;formal grammars;equivalent mutant;multiple mutants;linear time;random testing","","4","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Tracking mobile units for dependable message delivery","A. L. Murphy; G. -. Roman; G. Varghese","Rochester Univ., NY, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","5","433","448","As computing components get smaller and people become accustomed to having computational power at their disposal at any time, mobile computing is developing as an important research area. One of the fundamental problems in mobility is maintaining connectivity through message passing as the user moves through the network. An approach to this is to have a single home node constantly track the current location of the mobile unit and forward messages to this location. One problem with this approach is that, during the update to the home agent after movement, messages are often dropped, especially in the case of frequent movement. In this paper, we present a new algorithm which uses a home agent, but maintains information regarding a subnet within which the mobile unit must be present. We also present a reliable message delivery algorithm which is superimposed on the region maintenance algorithm. Our strategy is based on ideas from diffusing computations as first proposed by Dijkstra and Scholten. Finally, we present a second algorithm which limits the size of the subnet by keeping only a path from the home node to the mobile unit.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000448","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000448","","Maintenance;Mobile computing;Message passing","portable computers;mobile computing;message passing;software maintenance;computer communications software","mobile units tracking;dependable message delivery;mobile computing;connectivity;message passing;home agent;reliable message delivery algorithm;region maintenance algorithm","","2","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Model Checking Markov Chains with Actions and State Labels","C. Baier; L. Cloth; B. R. Haverkort; M. Kuntz; M. Siegle","Institute for Theoretical Computer Science, Technische Universita¨t Dresden, D-01062 Dresden, Germany; EWI/DACS, University of Twente, PO Box 217, 7500 AE Enschede, The Netherlands; EWI/DACS, University of Twente, PO Box 217, 7500 AE Enschede, The Netherlands; EWI/DACS, University of Twente, PO Box 217, 7500 AE Enschede, The Netherlands; Universita¨t der Bundeswehr Mu¨nchen, Institute fu¨r Technische Informatik, Fakulta¨t fu¨r Informatik, 85577 Neubiberg, Germany","IEEE Transactions on Software Engineering","","2007","33","4","209","224","In the past, logics of several kinds have been proposed for reasoning about discrete-time or continuous-time Markov chains. Most of these logics rely on either state labels (atomic propositions) or on transition labels (actions). However, in several applications it is useful to reason about both state properties and action sequences. For this purpose, we introduce the logic as CSL which provides a powerful means to characterize execution paths of Markov chains with actions and state labels. asCSL can be regarded as an extension of the purely state-based logic CSL (continuous stochastic logic). In asCSL, path properties are characterized by regular expressions over actions and state formulas. Thus, the truth value of path formulas depends not only on the available actions in a given time interval, but also on the validity of certain state formulas in intermediate states. We compare the expressive power of CSL and asCSL and show that even the state-based fragment of asCSL is strictly more expressive than CSL if time intervals starting at zero are employed. Using an automaton-based technique, an asCSL formula and a Markov chain with actions and state labels are combined into a product Markov chain. For time intervals starting at zero, we establish a reduction of the model checking problem for asCSL to CSL model checking on this product Markov chain. The usefulness of our approach is illustrated with an elaborate model of a scalable cellular communication system, for which several properties are formalized by means of asCSL formulas and checked using the new procedure","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123324","Protocol verification;performance of systems;model checking;automata;Markov processes.","Stochastic processes;Probabilistic logic;Algebra;Power system modeling;Law;Legal factors;Automata;Markov processes;Embedded system;Petri nets","automata theory;formal verification;Markov processes;temporal logic","state label;transition label;Markov chain;continuous stochastic logic;regular expression;automaton-based technique;model checking","","33","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Distributed Version Management for Read-Only Actions","W. E. Weihl","M. I. T. Laboratory for Computer Science, 545 Technology Square","IEEE Transactions on Software Engineering","","1987","SE-13","1","55","64","Typical concurrency control protocols for atomic actions, such as two-phase locking, perform poorly for long read-only actions. We present four new concurrency control protocols that eliminate all interference between read-only actions and update actions, and thus offer significantly improved performance for read-only actions. The protocols work by maintaining multiple versions of the system state; read-only actions read old versions, while update actions manipulate the most recent version. We focus on the problem of managing the storage required for old versions in a distributed system. One of the protocols uses relatively little space, but has a potentially significant communication cost. The other protocols use more space, but may be cheaper in terms of communication.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232835","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702133","Atomic actions;concurrency;distributed systems;multiple version protocols;read-only actions;storage management","Protocols;Concurrency control;Hardware;Interference elimination;Costs;Concurrent computing;Data mining;Application software;Delay effects;System recovery","","Atomic actions;concurrency;distributed systems;multiple version protocols;read-only actions;storage management","","22","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Virtual Time CSMA Protocols for Hard Real-Time Communication","Wei Zhao; K. Ramamritham","Department of Mathematics, Amherst College; NA","IEEE Transactions on Software Engineering","","1987","SE-13","8","938","952","We study virtual time CSMA protocols for hard real time communication systems, i, e., systems where messages have explicit deadlines. In this class of CSMA protocols, each node maintains two clocks; a real time clock and a virtual time clock. Whenever a node finds the channel to be idle, it resets its virtual clock. The virtual clock then runs at a higher rate than the real clock. A node transmits a waiting message when the time on the virtual clock is equal to some parameter of the message. Using different message parameters in conjunction with the virtual clock, different transmission policies can be implemented. In particular, use of message arrival time, message length, message laxity, and message deadline implements FCFS, Minimum-Length-First, Minimum-Laxity-First, and Minimum-Deadline-First transmission policies, respectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233512","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702312","CSMA protocols;multiaccess networks;performance evaluation;real-time communications;simulation;virtual time","Multiaccess communication;Clocks;Access protocols;Real time systems;Measurement;Delay;Delta modulation;Road accidents;Telecommunication traffic;Springs","","CSMA protocols;multiaccess networks;performance evaluation;real-time communications;simulation;virtual time","","59","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A Look into Programmers' Heads","N. Peitek; J. Siegmund; S. Apel; C. Kästner; C. Parnin; A. Bethmann; T. Leich; G. Saake; A. Brechmann","SLNIB, Leibniz-Institut fur Neurobiologie, 28390 Magdeburg, Saxony Anhalt Germany (e-mail: norman.peitek@lin-magdeburg.de); Department of Informatics and Mathematics, University of Passau, Passau, Bavaria Germany 94032 (e-mail: Janet.Siegmund@uni-passau.de); Department of Informatics and Mathematics, University of Passau, Passau, Bavaria Germany 94032 (e-mail: apel@uni-passau.de); Institute for Software Reserach, Carnegie Mellon University, Pittsburgh, Pennsylvania United States 15213-3890 (e-mail: kaestner@cs.cmu.edu); Computer Science, North Carolina State University, Raleigh, North Carolina United States 27608 (e-mail: cjparnin@ncsu.edu); SLNIB, Leibniz-Institut fur Neurobiologie, 28390 Magdeburg, Sachsen-Anhalt Germany (e-mail: bethmann@lin-magdeburg.de); Applied Computer Science, Metop Research Institute, Magdeburg, Saxony-Anhalt Germany (e-mail: thomas.leich@metop.de); Department of Computer Science, Otto-von-Guericke Universitaet Magdeburg, Magdeburg, Saxony Anhalt Germany (e-mail: saake@iti.cs.uni-magdeburg.de); SLNIB, Leibniz-Institut fur Neurobiologie, 28390 Magdeburg, Sachsen-Anhalt Germany (e-mail: brechmann@lin-magdeburg.de)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Program comprehension is an important, but hard to measure cognitive process. This makes it difficult to provide suitable programming languages, tools, or coding conventions to support developers in their everyday work. Here, we explore whether functional magnetic resonance imaging (fMRI) is feasible for soundly measuring program comprehension. To this end, we observed 17 participants inside an fMRI scanner while they were comprehending source code. The results show a clear, distinct activation of five brain regions, which are related to working memory, attention, and language processing, which all fit well to our understanding of program comprehension. Furthermore, we found reduced activity in the default mode network, indicating the cognitive effort necessary for program comprehension. We also observed that familiarity with Java as underlying programming language reduced cognitive effort during program comprehension. To gain confidence in the results and the method, we replicated the study with 11 new participants and largely confirmed our findings. Our results encourage us and, hopefully, others to use fMRI to observe programmers and, in the long run, answer questions, such as: How should we train programmers? Can we train someone to become an excellent programmer? How effective are new languages and tools for program comprehension?","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2863303","National Science Foundation; Deutsche Forschungsgemeinschaft; Defense Advanced Research Projects Agency; Bayerisches Staatsministerium fur Bildung und Kultus Wissenschaft und Kunst; H2020 European Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8425769","Functional magnetic resonance imaging;program comprehension","Functional magnetic resonance imaging;Task analysis;Cognition;Brain;Programming;Blood","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Formal specification and analysis of software architectures using the chemical abstract machine model","P. Inverardi; A. L. Wolf","Dipartimento di Matematica Pura ed Applicata, L'Aquila Univ., Italy; NA","IEEE Transactions on Software Engineering","","1995","21","4","373","386","We are exploring an approach to formally specifying and analyzing software architectures that is based on viewing software systems as chemicals whose reactions are controlled by explicitly stated rules. This powerful metaphor was devised in the domain of theoretical computer science by Bana/spl circ/tre and Le Me/spl acute/tayer (1990) and then reformulated as the CHAM (CHemical Abstract Machine) by Berry and Boudol (1992). The CHAM formalism provides a framework for developing operational specifications that does not bias the described system toward any particular computational model. It also encourages the construction and use of modular specifications at different levels of detail. We illustrate the use of the CHAM for architectural description and analysis by applying it to two different architectures for a simple but familiar software system, the multiphase compiler.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.385973","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=385973","","Chemical analysis;Formal specifications;Computer architecture;Software architecture;Software systems;Computer science;Power system modeling;Computational modeling;Modular construction;Software engineering","formal specification;program compilers;chemical reactions;finite automata;programming theory","formal specification;formal analysis;software architectures;chemical abstract machine model;chemical reactions;explicitly stated rules;theoretical computer science;CHAM formalism;operational specifications;computational model;modular specifications;levels of detail;architectural description;multiphase compiler","","106","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Developing interactive information systems with the User Software Engineering methodology","A. I. Wasserman; P. A. Pircher; D. T. Shewmake; M. L. Kersten","Section of Medical Information Science, University of California, San Francisco, CA 94143; Section of Medical Information Science, University of California, San Francisco, CA 94143; Section of Medical Information Science, University of California, San Francisco, CA 94143; Centrum voor Wiskunde en Informatica, Amsterdam, The Netherlands","IEEE Transactions on Software Engineering","","1986","SE-12","2","326","345","User software engineering (USE) is a methodology, supported by automated tools, for the systematic development of interactive information systems. The USE methodology gives particular attention to effective user involvement in the early stages of the software development process, concentrating on external design and the use of rapidly created and modified prototypes of the user interface. The USE methodology is supported by an integrated set of graphically based tools. The USE methodology and the tools that support it are described.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312947","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312947","Human/computer interaction;interactive information systems;rapid prototyping;RAPID/USE;software development methodology;transition diagrams;User Software Engineering","Libraries;Books;Data models;Software engineering;Software;Databases;Information systems","interactive systems;software engineering;user interfaces","interactive information systems;user software engineering methodology;automated tools;user involvement;software development process;prototypes;user interface;graphically based tools;USE methodology","","14","","","","","","","","IEEE","IEEE Journals & Magazines"
"Reusability of mathematical software: a contribution","P. Di Felice","Dipartimento di Ingegneria Elettrica, Univ. di L'Aquila, Italy","IEEE Transactions on Software Engineering","","1993","19","8","835","843","Mathematical software is devoted to solving problems involving matrix computation and manipulation. The main problem limiting the reusability of existing mathematical software is that programs are often not initially designed for being reused. Therefore, it is hard to find programs that can be easily reused. A programming methodology useful for designing and implementing reusable code is presented. A portion of code designed and implemented for being reused is called a unit. The units are self-contained software components featuring a high degree of information hiding. This way of organizing software facilitates the reuse process and improves the understandability of units. To speed up the implementation process, a system supporting the reusability of units from an existing software library is particularly useful. The functionality of the EasyCard system, which creates, maintains, and queries a catalog of units is discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.238586","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=238586","","Software reusability;Programming profession;Software tools;Sparse matrices;Application software;Design methodology;Software libraries;Object oriented programming;Standardization;Organizing","mathematics computing;software reusability","matrix computation;reusability;mathematical software;programming methodology;reusable code;self-contained software components;information hiding;understandability;existing software library;EasyCard system","","6","","30","","","","","","IEEE","IEEE Journals & Magazines"
"An Experience in Testing the Security of Real-World Electronic Voting Systems","D. Balzarotti; G. Banks; M. Cova; V. Felmetsger; R. Kemmerer; W. Robertson; F. Valeur; G. Vigna","Eurecom Institute, Sophia Antipolis, France; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara","IEEE Transactions on Software Engineering","","2010","36","4","453","473","Voting is the process through which a democratic society determines its government. Therefore, voting systems are as important as other well-known critical systems, such as air traffic control systems or nuclear plant monitors. Unfortunately, voting systems have a history of failures that seems to indicate that their quality is not up to the task. Because of the alarming frequency and impact of the malfunctions of voting systems, in recent years a number of vulnerability analysis exercises have been carried out against voting systems to determine if they can be compromised in order to control the results of an election. We have participated in two such large-scale projects, sponsored by the Secretaries of State of California and Ohio, whose goals were to perform the security testing of the electronic voting systems used in their respective states. As the result of the testing process, we identified major vulnerabilities in all of the systems analyzed. We then took advantage of a combination of these vulnerabilities to generate a series of attacks that would spread across the voting systems and would “steal” votes by combining voting record tampering with social engineering approaches. As a response to the two large-scale security evaluations, the Secretaries of State of California and Ohio recommended changes to improve the security of the voting process. In this paper, we describe the methodology that we used in testing the two real-world electronic voting systems we evaluated, the findings of our analysis, our attacks, and the lessons we learned.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5210119","Voting systems;security testing;vulnerability analysis.","Electronic equipment testing;System testing;Security;Electronic voting systems;Large-scale systems;Government;Air traffic control;History;Frequency;Control systems","data privacy;government data processing;security of data","security testing;electronic voting system;alarming frequency;vulnerability analysis exercise;California;Ohio;social engineering approache;large scale security evaluation","","18","","62","","","","","","IEEE","IEEE Journals & Magazines"
"The optimal class size for object-oriented software","K. El Emam; S. Benlarbi; N. Goel; W. Melo; H. Lounis; S. N. Rai","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","5","494","509","A growing body of literature suggests that there is an optimal size for software components. This means that components that are too small or too big will have a higher defect content (i.e., there is a U-shaped curve relating defect content to size). The U-shaped curve has become known as the ""Goldilocks Conjecture."" Recently, a cognitive theory has been proposed to explain this phenomenon and it has been expanded to characterize object-oriented software. This conjecture has wide implications for software engineering practice. It suggests 1) that designers should deliberately strive to design classes that are of the optimal size, 2) that program decomposition is harmful, and 3) that there exists a maximum (threshold) class size that should not be exceeded to ensure fewer faults in the software. The purpose of the current paper is to evaluate this conjecture for object-oriented systems. We first demonstrate that the claims of an optimal component/class size (1) above) and of smaller components/classes having a greater defect content (2) above) are due to a mathematical artifact in the analyses performed previously. We then empirically test the threshold effect claims of this conjecture (3) above). To our knowledge, the empirical test of size threshold effects for object-oriented systems has not been performed thus far. We performed an initial study with an industrial C++ system and repeated it twice on another C++ system and on a commercial Java application. Our results provide unambiguous evidence that there is no threshold effect of class size. We obtained the same result for three systems using four different size measures. These findings suggest that there is a simple continuous relationship between class size and faults, and that, optimal class size, smaller classes are better and threshold effects conjectures have no sound theoretical nor empirical basis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000452","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000452","","Software engineering;Performance analysis;System testing;Performance evaluation;Java;Size measurement","object-oriented programming;software quality;software metrics","optimal class size;object-oriented software;software components;U-shaped curve;program decomposition;object-oriented metrics;software quality","","46","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Balancing Privacy and Utility in Cross-Company Defect Prediction","F. Peters; T. Menzies; L. Gong; H. Zhang","West Virginia University, Morgantown; West Virginia University, Morgantown; Tsinghua University, Beijing; Tsinghua University, Beijing","IEEE Transactions on Software Engineering","","2013","39","8","1054","1068","Background: Cross-company defect prediction (CCDP) is a field of study where an organization lacking enough local data can use data from other organizations for building defect predictors. To support CCDP, data must be shared. Such shared data must be privatized, but that privatization could severely damage the utility of the data. Aim: To enable effective defect prediction from shared data while preserving privacy. Method: We explore privatization algorithms that maintain class boundaries in a dataset. CLIFF is an instance pruner that deletes irrelevant examples. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. CLIFF+MORPH are tested in a CCDP study among 10 defect datasets from the PROMISE data repository. Results: We find: 1) The CLIFFed+MORPHed algorithms provide more privacy than the state-of-the-art privacy algorithms; 2) in terms of utility measured by defect prediction, we find that CLIFF+MORPH performs significantly better. Conclusions: For the OO defect data studied here, data can be privatized and shared without a significant degradation in utility. To the best of our knowledge, this is the first published result where privatization does not compromise defect prediction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6419712","Privacy;classification;defect prediction","Testing;Software;Genetic algorithms;Sociology;Statistics;Search problems;Arrays","data privacy;program debugging","privacy balancing;utility balancing;cross-company defect prediction;CCDP;defect predictors;privacy preservation;privatization algorithms;class boundaries;pruner;data mutator;PROMISE data repository;CLIFFed-MORPHed algorithm;OO defect data","","42","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Knowledge-based repository scheme for storing and retrieving business components: a theoretical design and an empirical analysis","P. Vitharana; F. M. Zahedi; H. Jain","Sch. of Manage., Syracuse Univ., NY, USA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","7","649","664","Component-based development (CDB) promises to reduce complexity and cost of software development and maintenance through reuse. For CBD to be successful, a vibrant market for commercial business components is essential. One of the key requirements of an active market for business components is an effective scheme for classifying and describing them at various levels of detail, as well as a corresponding repository for storing and retrieving these components. Such a scheme needs to support various constituents such as business users, managers, and application assemblers. The scheme and repository should help users and managers to select components that match their requirements and aid application assemblers in identifying components most compatible with their deployment environment (such as the platform) and system inputs (such as data types). Drawing from the concepts of group technology and software reuse paradigm, this paper proposes a scheme for classifying and describing business components and the design of a knowledge-based repository for their storage and retrieval. The proposed scheme is implemented in a prototype repository. The effectiveness of the prototype and the underlying classification and coding scheme is assessed empirically through controlled experiments. Results support the assertion that the scheme is effective in enhancing the users' and analysts' ability to find the needed business components.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214328","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214328","","Application software;Software prototyping;Prototypes;Costs;Programming;Software maintenance;Business communication;Environmental management;Assembly systems;Group technology","business data processing;knowledge management;object-oriented programming;information retrieval;information storage;software libraries;hypermedia markup languages;classification","component-based development;software reuse;software library;component repository;commercial business components;group technology;knowledge-based repository;component storage;component retrieval","","30","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Reducing Features to Improve Code Change-Based Bug Prediction","S. Shivaji; E. James Whitehead; R. Akella; S. Kim","University of California, Santa Cruz, Santa Cruz; University of California, Santa Cruz, Santa Cruz; University of California, Santa Cruz, Santa Cruz; Hong Kong University of Science and, Hong Kong","IEEE Transactions on Software Engineering","","2013","39","4","552","569","Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine (SVM) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The SVM's improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6226427","Reliability;bug prediction;machine learning;feature selection","Software;Support vector machines;History;Machine learning;Feature extraction;Measurement;Computer bugs","belief networks;learning (artificial intelligence);pattern classification;program debugging;support vector machines","code change-based bug prediction;machine learning classifier;source code file;software history;classifier-based bug prediction;machine learned feature reduction;feature selection technique;classification performance;naive Bayes classifier;support vector machine;SVM classifier;software project;buggy F-measure","","75","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Compartmented mode workstation: prototype highlights","J. L. Berger; J. Picciotto; J. P. L. Woodward; P. T. Cummings","MITRE Corp., Bedford, MA, USA; MITRE Corp., Bedford, MA, USA; MITRE Corp., Bedford, MA, USA; MITRE Corp., Bedford, MA, USA","IEEE Transactions on Software Engineering","","1990","16","6","608","618","The primary goal of the MITRE compartmented mode workstation (CMW) project was to articulate the security requirements that workstations must meet to process highly classified intelligence data. As a basis for the validity of the requirements developed, a prototype was implemented which demonstrated that workstations could meet the requirements in an operationally useful manner while still remaining binary compatible with off-the-shelf software. The security requirements not only addressed traditional security concerns but also introduced concepts in areas such as labeling and the use of a trusted window management system. The CMW labeling paradigm is based on associating two types of security labels with objects: sensitivity levels and information labels. Sensitivity levels describe the levels at which objects must be protected. Information labels are used to prevent data overclassification and also provide a mechanism for associating with data those markings that are required for accurate data labeling, but which play no role in access control decisions. The use of a trusted window manager allows users to easily operate at multiple sensitivity levels and provides a convenient mechanism for communicating security information to users in a relatively unobtrusive manner.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.55089","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=55089","","Workstations;Prototypes;Data security;Information security;Computer security;Labeling;Access control;Protection;Power generation economics;Environmental economics","security of data;software engineering;workstations","data overclassification prevention;MITRE compartmented mode workstation;security requirements;highly classified intelligence data;binary compatible;trusted window management system;security labels;objects;sensitivity levels;information labels;markings;accurate data labeling;multiple sensitivity levels","","8","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Retargeting sequential image-processing programs for data parallel execution","L. B. Baumstark; L. M. Wills","Dept. of Comput. Sci., State Univ. of West Georgia, Carollton, GA, USA; NA","IEEE Transactions on Software Engineering","","2005","31","2","116","136","New compact, low-power implementation technologies for processors and imaging arrays can enable a new generation of portable video products. However, software compatibility with large bodies of existing applications written in C prevents more efficient, higher performance data parallel architectures from being used in these embedded products. If this software could be automatically retargeted explicitly for data parallel execution, product designers could incorporate these architectures into embedded products. The key challenge is exposing the parallelism that is inherent in these applications but that is obscured by artifacts imposed by sequential programming languages. This paper presents a recognition-based approach for automatically extracting a data parallel program model from sequential image processing code and retargeting it to data parallel execution mechanisms. The explicitly parallel model presented, called multidimensional data flow (MDDF), captures a model of how operations on data regions (e.g., rows, columns, and tiled blocks) are composed and interact. To extract an MDDF model, a partial recognition technique is used that focuses on identifying array access patterns in loops, transforming only those program elements that hinder parallelization, while leaving the core algorithmic computations intact. The paper presents results of retargeting a set of production programs to a representative data parallel processor array to demonstrate the capacity to extract parallelism using this technique. The retargeted applications yield a potential execution throughput limited only by the number of processing elements, exceeding thousands of instructions per cycle in massively parallel implementations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401928","Index Terms- Reengineering;SIMD processors;data-level parallelization;explicitly parallel program representation;program recognition.","Data mining;Embedded software;Application software;Parallel processing;Software performance;Parallel architectures;Product design;Computer architecture;Computer languages;Image recognition","parallel programming;parallel architectures;video signal processing;data flow computing;systems re-engineering;imaging","sequential image-processing program;data parallel execution;imaging array;portable video product;data parallel architecture;sequential programming language;data parallel program model;multidimensional data flow;parallel processor array","","5","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Amorphous Slicing of Extended Finite State Machines","K. Androutsopoulos; D. Clark; M. Harman; R. M. Hierons; Z. Li; L. Tratt","University College London, London; University College London, London; University College London, London; Brunel University, Uxbridge, Middlesex; Beijing University of Chemical Technology, Beijing; King's Colledge London, London","IEEE Transactions on Software Engineering","","2013","39","7","892","909","Slicing is useful for many software engineering applications and has been widely studied for three decades, but there has been comparatively little work on slicing extended finite state machines (EFSMs). This paper introduces a set of dependence-based EFSM slicing algorithms and an accompanying tool. We demonstrate that our algorithms are suitable for dependence-based slicing. We use our tool to conduct experiments on 10 EFSMs, including benchmarks and industrial EFSMs. Ours is the first empirical study of dependence-based program slicing for EFSMs. Compared to the only previously published dependence-based algorithm, our average slice is smaller 40 percent of the time and larger only 10 percent of the time, with an average slice size of 35 percent for termination insensitive slicing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.72","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6374192","Slicing;extended finite state machines","Automata;Algorithm design and analysis;Approximation algorithms;Software algorithms;Unified modeling language;Educational institutions;Electronic mail","finite state machines;program slicing;software engineering","software engineering application;extended finite state machine slicing;dependence-based EFSM slicing algorithm;benchmarks EFSM;industrial EFSM;dependence-based program slicing;termination insensitive slicing;amorphous slicing","","8","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Engineering software design processes to guide process execution","Xiping Song; L. J. Osterweil","Corporate Res. Inc., Princeton, NJ, USA; NA","IEEE Transactions on Software Engineering","","1998","24","9","759","775","Using systematic development processes is an important characteristic of any mature engineering discipline. In current software practice, software design methodologies (SDMs) are intended to be used to help design software more systematically. This paper shows, however, that one well-known example of such an SDM, Booch Object-Oriented Design (BOOD), as described in the literature is too imprecise and incomplete to be considered as a fully systematic process for specific projects. To provide more effective and appropriate guidance and control in software design processes, we applied the process programming concept to the design process. Given two different sets of plausible design process requirements, we elaborated two more detailed and precise design processes that are responsive to these requirements. We have also implemented, experimented with, and evaluated a prototype (called Debus-Booch) that supports the execution of these detailed processes.","0098-5589;1939-3520;2326-3881","","10.1109/32.713330","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=713330","","Design engineering;Software design;Design methodology;Process design;Software engineering;Programming;Computer Society;Software systems;Software prototyping;Prototypes","object-oriented programming;software engineering","process execution;systematic development processes;software design methodologies;Booch Object-Oriented Design;BOOD;software design processes;process programming","","5","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Information resources management in heterogeneous, distributed environments: A metadatabase approach","C. Hsu; M. Bouziane; L. Rattner; L. Yee","Rensselaer Polytech. Inst., Troy, NY, USA; Rensselaer Polytech. Inst., Troy, NY, USA; Rensselaer Polytech. Inst., Troy, NY, USA; Rensselaer Polytech. Inst., Troy, NY, USA","IEEE Transactions on Software Engineering","","1991","17","6","604","625","The core structure of a metadatabase system for information integration in heterogeneous and distributed environments, the global information resources dictionary (GIRD) model for unified metadata representation and management (both data and knowledge), is discussed. Overviews of metadatabase systems and the two-stage entity relationship (TSER) representation method are presented. To illustrate some major properties of the metadatabase model, and to show how the GIRD elements fit together to deliver these properties, manufacturing information management examples are given. The GIRD and the information resources dictionary system (IRDS) standard are compared.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.87285","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=87285","","Information resources;Environmental management;Information management;Resource management;Dictionaries;Computer aided software engineering;Productivity;Software development management;Engineering management;Knowledge management","CAD/CAM;database management systems;database theory;distributed processing;manufacturing data processing;software engineering","information resource management;core structure;metadatabase system;information integration;distributed environments;global information resources dictionary;unified metadata representation;two-stage entity relationship;TSER;metadatabase model;GIRD elements;manufacturing information management examples;information resources dictionary system;IRDS","","47","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Linking Model-Driven Development and Software Architecture: A Case Study","A. Mattsson; B. Lundell; B. Lings; B. Fitzgerald","Combitech AB, Jönköping; University of Skövde, Skövde; University of Skövde, Skövde; Univerity of Limerick, Limerick","IEEE Transactions on Software Engineering","","2009","35","1","83","93","A basic premise of model driven development (MDD) is to capture all important design information in a set of formal or semi-formal models which are then automatically kept consistent by tools. The concept however is still relatively immature and there is little by way of empirically validated guidelines. In this paper we report on the use of MDD on a significant real-world project over several years. Our research found the MDD approach to be deficient in terms of modelling architectural design rules. Furthermore, the current body of literature does not offer a satisfactory solution as to how architectural design rules should be modelled. As a result developers have to rely on time-consuming and error-prone manual practices to keep a system consistent with its architecture. To realise the full benefits of MDD it is important to find ways of formalizing architectural design rules which then allow automatic enforcement of the architecture on the system model. Without this, architectural enforcement will remain a bottleneck in large MDD projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4657364","Software Architecture;Model-Driven Development;Case Study Research;Software Architecture;Model-Driven Development;Case Study Research","Joining processes;Software architecture;Computer architecture;Guidelines;Context modeling;Computer industry;Computer errors;Programming;Keyword search;Portals","formal verification;software architecture;systems analysis","model-driven development;software architecture;formal models;semi-formal models;architectural design rules","","22","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Toward a Tool-Based Development Methodology for Pervasive Computing Applications","D. Cassou; J. Bruneau; C. Consel; E. Balland","University of Bordeaux and INRIA, Talence; University of Bordeaux and INRIA, Talence; University of Bordeaux and INRIA, Talence; University of Bordeaux and INRIA, Talence","IEEE Transactions on Software Engineering","","2012","38","6","1445","1463","Despite much progress, developing a pervasive computing application remains a challenge because of a lack of conceptual frameworks and supporting tools. This challenge involves coping with heterogeneous devices, overcoming the intricacies of distributed systems technologies, working out an architecture for the application, encoding it in a program, writing specific code to test the application, and finally deploying it. This paper presents a design language and a tool suite covering the development life-cycle of a pervasive computing application. The design language allows us to define a taxonomy of area-specific building-blocks, abstracting over their heterogeneity. This language also includes a layer to define the architecture of an application, following an architectural pattern commonly used in the pervasive computing domain. Our underlying methodology assigns roles to the stakeholders, providing separation of concerns. Our tool suite includes a compiler that takes design artifacts written in our language as input and generates a programming framework that supports the subsequent development stages, namely, implementation, testing, and deployment. Our methodology has been applied on a wide spectrum of areas. Based on these experiments, we assess our approach through three criteria: expressiveness, usability, and productivity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.107","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6051438","Methodology;domain-specific language;generative programming;pervasive computing;toolkit;programming support;simulation","Pervasive computing;Taxonomy;Computer architecture;Programming;Domain specific languages;Computational modeling;Software architecture","program compilers;software architecture;ubiquitous computing","tool-based development methodology;pervasive computing applications;distributed systems technologies;development life-cycle;area-specific building-blocks;architectural pattern;compiler;design artifacts","","21","","53","","","","","","IEEE","IEEE Journals & Magazines"
"A model for multilevel security in computer networks","W. -. Lu; M. K. Sundareshan","Dept. of Electr. & Comput. Eng., Arizona Univ., Tucson, AZ, USA; Dept. of Electr. & Comput. Eng., Arizona Univ., Tucson, AZ, USA","IEEE Transactions on Software Engineering","","1990","16","6","647","659","A model is presented that precisely describes the mechanism that enforces the security policy and requirements for a multilevel secure network. The mechanism attempts to ensure secure flow of information between entities assigned to different security classes in different computer systems connected to the network. The mechanism also controls the access to the network devices by the subjects (users and processes executed on behalf of the users) with different security clearances. The model integrates the notions of nondiscretionary access control and information flow control to provide a trusted network base that imposes appropriate restrictions on the flow of information among the various devices. Utilizing simple set-theoretic concepts, a procedure is given to verify the security of a network that implements the model.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.55093","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=55093","","Multilevel systems;Intelligent networks;Computer networks;Information security;Protection;Communication system control;Access control;Computer security;Computer architecture;Data security","computer networks;security of data","multilevel security;computer networks;security policy;multilevel secure network;entities;security classes;computer systems;network devices;subjects;security clearances;nondiscretionary access control;information flow control;trusted network base;set-theoretic concepts","","7","","53","","","","","","IEEE","IEEE Journals & Magazines"
"A Theoretical Basis for the Analysis of Multiversion Software Subject to Coincident Errors","D. E. Eckhardt; L. D. Lee","NASA Langley Research Center; NA","IEEE Transactions on Software Engineering","","1985","SE-11","12","1511","1517","Fundamental to the development of redundant software techniques (known as fault-tolerant software) is an understanding of the impact of multiple joint occurrences of errors, referred to here as coincident errors. A theoretical basis for the study of redundant software is developed which 1) provides a probabilistic framework for empirically evaluating the effectiveness of a general multiversion strategy when component versions are subject to coincident errors, and 2) permits an analytical study of the effects of these errors. An intensity function, called the intensity of coincident errors, has a central role in this analysis. This function describes the propensity of programmers to introduce design faults in such a way that software components fail together when executing in the application environment. We give a condition under which a multiversion system is a better strategy than relying on a single version and we study some differences between the coincident errors model developed here and the model that assumes independent failures of component verions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231895","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701974","Coincident errors;fault-tolerant software;intensity distribution;intensity of coincident errors;multiversion software;N-version programming;reliability of redundant software","Redundancy;Fault tolerance;Application software;Programming profession;Hardware;Probability;Fault tolerant systems;Software safety;Aerospace electronics;Control systems","","Coincident errors;fault-tolerant software;intensity distribution;intensity of coincident errors;multiversion software;N-version programming;reliability of redundant software","","194","","14","","","","","","IEEE","IEEE Journals & Magazines"
"The confounding effect of class size on the validity of object-oriented metrics","K. El Emam; S. Benlarbi; N. Goel; S. N. Rai","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","7","630","650","Much effort has been devoted to the development and empirical validation of object-oriented metrics. The empirical validations performed thus far would suggest that a core set of validated metrics is close to being identified. However, none of these studies allow for the potentially confounding effect of class size. We demonstrate a strong size confounding effect and question the results of previous object-oriented metrics validation studies. We first investigated whether there is a confounding effect of class size in validation studies of object-oriented metrics and show that, based on previous work, there is reason to believe that such an effect exists. We then describe a detailed empirical methodology for identifying those effects. Finally, we perform a study on a large C++ telecommunications framework to examine if size is really a confounder. This study considered the Chidamber and Kemerer metrics and a subset of the Lorenz and Kidd metrics. The dependent variable was the incidence of a fault attributable to a field failure (fault-proneness of a class). Our findings indicate that, before controlling for size, the results are very similar to previous studies. The metrics that are expected to be validated are indeed associated with fault-proneness.","0098-5589;1939-3520;2326-3881","","10.1109/32.935855","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=935855","","Size control;Software quality;Testing;Telecommunication control;Helium;Software engineering;Maintenance;Guidelines;Object oriented modeling;Predictive models","software metrics;object-oriented programming;C++ language;software fault tolerance;software quality","class size;object-oriented metrics;software metrics validation;C++;telecommunications framework;fault-proneness;software quality","","193","","110","","","","","","IEEE","IEEE Journals & Magazines"
"A graph model for software evolution","Luqi","Dept. of Comput. Sci., US Naval Post-graduate Sch., Monterey, CA, USA","IEEE Transactions on Software Engineering","","1990","16","8","917","927","A graph model of software evolution is presented. The author seeks to formalize the objects and activities involved in software evolution in sufficient detail to enable automatic assistance for maintaining the consistency and integrity of an evolving software system. This includes automated support for propagating the consequences of a change to a software system. The evolution of large and complex software systems receives particular attention.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.57627","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=57627","","Software systems;Software maintenance;Software prototyping;Prototypes;Scheduling;Automatic control;Costs;Programming;History;Humans","automatic programming;data integrity;graph theory;software tools","graph model;software evolution;automatic assistance;consistency;integrity;evolving software system;automated support;complex software systems","","45","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional Verification for Hierarchical Scheduling of Real-Time Systems","L. Carnevali; A. Pinzuti; E. Vicario","Università di Firenze; Università di Firenze; Università di Firenze","IEEE Transactions on Software Engineering","","2013","39","5","638","657","Hierarchical Scheduling (HS) techniques achieve resource partitioning among a set of real-time applications, providing reduction of complexity, confinement of failure modes, and temporal isolation among system applications. This facilitates compositional analysis for architectural verification and plays a crucial role in all industrial areas where high-performance microprocessors allow growing integration of multiple applications on a single platform. We propose a compositional approach to formal specification and schedulability analysis of real-time applications running under a Time Division Multiplexing (TDM) global scheduler and preemptive Fixed Priority (FP) local schedulers, according to the ARINC-653 standard. As a characterizing trait, each application is made of periodic, sporadic, and jittering tasks with offsets, jitters, and nondeterministic execution times, encompassing intra-application synchronizations through semaphores and mailboxes and interapplication communications among periodic tasks through message passing. The approach leverages the assumption of a TDM partitioning to enable compositional design and analysis based on the model of preemptive Time Petri Nets (pTPNs), which is expressly extended with a concept of Required Interface (RI) that specifies the embedding environment of an application through sequencing and timing constraints. This enables exact verification of intra-application constraints and approximate but safe verification of interapplication constraints. Experimentation illustrates results and validates their applicability on two challenging workloads in the field of safety-critical avionic systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6264049","Real-time systems;hierarchical scheduling;ARINC-653;time division multiplexing;preemptive fixed priority;compositional verification;preemptive time Petri nets;symbolic state-space analysis","Real time systems;Complexity theory;Time division multiplexing;Job shop scheduling;Timing;Resource management;Petri nets","aerospace computing;formal verification;message passing;microprocessor chips;Petri nets;resource allocation;safety-critical software;scheduling","compositional verification;hierarchical scheduling;realtime system;HS technique;resource partitioning;compositional analysis;architectural verification;high-performance microprocessor;formal specification;schedulability analysis;time division multiplexing;TDM global scheduler;preemptive fixed priority local scheduler;FP local scheduler;ARINC-653 standard;periodic task;sporadic task;jittering task;nondeterministic execution time;intra-application synchronization;interapplication communication;message passing;semaphore;mailbox;compositional design;preemptive time Petri nets model;required interface concept;RI concept;sequencing constraint;timing constraint;safety-critical avionic system","","19","","41","","","","","","IEEE","IEEE Journals & Magazines"
"A software science model of compile time","W. H. Shaw; J. W. Howatt; R. S. Maness; D. M. Miller","Dept. of Electr. & Comput. Eng., Air Force Inst. of Technol., Wright-Patterson AFB, OH, USA; Dept. of Electr. & Comput. Eng., Air Force Inst. of Technol., Wright-Patterson AFB, OH, USA; Dept. of Electr. & Comput. Eng., Air Force Inst. of Technol., Wright-Patterson AFB, OH, USA; Dept. of Electr. & Comput. Eng., Air Force Inst. of Technol., Wright-Patterson AFB, OH, USA","IEEE Transactions on Software Engineering","","1989","15","5","543","549","The Halstead theory of software science is used to describe the compilation process and generate a compiler performance index. A nonlinear model of compile time is estimated for four Ada compilers. A fundamental relation between compile time and program modularity is proposed. Issues considered include data collection procedures, the development of a counting strategy, the analysis of the complexity measures used, and the investigation of significant relationships between program characteristics and compile time. The results indicate that the model has a high predictive power and provides interesting insights into compiler performance phenomena. The research suggests that the discrimination rate of a compiler is a valuable performance index and is preferred to average compile-time statistics.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24703","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24703","","Military computing;Embedded computing;Software performance;Time measurement;Force control;Performance analysis;Aerospace electronics;Marine vehicles;Predictive models;Statistics","performance evaluation;program compilers","software science model;compile time;Halstead theory of software science;compilation process;compiler performance index;nonlinear model;Ada compilers;fundamental relation;program modularity;data collection;counting strategy;complexity measures;significant relationships;program characteristics;predictive power;compiler performance;discrimination rate;performance index","","2","","11","","","","","","IEEE","IEEE Journals & Magazines"
"A logical theory of interfaces and objects","P. S. C. Alencar; D. D. Cowan; C. J. P. Lucena","Dept. of Comput. Sci., Waterloo Univ., Ont., Canada; Dept. of Comput. Sci., Waterloo Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","2002","28","6","548","575","This paper motivates and describes a logic-based approach to specifying and reasoning about interfaces and objects that focuses on separation of concerns issues. The approach is based on the abstract design view (ADV), a software design model for object-oriented systems. The model was originally introduced to characterize, in an informal and practical setting, a clear separation between objects, which we called abstract design objects and their interfaces (ADVs). The objects capture the basic concern, while the interfaces capture special concerns such as user interface, control, timing, and distribution. First, we analyze the ADV design model in order to precisely characterize the interfaces, their associated objects, and the relationship between them. Then, we present one possible approach to formalizing interfaces, objects, and the ""views-a"" relationship. The central mathematical tools used for this purpose are temporal logic and some tools from the category theory. The formal approach is illustrated by examples that show how the interface and related objects and the views-a relationship can be used in object-oriented specifications. We also show how the theory enables the designer to perform relevant analysis activities while modeling with separation of concerns in mind. The theory can be used to derive dynamic and structural properties of the interface objects and the views-a relationship. In particular, we can use the theory to derive global properties of interfaces that capture special concerns from the local properties of their related objects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010059","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010059","","Object oriented modeling;Application software;User interfaces;Computer interfaces;Distributed computing;Timing;Software design;Performance analysis;Logic design;Concurrent computing","object-oriented programming;category theory;temporal logic;user interfaces;formal specification;formal verification","abstract design views;user interfaces;concurrency;specification;verification;abstract design objects;temporal logic;category theory;object-oriented program;separation of concerns","","7","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Determining inspection cost-effectiveness by combining project data and expert opinion","B. Freimut; L. C. Briand; F. Vollei","Fraunhofer Inst. for Exp. Software, Kaiserslautern, Germany; Fraunhofer Inst. for Exp. Software, Kaiserslautern, Germany; Fraunhofer Inst. for Exp. Software, Kaiserslautern, Germany","IEEE Transactions on Software Engineering","","2005","31","12","1074","1092","There is a general agreement among software engineering practitioners that software inspections are an important technique to achieve high software quality at a reasonable cost. However, there are many ways to perform such inspections and many factors that affect their cost-effectiveness. It is therefore important to be able to estimate this cost-effectiveness in order to monitor it, improve it, and convince developers and management that the technology and related investments are worth while. This work proposes a rigorous but practical way to do so. In particular, a meaningful model to measure cost-effectiveness is proposed and a method to determine cost-effectiveness by combining project data and expert opinion is described. To demonstrate the feasibility of the proposed approach, the results of a large-scale industrial case study are presented and an initial validation is performed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.136","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566608","Index Terms- Software inspection;cost-effectiveness model;Monte Carlo simulation;case study;expert opinion.","Inspection;Costs;Software quality;Monitoring;Programming;Software engineering;Technology management;Investments;Particle measurements;Large-scale systems","inspection;software quality;software cost estimation","software engineering;software inspection;software quality;cost-effectiveness estimation;project data;expert opinion","","13","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis of Database System Architectures Using Benchmarks","S. B. Yao; A. R. Hevner; H. Young-Myers","Database Systems Research Center, College of Business and Management, University of Maryland; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","6","709","725","Database machine architectures have been proposed as a promising alternative to improve database system performance, control, and flexibility. While many claims have been made for the database machine concept, few studies have been made to test the performance advantages and disadvantages of a database machine in an application environment. A comprehensive benchmark study comparing the performance of database systems on a conventional computer system and a database machine is reported in this paper. The results show the database machine architecture to have superior performance in most cases. The performance advantage is sensitive to the communication line speed between the host computer and the database machine. The effects of line speed are studied and displayed in the benchmark results. A summary of the similarities and differences between the architectures based upon our benchmark results completes the paper.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233476","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702276","Benchmarking;database machines;database system architectures;performance evaluation","Data analysis;Database systems;Database machines;Computer architecture;Application software;Benchmark testing;Operating systems;Control systems;Hardware;Computer interfaces","","Benchmarking;database machines;database system architectures;performance evaluation","","3","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Formal analysis of a space-craft controller using SPIN","K. Havelund; M. Lowry; J. Penix","Kestrel Technol., NASA Ames Res. Center, Moffett Field, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","8","749","765","The paper documents an application of the finite state model checker SPIN to formally analyze a multithreaded plan execution module. The plan execution module is one component of NASA's New Millennium Remote Agent, an artificial intelligence-based spacecraft control system architecture which launched in October of 1998 as part of the DEEP SPACE 1 mission. The bottom layer of the plan execution module architecture is a domain specific language, named ESL (Executive Support Language), implemented as an extension to multithreaded COMMON LISP. ESL supports the construction of reactive control mechanisms for autonomous robots and spacecraft. For the case study, we translated the ESL services for managing interacting parallel goal-and-event driven processes into the PROMELA input language of SPIN. A total of five previously undiscovered concurrency errors were identified within the implementation of ESL. According to the Remote Agent programming team, the effort has had a major impact, locating errors that would not have been located otherwise and, in one case, identifying a major design flaw. In fact, in a different part of the system, a concurrency bug identical to one discovered by this study escaped testing and caused a deadlock during an in-flight experiment, 96 million kilometers from Earth. The work additionally motivated the introduction of procedural abstraction in terms of inline procedures into SPIN.","0098-5589;1939-3520;2326-3881","","10.1109/32.940728","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=940728","","Space vehicles;Concurrent computing;Intelligent systems;Artificial intelligence;Intelligent agent;Intelligent control;Control systems;Space missions;Domain specific languages;Orbital robotics","space vehicles;aerospace control;multi-threading;intelligent control;program verification;software agents","formal analysis;spacecraft controller;SPIN;finite state model checker;multithreaded plan execution module;New Millennium Remote Agent;artificial intelligence-based spacecraft control system architecture;DEEP SPACE 1 mission;plan execution module architecture;domain specific language;Executive Support Language;multithreaded COMMON LISP;reactive control mechanisms;autonomous robots;ESL services;interacting parallel goal-and-event driven processes;PROMELA input language;concurrency errors;Remote Agent programming team;design flaw;concurrency bug;in-flight experiment;procedural abstraction;inline procedures","","59","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Processing Implication on Queries","Xian-he Sun; N. N. Kamel; L. M. Ni","Departmcnt of Computer Science. Michigan State University. East Lansing. Ml; NA; NA","IEEE Transactions on Software Engineering","","1989","15","10","1168","1175","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1989.559764","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=559764","","Database systems;Relational databases;Sun;Distributed databases;Costs;Polynomials;Senior members;Protocols;Query processing","","Database;Derivability Problem;Directed Graph;Implication Problem;Mathematical Logic;NP-hard;project-select-joint queries;satisfiability","","8","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Learning a Metric for Code Readability","R. P. L. Buse; W. R. Weimer","University of Virginia, Charlottesville; University of Virginia, Charlottesville","IEEE Transactions on Software Engineering","","2010","36","4","546","558","In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from 120 human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80 percent effective and better than a human, on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with three measures of software quality: code changes, automated defect reports, and defect log messages. We measure these correlations on over 2.2 million lines of code, as well as longitudinally, over many releases of selected projects. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggest that comments, in and of themselves, are less important than simple blank lines to local judgments of readability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5332232","Software readability;program understanding;machine learning;software maintenance;code metrics;FindBugs.","Software quality;Humans;Software maintenance;Readability metrics;Documentation;Software measurement;Computer languages;Design engineering;Machine learning;Costs","human factors;software quality","code readability;software quality;local code features;human notions;code changes;automated defect reports;defect log messages;programming language design","","64","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Fixing of Programs with Contracts","Y. Pei; C. A. Furia; M. Nordio; Y. Wei; B. Meyer; A. Zeller","Chair of Software Engineering, Department of Computer Science, ETH Zürich, Switzerland; Chair of Software Engineering, Department of Computer Science, ETH Zürich, Switzerland; Chair of Software Engineering, Department of Computer Science, ETH Zürich, Switzerland; Constraint Reasoning Group, Microsoft Research Cambridge, United Kingdom; Chair of Software Engineering, Department of Computer Science, ETH Zürich, Switzerland; Software Engineering Chair, Saarland University, Germany","IEEE Transactions on Software Engineering","","2014","40","5","427","449","This paper describes AutoFix, an automatic debugging technique that can fix faults in general-purpose software. To provide high-quality fix suggestions and to enable automation of the whole debugging process, AutoFix relies on the presence of simple specification elements in the form of contracts (such as pre- and postconditions). Using contracts enhances the precision of dynamic analysis techniques for fault detection and localization, and for validating fixes. The only required user input to the AutoFix supporting tool is then a faulty program annotated with contracts; the tool produces a collection of validated fixes for the fault ranked according to an estimate of their suitability. In an extensive experimental evaluation, we applied AutoFix to over 200 faults in four code bases of different maturity and quality (of implementation and of contracts). AutoFix successfully fixed 42 percent of the faults, producing, in the majority of cases, corrections of quality comparable to those competent programmers would write; the used computational resources were modest, with an average time per fix below 20 minutes on commodity hardware. These figures compare favorably to the state of the art in automated program fixing, and demonstrate that the AutoFix approach is successfully applicable to reduce the debugging burden in real-world scenarios.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2312918","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6776507","Automatic program repair;contracts;dynamic analysis","Indexes;Contracts;Debugging;Libraries;Software engineering;Software;Automation","contracts;fault diagnosis;program debugging;program diagnostics;software tools","automated program fixing;contracts;automatic debugging technique;general-purpose software fault fixing;high-quality fix suggestions;debugging process;dynamic analysis techniques;fault localization;fault detection;AutoFix supporting tool;computational resources;commodity hardware;AutoFix approach","","34","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Algorithms for the generation of state-level representations of stochastic activity networks with general reward structures","M. A. Qureshi; W. H. Sanders; A. P. A. van Moorsel; R. German","AT&T Bell Labs., Holmdel, NJ, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1996","22","9","603","614","Stochastic Petri nets (SPNs) and extensions are a popular method for evaluating a wide variety of systems. In most cases, their numerical solution requires generating a state-level stochastic process, which captures the behavior of the SPN with respect to a set of specified performance measures. These measures are commonly defined at the net level by means of a reward variable. In this paper, we discuss issues regarding the generation of state-level reward models for systems specified as stochastic activity networks (SANs) with ""step-based reward structures"". Step-based reward structures are a generalization of previously proposed reward structures for SPNs and can represent all reward variables that can be defined on the marking behavior of a net. While discussing issues related to the generation of the underlying state-level reward model, we provide an algorithm to determine whether a given SAN is ""well-specified"" A SAN is well-specified if choices about which instantaneous activity completes among multiple simultaneously-enabled instantaneous activities do not matter, with respect to the probability of reaching next possible stable markings and the distribution of reward obtained upon completion of a timed activity. The fact that a SAN is well specified is both a necessary and sufficient condition for its behavior to be completely probabilistically specified, and hence is an important property to determine.","0098-5589;1939-3520;2326-3881","","10.1109/32.541432","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=541432","","Stochastic processes;Storage area networks;Petri nets;Stochastic systems;Terminology;Sufficient conditions;Markov processes;Computer networks;Fault tolerant systems;Degradation","Petri nets;stochastic systems;Markov processes;probability;performance index","state-level representations;stochastic activity networks;general reward structures;stochastic Petri nets;performance measures;reward model generation;step-based reward structures;reward variables;marking behavior;well-specified networks;multiple simultaneously enabled instantaneous activities;timed activity completion;sufficient condition;completely probabilistically specified behaviour;Markov processes","","11","","13","","","","","","IEEE","IEEE Journals & Magazines"
"A Field Evaluation of Natural Language for Data Retrieval","M. Jarke; J. A. Tuner; E. A. Stohr; Y. Vassiliou; N. H. White; K. Michielsen","Graduate School of Business Administration, Computer Applications and Information Systems Area, New York University; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","1","97","114","Although a large number of natural language database interfaces have been developed, there have been few empirical studies of their practical usefulness. This paper presents the design and results of a field evaluation of a natural language system-NLS-used for data retrieval.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231847","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701902","Human-machine interaction;interface design;language evaluation;natural language query;query languages","Natural languages;Information retrieval;Laboratories;Application software;Database languages;Relational databases;Problem-solving;Information systems;Data analysis;Performance evaluation","","Human-machine interaction;interface design;language evaluation;natural language query;query languages","","10","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Cross versus Within-Company Cost Estimation Studies: A Systematic Review","B. A. Kitchenham; E. Mendes; G. H. Travassos","IEEE Computer Society; NA; NA","IEEE Transactions on Software Engineering","","2007","33","5","316","329","The objective of this paper is to determine under what circumstances individual organizations would be able to rely on cross-company-based estimation models. We performed a systematic review of studies that compared predictions from cross-company models with predictions from within-company models based on analysis of project data. Ten papers compared cross-company and within-company estimation models; however, only seven presented independent results. Of those seven, three found that cross-company models were not significantly different from within-company models, and four found that cross-company models were significantly worse than within-company models. Experimental procedures used by the studies differed making it impossible to undertake formal meta-analysis of the results. The main trend distinguishing study results was that studies with small within-company data sets (i.e., $20 projects) that used leave-one-out cross validation all found that the within-company model was significantly different (better) from the cross-company model. The results of this review are inconclusive. It is clear that some organizations would be ill-served by cross-company models whereas others would benefit. Further studies are needed, but they must be independent (i.e., based on different data bases or at least different single company data sets) and should address specific hypotheses concerning the conditions that would favor cross-company or within-company models. In addition, experimenters need to standardize their experimental procedures to enable formal meta-analysis, and recommendations are made in Section 3.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1001","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160970","Cost estimation;management;systematic review;software engineering.","Costs;Predictive models;Computer Society;Data analysis;Performance analysis;Engineering management;Software engineering;Proposals;Productivity;Accuracy","software cost estimation","software engineering;software cost estimation models;cross-company-based estimation","","152","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Process Aspects and Social Dynamics of Contemporary Code Review: Insights from Open Source Development and Industrial Practice at Microsoft","A. Bosu; J. C. Carver; C. Bird; J. Orbeck; C. Chockley","Department of Computer Science, Southern Illinois University, Carbondale, IL; Department of Computer Science, University of Alabama, Tuscaloosa, AL; Microsoft Research, Microsoft Corportation, Redmond, WA; Department of Computer Science, University of Alabama, Tuscaloosa, AL; Department of Computer Science, University of Alabama, Tuscaloosa, AL","IEEE Transactions on Software Engineering","","2017","43","1","56","75","Many open source and commercial developers practice contemporary code review, a lightweight, informal, tool-based code review process. To better understand this process and its benefits, we gathered information about code review practices via surveys of open source software developers and developers from Microsoft. The results of our analysis suggest that developers spend approximately 10-15 percent of their time in code reviews, with the amount of effort increasing with experience. Developers consider code review important, stating that in addition to finding defects, code reviews offer other benefits, including knowledge sharing, community building, and maintaining code quality. The quality of the code submitted for review helps reviewers form impressions about their teammates, which can influence future collaborations. We found a large amount of similarity between the Microsoft and OSS respondents. One interesting difference is that while OSS respondents view code review as an important method of impression formation, Microsoft respondents found knowledge dissemination to be more important. Finally, we found little difference between distributed and co-located Microsoft teams. Our findings identify the following key areas that warrant focused research: 1) exploring the non-technical benefits of code reviews, 2) helping developers in articulating review comments, and 3) assisting reviewers' program comprehension during code reviews.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2576451","US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7484733","Code review;open source;OSS;survey;peer impressions;commercial projects","Inspection;Organizations;Collaboration;Context;Instruments;Measurement;Human factors","public domain software;software engineering;software management;software reviews;team working","contemporary code review;open source software developers;code quality;OSS;knowledge dissemination;Microsoft teams","","2","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Controls for Interorganization Networks","D. Estrin","Department of Computer Science, University of Southern California","IEEE Transactions on Software Engineering","","1987","SE-13","2","249","261","Interorganization computer networks support person-to-person communication via electronic mail; exchange of cad/cam data, software modules, or documents via file transfer; input to an order-entry or accounting system via a database query and update protocol; and use of shared computational resources via an asynchronous message protocol or remote login. In most such interorganization arrangements, the set of resources that an organization wants to make accessible to outsiders is significantly smaller than the set of resources that it wants to remain strictly-internal (i.e., accessible to employees of the organization only). In addition, because the potential user is a person (or machine) outside the boundaries of the organization, the damage associated with undesired use can be high. Because of these characteristics, Interorganization Networks (ION's) have unique usage-control requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233149","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702204","Access control;authentication;network interconnection;network security","Communication system control;Computer networks;Access protocols;Authentication;Electronic mail;Communication system software;CADCAM;Communication system security;Data security;Control systems","","Access control;authentication;network interconnection;network security","","11","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Ada program partitioning language: a notion for distributing Ada programs","R. Jha; J. M. Kamrad; D. T. Cornhill","Honeywell Syst. & Res. Center, Minneapolis, MN, USA; Honeywell Syst. & Res. Center, Minneapolis, MN, USA; Honeywell Syst. & Res. Center, Minneapolis, MN, USA","IEEE Transactions on Software Engineering","","1989","15","3","271","280","Ada Program Partitioning Language (APPL) has been designed as part of Honeywell's Distributed Ada project. The goal of the project is to develop an approach for reducing the complexity of building distributed applications in Ada. In the proposed approach, an application is written as a single Ada program using the full capabilities of the Ada language. It is not necessary to factor the underlying hardware configuration into the program design. Once the program has been completed and tested in the host development environment, it is partitioned into fragments and mapped onto the distributed hardware. The partitioning and mapping are expressed in APPL and do not require changes to the Ada source. The main thrusts of the project include the design of APPL and the development of language translation tools and the run-time system to support Ada and APPL for a distributed target. The authors present an overview of APPL, the goals considered in the design, and issues that impact its implementation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21755","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21755","","Application software;Hardware;Embedded system;Buildings;Embedded software;Software systems;Writing;Testing;Distributed computing;Software design","Ada;distributed processing;program interpreters;software tools","Honeywell;Ada program partitioning language;APPL;Distributed Ada project;complexity;distributed applications;hardware configuration;program design;host development environment;Ada;language translation tools;run-time system","","14","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Constructing meta-CASE workbenches by exploiting visual language generators","G. Costagliola; V. Deufemia; F. Ferrucci; C. Gravino","Dipt. di Matematica e Inf., Salerno Univ., Italy; Dipt. di Matematica e Inf., Salerno Univ., Italy; Dipt. di Matematica e Inf., Salerno Univ., Italy; Dipt. di Matematica e Inf., Salerno Univ., Italy","IEEE Transactions on Software Engineering","","2006","32","3","156","175","In this paper, we propose an approach for the construction of meta-CASE workbenches, which suitably integrates the technology of visual language generation systems, UML metamodeling, and interoperability techniques based on the GXL (graph exchange language) format. The proposed system consists of two major components. Environments for single visual languages are generated by using the modeling language environment generator (MEG), which follows a metamodel/grammar-approach. The abstract syntax of a visual language is defined by UML class diagrams, which serve as a base for the grammar specification of the language. The workbench generator (WoG) allows designers to specify the target workbench by means of a process model given in terms of a suitable activity diagram. Starting from the supplied specification WoG generates the customized workbench by integrating the required environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610608","Meta-CASE tools;metamodeling techniques;visual languages;visual programming environment generators.","Computer aided software engineering;Power system modeling;Unified modeling language;Metamodeling;Programming environments;Software design;Collaborative work;Software tools;Employment;Debugging","visual languages;visual programming;automatic programming;programming environments;computer aided software engineering;formal specification;Unified Modeling Language;XML;grammars;open systems","meta-CASE workbench tool;visual language generation system;UML metamodeling;interoperability techniques;GXL;graph exchange language format;modeling language environment generator;MEG;metamodel/grammar-approach;UML class diagram;language grammar specification;workbench generator;WoG;visual programming environment generator","","10","","73","","","","","","IEEE","IEEE Journals & Magazines"
"Orca: a language for parallel programming of distributed systems","H. E. Bal; M. F. Kaashoek; A. S. Tanenbaum","Dept. of Math. & Comput. Sci., Vrije Univ., Amsterdam, Netherlands; Dept. of Math. & Comput. Sci., Vrije Univ., Amsterdam, Netherlands; Dept. of Math. & Comput. Sci., Vrije Univ., Amsterdam, Netherlands","IEEE Transactions on Software Engineering","","1992","18","3","190","205","A detailed description is given of the Orca language design and the design choices are discussed. Orca is intended for applications programmers rather than systems programmers. This is reflected in its design goals to provide a simple, easy-to-use language that is type-secure and provides clean semantics. Three example parallel applications in Orca, one of which is described in detail, are discussed. One of the existing implementations, which is based on reliable broadcasting, is described. Performance measurements of this system are given for three parallel applications. The measurements show that significant speedups can be obtained for all three applications. The authors compare Orca with several related languages and systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.126768","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=126768","","Parallel programming;Programming profession;Broadcasting;Workstations;Local area networks;Application software;Message passing;Velocity measurement;Costs;Sun","parallel languages;parallel programming","parallel programming;distributed systems;Orca language design;design choices;applications programmers;easy-to-use language;type-secure;clean semantics;parallel applications;Orca;reliable broadcasting","","160","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Tranquility: A Low Disruptive Alternative to Quiescence for Ensuring Safe Dynamic Updates","Y. Vandewoude; P. Ebraert; Y. Berbers; T. D'Hondt","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","856","868","This paper revisits a problem that was identified by Kramer and Magee: placing a system in a consistent state before and after runtime changes. We show that their notion of quiescence as a necessary and sufficient condition for safe runtime changes is too strict and results in a significant disruption in the application being updated. In this paper, we introduce a weaker condition: tranquillity. We show that tranquillity is easier to obtain and less disruptive for the running application but still a sufficient condition to ensure application consistency. We present an implementation of our approach on a component middleware platform and experimentally verify the validity and practical applicability of our approach using data retrieved from a case study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70733","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359466","Distributed objects;Componentware;Application-aware adaptation;Distributed objects;components;containers;Distributed objects;Componentware;Application-aware adaptation;Distributed objects;components;containers","Application software;Runtime;Sufficient conditions;Software engineering;Middleware;Information retrieval;Computer languages;Operating systems;Software systems;Containers","middleware;object-oriented programming","tranquillity;quiescence;necessary and sufficient condition;runtime changes;application consistency;component middleware platform","","70","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Program readability: procedures versus comments","T. Tenny","Dept. of Comput. Sci., Texas Christian Univ., Fort Worth, TX, USA","IEEE Transactions on Software Engineering","","1988","14","9","1271","1279","A 3*2 factorial experiment was performed to compare the effects of procedure format (none, internal, or external) with those of comments (absent or present) on the readability of a PL/1 program. The readability of six editions of the program, each having a different combination of these factors, was inferred from the accuracy with which students could answer questions about the program after reading it. Both extremes in readability occurred in the program editions having no procedures: without comments the procedureless program was the least readable and with comments it was the most readable.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6171","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6171","","Testing;Programming profession;Software engineering;Proposals;Costs;Computer science","PL/1;programming","procedures;comments;factorial experiment;procedure format;PL/1 program;readability","","46","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Tractable dataflow analysis for distributed systems","Shing Chi Cheung; J. Kramer","Dept. of Comput. Sci., Hong Kong Univ. of Sci. & Technol., Hong Kong; NA","IEEE Transactions on Software Engineering","","1994","20","8","579","593","Automated behavior analysis is a valuable technique in the development and maintenance of distributed systems. In this paper, we present a tractable dataflow analysis technique for the detection of unreachable states and actions in distributed systems. The technique follows an approximate approach described by Reif and Smolka, but delivers a more accurate result in assessing unreachable states and actions. The higher accuracy is achieved by the use of two concepts: action dependency and history sets. Although the technique does not exhaustively detect all possible errors, it detects nontrivial errors with a worst-case complexity quadratic to the system size. It can be automated and applied to systems with arbitrary loops and nondeterministic structures. The technique thus provides practical and tractable behavior analysis for preliminary designs of distributed systems. This makes it an ideal candidate for an interactive checker in software development tools. The technique is illustrated with case studies of a pump control system and an erroneous distributed program. Results from a prototype implementation are presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310668","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310668","","Data analysis;History;Programming;Software engineering;Computational efficiency;Computer errors;Automatic control;Control systems;Software prototyping;Prototypes","distributed processing;software engineering","distributed systems;dataflow analysis;action dependency;history sets;worst-case complexity;pump control system;software development tools;arbitrary loops;nondeterministic structures;labeled transition systems;static analysis;program verification;distributed software engineering;synchronous communicating systems;reachability analysis","","7","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Advancing candidate link generation for requirements tracing: the study of methods","J. H. Hayes; A. Dekhtyar; S. K. Sundaram","Dept. of Comput. Sci., Kentucky Univ., Lexington, KY, USA; Dept. of Comput. Sci., Kentucky Univ., Lexington, KY, USA; Dept. of Comput. Sci., Kentucky Univ., Lexington, KY, USA","IEEE Transactions on Software Engineering","","2006","32","1","4","19","This paper addresses the issues related to improving the overall quality of the dynamic candidate link generation for the requirements tracing process for verification and validation and independent verification and validation analysts. The contribution of the paper is four-fold: we define goals for a tracing tool based on analyst responsibilities in the tracing process, we introduce several new measures for validating that the goals have been satisfied, we implement analyst feedback in the tracing process, and we present a prototype tool that we built, RETRO (REquirements TRacing On-target), to address these goals. We also present the results of a study used to assess RETRO's support of goals and goal elements that can be measured objectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.3","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583599","Requirements tracing;dynamic link generation;Verification and Validation (V&amp;V);Independent Validation and Verification (IV&amp;V);information retrieval;TF-IDF;LSI;recall;precision.","Risk analysis;System testing;Spine;Computer Society;Feedback;Prototypes;Information retrieval;Large scale integration;Computer aided software engineering","formal verification;formal specification;software tools","dynamic candidate link generation;RETRO tool;Requirements Tracing On-target tool;independent verification analysis;independent validation analysis","","232","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Privately Finding Specifications","W. Weimer; N. Mishra","NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","21","32","Buggy software is a reality and automated techniques for discovering bugs are highly desirable. A specification describes the correct behavior of a program. For example, a file must eventually be closed once it has been opened. Specifications are learned by finding patterns in normal program execution traces versus erroneous ones. With more traces, more specifications can be learned more accurately. By combining traces from multiple parties that possess distinct programs but use a common library, it is possible to obtain sufficiently many traces. However, obtaining traces from competing parties is problematic: By revealing traces, it may be possible to learn that one party writes buggier code than another. We present an algorithm by which mutually distrusting parties can work together to learn program specifications while preserving their privacy. We use a perturbation algorithm to obfuscate individual trace values while still allowing statistical trends to be mined from the data. Despite the noise introduced to safeguard privacy, empirical evidence suggests that our algorithm learns specifications that find 85 percent of the bugs that a no-privacy approach would find.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70744","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359470","F.3.1.f Specification techniques;D.2.19 Software Quality/SQA;I.2.6 Learning;K.4.1.f Privacy;F.3.1.f Specification techniques;D.2.19 Software Quality/SQA;I.2.6 Learning;K.4.1.f Privacy","Computer bugs;Privacy;Software testing;Data security;Operating systems;Libraries;Software quality;Formal specifications;Information security;Concurrent computing","data mining;perturbation techniques;program debugging;software maintenance","buggy software;specification techniques;perturbation algorithm;data mining","","3","","40","","","","","","IEEE","IEEE Journals & Magazines"
"<italic>SITAR</italic>: GUI Test Script Repair","Z. Gao; Z. Chen; Y. Zou; A. M. Memon","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science, University of Maryland, College Park, MD, USA","IEEE Transactions on Software Engineering","","2016","42","2","170","186","System testing of a GUI-based application requires that test cases, consisting of sequences of user actions/events, be executed and the software's output be verified. To enable automated re-testing, such test cases are increasingly being coded as low-level test scripts, to be replayed automatically using test harnesses. Whenever the GUI changes-widgets get moved around, windows get merged-some scripts become unusable because they no longer encode valid input sequences. Moreover, because the software's output may have changed, their test oracles-assertions and checkpoints-encoded in the scripts may no longer correctly check the intended GUI objects. We present ScrIpT repAireR (SITAR), a technique to automatically repair unusable low-level test scripts. SITAR uses reverse engineering techniques to create an abstract test for each script, maps it to an annotated event-flow graph (EFG), uses repairing transformations and human input to repair the test, and synthesizes a new “repaired” test script. During this process, SITAR also repairs the reference to the GUI objects used in the checkpoints yielding a final test script that can be executed automatically to validate the revised software. SITAR amortizes the cost of human intervention across multiple scripts by accumulating the human knowledge as annotations on the EFG. An experiment using QTP test scripts suggests that SITAR is effective in that 41-89 percent unusable test scripts were repaired. Annotations significantly reduced human cost after 20 percent test scripts had been repaired.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2454510","National Basic Research Program of China; NSFC; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7214294","GUI testing;GUI test script;test script repair;human knowledge accumulation;GUI testing;GUI test script;test script repair;human knowledge accumulation","Maintenance engineering;Graphical user interfaces;Software;Testing;Automation;Computational modeling;Electronic mail","graph theory;graphical user interfaces;program testing;software maintenance","SITAR;test script repairer;graphical user interface;GUI-based application;system testing;automated retesting;event-flow graph;EFG;software validation","","9","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Resolving race conditions in asynchronous partial order scenarios","B. Mitchell","Dept. of Comput., Surrey Univ., Guildford, UK","IEEE Transactions on Software Engineering","","2005","31","9","767","784","Scenario-based requirements specifications are the industry norm for defining communication protocols. However, such scenarios often contain race conditions. A race condition occurs when events are specified to occur in a particular order, but in practice, this order cannot be guaranteed. The paper considers UML/MSC scenarios that can be described with standard partial order theoretic asynchronous behavioral semantics. We define these to be partial order scenarios. The paper proves there is a unique minimal generalization of a partial order scenario that is race free. The paper also proves there is a unique minimal race free refinement of the behavioral semantics of a partial order scenario. Unlike the generalization, the refinement cannot be realized in the form of a partial order scenario, although it can always be embedded in one. The paper, also proves the results can be generalized to a subclass of iterative scenarios.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.104","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514445","Index Terms- Requirements analysis;formal methods;distributed programming.","Unified modeling language;Protocols;Communication industry;Communication systems;Specification languages;Message passing;Creep;Inspection;Software tools","Unified Modeling Language;formal specification;distributed programming;programming language semantics","race condition;asynchronous partial order scenarios;formal specifications;UML;asynchronous behavioral semantics;distributed programming","","14","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Design and specification of iterators using the swapping paradigm","B. W. Weide; S. H. Edwards; D. E. Harms; D. A. Lamb","Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA; NA; NA","IEEE Transactions on Software Engineering","","1994","20","8","631","643","How should iterators be abstracted and encapsulated in modern imperative languages? We consider the combined impact of several factors on this question: the need for a common interface model for user defined iterator abstractions, the importance of formal methods in specifying such a model, and problems involved in modular correctness proofs of iterator implementations and clients. A series of iterator designs illustrates the advantages of the swapping paradigm over the traditional copying paradigm. Specifically, swapping based designs admit more efficient implementations while offering relatively straightforward formal specifications and the potential for modular reasoning about program behavior. The final proposed design schema is a common interface model for an iterator for any generic collection.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310672","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310672","","Formal specifications;Reasoning about programs;Information science;Senior members;Forward contracts;Modems;Packaging;Proposals;Computerized monitoring;Military computing","formal specification;program verification;data encapsulation","iterators;swapping paradigm;imperative languages;common interface model;user defined iterator abstractions;formal methods;modular correctness proofs;iterator designs;formal specification;modular reasoning;program verification;proof of correctness;swapping","","2","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Design Pattern Detection Using Similarity Scoring","N. Tsantalis; A. Chatzigeorgiou; G. Stephanides; S. T. Halkidis","NA; IEEE Computer Society; IEEE Computer Society; NA","IEEE Transactions on Software Engineering","","2006","32","11","896","909","The identification of design patterns as part of the reengineering process can convey important information to the designer. However, existing pattern detection methodologies generally have problems in dealing with one or more of the following issues: identification of modified pattern versions, search space explosion for large systems and extensibility to novel patterns. In this paper, a design pattern detection methodology is proposed that is based on similarity scoring between graph vertices. Due to the nature of the underlying graph algorithm, this approach has the ability to also recognize patterns that are modified from their standard representation. Moreover, the approach exploits the fact that patterns reside in one or more inheritance hierarchies, reducing the size of the graphs to which the algorithm is applied. Finally, the algorithm does not rely on any pattern-specific heuristic, facilitating the extension to novel design structures. Evaluation on three open-source projects demonstrated the accuracy and the efficiency of the proposed method","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.112","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015512","Patterns;object-oriented design methods;graph algorithms;restructuring;reverse engineering;reengineering.","Computer Society;Explosions;Design methodology;Open source software;Software systems;Space exploration;Clustering algorithms;Pattern recognition;Algorithm design and analysis;Reverse engineering","graph theory;object-oriented methods;object-oriented programming;systems re-engineering","design pattern detection;reengineering process;graph vertices;open-source project;graph algorithms","","195","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting Self-Adaptation via Quantitative Verification and Sensitivity Analysis at Run Time","A. Filieri; G. Tamburrelli; C. Ghezzi","Reliable Software Systems Group, University of Stuttgart, Stuttgart, Germany; Vrije University of Amsterdam, Netherlands; Dipartimento di Elettronica, Informazione e Bioingegneria at Politecnico di Milano, Milan, Italy","IEEE Transactions on Software Engineering","","2016","42","1","75","99","Modern software-intensive systems often interact with an environment whose behavior changes over time, often unpredictably. The occurrence of changes may jeopardize their ability to meet the desired requirements. It is therefore desirable to design software in a way that it can self-adapt to the occurrence of changes with limited, or even without, human intervention. Self-adaptation can be achieved by bringing software models and model checking to run time, to support perpetual automatic reasoning about changes. Once a change is detected, the system itself can predict if requirements violations may occur and enable appropriate counter-actions. However, existing mainstream model checking techniques and tools were not conceived for run-time usage; hence they hardly meet the constraints imposed by on-the-fly analysis in terms of execution time and memory usage. This paper addresses this issue and focuses on perpetual satisfaction of non-functional requirements, such as reliability or energy consumption. Its main contribution is the description of a mathematical framework for run-time efficient probabilistic model checking. Our approach statically generates a set of verification conditions that can be efficiently evaluated at run time as soon as changes occur. The proposed approach also supports sensitivity analysis, which enables reasoning about the effects of changes and can drive effective adaptation strategies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2421318","European Commission; Programme IDEAS-ERC; Project 227977-SMScom; Programme FP7-PEOPLE-2011-IEF; Project 302648-RunMore; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7083754","Self-adaptive Systems;Software Evolution;Non-functional Requirements;Discrete-Time Markov models;Rewards;Software Reliability;Costs;Probabilistic Model Checking;Models at Runtime;Self-adaptive systems;software evolution;non-functional requirements;discrete-time Markov models;rewards;software reliability;costs;probabilistic model checking;models at runtime","Model checking;Adaptation models;Software;Markov processes;Probabilistic logic;Computational modeling;Reliability","probability;program verification;software reliability","self-adaptation;quantitative verification;run time analysis;software-intensive systems;software design;software models;perpetual automatic reasoning;requirements violation;on-the-fly analysis;execution time;memory usage;perpetual satisfaction;nonfunctional requirements;mathematical framework;probabilistic model checking;sensitivity analysis","","24","","91","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Test Suites and Adequacy Criteria Using Simulation-Based Models of Distributed Systems","M. J. Rutherford; A. Carzaniga; A. L. Wolf","University of Colorado at Boulder, Boulder; University of Lugano, Lugano; Imperial College London, London","IEEE Transactions on Software Engineering","","2008","34","4","452","470","Test adequacy criteria provide the engineer with guidance on how to populate test suites. While adequacy criteria have long been a focus of research, existing testing methods do not address many of the fundamental characteristics of distributed systems, such as distribution topology, communication failure, and timing. Furthermore, they do not provide the engineer with a means to evaluate the relative effectiveness of different criteria, nor the relative effectiveness of adequate test suites satisfying a given criterion. This paper makes three contributions to the development and use of test adequacy criteria for distributed systems: (1) a testing method based on discrete-event simulations; (2) a fault-based analysis technique for evaluating test suites and adequacy criteria; and (3) a series of case studies that validate the method and technique. The testing method uses a discrete-event simulation as an operational specification of a system, in which the behavioral effects of distribution are explicitly represented. Adequacy criteria and test cases are then defined in terms of this simulation-based specification. The fault-based analysis involves mutation of the simulation-based specification to provide a foil against which test suites and the criteria that formed them can be evaluated. Three distributed systems were used to validate the method and technique, including DNS, the domain name system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527254","Specification;Test coverage of specifications;Specification;Test coverage of specifications","System testing;Discrete event simulation;Computational modeling;Computer Society;Domain Name System;Web server;Network servers;Computer simulation;Topology;Timing","distributed processing;program testing","distributed system;test adequacy criteria;discrete-event simulation;fault-based analysis technique;simulation-based specification","","13","","61","","","","","","IEEE","IEEE Journals & Magazines"
"Early experience with the Visual Programmer's WorkBench","R. V. Rubin; J. Walker; E. J. Golin","GTE Lab., Waltham, MA, USA; GTE Lab., Waltham, MA, USA; NA","IEEE Transactions on Software Engineering","","1990","16","10","1107","1121","The Visual Programmer's WorkBench (VPW) addresses the rapid synthesis and customization of environments for the specification, analysis, and execution of visual programs. The goal of VPW is to enable the easy creation of environments for visual languages. The design of VPW and experience using it to generate a distributed programming environment for a concurrent visual language are described. A visual programming environment for the PetriFSA language generated with VPW is outlined. An overview is provided of the language definition model and its relation to the logical architecture of VPW. Details are given of the language specifications used in VPW, and its application in defining the PetriFSA language. A language-based environment for a specific visual language is generated in VPW from a specification of the syntactic structure, the abstract structure, the static semantics, and the dynamic semantics of the language. VPW is built around a model of distributed processing based on a shared distributed memory. This framework is used in defining the architecture of the environment and for the execution model of visual languages.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60292","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60292","","Programming environments;Dynamic programming;Software engineering;Synthesizers;Application software;Visual databases;Interactive systems;Distributed processing;Functional programming","data structures;formal specification;program verification;programming environments;specification languages;visual programming","Visual Programmer's WorkBench;synthesis;customization of environments;specification;visual languages;distributed programming environment;PetriFSA language;syntactic structure;abstract structure;static semantics;dynamic semantics","","18","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Early Detection of Collaboration Conflicts and Risks","Y. Brun; R. Holmes; M. D. Ernst; D. Notkin","University of Massachusetts, Amherst; University of Waterloo, Waterloo; University of Washington, Seattle; University of Washington, Seattle","IEEE Transactions on Software Engineering","","2013","39","10","1358","1375","Conflicts among developers' inconsistent copies of a shared project arise in collaborative development and can slow progress and decrease quality. Identifying and resolving such conflicts early can help. Identifying situations which may lead to conflicts can prevent some conflicts altogether. By studying nine open-source systems totaling 3.4 million lines of code, we establish that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. Motivated by this finding, we develop a speculative analysis technique that uses previously unexploited information from version control operations to precisely diagnose important classes of conflicts. Then, we design and implement Crystal, a publicly available tool that helps developers identify, manage, and prevent conflicts. Crystal uses speculative analysis to make concrete advice unobtrusively available to developers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6520859","Collaborative development;collaboration conflicts;developer awareness;speculative analysis;version control;Crystal","Crystals;Collaboration;History;Open source software;Control systems;Terminology;Computer science","configuration management;groupware;program diagnostics;public domain software;software engineering","collaboration conflicts;collaborative development;open-source systems;overlapping textual edits;subsequent build-and-test failures;speculative analysis technique;version control operations;Crystal;publicly available tool;source code","","21","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Interface Mutation: an approach for integration testing","M. E. Delamaro; J. C. Maidonado; A. P. Mathur","Dept. of Inf., Maringa State Univ., Brazil; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","228","247","The need for test adequacy criteria is widely recognized. Several criteria have been proposed for the assessment of adequacy of tests at the unit level. However, there remains a lack of criteria for the assessment of the adequacy of tests generated during integration testing. We present a mutation based interprocedural criterion, named Interface Mutation (IM), suitable for use during integration testing. A case study to evaluate the proposed criterion is reported. In the study, the UNIX sort utility was seeded with errors and Interface Mutation evaluated by measuring the cost of its application and its error revealing effectiveness. Alternative IM criteria using different sets of Interface Mutation operators were also evaluated. While comparing the error revealing effectiveness of these Interface Mutation-based test sets with same size randomly generated test sets, we observed that in most cases Interface Mutation based test sets are superior. The results suggest that Interface Mutation offers a viable test adequacy criteria for use at the integration level.","0098-5589;1939-3520;2326-3881","","10.1109/32.910859","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910859","","Genetic mutations;Software testing;Software systems;System testing;Computer Society;Costs;Programming;Redundancy;Software design","program testing;Unix;sorting","Interface Mutation;integration testing;test adequacy criteria;unit level;mutation based interprocedural criterion;case study;UNIX sort utility;error revealing effectiveness;alternative IM criteria;Interface Mutation operators;test sets;randomly generated test sets;integration level","","113","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Ethical issues in empirical studies of software engineering","J. Singer; N. G. Vinson","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada","IEEE Transactions on Software Engineering","","2002","28","12","1171","1180","The popularity of empirical methods in software engineering research is on the rise. Surveys, experiments, metrics, case studies, and field studies are examples of empirical methods used to investigate both software engineering processes and products. The increased application of empirical methods has also brought about an increase in discussions about adapting these methods to the peculiarities of software engineering. In contrast, the ethical issues raised by empirical methods have received little, if any, attention in the software engineering literature. This article is intended to introduce the ethical issues raised by empirical research to the software engineering research community and to stimulate discussion of how best to deal with these ethical issues. Through a review of the ethical codes of several fields that commonly employ humans and artifacts as research subjects, we have identified major ethical issues relevant to empirical studies of software engineering. These issues are illustrated with real empirical studies of software engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158289","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158289","","Software engineering;Ethics;Humans;Collaborative work;Application software;Guidelines;Risk management;Law;Legal factors;Collaborative software","software engineering;professional aspects;legislation","ethical issues;empirical studies;software engineering;software engineering processes;software engineering products;legal issues","","48","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Technology Selection: An Educational Approach","W. E. Riddle; L. G. Williams","Software Productivity Consortium; NA","IEEE Transactions on Software Engineering","","1987","SE-13","11","1199","1206","Creating and enhancing a software engineering work force requires several different types of continuing education for software professionals, including: task-oriented education, enhancement-oriented education and selection-oriented education. In this paper, we focus on the important, but often neglected, category of selection-oriented education. We begin with a discussion of technology selection, indicating what it involves, how it contributes to improving the state of practice, and why it is key to technology improvement in general. This is followed by a discussion of some criteria for selection-oriented education programs. We then describe the selection-oriented education activities at the Rocky Mountain Institute of Software Engineering and relate some problems encountered in establishing these activities.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232870","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702168","Software engineering education;technology selection;technology transfer","Educational technology;Educational programs;Software engineering;Continuing education;Computer science education;Technology transfer;Personnel;Educational products;Engineering education;Computer science","","Software engineering education;technology selection;technology transfer","","1","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Development life cycle of computer networks: the executable model approach","J. Etkin; J. A. Zinky","Coll. of Eng., Boston Univ., MA, USA; NA","IEEE Transactions on Software Engineering","","1989","15","9","1078","1089","An approach is proposed for extending the use of design models to the implementation and operational phases of the network development life cycle. A conceptual approach is offered for using executable models in the day-to-day operation of computer networks. Several strategies are given for integrating models into different development tasks. It is shown why these strategies are feasible. Characteristics of executable models that differ from those of traditional models are identified along with new technologies that reduce the cost of implementing and using executable models.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31366","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31366","","Computer networks;Computer network management;Predictive models;Analytical models;Embedded system;Costs;Computational modeling;Performance analysis;Software testing;Computer graphics","computer networks;digital simulation;software engineering","computer networks;design models;network development life cycle;executable models;day-to-day operation;development tasks","","4","","32","","","","","","IEEE","IEEE Journals & Magazines"
"On the Distribution of Bugs in the Eclipse System","G. Concas; M. Marchesi; A. Murgia; R. Tonelli; I. Turnu","University of Cagliari, Cagliari; University of Cagliari, Cagliari; University of Cagliari, Cagliari; University of Cagliari, Cagliari; University of Cagliari, Cagliari","IEEE Transactions on Software Engineering","","2011","37","6","872","877","The distribution of bugs in software systems has been shown to satisfy the Pareto principle, and typically shows a power-law tail when analyzed as a rank-frequency plot. In a recent paper, Zhang showed that the Weibull cumulative distribution is a very good fit for the Alberg diagram of bugs built with experimental data. In this paper, we further discuss the subject from a statistical perspective, using as case studies five versions of Eclipse, to show how log-normal, Double-Pareto, and Yule-Simon distributions may fit the bug distribution at least as well as the Weibull distribution. In particular, we show how some of these alternative distributions provide both a superior fit to empirical data and a theoretical motivation to be used for modeling the bug generation process. While our results have been obtained on Eclipse, we believe that these models, in particular the Yule-Simon one, can generalize to other software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928349","Software bug distribution;empirical research;object-oriented systems.","Computer bugs;Software systems;Data models;Computational modeling;Weibull distribution;Object oriented modeling","eclipses;Pareto analysis;Weibull distribution","software systems;Pareto principle;Weibull cumulative distribution;statistical perspective;eclipse system","","19","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional validation of time-critical systems using communicating time Petri nets","G. Bucci; E. Vicario","Dept. of Syst. & Inf., Florence Univ., Italy; Dept. of Syst. & Inf., Florence Univ., Italy","IEEE Transactions on Software Engineering","","1995","21","12","969","992","An extended Petri net model which considers modular partitioning along with timing restrictions and environment models is presented. Module constructs permit the specification of a complex system as a set of message passing modules with the timing semantics of time Petri nets. The state space of each individual module can be separately enumerated and assessed under the assumption of a partial specification of the intended module operation environment. State spaces of individual modules can be recursively integrated, to permit the assessment of module clusters and of the overall model, and to check the satisfaction of the assumptions made in the separate analysis of elementary component modules. In the intermediate stages between subsequent integration steps, the state spaces of module and module clusters can be projected onto reduced representations concealing local events that are not essential to the purposes of the analysis. The joint use of incremental enumeration and intermediate concealment of local events allows for a flexible management of state explosion, and permits a scalable approach to the validation of complex systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.489073","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=489073","","Time factors;Petri nets;State-space methods;Timing;Reachability analysis;Automata;Message passing;Explosions;System recovery;Real time systems","Petri nets;message passing;timing;formal specification;reachability analysis;program diagnostics;program verification","extended Petri net model;modular partitioning;timing restrictions;environment models;compositional validation;time-critical systems;communicating time Petri nets;module constructs;complex system specification;message passing modules;timing semantics;state space;partial specification;module operation environment;module clusters;elementary component modules;concealed local events;incremental enumeration;state explosion management","","65","","37","","","","","","IEEE","IEEE Journals & Magazines"
"A predicate-transition net model for parallel interpretation of logic programs","T. Murata; D. Zhang","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA","IEEE Transactions on Software Engineering","","1988","14","4","481","497","A predicate/transition net model for a subset of Horn clause logic programs is presented. The syntax, transformation procedure, semantics, and deduction process for the net model are discussed. A possible parallel implementation for the net model is described, which is based on the concepts of communicating processes and relations. The proposed net model offers a syntactical variant of Horn clause logic and has two distinctions from other existing schemes for the logic programs: representation formalism and the deduction method. The net model provides an approach towards the solutions of the separation of logic from control and the improvement of the execution efficiency through parallel processing for the logic programs. The abstract nature of the net model also lends itself to different implementation strategies.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4671","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4671","","Logic programming;Petri nets;Automatic control;Parallel processing;Computer languages;Humans;Computer science;Software engineering;Proposals","formal logic;logic programming;parallel programming;programming theory","logic programming;parallel programming;predicate-transition net model;parallel interpretation;logic programs;Horn clause logic;deduction process;parallel processing","","74","","64","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical comparison of textual and graphical data structure documentation for Cobol programs","J. A. Lehman","Dept. of Manage. Inf. Syst., Alaska Univ., Fairbanks, AK, USA","IEEE Transactions on Software Engineering","","1989","15","9","1131","1135","The author presents the results of an experimental investigation into the comparative usefulness of textual tools and graphical tools for the program understanding phase of Cobol program maintenance. Both novice and experienced programmers are used as subjects. The results show a slight superiority for graphical tools when they are used by less experienced programmers. They cast doubt on the importance of rigid adherence to program design methodologies for experienced programmers and on the extensibility of experiments using relatively inexperienced student subjects.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31370","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31370","","Data structures;Documentation;Design methodology;Programming profession;Flowcharts;Displays;Graphics;Modems;Information management;Control systems","COBOL;computer graphics;data structures;system documentation;word processing","textual data structure documentation;graphical data structure documentation;Cobol programs;textual tools;graphical tools;program understanding phase;Cobol program maintenance","","1","","19","","","","","","IEEE","IEEE Journals & Magazines"
"BURN: Enabling Workload Burstiness in Customized Service Benchmarks","G. Casale; A. Kalbasi; D. Krishnamurthy; J. Rolia","Imperial College London, London; University of Calgary, Calgary; University of Calgary, Calgary; HP Labs, Palo Alto","IEEE Transactions on Software Engineering","","2012","38","4","778","793","We introduce BURN, a methodology to create customized benchmarks for testing multitier applications under time-varying resource usage conditions. Starting from a set of preexisting test workloads, BURN finds a policy that interleaves their execution to stress the multitier application and generate controlled burstiness in resource consumption. This is useful to study, in a controlled way, the robustness of software services to sudden changes in the workload characteristics and in the usage levels of the resources. The problem is tackled by a model-based technique which first generates Markov models to describe resource consumption patterns of each test workload. Then, a policy is generated using an optimization program which sets as constraints a target request mix and user-specified levels of burstiness at the different resources in the system. Burstiness is quantified using a novel metric called overdemand, which describes in a natural way the tendency of a workload to keep a resource congested for long periods of time and across multiple requests. A case study based on a three-tier application testbed shows that our method is able to control and predict burstiness for session service demands at a fine-grained scale. Furthermore, experiments demonstrate that for any given request mix our approach can expose latency and throughput degradations not found with nonbursty workloads having the same request mix.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.58","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928353","Benchmarking;performance;burstiness;bottleneck migration;overdemand","Benchmark testing;Markov processes;Servers;Aggregates;Analytical models;Computational modeling;Linear regression","benchmark testing;Markov processes","BURN;workload burstiness;customized service benchmarks;customized benchmarks;multitier application;time-varying resource usage condition;controlled burstiness;software services;model-based technique;Markov models;resource consumption pattern;optimization program;target request mix;user-specified levels;three-tier application testbed;session service demands;fine-grained scale;latency;throughput degradation;nonbursty workloads","","8","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal allocation of file servers in a local network environment","C. M. Woodside; S. K. Tripathi","ISEM Laboratory, Universit&#x00E9; Paris-Sud, 91405 Orsay, France; Centre Mondial Informatique, Paris, France; Department of Systems and Computer Engineering, Carleton University, Ottawa, Ont. K1S 5B6, Canada; ISEM Laboratory, Universit&#x00E9; Paris-Sud, 91405 Orsay, France; Department of Computer Science, University of Maryland, College Park, MD 20742","IEEE Transactions on Software Engineering","","1986","SE-12","8","844","848","A globally optimal allocation for files in a local network environment is presented. The principal concern is the delays due to contention at the file servers; storage space is assumed to be adequate. A queuing network model is used to represent the file servers and the workstations. The workloads generated by the workstations are statistically identical. The model assumes that the communications medium is lightly loaded. In this case there is very little queuing, so that a message transmission requires an approximately constant average delay which can be included in the local processing time of the workstation. Under these assumptions the model can be applied to any of the various LAN technologies. It is shown that all the files of each workstation should be placed on one file server, with the workstations divided as equally as possible among the file servers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312986","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312986","Design studies;file assignment;local networks;modeling techniques;multiclass closed queueing networks;optimization;performance of systems","File servers;Workstations;Resource management;Educational institutions;Load modeling;Computational modeling;Local area networks","file organisation;local area networks;queueing theory","optimal file allocation;file servers;local network environment;storage space;queuing network model;communications medium;message transmission;LAN","","6","","","","","","","","IEEE","IEEE Journals & Magazines"
"Automating output size and reuse metrics in a repository-based computer-aided software engineering (CASE) environment","R. D. Banker; R. J. Kauffman; C. Wright; D. Zweig","Carlson Sch. of Manage., Minnesota Univ., Minneapolis, MN, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1994","20","3","169","187","Measurement of software development productivity is needed in order to control software costs, but it is discouragingly labor-intensive and expensive. Computer-aided software engineering (CASE) technologies/spl minus/especially repository-based, integrated CASE/spl minus/have the potential to support the automation of this measurement. We discuss the conceptual basis for the development of automated analyzers for function point and software reuse measurement for object-based CASE. Both analyzers take advantage of the existence of a representation of the application system that is stored within an object repository, and that contains the necessary information about the application system. We also discuss metrics for software reuse measurement, including reuse leverage, reuse value, and reuse classification that are motivated by managerial requirements and the efforts, within industry and the IEEE, to standardize measurement. The functionality and the analytical capabilities of state-of-the-art automated software metrics analyzers are illustrated in the context of an investment banking industry application that is similar to systems deployed at the New York City-based investment bank where these tools were developed and tested.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.268919","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=268919","","Software measurement;Computer aided software engineering;Application software;Investments;Programming;Productivity;Automatic control;Costs;Automation;Information analysis","software tools;object-oriented programming;software metrics;software reusability;bank data processing","output size;reuse metrics;repository-based computer-aided software engineering environment;CASE environment;software development productivity;software cost control;integrated CASE;conceptual basis;automated analyzers;function point;software reuse measurement;object-based CASE;object repository;reuse leverage;reuse classification;managerial requirements;state-of-the-art automated software metrics analyzers;investment banking industry application","","27","","70","","","","","","IEEE","IEEE Journals & Magazines"
"Software bottlenecking in client-server systems and rendezvous networks","J. E. Neilson; C. M. Woodside; D. C. Petriu; S. Majumdar","Real-time & Distributed Syst. Group, Carleton Univ., Ottawa, Ont., Canada; Real-time & Distributed Syst. Group, Carleton Univ., Ottawa, Ont., Canada; Real-time & Distributed Syst. Group, Carleton Univ., Ottawa, Ont., Canada; Real-time & Distributed Syst. Group, Carleton Univ., Ottawa, Ont., Canada","IEEE Transactions on Software Engineering","","1995","21","9","776","782","Software bottlenecks are performance constraints caused by slow execution of a software task, in typical client-server systems a client task must wait in a blocked state for the server task to respond to its requests, so a saturated server will slow down all its clients. A rendezvous network generalizes this relationship to multiple layers of servers with send-and-wait interactions (rendezvous), a two-phase model of task behavior, and to a unified model for hardware and software contention. Software bottlenecks have different symptoms, different behavior when the system is altered, and a different cure from the conventional bottlenecks seen in queueing network models of computer systems, caused by hardware limits. The differences are due to the ""push-back"" effect of the rendezvous, which spreads the saturation of a server to its clients. The paper describes software bottlenecks by examples, gives a definition, shows how they can be located and alleviated, and gives a method for estimating the performance benefit to be obtained. Ultimately, if all the software bottlenecks can be removed, the performance limit will be due to a conventional hardware bottleneck.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.464543","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=464543","","Intelligent networks;Client-server systems;Network servers;Hardware;Object oriented modeling;Software performance;Computer networks;Distributed computing;Operating systems;Concurrent computing","software performance evaluation;client-server systems;queueing theory;performance evaluation;resource allocation","software bottlenecks;client-server systems;rendezvous networks;performance constraints;slow task execution;server task;send-and-wait interactions;two-phase model;task behavior;software contention;hardware contention;queueing network models;hardware limits;push-back effect;performance benefit","","44","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A survey on software architecture analysis methods","L. Dobrica; E. Niemela","Fac. of Autom. Control & Comput., Univ. Politehnica of Bucharest, Romania; NA","IEEE Transactions on Software Engineering","","2002","28","7","638","653","The purpose of the architecture evaluation of a software system is to analyze the architecture to identify potential risks and to verify that the quality requirements have been addressed in the design. This survey shows the state of the research at this moment, in this domain, by presenting and discussing eight of the most representative architecture analysis methods. The selection of the studied methods tries to cover as many particular views of objective reflections as possible to be derived from the general goal. The role of the discussion is to offer guidelines related to the use of the most suitable method for an architecture assessment process. We will concentrate on discovering similarities and differences between these eight available methods by making classifications, comparisons and appropriateness studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019479","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019479","","Software architecture;Computer architecture;Software systems;Software quality;Risk analysis;Reflection;Software metrics;Computer Society;Guidelines;Computer industry","software architecture;software quality;reviews","survey;software architecture analysis methods;potential risk identification;software quality requirements;objective reflections;software architecture assessment process;classifications;appropriateness studies;software quality attributes;scenarios","","226","","52","","","","","","IEEE","IEEE Journals & Magazines"
"A comparison of computed chaining to predictors","K. Tai; A. L. Tharp","Department of Computer Science, North Carolina State University, Raleigh, NC 27695; Department of Computer Science, North Carolina State University, Raleigh, NC 27695","IEEE Transactions on Software Engineering","","1986","SE-12","8","870","874","Computed chaining and predictors are two recent techniques for resolving hashing collisions which use a pseudolink field instead of an actual address link to group records which map to the same home address. With additional computation on the pseudolink, the actual address can be determined. The advantage of the pseudolink is that it often takes much less storage than an actual address would take. The authors note problems with the predictor method which must be overcome if the method is to be used successfully. They also compare the predictor method to computed chaining. They conclude with a discussion of the utility of multiple predictor, i.e. having more than one chain of pseudolinks for records with the same home address.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312990","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312990","Collision;hashing;resolution","Probes;Databases;Computational modeling;Computer science;Equations;Access control","file organisation","computed chaining;hashing collisions;pseudolink field;predictor method;multiple predictor","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Enhanced Code Conversion Approach for the Integrated Cross-Platform Mobile Development (ICPMD)","W. S. El-Kassas; B. A. Abdullah; A. H. Yousef; A. M. Wahba","Department of Computer and Systems Engineering, Faculty of Engineering, Ain Shams University, Cairo, Egypt; Department of Computer and Systems Engineering, Faculty of Engineering, Ain Shams University, Cairo, Egypt; Department of Computer and Systems Engineering, Faculty of Engineering, Ain Shams University, Cairo, Egypt; Department of Computer and Systems Engineering, Faculty of Engineering, Ain Shams University, Cairo, Egypt","IEEE Transactions on Software Engineering","","2016","42","11","1036","1053","Mobile development companies aim to maximize the return on investments by making their mobile applications (Apps) available on different mobile platforms. Consequently, the same App is developed several times; each time the developer uses the programming languages and development tools of a specific platform. Therefore, there is a need to have cross-platform mobile applications development solutions that enable the developers to develop the App once and run it everywhere. The Integrated Cross-Platform Mobile Applications Development (ICPMD) solution is one of the attempts that enables the developers to use the most popular programming languages like Java for Android and C# for Windows Phone 8 (WP8). ICPMD is used to transform both the source code and user interface to another language to generate full Apps on the target platform. This paper extends ICPMD by proposing a new code conversion approach based on XSLT and Regular Expressions to ease the conversion process. In addition, it provides the assessment method to compare the ICPMD efficiency with competing approaches. Several Apps are converted from WP8 to Android and vice versa. The ICPMD evaluation results show reasonable improvement over commercial cross-platform mobile development tools (Titanium and Xamarin).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2543223","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7442177","Cross-platform mobile development;code conversion;code reuse;generated apps;ICPMD, source code patterns","Mobile communication;Java;Runtime;Titanium;Application programming interfaces;Smart phones","Android (operating system);mobile computing;programming languages;smart phones;software tools;source code (software);user interfaces","code conversion;integrated cross-platform mobile development;ICPMD;mobile application development;mobile platform;programming language;development tool;source code;user interface;smart phone","","1","","39","","","","","","IEEE","IEEE Journals & Magazines"
"makeSense: Simplifying the Integration of Wireless Sensor Networks into Business Processes","L. Mottola; G. P. Picco; F. J. Opperman; J. Eriksson; N. Finne; H. Fuchs; A. Gaglione; S. Karnouskos; P. Montero; N. Oertel; K. R&#xf6;mer; P. Spiess; S. Tranquillini; T. Voigt","DEIB, Politenico di Milano, Milano, Milano Italy 20133 (e-mail: luca.mottola@polimi.it); Department of Information Engineering and Computer Science, University of Trento, Trento, TN Italy 38100 (e-mail: gianpietro.picco@unitn.it); Institut f&#xfc;r Technische Informatik, Technische Universitat Graz, 27253 Graz, Steiermark Austria (e-mail: oppermann@tugraz.at); NES, Swedish Institute of Computer Science, 4277 Kista, Kista Sweden (e-mail: joakime@sics.se); NES, Swedish Institute of Computer Science, 4277 Kista, Kista Sweden (e-mail: nfi@sics.se); SAP Research, SAP SE, Karlsruhe, Karlsruhe Germany (e-mail: harald.fuchs@sap.com); Digital Catapult, London NW1 2RA, U.K. (e-mail: andrea.gaglione@digicatapult.org.uk); SAP Research, SAP, Karlsruhe, BW Germany D-76131 (e-mail: karnouskos@ieee.org); ACCIONA, ACCIONA Ingenieria SA, 152752 Alcobendas, Madrid Spain (e-mail: patricio.moreno.montero@acciona.es); SAP Research, SAP SE, Karlsruhe, Karlsruhe Germany (e-mail: nina.oertel@sap.com); Institut f&#xfc;r Technische Informatik, Technische Universitat Graz, 27253 Graz, Steiermark Austria (e-mail: roemer@tugraz.at); SAP Research, SAP AG, Karlsruhe, Karlsruhe Germany (e-mail: patrik.spiess@sap.com); DISI, University of Trento, Trento, Trento Italy (e-mail: stefano.tranquillini@gmail.com); CSL, Swedish Institute of Computer Science, Kista, Kista Sweden (e-mail: thiemo@sics.se)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","A wide gap exists between the state of the art in developing Wireless Sensor Network (WSN) software and current practices concerning the design, execution, and maintenance of business processes. WSN software is most often developed based on low-level OS abstractions, whereas business process development leverages high-level languages and tools. This state of affairs places WSNs at the fringe of industry. The makeSense system addresses this problem by simplifying the integration of WSNs into business processes. Developers use BPMN models extended with WSN-specific constructs to specify the application behavior across both traditional business process execution environments and the WSN itself, which is to be equipped with application-specific software. We compile these models into a high-level intermediate language&#x2014;also directly usable by WSN developers&#x2014;and then into OS-specific deployment-ready binaries. Key to this process is the notion of meta-abstraction, which we define to capture fundamental patterns of interaction with and within the WSN. The concrete realization of meta-abstractions is application-specific; developers tailor the system configuration by selecting concrete abstractions out of the existing codebase or by providing their own. Our evaluation of makeSense shows that i) users perceive our approach as a significant advance over the state of the art, providing evidence of the increased developer productivity when using makeSense; ii) in large-scale simulations, our prototype exhibits an acceptable system overhead and good scaling properties, demonstrating the general applicability of makeSense; and, iii) our prototype&#x2014;including the complete tool-chain and underlying system support&#x2014;sustains a real-world deployment where estimates by domain specialists indicate the potential for drastic reductions in the total cost of ownership compared to wired and conventional WSN-based solutions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2787585","European Commission; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8240710","","Wireless sensor networks;Business;Programming;Ventilation;Software;Concrete","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"An Improved SDA Based Defect Prediction Framework for Both Within-Project and Cross-Project Class-Imbalance Problems","X. Jing; F. Wu; X. Dong; B. Xu","State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, China; State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, China; State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China","IEEE Transactions on Software Engineering","","2017","43","4","321","339","Background. Solving the class-imbalance problem of within-project software defect prediction (SDP) is an important research topic. Although some class-imbalance learning methods have been presented, there exists room for improvement. For cross-project SDP, we found that the class-imbalanced source usually leads to misclassification of defective instances. However, only one work has paid attention to this cross-project class-imbalance problem. Objective. We aim to provide effective solutions for both within-project and cross-project class-imbalance problems. Method. Subclass discriminant analysis (SDA), an effective feature learning method, is introduced to solve the problems. It can learn features with more powerful classification ability from original metrics. For within-project prediction, we improve SDA for achieving balanced subclasses and propose the improved SDA (ISDA) approach. For cross-project prediction, we employ the semi-supervised transfer component analysis (SSTCA) method to make the distributions of source and target data consistent, and propose the SSTCA+ISDA prediction approach. Results. Extensive experiments on four widely used datasets indicate that: 1) ISDA-based solution performs better than other state-of-the-art methods for within-project class-imbalance problem; 2) SSTCA+ISDA proposed for cross-project class-imbalance problem significantly outperforms related methods. Conclusion. Within-project and cross-project class-imbalance problems greatly affect prediction performance, and we provide a unified and effective prediction framework for both problems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2597849","National Nature Science Foundation of China; The Chinese 973 Program; Research Project of NJUPT; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7530877","Software defect prediction (SDP);within-project class-imbalance;cross-project class-imbalance;improved subclass discriminant analysis (ISDA);ISDA based defect prediction framework","Support vector machines;Learning systems;Predictive models;Software;Software engineering;Measurement","learning (artificial intelligence);software engineering","SDA based defect prediction;within-project class-imbalance problem;cross-project class-imbalance problem;software defect prediction;SDP;class-imbalance learning;subclass discriminant analysis;semisupervised transfer component analysis;SSTCA","","16","","78","","","","","","IEEE","IEEE Journals & Magazines"
"Timing constraint Petri nets and their application to schedulability analysis of real-time system specifications","J. J. P. Tsai; S. Jennhwa Yang; Yao-Hsiung Chang","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; NA","IEEE Transactions on Software Engineering","","1995","21","1","32","49","We present timing constraint Petri nets (or TCPN's for short) and describe how to use them to model a real-time system specification and determine whether the specification is schedulable with respect to imposed timing constraints. The strength of TCPN's over other time-related Petri nets is in the modeling and analysis of conflict structures. Schedulability analysis is conducted in three steps: specification modeling, reachability simulation, and timing analysis. First, we model a real-time system by transforming its system specification along with its imposed timing constraints into a TCPN; we call this net N/sub s/. Then we simulate the reachability of N/sub s/ to verify whether a marking, M/sub n/, is reachable from an initial marking, M/sub o/. It is important to note that a reachable marking in Petri nets is not necessarily reachable in TCPN's due to the imposed timing constraints, Therefore, in the timing analysis step, a reachable marking M/sub n/, found in the reachability simulation step is analyzed to verify whether M/sub n/, is reachable with the timing constraints. M/sub n/ is said to be reachable in the TCPN's if and only if we can find at least one firing sequence /spl sigma/ so that all transitions in /spl sigma/ are strongly schedulable with respect to M/sub o/ under the timing constraints. If such M/sub n/ can be found, then we can assert that the specification is schedulable under the imposed timing constraints, otherwise the system specification needs to be modified or the timing constraints need to be relaxed. We also present a synthesis method for determining the best approximation of the earliest fire beginning time (EFBT) and the latest fire ending time (LFET) of each strongly schedulable transition.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.341845","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=341845","","Timing;Petri nets;Real time systems;Job shop scheduling;Monitoring;Analytical models;Fires;Runtime;Logic;Time factors","Petri nets;real-time systems;formal specification;formal verification;scheduling;timing;distributed processing","timing constraint Petri nets;schedulability analysis;real-time system specifications;time-related Petri nets;specification modeling;reachability simulation;real-time system;system specification;reachability simulation step;synthesis method;latest fire ending time;earliest fire beginning time;strongly schedulable transition","","71","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Quality Requirements in Industrial Practice—An Extended Interview Study at Eleven Companies","R. Berntsson Svensson; T. Gorschek; B. Regnell; R. Torkar; A. Shahrokni; R. Feldt","Lund University, Lund; Blekinge Institute of Technology, Karlskrona; Lund University, Lund; Blekinge Institute of Technology, Karlskrona; Chalmers University of Technology, Göteborg; Chalmers University of Technology, Göteborg","IEEE Transactions on Software Engineering","","2012","38","4","923","935","In order to create a successful software product and assure its quality, it is not enough to fulfill the functional requirements, it is also crucial to find the right balance among competing quality requirements (QR). An extended, previously piloted, interview study was performed to identify specific challenges associated with the selection, tradeoff, and management of QR in industrial practice. Data were collected through semistructured interviews with 11 product managers and 11 project leaders from 11 software companies. The contribution of this study is fourfold: First, it compares how QR are handled in two cases, companies working in business-to-business markets and companies that are working in business-to-consumer markets. These two are also compared in terms of impact on the handling of QR. Second, it compares the perceptions and priorities of QR by product and project management, respectively. Third, it includes an examination of the interdependencies among quality requirements perceived as most important by the practitioners. Fourth, it characterizes the selection and management of QR in downstream development activities.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5753901","Management;process;requirements/specifications","Companies;Interviews;Industries;Usability;Telecommunications;Reliability","DP industry;project management;software management;software quality","quality requirements;industrial practice;software product;software quality;QR selection;QR tradeoff;QR management;software company;business-to-business market;business-to-consumer market;project management;QR handling","","25","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Certifying the reliability of software","P. A. Currit; M. Dyer; H. D. Mills","IBM Corporation, Federal Systems Division, 6600 Rockledge Drive, Bethesda, MD 20817; IBM Corporation, Federal Systems Division, 6600 Rockledge Drive, Bethesda, MD 20817; IBM Corporation, Federal Systems Division, 6600 Rockledge Drive, Bethesda, MD 20817","IEEE Transactions on Software Engineering","","1986","SE-12","1","3","11","A description is given of a procedure for certifying the reliability of software before its release to users. The ingredients of this procedure are a life cycle of executable product increments, representative statistical testing, and a standard estimate of the MTTF (mean time to failure) of the product at the time of its release. The authors also discuss the development of certified software products and the derivation of a statistical model used for reliability projection. Available software test data are used to demonstrate the application of the model in certification process.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312914","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312914","Incremental development;software reliability certification;software reliability models;statistical quality control;statistical testing process","Software;Testing;Software reliability;Certification;Statistical analysis;Standards","reliability theory;software reliability;statistical analysis","reliability;software;life cycle;executable product increments;statistical testing;standard estimate;MTTF;mean time to failure;certified software products;statistical model;reliability projection;certification process","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting Change Impact Analysis Using a Recommendation System: An Industrial Case Study in a Safety-Critical Context","M. Borg; K. Wnuk; B. Regnell; P. Runeson","SICS Swedish ICT AB, Ideon Science Park, Building Beta 2, Scheelevägen 17, Lund, Sweden; Blekinge Institute of Technology, Karlskrona, Sweden; Lund University, Lund, Sweden; Lund University, Lund, Sweden","IEEE Transactions on Software Engineering","","2017","43","7","675","700","Change Impact Analysis (CIA) during software evolution of safety-critical systems is a labor-intensive task. Several authors have proposed tool support for CIA, but very few tools were evaluated in industry. We present a case study on ImpRec, a recommendation System for Software Engineering (RSSE), tailored for CIA at a process automation company. ImpRec builds on assisted tracing, using information retrieval solutions and mining software repositories to recommend development artifacts, potentially impacted when resolving incoming issue reports. In contrast to the majority of tools for automated CIA, ImpRec explicitly targets development artifacts that are not source code. We evaluate ImpRec in a two-phase study. First, we measure the correctness of ImpRec's recommendations by a simulation based on 12 years' worth of issue reports in the company. Second, we assess the utility of working with ImpRec by deploying the RSSE in two development teams on different continents. The results suggest that ImpRec presents about 40 percent of the true impact among the top-10 recommendations. Furthermore, user log analysis indicates that ImpRec can support CIA in industry, and developers acknowledge the value of ImpRec in interviews. In conclusion, our findings show the potential of reusing traceability associated with developers' past activities in an RSSE.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2620458","Embedded Applications Software Engineering; ORION; Knowledge Foundation; Lund University; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7637029","Case;maintenance management;software and system safety;tracing","Context;Industries;Software engineering;Unified modeling language;Automation;Software systems","program diagnostics;recommender systems;safety-critical software;software engineering","change impact analysis;safety-critical context;CIA;software evolution;safety-critical system;recommendation system for software engineering;RSSE;ImpRec;information retrieval solution;software repository mining","","2","","120","","","","","","IEEE","IEEE Journals & Magazines"
"Yeast: a general purpose event-action system","B. Krishnamurthy; D. S. Rosenblum","Dept. of Software Eng Res., AT&T Bell Labs., Murray Hill, NJ, USA; NA","IEEE Transactions on Software Engineering","","1995","21","10","845","857","Distributed networks of personal workstations are becoming the dominant computing environment for software development organizations. Many cooperative activities that are carried out in such environments are particularly well suited for automated support. Taking the point of view that such activities are modeled most naturally as the occurrence of events requiring actions to be performed, we developed a system called Yeast (Yet another Event Action Specification Tool). Yeast is a client server system in which distributed clients register event action specifications with a centralized server, which performs event detection and specification management. Each specification submitted by a client defines a pattern of events that is of interest to the client's application plus an action that is to be executed in response to an occurrence of the event pattern; the server triggers the action of a specification once it has detected an occurrence of the associated event pattern. Yeast provides a global space of events that is visible to and shared by all users. In particular, events generated by one user can trigger specifications registered by another user. Higher level applications are built as collections of Yeast specifications. We use Yeast on a daily basis for a variety of applications, from deadline notification to software process automation. The paper presents an in depth description of Yeast and an example application of Yeast, in which Yeast specifications are used to automate a software distribution process involving several interdependent software tools.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.469456","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=469456","","Fungi;Application software;Event detection;Software tools;Workstations;Computer networks;Distributed computing;Programming;Client server systems;Automation","client-server systems;network servers;formal specification","Yeast;general purpose event action system;general purpose event-action system;distributed networks;personal workstations;software development organizations;cooperative activities;automated support;Yet another Event Action Specification Tool;client server system;distributed clients;centralized server;event detection;specification management;associated event pattern;global space;higher level applications;deadline notification;software process automation;software distribution process;interdependent software tools","","37","","25","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Evaluation of Mutation Testing for Improving the Test Quality of Safety-Critical Software","R. Baker; I. Habli","Aero Engine Controls, Birmingham; University of York, York","IEEE Transactions on Software Engineering","","2013","39","6","787","805","Testing provides a primary means for assuring software in safety-critical systems. To demonstrate, particularly to a certification authority, that sufficient testing has been performed, it is necessary to achieve the test coverage levels recommended or mandated by safety standards and industry guidelines. Mutation testing provides an alternative or complementary method of measuring test sufficiency, but has not been widely adopted in the safety-critical industry. In this study, we provide an empirical evaluation of the application of mutation testing to airborne software systems which have already satisfied the coverage requirements for certification. Specifically, we apply mutation testing to safety-critical software developed using high-integrity subsets of C and Ada, identify the most effective mutant types, and analyze the root causes of failures in test cases. Our findings show how mutation testing could be effective where traditional structural coverage analysis and manual peer review have failed. They also show that several testing issues have origins beyond the test activity, and this suggests improvements to the requirements definition and coding process. Our study also examines the relationship between program characteristics and mutation survival and considers how program size can provide a means for targeting test areas most likely to have dormant faults. Industry feedback is also provided, particularly on how mutation testing can be integrated into a typical verification life cycle of airborne software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6298894","Mutation;safety-critical software;verification;testing;certification","Testing;Certification;Software systems;Safety;Industries;Guidelines","Ada;aerospace computing;C language;certification;integrated software;program testing;program verification;safety-critical software;software quality","software test quality;safety-critical software;test coverage level;safety standard;industry guideline;test sufficiency measurement;empirical evaluation;mutation testing;airborne software system;certification;coverage requirement satisfaction;C;Ada;software integration;mutant type;software failure;structural coverage analysis;coding process;verification life cycle","","18","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Logical clock requirements for reverse engineering scenarios from a distributed system","C. E. Hrischuk; C. M. Woodside","Rational Software, Woodinville, WA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","4","321","339","To reverse engineer scenarios from event traces, one must infer causal relationships between events. The inferences are usually based on a trace with sequence numbers or timestamps corresponding to some kind of logical clock. In practice, there is an explosion of potentially causal relationships in the trace, which limits one's ability to extract scenarios. This work defines a more parsimonious form of causality called scenario causality that concentrates on certain major causal relationships and ignores more subtle, potentially causal links. The influence of an event is restricted to the particular scenario it is part of. An event which is not a message reception is defined to be caused by the previous event in the same software object, while a message reception is caused by a sending event in another object. The events are ordered to form a scenario event graph where typed nodes are events and the typed edges are certain causal relationships. Intuitively, we might say that most logical clocks, which identify events which ""happened before"" a given event and, thus, are potentially causal, give an upper bound on the set of causal events; scenario causality identifies a lower bound. The much smaller lower bound set makes it possible to reverse engineer and automate the analysis of scenarios.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995416","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995416","","Clocks;Reverse engineering","reverse engineering;clocks;distributed programming;causality;program diagnostics;graph grammars;program debugging;software engineering","logical clock requirements;scenario reverse engineering;distributed system;event traces;causal relationship inference;trace sequence numbers;timestamps;parsimonious causality;scenario causality;message reception;software objects;sending event;scenario event graph;typed nodes;typed edges;upper bound;lower bound;automatic scenario analysis;causal order;software tracing;graph grammar;trace analysis;distributed programming;event labeling;debugging;World Wide Web services","","3","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Schlumberger's software improvement program","H. Wohlwend; S. Rosenbaum","Sematech, Austin, TX, USA; NA","IEEE Transactions on Software Engineering","","1994","20","11","833","839","A corporate-wide software process improvement effort has been ongoing at Schlumberger for several years. Through the motivation efforts of a small group, productive changes have occurred across the company. We see improvements in many development areas, including project planning and requirements management. The catalysts behind these advances include capability assessments, training, and collaboration.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.368125","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=368125","","Software quality;Software engineering;Laboratories;Productivity;Programming;Software measurement;Project management;Industrial training;Management training;Collaboration","DP industry;project management;software development management","Schlumberger;software improvement program;corporate-wide software process improvement effort;productive changes;project planning;requirements management;capability assessments;training;collaboration;software process management;SEI;software engineering","","46","","11","","","","","","IEEE","IEEE Journals & Magazines"
"An Approach to Performance Specification of Communication Protocols Using Timed Petri Nets","K. Garg","Department of Electronics and Communication Engineering, University of Roorkee","IEEE Transactions on Software Engineering","","1985","SE-11","10","1216","1225","There has been a lot of interest in the past decade in using timed Petri nets to model computer systems. In this paper we show how such timed Petri nets can be used to great advantage in describing and algebraically specifying communication system performance. We make use of the time parameter of timed Petri nets to model the delay in performing certain operations of a communication protocol. The specification is borrowed from the recently reported AFFIRM language, and the protocol chosen for illustration is the ECMA transfer protocol, proposed for the ISO reference model. However, the methodology can be used with other protocols as well. We also show how liveness properties can be specified, easily using timed Petri nets.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231869","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701937","Distributed computer systems;natural-deduction theorem proving;performance evaluation;performance modeling and analysis;Petri nets;protocol;protocol specifications;timed Petri nets","Protocols;Petri nets;Performance analysis;Communication systems;Delay effects;ISO;Distributed computing;Concurrent computing;Logic;Timing","","Distributed computer systems;natural-deduction theorem proving;performance evaluation;performance modeling and analysis;Petri nets;protocol;protocol specifications;timed Petri nets","","24","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Semi-automatic program construction from specifications using library modules","F. Nishida; S. Takamatsu; Y. Fujita; T. Tani","Dept. of Electr. Eng., Osaka Prefectural Univ., Japan; Dept. of Electr. Eng., Osaka Prefectural Univ., Japan; NA; NA","IEEE Transactions on Software Engineering","","1991","17","9","853","871","A method of semiautomatic specification refinement and program generation using library modules, is described. Users write their specifications and modify and rearrange them so that they can be refined with the aid of the library modules. When a specification is given, a refinement system, called MAPS (module-aided program construction system) searches for library modules applicable to the given specification, replaces the specification with a more detailed description written in the operation part of the modules, and converts the refined specification into a program written in a programming language designated by the user. Case-like expressions or pseudo-natural language expressions are used for describing user's specifications and specifications for library modules.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92909","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92909","","Modular construction;Natural languages;Software libraries;Refining;Computer languages;Documentation;Joining processes;Software quality;Productivity;Software engineering","automatic programming;formal specification;software tools;subroutines","case-like expressions;semiautomatic specification refinement;program generation;library modules;refinement system;MAPS;module-aided program construction system;programming language;pseudo-natural language expressions","","10","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Including scalars in a programming language based on the relational algebra","T. H. Merrett; N. Laliberte","Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada; Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada","IEEE Transactions on Software Engineering","","1989","15","11","1437","1443","Scalars, arrays, and records, together with associated operations and syntax, have been introduced as special cases of relations into the relational programming system, relix. This permits all of these data types, as well as relations, to be stored persistently. The requirement in most languages that array elements and record fields can be assigned to leads in this case to the general implementation of QT-selectors as l-expressions, with, in particular, systematic interpretations of assignment to projections and selections of relations. The authors discuss the principles and the implementation of this extension to the relational algebra. They take advantage of the very specialized syntax of array access to build a tuned access method, using B-trees and Z-order. The performance results show the advantage of this implementation over the slower implementation required for general QT-selectors.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41335","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41335","","Computer languages;Algebra;Database systems;Writing;Documentation;Functional programming;Councils;Employment;Automation","data structures;database theory;high level languages;relational databases","scalars;programming language;relational algebra;records;syntax;relations;relational programming system;relix;data types;QT-selectors;l-expressions;systematic interpretations;projections;array access;tuned access method;B-trees;Z-order","","","","18","","","","","","IEEE","IEEE Journals & Magazines"
"On the frame problem in procedure specifications","A. Borgida; J. Mylopoulos; R. Reiter","Dept. of Comput. Sci., Rutgers Univ., New Brunswick, NJ, USA; NA; NA","IEEE Transactions on Software Engineering","","1995","21","10","785","798","The paper provides examples of situations where formal specifications of procedures in the standard pre/postcondition style become lengthy, cumbersome and difficult to change, a problem which is particularly acute in the case of object oriented specifications with inheritance. We identify the problem as the inability to express that a procedure changes only those things it has to, leaving everything else unmodified, and review some attempts at dealing with this ""frame problem"" in the software specification community. The second part of the paper adapts a recent proposal for a solution to the frame problem in artificial intelligence-the notion of explanation closure axioms-to provide an approach whereby one can state such conditions succinctly and modularly, with the added advantage of having the specifier be reminded of things that she may have omitted saying in procedure specifications. Since this approach is based on standard predicate logic, its semantics are relatively straightforward. The paper also suggests an algorithm which generates syntactically the explanation closure axioms from the pre/postcondition specifications, provided they are written in a restricted language; it also suggests a model theory supporting it.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.469460","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=469460","","Formal specifications;Databases;Logic;Computer science;Proposals;Artificial intelligence;Object oriented modeling;Specification languages;Computer languages","formal specification;specification languages;object-oriented programming;explanation;formal logic","frame problem;procedure specifications;formal specifications;standard pre/postcondition style;object oriented specifications;inheritance;software specification community;artificial intelligence;explanation closure axioms;standard predicate logic;restricted language;model theory","","61","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Success and failure factors in software reuse","M. Morisio; M. Ezran; C. Tully","Dipt. Autom. e Inf., Torino Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","2002","28","4","340","357","This paper aims at identifying some of the key factors in adopting or running a company-wide software reuse program. Key factors are derived from empirical evidence of reuse practices, as emerged from a survey of projects for the introduction of reuse in European companies: 24 such projects performed from 1994 to 1997 were analyzed using structured interviews. The projects were undertaken in both large and small companies, working in a variety of business domains, and using both object-oriented and procedural development approaches. Most of them produce software with high commonality between applications, and have at least reasonably mature processes. Despite that apparent potential for success, around one-third of the projects failed. Three main causes of failure were not introducing reuse-specific processes, not modifying nonreuse processes, and not considering human factors. The root cause was a lack of commitment by top management, or nonawareness of the importance of those factors, often coupled with the belief that using the object-oriented approach or setting up a repository seamlessly is all that is necessary to achieve success in reuse. Conversely, successes were achieved when, given a potential for reuse because of commonality among applications, management committed to introducing reuse processes, modifying nonreuse processes, and addressing human factors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995420","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995420","","Companies;Human factors;Performance analysis;Application software","software reusability;object-oriented programming;software development management;human factors","company-wide software reuse program;survey;European companies;small companies;large companies;business;object-oriented programming;project failure;procedural development;human factors;top management","","106","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Automated module testing in Prolog","D. M. Hoffman; P. Strooper","Dept. of Comput. Sci., Victoria Univ., BC, Canada; Dept. of Comput. Sci., Victoria Univ., BC, Canada","IEEE Transactions on Software Engineering","","1991","17","9","934","943","Tools and techniques for writing scripts in Prolog that automatically test modules implemented in C are presented. Both the input generation and the test oracle problems are addressed, focusing on a balance between the adequacy of the test inputs and the cost of developing the output oracle. The authors investigate automated input generation according to functional testing, random testing, and a novel approach based on trace invariants. For each input generation scheme, a mechanism for generating the expected outputs has been developed. The methods are described and illustrated in detail. Script development and maintenance costs appear to be reasonable, and run-time performance appears to be acceptable.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92913","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92913","","Automatic testing;System testing;Sequential analysis;Costs;Production systems;Runtime;Automation;Writing;Design for testability;Power generation economics","C language;logic programming;program testing;PROLOG","Prolog;C;input generation;test oracle problems;test inputs;output oracle;automated input generation;functional testing;random testing;trace invariants;maintenance costs;run-time performance","","22","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Performance Criteria for Constrained Nonlinear Programming Codes","H. W. Robb; H. R. Weistroffer","Unicorn Lines (Pty) Ltd.; NA","IEEE Transactions on Software Engineering","","1987","SE-13","4","479","489","A set of performance criteria for evaluating optimization software with respect to efficiency, reliability, and accuracy is presented and discussed. A numerical comparison of five constrained nonlinear programming codes is described, which was carried out in order to test the usefulness and general applicability of the proposed performance criteria. The results of the numerical comparison are discussed, and the proposed criteria are compared to the criteria traditionally used in comparative evaluations of nonlinear programming codes, with particular reference to machine dependence and the applicability to test problems with unknown solutions. A separate small scale computational experiment is described which was carried out specifically to test the machine dependence of the criteria. The observed deficiencies of the proposed new criteria are also discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233184","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702239","Constrained optimization;evaluation;mathematical programming;nonlinear programming;numerical comparison;optimization software;performance criteria","Mathematical programming;Robustness;Software performance;Africa;Writing;Councils;Guidelines;Life testing;Particle measurements;Time measurement","","Constrained optimization;evaluation;mathematical programming;nonlinear programming;numerical comparison;optimization software;performance criteria","","1","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Composite Constant Propagation and its Application to Android Program Analysis","D. Octeau; D. Luchaup; S. Jha; P. McDaniel","Department of Computer Sciences, University of Wisconsin, Madison, WI; CyLab, Carnegie Mellon University, Pittsburgh, PA; Department of Computer Sciences, University of Wisconsin, Madison, WI; Department of Computer Science and Engineering, Pennsylvania State University, University Park, PA","IEEE Transactions on Software Engineering","","2016","42","11","999","1014","Many program analyses require statically inferring the possible values of composite types. However, current approaches either do not account for correlations between object fields or do so in an ad hoc manner. In this paper, we introduce the problem of composite constant propagation. We develop the first generic solver that infers all possible values of complex objects in an interprocedural, flow and context-sensitive manner, taking field correlations into account. Composite constant propagation problems are specified using COAL, a declarative language. We apply our COAL solver to the problem of inferring Android Inter-Component Communication (ICC) values, which is required to understand how the components of Android applications interact. Using COAL, we model ICC objects in Android more thoroughly than the state-of-the-art. We compute ICC values for 489 applications from the Google Play store. The ICC values we infer are substantially more precise than previous work. The analysis is efficient, taking two minutes per application on average. While this work can be used as the basis for many whole-program analyses of Android applications, the COAL solver can also be used to infer the values of composite objects in many other contexts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2550446","National Science Foundation; National Science Foundation; Google; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7447806","Composite constant;constant propagation;inter-component communication;ICC;Android application analysis","Androids;Humanoid robots;Coal;Correlation;Context;Object oriented modeling;Receivers","Android (operating system);mobile computing;program diagnostics","composite constant propagation;Android program analysis;COAL declarative language;Android inter-component communication value;ICC values;Google Play Store;whole-program analysis;Android applications","","5","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Utilizing an Executable Specification Language for an Information System","S. D. Urban; J. E. Urban; W. D. Dominick","Center for Advanced Computer Studies, University of Southwestern Louisiana; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","7","598","605","This paper describes an approach to software specification development with interpretation as applied to an information storage and retrieval system. Machine execution of software specifications is possible with both partial and complete specifications. A partial specification is interpreted using abstract execution. The Descartes specification language is utilized to describe a functional aspect of an existing information storage and retrieval system, namely, the MADAM (Multics Approach to Data Access and Management) system at the University of Southwestern Louisiana. Brief descriptions of both the Descartes language and the MADAM system precede the example specification. The paper concludes with a discussion of the expected results that this methodology could have on the pragmatic development and evolution of information systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232504","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702064","Abstract execution;executable specifications;information storage and retrieval;prototyping","Specification languages;Information systems;Software engineering;Information retrieval;Database systems;Application software;Software systems;Management information systems;Statistical analysis;Laboratories","","Abstract execution;executable specifications;information storage and retrieval;prototyping","","2","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Fragment class analysis for testing of polymorphism in Java software","A. Rountev; A. Milanova; B. G. Ryder","Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","6","372","387","Testing of polymorphism in object-oriented software may require coverage of all possible bindings of receiver classes and target methods at call sites. Tools that measure this coverage need to use class analysis to compute the coverage requirements. However, traditional whole-program class analysis cannot be used when testing incomplete programs. To solve this problem, we present a general approach for adapting whole-program class analyses to operate on program fragments. Furthermore, since analysis precision is critical for coverage tools, we provide precision measurements for several analyses by determining which of the computed coverage requirements are actually feasible for a set of subject components. Our work enables the use of whole-program class analyses for testing of polymorphism in partial programs, and identifies analyses that potentially are good candidates for use in coverage tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321060","Program analysis;class analysis;test coverage;object-oriented software.","Software testing;Java;Computer Society;Collaboration;Programming profession;Contracts;Fault detection;Performance evaluation;Guidelines;Computer science","Java;program testing;object-oriented methods;program diagnostics","fragment class analysis;polymorphism testing;Java software;object-oriented software;whole-program class analysis;partial program","","19","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Some Comments on ""Transition-Oriented"" Versus ""Structured"" Specification of Distributed Algorithms and Protocols","G. v. Bochmann; J. P. Verjus","Department d'IRO, University of Montreal; NA","IEEE Transactions on Software Engineering","","1987","SE-13","4","501","505","Formal description techniques (FDT's) are being developed for the specification of communication protocols and other distributed systems. Some of them (namely SDL and Estelle) are based on an extended state transition model and promote a ""transition-oriented"" specification style. Another one (namely Lotos) and most highlevel programming languages promote a style which is called ""structured."" The correspondence compares these two specification styles in the framework of rendezvous interactions between different system modules. The advantages of each of the two styles are discussed in relation with an example of a virtual ring mutual exclusion protocol. Transformation rules between the two approaches are given. An extension to the state transition oriented FDT's is also suggested in order to allow for a structured specification style.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233188","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702243","Distributed algorithms;Estelle;exception handling;extended state machines;mutual exclusion;SDL;structured programming","Distributed algorithms;Protocols;Carbon capture and storage;Computer languages;Specification languages;LAN interconnection;ISO;Automata;Signal generators;Modems","","Distributed algorithms;Estelle;exception handling;extended state machines;mutual exclusion;SDL;structured programming","","1","","16","","","","","","IEEE","IEEE Journals & Magazines"
"BLISS: Improved Symbolic Execution by Bounded Lazy Initialization with SAT Support","N. Rosner; J. Geldenhuys; N. M. Aguirre; W. Visser; M. F. Frias","Department of Computer Science, FCEyN, Universidad de Buenos Aires, Buenos Aires, Argentina; Department of Computer Science, University of Stellenbosch, Stellenbosch, South Africa; Department of Computer Science, FCEFQyN, Universidad Nacional de Rio Cuarto, and CONICET, Río Cuarto, Argentina; Department of Computer Science, University of Stellenbosch, Stellenbosch, South Africa; Department of Software Engineering, Instituto Tecnológico de Buenos Aires, and CONICET, Buenos Aires, Argentina","IEEE Transactions on Software Engineering","","2015","41","7","639","660","Lazy Initialization (LI) allows symbolic execution to effectively deal with heap-allocated data structures, thanks to a significant reduction in spurious and redundant symbolic structures. Bounded lazy initialization (BLI) improves on LI by taking advantage of precomputed relational bounds on the interpretation of class fields in order to reduce the number of spurious structures even further. In this paper we present bounded lazy initialization with SAT support (BLISS), a novel technique that refines the search for valid structures during the symbolic execution process. BLISS builds upon BLI, extending it with field bound refinement and satisfiability checks. Field bounds are refined while a symbolic structure is concretized, avoiding cases that, due to the concrete part of the heap and the field bounds, can be deemed redundant. Satisfiability checks on refined symbolic heaps allow us to prune these heaps as soon as they are identified as infeasible, i.e., as soon as it can be confirmed that they cannot be extended to any valid concrete heap. Compared to LI and BLI, BLISS reduces the time required by LI by up to four orders of magnitude for the most complex data structures. Moreover, the number of partially symbolic structures obtained by exploring program paths is reduced by BLISS by over 50 percent, with reductions of over 90 percent in some cases (compared to LI). BLISS uses less memory than LI and BLI, which enables the exploration of states unreachable by previous techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2389225","NPRP; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7004061","Symbolic execution;lazy initialization;tight field bounds;Symbolic PathFinder","Concrete;Binary trees;Java;Periodic structures;Software","computability;data structures;program verification","BLISS;symbolic execution;bounded lazy initialization with SAT support;heap-allocated data structures;symbolic structures;relational bounds;class fields;field bound refinement;satisfiability check;symbolic heap;program path","","4","","26","","","","","","IEEE","IEEE Journals & Magazines"
"A Data Mining Approach for Detecting Higher-Level Clones in Software","H. A. Basit; S. Jarzabek","Lahore University of Management Sciences, Lahore; National University of Singapore, Singapore","IEEE Transactions on Software Engineering","","2009","35","4","497","514","Code clones are similar program structures recurring in variant forms in software system(s). Several techniques have been proposed to detect similar code fragments in software, so-called simple clones. Identification and subsequent unification of simple clones is beneficial in software maintenance. Even further gains can be obtained by elevating the level of code clone analysis. We observed that recurring patterns of simple clones often indicate the presence of interesting higher-level similarities that we call structural clones. Structural clones show a bigger picture of similarity situation than simple clones alone. Being logical groups of simple clones, structural clones alleviate the problem of huge number of clones typically reported by simple clone detection tools, a problem that is often dealt with postdetection visualization techniques. Detection of structural clones can help in understanding the design of the system for better maintenance and in reengineering for reuse, among other uses. In this paper, we propose a technique to detect some useful types of structural clones. The novelty of our approach includes the formulation of the structural clone concept and the application of data mining techniques to detect these higher-level similarities. We describe a tool called clone miner that implements our proposed technique. We assess the usefulness and scalability of the proposed techniques via several case studies. We discuss various usage scenarios to demonstrate in what ways the knowledge of structural clones adds value to the analysis based on simple clones alone.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4796208","Design concepts;maintainability;restructuring;reverse engineering;reengineering;reusable software.","Data mining;Cloning;Software systems;Software maintenance;Collaboration;Portals;Computer Society;Visualization;Scalability;Reverse engineering","data mining;software maintenance;software reusability","data mining approach;higher-level clone detection;program structures;software system;software maintenance;code clone analysis;postdetection visualization techniques;software reusability","","44","","53","","","","","","IEEE","IEEE Journals & Magazines"
"A hybrid knowledge representation as a basis of requirement specification and specification analysis","J. J. P. Tsai; T. Weigert; H. -. Jang","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; NA; NA","IEEE Transactions on Software Engineering","","1992","18","12","1076","1100","A formal requirement specification language, the frame-and-rule oriented requirement specification language FRORL, developed to facilitate the specification, analysis, and development of a software system is presented. The surface syntax of FRORL is based on the concepts of frames and production rules that may bear hierarchical relationships to each other, relying on multiple inheritance. To provide thorough semantic foundations, FRORL is based on a nonmonotonic variant of Horn-clause logic. Using the machinery of Horn-clause logic, various properties of a FRORL specification can be analyzed. Among the external properties of FRORL are formality, object-orientedness, and a wide spectrum of life cycle phases. Intrinsic properties are modularity, provision for incremental development, inheritance, refinement, reusability, prototyping, and executability. A software development environment based on FRORL has been implemented using the C language on a Sun workstation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.184762","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=184762","","Knowledge representation;Specification languages;Logic;Software systems;Production;Machinery;Software prototyping;Prototypes;Programming;Sun","formal specification;Horn clauses;knowledge representation;logic programming languages;specification languages","nonmonotonic Horn clause logic;object oriented;hybrid knowledge representation;formal requirement specification language;frame-and-rule oriented requirement specification language;FRORL;surface syntax;frames;production rules;hierarchical relationships;multiple inheritance;inheritance;reusability;prototyping;executability;software development environment;C language","","39","","54","","","","","","IEEE","IEEE Journals & Magazines"
"A Comprehensive Approach to Naming and Accessibility in Refactoring Java Programs","M. Schäfer; A. Thies; F. Steimann; F. Tip","IBM T.J. Watson Research Center, Hawthorne; Fernuniversität in Hagen, Hagen; Fernuniversität in Hagen, Hagen; IBM, Hawthorne","IEEE Transactions on Software Engineering","","2012","38","6","1233","1257","Automated tool support for refactoring is now widely available for mainstream programming languages such as Java. However, current refactoring tools are still quite fragile in practice and often fail to preserve program behavior or compilability. This is mainly because analyzing and transforming source code requires consideration of many language features that complicate program analysis, in particular intricate name lookup and access control rules. This paper introduces J<sub>L</sub>, a lookup-free, access control-free representation of Java programs. We present algorithms for translating Java programs into J<sub>L</sub>and vice versa, thereby making it possible to formulate refactorings entirely at the level of J<sub>L</sub>and to rely on the translations to take care of naming and accessibility issues. We demonstrate how complex refactorings become more robust and powerful when lifted to J<sub>L</sub>. Our approach has been implemented using the JastAddJ compiler framework, and evaluated by systematically performing two commonly used refactorings on an extensive suite of real-world Java applications. The evaluation shows that our tool correctly handles many cases where current refactoring tools fail to handle the complex rules for name binding and accessibility in Java.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6152131","Restructuring;reverse engineering;and reengineering;object-oriented languages;Java","Java;Access control;Feature extraction;Reverse engineering;Object oriented programming;Shadow mapping;Program processors","authorisation;Java;naming services;program compilers;program diagnostics;software maintenance","comprehensive approach;naming issues;accessibility issues;Java program refactoring;mainstream programming languages;source code analysis;source code transformation;language features;program analysis;name lookup;access control rules;JL;lookup-free access control-free representation;JastAddJ compiler framework","","8","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Xstream: a middleware for streaming XML contents over wireless environments","E. Y. C. Wong; A. T. S. Chan; Hong Va Leong","Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China; Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China; Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China","IEEE Transactions on Software Engineering","","2004","30","12","918","935","XML (extensible Markup Language) has been developed and deployed by domain-specific standardization bodies and commercial companies. Studies have been conducted on a wide variety of issues encompassing XML. In the use of XML for wireless computing, the focus has been on investigating ways to efficiently represent XML data for transmission over a wireless environment. We propose a middleware, Xstream (XML Streaming), for efficiently streaming XML contents over a wireless environment by leveraging the rich semantics and structural characteristics of XML documents and by flexibly managing units containing fragments of data into autonomous units, known as XDU (Xstream Data Unit) fragments. The concept of an XDU is fundamental to the operation of Xstream. It provides for the efficient transfer of documents across a wireless link and allows other issues and challenges pertaining to wireless transmission to be addressed. By fragmenting and organizing an XML document into XDU fragments, we are able to incrementally send fragments across a wireless link, while the receiver is able to perform look-ahead processing of the document without having to wait for the entire document to be downloaded. We propose a fragmenting strategy based on the value of the wireless link's Maximum Transfer Units (MTUs). In addition, we present and evaluate several packetizing strategies, i.e., strategies wherein a collection of XDUs are grouped into a packet to optimize packet delivery and processing. At the receiving end of this process, a reassembly strategy incrementally reconstructs the XML document as XDU fragments are being received, thereby facilitating client application implementation of look-ahead processing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.108","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377189","Index Terms- XML;streaming;multiresolution;wireless;fragmentation;packetizing;middleware.","Middleware;XML;Wireless application protocol;Markup languages;Mobile computing;Standardization;Content management;Environmental management;Organizing","XML;middleware;mobile computing;multimedia computing;formal specification","Xstream middleware;XML content streaming;wireless environments;extensible Markup Language;domain-specific standardization;wireless computing;XML documents;Xstream Data Unit fragments;Maximum Transfer Units;packetizing strategies","","5","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Conflicts in policy-based distributed systems management","E. C. Lupu; M. Sloman","Dept. of Comput., Imperial Coll. of Sci., Technol. & Med., London, UK; NA","IEEE Transactions on Software Engineering","","1999","25","6","852","869","Modern distributed systems contain a large number of objects and must be capable of evolving, without shutting down the complete system, to cater for changing requirements. There is a need for distributed, automated management agents whose behavior also has to dynamically change to reflect the evolution of the system being managed. Policies are a means of specifying and influencing management behavior within a distributed system, without coding the behavior into the manager agents. Our approach is aimed at specifying implementable policies, although policies may be initially specified at the organizational level and then refined to implementable actions. We are concerned with two types of policies. Authorization policies specify what activities a manager is permitted or forbidden to do to a set of target objects and are similar to security access-control policies. Obligation policies specify what activities a manager must or must not do to a set of target objects and essentially define the duties of a manager. Conflicts can arise in the set of policies. Conflicts may also arise during the refinement process between the high level goals and the implementable policies. The system may have to cater for conflicts such as exceptions to normal authorization policies. The paper reviews policy conflicts, focusing on the problems of conflict detection and resolution. We discuss the various precedence relationships that can be established between policies in order to allow inconsistent policies to coexist within the system and present a conflict analysis tool which forms part of a role based management framework. Software development and medical environments are used as example scenarios.","0098-5589;1939-3520;2326-3881","","10.1109/32.824414","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=824414","","Authorization;Security;Computer Society;Humans;File systems","distributed processing;authorisation;management of change;systems analysis;bibliographies","policy based distributed systems management;modern distributed systems;changing requirements;automated management agents;management behavior;manager agents;implementable policies;organizational level;implementable action;authorization policies;target objects;security access-control policies;obligation policies;refinement process;high level goals;conflict detection;precedence relationships;conflict analysis tool;role based management framework;software development;medical environments","","284","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluation of Accuracy in Design Pattern Occurrence Detection","N. Pettersson; W. Lowe; J. Nivre","Växjö University, Växjö; Växjö University, Växjö; Växjö University, Växjö","IEEE Transactions on Software Engineering","","2010","36","4","575","590","Detection of design pattern occurrences is part of several solutions to software engineering problems, and high accuracy of detection is important to help solve the actual problems. The improvement in accuracy of design pattern occurrence detection requires some way of evaluating various approaches. Currently, there are several different methods used in the community to evaluate accuracy. We show that these differences may greatly influence the accuracy results, which makes it nearly impossible to compare the quality of different techniques. We propose a benchmark suite to improve the situation and a community effort to contribute to, and evolve, the benchmark suite. Also, we propose fine-grained metrics assessing the accuracy of various approaches in the benchmark suite. This allows comparing the detection techniques and helps improve the accuracy of detecting design pattern occurrences.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.92","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374428","Patterns;object-oriented design methods;measurement techniques;evaluation;reverse engineering;reengineering;restructuring.","Design methodology;Software systems;Computer science;Natural languages;Software engineering;Measurement techniques;Reverse engineering;Software tools;Application software;Software quality","object-oriented methods;software engineering;software metrics;software performance evaluation","design pattern occurrence detection;software engineering problem;fine grained metric;benchmark suite","","33","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Refactoring the aspectizable interfaces: an empirical assessment","P. Tonella; M. Ceccato","Centro per la Ricerca Scientifica e Tecnologica, ITC, Povo, Italy; Centro per la Ricerca Scientifica e Tecnologica, ITC, Povo, Italy","IEEE Transactions on Software Engineering","","2005","31","10","819","832","Aspect oriented programming aims at addressing the problem of the crosscutting concerns, i.e., those functionalities that are scattered among several modules in a given system. Aspects can be defined to modularize such concerns. In this work, we focus on a specific kind of crosscutting concerns, the scattered implementation of methods declared by interfaces that do not belong to the principal decomposition. We call such interfaces aspectizable. All the aspectizable interfaces identified within a large number of classes from the Java Standard Library and from three Java applications have been automatically migrated to aspects. To assess the effects of the migration on the internal and external quality attributes of these systems, we collected a set of metrics and we conducted an empirical study, in which some maintenance tasks were executed on the two alternative versions (with and without aspects) of the same system. In this paper, we report the results of such a comparison.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.115","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542065","Index Terms- Aspect oriented programming;refactoring;program transformations;empirical studies.","Scattering;Java;Functional programming;Libraries;Object oriented programming;Packaging;Containers","object-oriented programming;software metrics;Java;software libraries;software quality;software maintenance;application program interfaces","aspectizable interfaces;aspect oriented programming;crosscutting concerns;Java Standard Library;software metrics;software maintenance","","24","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Reuse of software through generation of partial systems","F. J. Polster","Kernforschungszentrum Karlsruhe GmbH, Institut f&#x00FC;r Datenverarbeitung in der Technik, Postfach 3640, D-7500 Karlsruhe 1, West Germany; Dornier GmbH, WF30, D-7990 Friedrichshafen 1, West Germany","IEEE Transactions on Software Engineering","","1986","SE-12","3","402","416","The author considers the problem of constructing partial systems, where the program of a partial system is obtained by selecting only those code segments of the complete program that implement the capabilities needed. A heuristic for determining fragments of a program system, which can serve as the building blocks for the programs of partial systems, is presented. The notion of `<i>B</i>-program' is introduced: a <i>B</i>-program contains, in addition to the fragments themselves for each fragment, substitute code and control information specifying the set of partial systems the fragment is relevant for. A representation of <i>B</i>-programs as a string is given such that generating a partial system consists in scanning this string and selecting substrings. A formal model for this type of program generation is developed. <i>B</i>-program reduction is dealt with; transformations for the elimination of superfluous vertices are presented; and the issue of uniqueness and the problem of constructing a minimal reduced <i>B</i>-program are discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312882","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312882","Code fragments;code selection;customizing;general software;generic systems;program generation;program tailoring;reuse of software","Indexes;Algorithm design and analysis;Computer aided software engineering;Software systems","software engineering","software reuse;software engineering;partial systems;code segments;fragments;building blocks;B-program;control information;formal model;program generation;transformations;superfluous vertices;uniqueness;minimal reduced B-program","","7","","","","","","","","IEEE","IEEE Journals & Magazines"
"Verifying the Evolution of Probability Distributions Governed by a DTMC","Y. Kwon; G. Agha","Microsoft Corporation, Redmond; University of Illinois at Urbana-Champaign, Urbana","IEEE Transactions on Software Engineering","","2011","37","1","126","141","We propose a new probabilistic temporal logic, iLTL, which captures properties of systems whose state can be represented by probability mass functions (pmfs). Using iLTL, we can specify reachability to a state (i.e., a pmf), as well as properties representing the aggregate (expected) behavior of a system. We then consider a class of systems whose transitions are governed by a Markov Chain-in this case, the set of states a system may be in is specified by the transitions of pmfs from all potential initial states to the final state. We then provide a model checking algorithm to check iLTL properties of such systems. Unlike existing model checking techniques, which either compute the portions of the computational paths that satisfy a specification or evaluate properties along a single path of pmf transitions, our model checking technique enables us to do a complete analysis on the expected behaviors of large-scale systems. Desirable system parameters may also be found as a counterexample of a negated goal. Finally, we illustrate the usefulness of iLTL model checking by means of two examples: assessing software reliability and ensuring the results of administering a drug.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.80","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5557891","Probabilistic model checking;linear temporal logic;Discrete Time Markov Chain;pharmacokinetics.","Markov processes;Limiting;Eigenvalues and eigenfunctions;Computational modeling;Transient analysis;Probability distribution;Steady-state","formal verification;Markov processes;statistical distributions;temporal logic","probability distribution;DTMC;temporal logic;iLTL;probability mass function;model checking;large scale system;discrete time Markov chain","","6","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Synthesis of behavioral models from scenarios","S. Uchitel; J. Kramer; J. Magee","Dept. of Comput., Imperial Coll., London, UK; Dept. of Comput., Imperial Coll., London, UK; Dept. of Comput., Imperial Coll., London, UK","IEEE Transactions on Software Engineering","","2003","29","2","99","115","Scenario-based specifications such as Message Sequence Charts (MSCs) are useful as part of a requirements specification. A scenario is a partial story, describing how system components, the environment, and users work concurrently and interact in order to provide system level functionality. Scenarios need to be combined to provide a more complete description of system behavior. Consequently, scenario synthesis is central to the effective use of scenario descriptions. How should a set of scenarios be interpreted? How do they relate to one another? What is the underlying semantics? What assumptions are made when synthesizing behavior models from multiple scenarios? In this paper, we present an approach to scenario synthesis based on a clear sound semantics, which can support and integrate many of the existing approaches to scenario synthesis. The contributions of the paper are threefold. We first define an MSC language with sound abstract semantics in terms of labeled transition systems and parallel composition. The language integrates existing approaches based on scenario composition by using high-level MSCs (hMSCs) and those based on state identification by introducing explicit component state labeling. This combination allows stakeholders to break up scenario specifications into manageable parts and reuse scenarios using hMCSs; it also allows them to introduce additional domain-specific information and general assumptions explicitly into the scenario specification using state labels. Second, we provide a sound synthesis algorithm which translates scenarios into a behavioral specification in the form of Finite Sequential Processes. This specification can be analyzed with the Labeled Transition System Analyzer using model checking and animation. Finally, we demonstrate how many of the assumptions embedded in existing synthesis approaches can be made explicit and modeled in our approach. Thus, we provide the basis for a common approach to scenario-based specification, synthesis, and analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178048","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178048","","Computer Society;Switches;Labeling;Animation;Software engineering;Unified modeling language","formal specification;specification languages","scenario-based specifications;message sequence charts;requirements specification;semantics;multiple scenarios;scenario synthesis;labeled transition systems;parallel composition;explicit component state labeling;synthesis algorithm;behavioral specification;finite sequential processes;labeled transition system analyzer","","124","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Reasons for software effort estimation error: impact of respondent role, information collection approach, and data analysis method","M. Jorgensen; K. Molokken-Ostvold","Simula Res. Lab., Lysaker, Norway; Simula Res. Lab., Lysaker, Norway","IEEE Transactions on Software Engineering","","2004","30","12","993","1007","This study aims to improve analyses of why errors occur in software effort estimation. Within one software development company, we collected information about estimation errors through: 1) interviews with employees in different roles who are responsible for estimation, 2) estimation experience reports from 68 completed projects, and 3) statistical analysis of relations between characteristics of the 68 completed projects and estimation error. We found that the role of the respondents, the data collection approach, and the type of analysis had an important impact on the reasons given for estimation error. We found, for example, a strong tendency to perceive factors outside the respondents' own control as important reasons for inaccurate estimates. Reasons given for accurate estimates, on the other hand, typically cited factors that were within the respondents' own control and were determined by the estimators' skill or experience. This bias in types of reason means that the collection only of project managers' viewpoints will not yield balanced models of reasons for estimation error. Unfortunately, previous studies on reasons for estimation error have tended to collect information from project managers only. We recommend that software companies combine estimation error information from in-depth interviews with stakeholders in all relevant roles, estimation experience reports, and results from statistical analyses of project characteristics","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.103","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377193","Index Terms- Cost estimation;review and evaluation;performance evaluation.","Estimation error;Data analysis;Project management;Statistical analysis;Computer errors;Information analysis;Programming;Error analysis;Information management","cost-benefit analysis;error handling;project management;software cost estimation;software development management;software performance evaluation","software effort estimation error;information collection approach;data analysis;software development;statistical analysis;project management;interviews;cost estimation;project evaluation;software review;performance evaluation","","40","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional semantics of a real-time prototyping language","B. Kramer; Luqi; V. Berzins","Dept. of Comput. Sci., US Naval Postgraduate Sch., Monterey, CA, USA; Dept. of Comput. Sci., US Naval Postgraduate Sch., Monterey, CA, USA; Dept. of Comput. Sci., US Naval Postgraduate Sch., Monterey, CA, USA","IEEE Transactions on Software Engineering","","1993","19","5","453","477","The formal semantics of a prototyping language for hard real-time systems, PSDL, is given. PSDL provides a data flow notation augmented by application-orientation timing and control constraints to describe a system as a hierarchy of networks of processing units communicating via data streams. The semantics of PSDL are defined in terms of algebraic high-level Petri nets. This formalism combines algebraic specifications of abstract data types with process and concurrency concepts of Petri nets. Its data abstraction facilities are used to define the meaning of PSDL data types, while high-level Petri nets serve to model the casual and timing behavior of a system. The net model exposes potential concurrency of computation and makes all synchronization needs implied by timing and control constraints explicit and precise. Time is treated as state of clocks, and clocks are modeled as ordinary system components. The net semantics provides the basis for applying analysis techniques and semantic tools available for high-level Petri nets.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232012","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232012","","Prototypes;Timing;Petri nets;Real time systems;Control systems;Concurrent computing;Clocks;Synchronization;Application software;Process control","abstract data types;formal specification;Petri nets;real-time systems;software prototyping;specification languages","compositional semantics;real-time prototyping language;formal semantics;hard real-time systems;PSDL;data flow notation;application-orientation timing;control constraints;algebraic high-level Petri nets;algebraic specifications;abstract data types;concurrency concepts;timing behavior;synchronization","","14","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods","G. J. Pai; J. Bechta Dugan","Fraunhofer Institute for Experimental Software Engineering (IESE), Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany; Charles L. Brown Department of Electrical and Computer Engineering, University of Virginia, 351 McCormick Road, PO Box 400743, Charlottesville, VA 22904-4743","IEEE Transactions on Software Engineering","","2007","33","10","675","686","We present a methodology for Bayesian analysis of software quality. We cast our research in the broader context of constructing a causal framework that can include process, product, and other diverse sources of information regarding fault introduction during the software development process. In this paper, we discuss the aspect of relating internal product metrics to external quality metrics. Specifically, we build a Bayesian network (BN) model to relate object-oriented software metrics to software fault content and fault proneness. Assuming that the relationship can be described as a generalized linear model, we derive parametric functional forms for the target node conditional distributions in the BN. These functional forms are shown to be able to represent linear, Poisson, and binomial logistic regression. The models are empirically evaluated using a public domain data set from a software subsystem. The results show that our approach produces statistically significant estimations and that our overall modeling method performs no worse than existing techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70722","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302779","Bayesian analysis;Bayesian networks;defects;fault proneness;metrics;object-oriented;regression;software quality","Bayesian methods;Software quality;Object oriented modeling;Software measurement;Programming;Quality assessment;Information resources;Software metrics;Logistics","Bayes methods;mathematics computing;object-oriented methods;regression analysis;software fault tolerance;software metrics;software quality","binomial logistic regression;Poisson logistic regression;linear logistic regression;object-oriented software metrics;Bayesian network;software quality;software fault proneness;software fault content","","86","","31","","","","","","IEEE","IEEE Journals & Magazines"
"A microprogramming logic","W. Damm","Dept. of Comput. Sci., Tech. Aachen Univ., West Germany","IEEE Transactions on Software Engineering","","1988","14","5","559","574","A universal syntax-directed proof system is presented for the verification of horizontal computer architectures. The system is based on the axiomatic architecture description language AADL, which is sufficiently rich to allow the specification of target architectures while providing a concise model for clocked microarchitectures. For each description A epsilon AADL of a host, it is shown how to construct systematically a (Hoare-style) axiomatic definition of an A-dependent high-level microprogramming language based on S*. The axiomatization of A's microoperations together with a powerful proof-rule dealing with the inherent low-level parallelism of horizontal architectures allow a complete axiomatic treatment of the timing behavior and dynamic conflicts of microprograms written in S*(A).<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6134","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6134","","Microprogramming;Logic;Computer architecture;Computer science;Architecture description languages;Power system modeling;Clocks;Microarchitecture;Parallel processing;Timing","computer architecture;formal logic;microprogramming;specification languages;theorem proving","microprogramming logic;syntax-directed proof system;horizontal computer architectures;architecture description language;AADL;specification;clocked microarchitectures;axiomatic definition;microoperations;low-level parallelism;timing behavior;dynamic conflicts","","2","","71","","","","","","IEEE","IEEE Journals & Magazines"
"Building knowledge through families of experiments","V. R. Basili; F. Shull; F. Lanubile","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","1999","25","4","456","473","Experimentation in software engineering is necessary but difficult. One reason is that there are a large number of context variables and, so, creating a cohesive understanding of experimental results requires a mechanism for motivating studies and integrating results. It requires a community of researchers that can replicate studies, vary context variables, and build models that represent the common observations about the discipline. The paper discusses the experience of the authors, based upon a collection of experiments, in terms of a framework for organizing sets of related studies. With such a framework, experiments can be viewed as part of common families of studies, rather than being isolated events. Common families of studies can contribute to important and relevant hypotheses that may not be suggested by individual experiments. A framework also facilitates building knowledge in an incremental manner through the replication of experiments within families of studies. To support the framework, the paper discusses the experiences of the authors in carrying out empirical studies, with specific emphasis on persistent problems encountered in experimental design, threats to validity, criteria for evaluation, and execution of experiments in the domain of software engineering.","0098-5589;1939-3520;2326-3881","","10.1109/32.799939","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=799939","","Software engineering;Buildings;Design for experiments;Testing;Computer science;Mathematical model;Computer Society;Context modeling;Organizing;Software measurement","software engineering","software engineering experimentation;studies;knowledge building;empirical studies;experimental design","","359","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Heuristics for join processing using nonclustered indexes","E. R. Omiecinski","Sch. of Inf. & Comput. Sci., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Transactions on Software Engineering","","1989","15","1","18","25","The author examines join processing when the access paths available are nonclustered indexes on the joining attribute(s) for both relations involved in the join. He uses a bipartite graph model to represent the pages from the two relations that contain tuples to be joined. The minimization of the number of page accesses needed to compute a join in the author's database environment is explored from two perspectives. The first is to reduce the maximum buffer size so that no page is accessed more than once, and the second is to reduce the number of page accesses for a fixed buffer size. The author has developed heuristics for these problems. He gives performance comparisons of these heuristics and another method that recently appeared in the literature. Results show that one particular heuristic performs very well for addressing the problem from either perspective.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21722","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21722","","Relational databases;Cost function;Buffer storage;Computational efficiency;Indexes;Bipartite graph;Query processing;Computer science;Indexing","relational databases","relational databases;query optimisation;join processing;nonclustered indexes;access paths;bipartite graph model;relations;tuples;page accesses;database environment;buffer size","","8","","12","","","","","","IEEE","IEEE Journals & Magazines"
"STATEMATE: a working environment for the development of complex reactive systems","D. Harel; H. Lachover; A. Naamad; A. Pnueli; M. Politi; R. Sherman; A. Shtull-Trauring; M. Trakhtenbrot","i-Logix Inc., Burlington, MA, USA; i-Logix Inc., Burlington, MA, USA; i-Logix Inc., Burlington, MA, USA; i-Logix Inc., Burlington, MA, USA; i-Logix Inc., Burlington, MA, USA; i-Logix Inc., Burlington, MA, USA; i-Logix Inc., Burlington, MA, USA; i-Logix Inc., Burlington, MA, USA","IEEE Transactions on Software Engineering","","1990","16","4","403","414","STATEMATE is a set of tools, with a heavy graphical orientation, intended for the specification, analysis, design, and documentation of large and complex reactive systems. It enables a user to prepare, analyze, and debug diagrammatic, yet precise, descriptions of the system under development from three interrelated points of view, capturing structure, functionality, and behavior. These views are represented by three graphical languages, the most intricate of which is the language of statecharts, used to depict reactive behavior over time. In addition to the use of statecharts, the main novelty of STATEMATE is in the fact that it understands the entire descriptions perfectly, to the point of being able to analyze them for crucial dynamic properties, to carry out rigorous executions and simulations of the described system, and to create running code automatically. These features are invaluable when it comes to the quality and reliability of the final outcome.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.54292","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=54292","","Real time systems;Control systems;Embedded software;Software systems;Research and development;Documentation;Communication system control;Communication system software;Software tools;Hardware","programming environments;software engineering;systems analysis","STATEMATE;working environment;development;complex reactive systems;graphical orientation;specification;analysis;design;documentation;debug diagrammatic;functionality;behavior;graphical languages;statecharts","","545","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Quantitative analysis of faults and failures in a complex software system","N. E. Fenton; N. Ohlsson","Dept. of Comput. Sci., Queen Mary & Westfield Coll., London, UK; NA","IEEE Transactions on Software Engineering","","2000","26","8","797","814","The authors describe a number of results from a quantitative study of faults and failures in two releases of a major commercial software system. They tested a range of basic software engineering hypotheses relating to: the Pareto principle of distribution of faults and failures; the use of early fault data to predict later fault and failure data; metrics for fault prediction; and benchmarking fault data. For example, we found strong evidence that a small number of modules contain most of the faults discovered in prerelease testing and that a very small number of modules contain most of the faults discovered in operation. We found no evidence to support previous claims relating module size to fault density nor did we find evidence that popular complexity metrics are good predictors of either fault-prone or failure-prone modules. We confirmed that the number of faults discovered in prerelease testing is an order of magnitude greater than the number discovered in 12 months of operational use. The most important result was strong evidence of a counter-intuitive relationship between pre- and postrelease faults; those modules which are the most fault-prone prerelease are among the least fault-prone postrelease, while conversely, the modules which are most fault-prone postrelease are among the least fault-prone prerelease. This observation has serious ramifications for the commonly used fault density measure. Our results provide data-points in building up an empirical picture of the software development process.","0098-5589;1939-3520;2326-3881","","10.1109/32.879815","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879815","","Failure analysis;Software systems;Density measurement;Software engineering;Software testing;Computer industry;Benchmark testing;Programming;Software metrics;Phase measurement","software performance evaluation;software metrics;software reliability","quantitative analysis;complex software system faults;quantitative study;commercial software system;basic software engineering hypotheses;Pareto principle;early fault data;failure data;software metrics;fault prediction;benchmarking;prerelease testing;module size;fault density;complexity metrics;failure-prone modules;operational use;counter-intuitive relationship;postrelease faults;fault-prone prerelease;fault-prone postrelease;fault density measure;data-points;software development process","","319","","46","","","","","","IEEE","IEEE Journals & Magazines"
"A layered approach to automating the verification of real-time systems","R. Gerber; I. Lee","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA","IEEE Transactions on Software Engineering","","1992","18","9","768","784","A layered approach to the specification and verification of real-time systems is described. Application processes are specified in the CSR Application Language, which includes high-level language constructs such as timeouts, deadlines, periodic processes, interrupts, and exception handling. A configuration schema is used to map the processes to system resources, and to specify the communication links between them. The authors automatically translate the result of the mapping into the CCSR process algebra, which characterizes CSR's resource-based computation model by a prioritized transition system. For the purposes of verification, a reachability analyzer based on the CCSR semantics has been implemented. This tool mechanically evaluates the correctness of the CSR specification by checking whether an exception state can be reached in its corresponding CCSR term. The effectiveness of this technique is illustrated by a multisensor robot example.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.159838","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=159838","","Real time systems;Timing;Algebra;Robots;Patient monitoring;Network topology;Computational modeling;Computer languages;Control systems","calculus of communicating systems;exception handling;formal specification;formal verification;high level languages;real-time systems","layered approach;specification;verification;real-time systems;CSR Application Language;high-level language constructs;timeouts;deadlines;periodic processes;interrupts;exception handling;configuration schema;system resources;communication links;CCSR process algebra;resource-based computation model;prioritized transition system;reachability analyzer;CCSR semantics;correctness;exception state;multisensor robot example","","27","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Effects of response and stability on scheduling in distributed computing systems","T. L. Casavant; J. G. Kuhl","Dept. of Electr. & Comput. Eng., Iowa Univ., Iowa City, IA, USA; Dept. of Electr. & Comput. Eng., Iowa Univ., Iowa City, IA, USA","IEEE Transactions on Software Engineering","","1988","14","11","1578","1588","An examination is made of the effects of response and stability on scheduling algorithms for general-purpose distributed computing systems. Response characterizes the time required, following a perturbation in the system state, to reach a new equilibrium state. Stability is a measure of the ability of a mechanism to detect when the effects of further actions will not improve the system state as defined by a user-defined objective. These results have implications for distributed computations in general. Analysis is based on formal communicating finite automata models of two distinct approaches to the scheduling problem, each using the objective of global optimal load balancing. The results indicate that absolute stability is not always necessary in dynamic systems for the same reasons that relatively small amounts of instability are tolerated in the design of analog control systems. It is shown that response is a very important first-order metric of dynamic scheduling behavior, and that response and stability are related.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9046","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9046","","Stability;Processor scheduling;Distributed computing;Scheduling algorithm;Time factors;Automata;Load management;Automatic control;Control systems;Dynamic scheduling","distributed processing;finite automata;scheduling","response;stability;scheduling;distributed computing systems;user-defined objective;communicating finite automata models;load balancing;dynamic systems;first-order metric","","27","","16","","","","","","IEEE","IEEE Journals & Magazines"
"On the optimal total processing time using checkpoints","B. Dimitrov; Z. Khalil; N. Kolev; P. Petrov","Inst. of Math., Acad. of Sci., Sofia, Bulgaria; NA; NA; NA","IEEE Transactions on Software Engineering","","1991","17","5","436","442","The authors investigate the problem of optimizing the expected blocking time duration by providing a schedule of checkpoints during the required job processing time. They give a general approach for determining the optimal checkpoint schedule and derive some cases when the optimal checkpointing is uniform. The model has applications in unreliable computing systems, multiclient computer service, data transmissions, etc.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90446","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=90446","","Electric breakdown;Processor scheduling;Checkpointing;Data communication;Testing;Performance evaluation;Councils;Mathematics;Application software;Computer applications","optimisation;programming theory;scheduling","optimal total processing time;optimal checkpoint schedule;unreliable computing systems;multiclient computer service;data transmissions","","7","","9","","","","","","IEEE","IEEE Journals & Magazines"
"An analysis of the Intel 80/spl times/86 security architecture and implementations","O. Sibert; P. A. Porras; R. Lindell","Oxford Syst. Inc., Lexington, MA, USA; NA; NA","IEEE Transactions on Software Engineering","","1996","22","5","283","293","An in depth analysis of the 80/spl times/86 processor families identifies architectural properties that may have unexpected, and undesirable, results in secure computer systems. In addition, reported implementation errors in some processor versions render them undesirable for secure systems because of potential security and reliability problems. We discuss the imbalance in scrutiny for hardware protection mechanisms relative to software, and why this imbalance is increasingly difficult to justify as hardware complexity increases. We illustrate this difficulty with examples of architectural subtleties and reported implementation errors.","0098-5589;1939-3520;2326-3881","","10.1109/32.502221","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=502221","","Computer architecture;Hardware;Computer errors;Computer security;Protection;Microprocessors;Trademarks;Materials testing;Performance analysis;Performance evaluation","microprocessor chips;computer architecture;security of data;computer testing;integrated circuit testing","Intel 80 x 86 security architecture;processor families;architectural properties;secure computer systems;processor versions;secure systems;hardware protection mechanisms;hardware complexity;architectural subtleties;implementation errors","","5","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Key Stakeholders' Value Propositions for Feature Selection in Software-intensive Products: An Industrial Case Study","P. Rodríguez; E. Mendes; B. Turhan","Department of Information Processing Sciences, University of Oulu, Oulu, Oulu Finland 90014 (e-mail: pilar.rodriguez@oulu.fi); Computer Science, Blekinge Tekniska Hogskola, 4206 Karlskrona, Blekinge Sweden 371 79 (e-mail: emilia.mendes@bth.se); Computer Science, Brunel University, 3890 Uxbridge, Middlesex United Kingdom of Great Britain and Northern Ireland UB8 3PH (e-mail: turhanb@computer.org)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Numerous software companies are adopting value-based decision making. However, what does value mean for key stakeholders making decisions? How do different stakeholder groups understand value? Without an explicit understanding of what value means, decisions are subject to ambiguity and vagueness, which are likely to bias them. This case study provides an in-depth analysis of key stakeholders' value propositions when selecting features for a large telecommunications company's software-intensive product. Stakeholder' value propositions were elicited via interviews, which were analyzed using Grounded Theory coding techniques (open and selective coding). Thirty-six value propositions were identified and classified into six dimensions: customer value, market competitiveness, economic value/profitability, cost efficiency, technology & architecture, and company strategy. Our results show that although propositions in the customer value dimension were those mentioned the most, the concept of value for feature selection encompasses a wide range of value propositions. Moreover, stakeholder groups focused on different and complementary value dimensions, calling to the importance of involving all key stakeholders in the decision making process. Although our results are particularly relevant to companies similar to the one described herein, they aim to generate a learning process on value-based feature selection for practitioners and researchers in general.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2878031","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8509148","Value-based Software Engineering (VBSE);Feature Selection;Release Planning;Decision-making;Value Proposition;Stakeholder Analysis;Key Stakeholders;Software-intensive Systems;Case Study;Grounded Theory","Stakeholders;Software;Companies;Feature extraction;Planning;Decision making;Economics","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Web structures: a tool for representing and manipulating programs","A. Maggiolo-Schettini; M. Napoli; G. Tortora","Dept. of Inf., Pisa Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","1988","14","11","1621","1639","The authors introduce web structures and their transformations and develop their theory in the framework of category theory. Once a program has been represented as a web structure, software tools, such as a high-level data flow analyzer or other general program transformers, can be written as sets of web structure production rules. An implementation of web structure transformations is in progress. The mathematical theory of web structure transformations allows form proofs of properties both at the metatheoretical and theoretical levels.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9050","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9050","","Data analysis;Software tools;Transformers;Production;Computer languages;Debugging;III-V semiconductor materials;Information analysis;Algorithm design and analysis","data structures;graph theory;program verification;programming theory;set theory;software tools","web structures;category theory;software tools;high-level data flow analyzer;program transformers;production rules;web structure transformations","","6","","11","","","","","","IEEE","IEEE Journals & Magazines"
"The estimation of parameters of the hypergeometric distribution and its application to the software reliability growth model","Y. Tohma; H. Yamano; M. Ohba; R. Jacoby","Dept. of Comput. Sci., Tokyo Inst. of Technol., Japan; Dept. of Comput. Sci., Tokyo Inst. of Technol., Japan; NA; NA","IEEE Transactions on Software Engineering","","1991","17","5","483","489","Six ways to estimate parameters of the hypergeometric distribution are investigated, and their accuracies are examined comparatively. It is demonstrated that the least-squares sum method is the best one among those tried, and can be applied to real test/debug data for estimating the number of faults still resident in a program after test/debugging. By this method the estimation time can be reduced greatly.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90450","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=90450","","Parameter estimation;Application software;Computer science;Jacobian matrices;Software reliability;Programming profession;Software debugging;Software testing;Costs;Computer science education","least squares approximations;parameter estimation;program debugging;program testing;software reliability","program testing;program debugging;parameter estimation;program fault estimation;software reliability growth model;hypergeometric distribution;least-squares sum method","","34","","9","","","","","","IEEE","IEEE Journals & Magazines"
"A formally verified application-level framework for real-time scheduling on POSIX real-time operating systems","P. Li; Binoy Ravindran; S. Suhaib; S. Feizabadi","Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., USA; Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., USA; Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., USA; Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., USA","IEEE Transactions on Software Engineering","","2004","30","9","613","629","We present a framework, called meta scheduler, for implementing real-time scheduling algorithms. The meta scheduler is a portable middleware layer component designed for implementations over POSIX-compliant operating systems. It accommodates pluggable real-time scheduling algorithms while offering the flexibility of platform independence - the singular underlying OS requirement is the now nearly ubiquitous POSIX compliance. The versatility of pluggable schedulers positions the meta scheduler for deployment in an interoperable heterogeneous real-time environment. We present the design of the meta scheduler and outline its implementation. Furthermore, we present a mechanized correctness verification using the UPPAAL model checker. Prototype implementation of the meta scheduler over QNX Neutrino real-time operating system demonstrates high performance and a small footprint.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324648","Index Terms- Real-time scheduling;time/utility functions;utility accrual scheduling;Portable Operating System Interface (POSIX);model checking.","Real time systems;Operating systems;Scheduling algorithm;Job shop scheduling;Time factors;Processor scheduling;Middleware;Prototypes;Neutrino sources;Adaptive systems","real-time systems;middleware;formal verification;scheduling;network operating systems;distributed object management","meta scheduler;real-time scheduling;portable middleware layer component;interoperable heterogeneous real-time environment;formal verification;UPPAAL model checker;QNX Neutrino real-time operating system;utility functions;utility accrual scheduling;portable operating system interface","","36","","45","","","","","","IEEE","IEEE Journals & Magazines"
"What Do We Know about the Effectiveness of Software Design Patterns?","C. Zhang; D. Budgen","Durham University, Durham; Durham University, Durham","IEEE Transactions on Software Engineering","","2012","38","5","1213","1231","Context. Although research in software engineering largely seeks to improve the practices and products of software development, many practices are based upon codification of expert knowledge, often with little or no underpinning from objective empirical evidence. Software design patterns seek to codify expert knowledge to share experience about successful design structures. Objectives. To investigate how extensively the use of software design patterns has been subjected to empirical study and what evidence is available about how and when their use can provide an effective mechanism for knowledge transfer about design. Method. We conducted a systematic literature review in the form of a mapping study, searching the literature up to the end of 2009 to identify relevant primary studies about the use of the 23 patterns catalogued in the widely referenced book by the “Gang of Four.” These studies were then categorized according to the forms of study employed, the patterns that were studied, as well as the context within which the study took place. Results. Our searches identified 611 candidate papers. Applying our inclusion/exclusion criteria resulted in a final set of 10 papers that described 11 instances of “formal” experimental studies of object-oriented design patterns. We augmented our analysis by including seven “experience” reports that described application of patterns using less rigorous observational forms. We report and review the profiles of the empirical evidence for those patterns for which multiple studies exist. Conclusions. We could not identify firm support for any of the claims made for patterns in general, although there was some support for the usefulness of patterns in providing a framework for maintenance, and some qualitative indication that they do not help novices learn about design. For future studies we recommend that researchers use case studies that focus upon some key patterns, and seek to identify the impact that their use can have upon maintenance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975176","Design patterns;systematic literature review;empirical software engineering","Software engineering;Software design;Systematics;Search engines;Terminology;Maintenance engineering","object-oriented programming;software maintenance","software design patterns;software engineering;software development;design structures;knowledge transfer;Gang-of-Four;object-oriented design patterns","","33","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Detecting Bugs by Discovering Expectations and Their Violations","P. Bian; B. Liang; Y. Zhang; C. Yang; W. Shi; Y. Cai","School of Information, Renmin University of China, 12471 Beijing, Beijing China (e-mail: bianpan@ruc.edu.cn); School of Information, Renmin University of China, 12471 Beijing, BEIJING China (e-mail: liangb@ruc.edu.cn); School of Information, Renmin University of China, 12471 Beijing, Beijing China (e-mail: annazhang@ruc.edu.cn); School of Information, Renmin University of China, 12471 Beijing, Beijing China (e-mail: cqyang@ruc.edu.cn); School of Information, Remin University of China, Beijing, Beijing China (e-mail: wenchang@ruc.edu.cn); Chinese Academy of Sciences, State Key Laboratory of Computer Science, Institute of Software, Beijing, Beijing China 100190 (e-mail: yancai@ios.ac.cn)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Code mining has been proven to be a promising approach to inferring implicit programming rules for finding software bugs. However, existing methods may report large numbers of false positives and false negatives. In this paper, we propose a novel approach called EAntMiner to improve the effectiveness of code mining. EAntMiner elaborately reduces noises from statements irrelevant to interesting rules and different implementation forms of the same logic. During preprocessing, we employ program slicing to decompose the original source repository into independent sub-repositories. In each sub-repository, statements irrelevant to critical operations (automatically extracted from source code) are excluded and various semantics-equivalent implementations are normalized into a canonical form as far as possible. Moreover, to tackle the challenge that some bugs are difficult to be detected by mining frequent patterns as rules, we further developed a kNN-based method to identify them. We have implemented EAntMiner and evaluated it on four large-scale C systems. EAntMiner successfully detected 105 previously unknown bugs that have been confirmed by corresponding development communities. A set of comparative evaluations also demonstrate that EAntMiner can effectively improve the precision of code mining.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2816639","National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8318656","Bug detection;code mining;program slicing;instance-based learning","Computer bugs;Data mining;Linux;Programming;Kernel;Semantics","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A VDM case study in mural","B. Fields; M. Elvang-Goransson","ICI, Manchester, UK; NA","IEEE Transactions on Software Engineering","","1992","18","4","279","295","The application of an interactive theorem-proving assistant and specification support tool called mural in the specification and verification of a small Vienna development method (VDM) development is described. It is the authors' intention to give a feel for how mural works and of mural's applicability as a tool in specifying and verifying software.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.129217","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=129217","","Computer aided software engineering;Concrete;Safety;Inductors;Application software;Software tools;Natural languages;Formal specifications;Process design;Refining","formal specification;interactive systems;program verification;software tools;theorem proving","VDM;interactive theorem-proving assistant;specification support tool;mural;specification;verification;Vienna development method","","5","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Performance comparison of three modern DBMS architectures","A. Delis; N. Roussopoulos","Inst. for Adv. Comput. Studies, Maryland Univ., College Park, MD, USA; Inst. for Adv. Comput. Studies, Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1993","19","2","120","138","The introduction of powerful workstations connected through local area networks (LANs) inspired new database management system (DBMS) architectures that offer high performance characteristics. The authors examine three such software architecture configurations: client-server (CS), the RAD-UNIFY type of DBMS (RU), and enhanced client-server (ECS). Their specific functional components and design rationales are discussed. Three simulation models are used to provide a performance comparison under different job workloads. Simulation results show that the RU almost always performs slightly better than the CS, especially under light workloads, and that ECS offers significant performance improvement over both CS and RU. Under reasonable update rates, the ECS over CS (or RU) performance ratio is almost proportional to the number of participating clients (for less than 32 clients). The authors also examine the impact of certain key parameters on the performance of the three architectures and show that ECS is more scalable that the other two.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.214830","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=214830","","Computer architecture;Workstations;Local area networks;Performance analysis;Throughput;Military computing;Computational modeling;Packaging;Software architecture;Indexes","database management systems;performance evaluation;software engineering","simulation results;DBMS architectures;workstations;local area networks;software architecture configurations;client-server;RAD-UNIFY type;functional components;design rationales;simulation models","","18","","26","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Automated Parameter Optimization on Defect Prediction Models","C. Tantithamthavorn; S. McIntosh; A. E. Hassan; K. Matsumoto","School of Computer Science, The University of Adelaide, Adelaide, South Australia Australia (e-mail: chakkrit.tantithamthavorn@adelaide.edu.au); Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec Canada H3A 0E9 (e-mail: shane.mcintosh@mcgill.ca); School of Computing, Queen&#x0027;s University, Kingston, Ontario Canada (e-mail: ahmed@cs.queensu.ca); Information Systems, Nara Institute of Science and Technology, Information Systems, Ikomashi, Naraken Japan (e-mail: matumoto@is.naist.jp)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Defect prediction models---classifiers that identify defect-prone software modules---have configurable parameters that control their characteristics (e.g., the number of trees in a random forest). Recent studies show that these classifiers underperform when default settings are used. In this paper, we study the impact of automated parameter optimization on defect prediction models. Through a case study of 18 datasets, we find that automated parameter optimization: (1) improves AUC performance by up to 40 percentage points; (2) yields classifiers that are at least as stable as those trained using default settings; (3) substantially shifts the importance ranking of variables, with as few as 28% of the top-ranked variables in optimized classifiers also being top-ranked in non-optimized classifiers; (4) yields optimized settings for 17 of the 20 most sensitive parameters that transfer among datasets without a statistically significant drop in performance; and (5) adds less than 30 minutes of additional computation to 12 of the 26 studied classification techniques. While widely-used classification techniques like random forest and support vector machines are not optimization-sensitive, traditionally overlooked techniques like C5.0 and neural networks can actually outperform widely-used techniques after optimization is applied. This highlights the importance of exploring the parameter space when using parameter-sensitive classification techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2794977","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8263202","Software defect prediction;search-based software engineering;experimental design;classification techniques;parameter optimization;grid search;random search;genetic algorithm;differential evolution","Optimization;Predictive models;Computational modeling;Software;Neural networks;Computational efficiency;Power system stability","","","","3","","","","","","","","IEEE","IEEE Early Access Articles"
"A taxonomy of scheduling in general-purpose distributed computing systems","T. L. Casavant; J. G. Kuhl","Dept. of Electr. & Comput. Eng., Iowa Univ., Iowa City, IA, USA; Dept. of Electr. & Comput. Eng., Iowa Univ., Iowa City, IA, USA","IEEE Transactions on Software Engineering","","1988","14","2","141","154","One measure of the usefulness of a general-purpose distributed computing system is the system's ability to provide a level of performance commensurate to the degree of multiplicity of resources present in the system. A taxonomy of approaches to the resource management problem is presented in an attempt to provide a common terminology and classification mechanism necessary in addressing this problem. The taxonomy, while presented and discussed in terms of distributed scheduling, is also applicable to most types of resource management.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4634","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4634","","Taxonomy;Processor scheduling;Distributed computing;Resource management;Energy management;Power system management;Cities and towns;Control theory;Operations research;Terminology","distributed processing;operating systems (computers);scheduling","operating systems;scheduling;distributed computing;resource management;distributed scheduling","","447","","118","","","","","","IEEE","IEEE Journals & Magazines"
"A Stub Generator for Multilanguage RPC in Heterogeneous Environments","P. B. Gibbons","Department of Electrical Engineering and Computer Science, Computer Science Division, University of California","IEEE Transactions on Software Engineering","","1987","SE-13","1","77","87","A stub generator for marshalling the arguments and results of remote procedure calls in heterogeneous environments is presented. The stub generator is itself language and machine independent, and derives all its knowledge of source languages and machine types from a set of language and machine specifications. These specifications can be paired in any combination to accommodate interlanguage calls between differing machines.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232837","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702135","Argument marshalling;distributed systems;heterogeneous environments;interface specification language;interlanguage procedure call;remote procedure call;stub generator","Specification languages;Programming profession;Data structures;Delay systems;Computer languages;Milling machines;Computer science;Hardware","","Argument marshalling;distributed systems;heterogeneous environments;interface specification language;interlanguage procedure call;remote procedure call;stub generator","","30","","23","","","","","","IEEE","IEEE Journals & Magazines"
"A Methodology for Developing Distributed Programs","S. Ramesh; S. L. Mehndiratta","Department of Computer Science and Engineering, Indian Institute of Technology; NA","IEEE Transactions on Software Engineering","","1987","SE-13","8","967","976","A methodology, different from the existing ones, for constructing distributed programs is presented. It is based on the well-known idea of developing distributed programs via synchronous and centralized programs. The distinguishing features of the methodology are: 1) specification include process structure information and distributed programs are developed taking this information into account, 2) a new class of programs, called PPSA's, is used in the development process, and 3) a transformational approach is suggested to solve the problems inherent in the method of developing distributed programs through synchronous and centralized programs. The methodology is illustrated with an example.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233514","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702314","Communicating processes;decentralization;distributed programming;programming methodology;program transformation","Centralized control;Control systems;Computer science;Process design;Network topology","","Communicating processes;decentralization;distributed programming;programming methodology;program transformation","","5","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Real-time software life cycle with the model system","J. S. Tseng; B. Szymanski; Y. Shi; N. S. Prywes","Department of Computer and Information Science, University of Pennsylvania, Philadephia, PA 19104; Naval Air Development Center, Warminster, PA 18974; Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY 12180; Department of Computer Science, Temple University, Philadelphia, PA 19122; Department of Computer and Information Science, University of Pennsylvania, Phildelphia, PA 19104","IEEE Transactions on Software Engineering","","1986","SE-12","2","358","373","The use of an assertive specification language for real-time software development and maintenance is considered. The language is used for asserting the acts or relations inherent in the problem to be solved; this is in contrast to conventional programming languages, which are used to express the computer solution. Expressing a problem in Model consists of declaring array variables and defining their relationships through equations. This is different from conventional programming, which relates the problem in terms of computer operations. The language is supported by an automatic system which interacts with the user in soliciting missing definitions or correcting inconsistencies, and which translates the specification into a near-optimal computer solution. The main advantages of this approach are indicated. The use of Model in real-time software development and maintenance is reviewed. Differences from conventional programming are stressed through an example, which also illustrates the use of the three automatic components of the Model system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312949","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312949","Assertive;compiler;configurator;nonprocedural;real time;software-development;timing","Real-time systems;Computers;Computational modeling;Mathematical model;Software;Delay","software engineering;specification languages","software life cycle;Model system;specification language;real-time software development;maintenance;programming languages;array variables;programming;missing definitions;inconsistencies","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Semantically extended dataflow diagrams: a formal specification tool","R. B. France","Inst. for Adv. Comput., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1992","18","4","329","346","A method for associating a dataflow diagram (DFD) with a formal specification is described. The intention is to enhance the use of the DFD as a formal specification tool, thus gaining a tool that can be used to document application functionality in an understandable manner and, at the same time, be capable of producing a formal specification that can be used to rigorously investigate the semantic properties of the application. It is shown how the formal specifications characterizing semantic models of DFDs can be used to investigate desired application properties of verify semantic decompositions of data transforms.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.129221","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=129221","","Formal specifications;Design for disassembly;Documentation;Application software;Programming;Communication system control;Concurrent computing;Formal languages;Testing;Economic forecasting","diagrams;formal specification;systems analysis","dataflow diagram;formal specification tool;application functionality;semantic properties;semantic decompositions;data transforms","","33","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Computational issues in secure interoperation","Li Gong; Xiaolei Qian","Comput. Sci. Lab., SRI Int., Menlo Park, CA, USA; Comput. Sci. Lab., SRI Int., Menlo Park, CA, USA","IEEE Transactions on Software Engineering","","1996","22","1","43","52","Advances in distributed systems and networking technology have made interoperation not only feasible but also increasingly popular. We define the interoperation of secure systems and its security, and prove complexity and composability results on obtaining optimal and secure interoperation. Most problems are NP-complete even for systems with very simple access control structures, while for a general setting the problem is undecidable. Nevertheless, composability reduces complexity in that secure global interoperation can be obtained incrementally by composing secure local interoperation. We illustrate, through an application in secure database interoperation, how these theoretical results can help system designers in practice.","0098-5589;1939-3520;2326-3881","","10.1109/32.481533","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=481533","","Access control;Data security;Information security;Information systems;National security;Intelligent networks;Distributed databases;Algorithm design and analysis;Database systems;Computational complexity","open systems;computational complexity;authorisation;security of data;distributed databases;decidability;software engineering;systems analysis","secure interoperation;distributed systems;networking technology;secure systems;security;complexity;composability;optimal interoperation;NP-complete problems;access control structures;undecidable problem;secure global interoperation;secure local interoperation;secure database interoperation;system designers","","76","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Static Analysis of Model Transformations","J. S. Cuadrado; E. Guerra; J. de Lara","Department of Languages and Systems, Universidad de Murcia, Murcia, Spain; Department of Computer Science, Universidad Autónoma de Madrid, Madrid, Spain; Department of Computer Science, Universidad Autónoma de Madrid, Madrid, Spain","IEEE Transactions on Software Engineering","","2017","43","9","868","897","Model transformations are central to Model-Driven Engineering (MDE), where they are used to transform models between different languages; to refactor and simulate models; or to generate code from models. Thus, given their prominent role in MDE, practical methods helping in detecting errors in transformations and automate their verification are needed. In this paper, we present a method for the static analysis of ATL model transformations. The method aims at discovering typing and rule errors, like unresolved bindings, uninitialized features or rule conflicts. It relies on static analysis and type inference, and uses constraint solving to assert whether a source model triggering the execution of a given problematic statement can possibly exist. Our method is supported by a tool that integrates seamlessly with the ATL development environment. To evaluate the usefulness of our method, we have used it to analyse a public repository of ATL transformations. The high number of errors discovered shows that static analysis of ATL transformations is needed in practice. Moreover, we have measured the precision and recall of the method by considering a synthetic set of transformations obtained by mutation techniques, and comparing with random testing. The experiment shows good overall results in terms of false positives and negatives.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2635137","Spanish MINECO; R&D programme of the Madrid Region; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7765073","Model-driven engineering;model transformation;ATL;static analysis;model finders;verification and testing","Unified modeling language;Analytical models;Testing;Model driven engineering;Transforms;Manuals;Computational modeling","constraint handling;error detection;formal verification;inference mechanisms;program diagnostics;random functions","static analysis;model-driven engineering;MDE;error detection;verification automation;ATL model transformations;typing errors;rule errors;unresolved bindings;uninitialized features;rule conflicts;type inference;constraint solving;public repository analysis;mutation techniques;random testing","","2","","70","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"What Industry Needs from Architectural Languages: A Survey","I. Malavolta; P. Lago; H. Muccini; P. Pelliccione; A. Tang","Università dell'Aquila, Italy; VU University Amsterdam, Amsterdam; Università dell'Aquila, Italy; Università dell'Aquila, Italy; Swinburne University of Technology, Melbourne","IEEE Transactions on Software Engineering","","2013","39","6","869","891","Many times we are faced with the proliferation of definitions, concepts, languages, and tools in certain (research) topics. But often there is a gap between what is provided by existing technologies and what is needed by their users. The strengths, limitations, and needs of the available technologies can be dubious. The same applies to software architectures, and specifically to languages designed to represent architectural models. Tens of different architectural languages have been introduced by the research and industrial communities in the last two decades. However, it is unclear if they fulfill the user's perceived needs in architectural description. As a way to plan for next generation languages for architectural description, this study analyzes practitioners' perceived strengths, limitations, and needs associated with existing languages for software architecture modeling in industry. We run a survey by interviewing 48 practitioners from 40 different IT companies in 15 countries. Each participant is asked to fill in a questionnaire of 51 questions. By analyzing the data collected through this study, we have concluded that 1) while practitioners are generally satisfied with the design capabilities provided by the languages they use, they are dissatisfied with the architectural language analysis features and their abilities to define extra-functional properties; 2) architectural languages used in practice mostly originate from industrial development instead of from academic research; 3) more formality and better usability are required of an architectural language.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.74","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6374194","Software architecture;architecture description languages;ADL;survey;empirical study","Unified modeling language;Software architecture;Computer architecture;Industries;Communities;Software;Google","data analysis;software architecture;specification languages","architectural languages;software architectures;architectural models;next generation languages;architectural description;data collection analysis;industrial development","","95","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Pair Programming with Respect to System Complexity and Programmer Expertise","E. Arisholm; H. Gallis; T. Dyba; D. I. K. Sjoberg","Simula Research Laboratory, PO Box 134, NO-1325 Lysaker, Norway; Simula Research Laboratory, PO Box 134, NO-1325 Lysaker, Norway; Simula Research Laboratory and SINTEF Information and Communication Technology, NO-7465 Trondheim, Norway; Simula Research Laboratory, PO Box 134, NO-1325 Lysaker, Norway","IEEE Transactions on Software Engineering","","2007","33","2","65","86","A total of 295 junior, intermediate, and senior professional Java consultants (99 individuals and 98 pairs) from 29 international consultancy companies in Norway, Sweden, and the UK were hired for one day to participate in a controlled experiment on pair programming. The subjects used professional Java tools to perform several change tasks on two alternative Java systems with different degrees of complexity. The results of this experiment do not support the hypotheses that pair programming in general reduces the time required to solve the tasks correctly or increases the proportion of correct solutions. On the other hand, there is a significant 84 percent increase in effort to perform the tasks correctly. However, on the more complex system, the pair programmers had a 48 percent increase in the proportion of correct solutions but no significant differences in the time taken to solve the tasks correctly. For the simpler system, there was a 20 percent decrease in time taken but no significant differences in correctness. However, the moderating effect of system complexity depends on the programmer expertise of the subjects. The observed benefits of pair programming in terms of correctness on the complex system apply mainly to juniors, whereas the reductions in duration to perform the tasks correctly on the simple system apply mainly to intermediates and seniors. It is possible that the benefits of pair programming will exceed the results obtained in this experiment for larger, more complex tasks and if the pair programmers have a chance to work together over a longer period of time","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052584","Empirical software engineering;pair programming;extreme programming;design principles;control styles;object-oriented programming;software maintainability;quasi-experiment.","Programming profession;Time measurement;Java;Software engineering;Software maintenance;Keyboards;Cost function;Power measurement;Computer industry","Java;object-oriented programming;task analysis;team working","pair programming;system complexity;programmer expertise;Java","","115","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Performance analysis of Time Warp with multiple homogeneous processors","A. Gupta; I. F. Akyildiz; R. M. Fujimoto","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Transactions on Software Engineering","","1991","17","10","1013","1027","The behavior of n interacting processors synchronized by the Time Warp protocol is analyzed using a discrete-state, continuous-time Markov chain model. The performance and dynamics of the processes (or processors) are analyzed under the following assumptions: exponential task times and timestamp increments on messages, each event message generates one new message that is sent to a randomly selected process, negligible rollback, state saving, and communication delay, unbounded message buffers, and homogeneous processors. Several performance measures are determined, such as: the fraction of processed events that commit, speedup, rollback probability, expected length of rollback, the probability mass function for the number of uncommitted processed events, the probability distribution function for the virtual time of a process, and the fraction of time the processors remain idle. The analysis is approximate, thus the results have been validated through performance measurements of a Time Warp testbed executing on a shared-memory multiprocessor.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.99190","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=99190","","Performance analysis;Discrete event simulation;Time measurement;Time warp simulation;Synchronization;Senior members;Protocols;Delay;Length measurement;Velocity measurement","discrete event simulation;Markov processes;multiprocessing systems;performance evaluation;protocols","parallel simulation;interacting processors;Time Warp protocol;discrete-state;continuous-time Markov chain model;exponential task times;timestamp increments;event message;negligible rollback;state saving;communication delay;unbounded message buffers;homogeneous processors;performance measures;processed events;speedup;rollback probability;probability mass function;uncommitted processed events;probability distribution function;virtual time;Time Warp testbed;shared-memory multiprocessor","","20","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Entropy Based Software Reliability Analysis of Multi-Version Open Source Software","V. B. Singh; M. Sharma; H. Pham","University of Delhi, Delhi, India; University of Delhi, Delhi, India; Department of Industrial and Systems Engineering, Rutgers, State University of New Jersey, Piscataway, NJ","IEEE Transactions on Software Engineering","","2018","44","12","1207","1223","The number of issues fixed in the current release of the software is one of the factors which decides the next release of the software. The source code files get changed during fixing of these issues. The uncertainty arises due to these changes is quantified using entropy based measures. We developed a Non-Homogeneous Poisson Process model for Open Source Software to understand the fixing of issues across releases. Based on this model, optimal release-updating using entropy and maximizing the active user's satisfaction level subject to fixing of issues up to a desired level, is investigated as well. The proposed models have been validated on five products of the Apache open source project. The optimal release time estimated from the proposed model is close to the observed release time at different active user's satisfaction levels. The proposed decision model can assist management to appropriately determine the optimal release-update time. The proposed entropy based model for issues estimation shows improvement in performance for 21 releases out of total 23 releases, when compared with well-known traditional software reliability growth models, namely GO model [1] and S-shaped model [2] . The proposed model is also found statistically significant.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2766070","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8081836","Entropy;feature improvement;new feature;release time problem;software repositories;cobb-douglas","Entropy;Software reliability;Software product lines;Computer bugs;Open source software","entropy;program diagnostics;public domain software;software maintenance;software reliability;source code (software);stochastic processes","multiversion Open Source Software;source code files;NonHomogeneous Poisson Process model;Apache open source project;GO model;S-shaped model;software reliability analysis;entropy","","","","55","","","","","","IEEE","IEEE Journals & Magazines"
"On the Understandability of Temporal Properties Formalized in Linear Temporal Logic, Property Specification Patterns and Event Processing Language","C. Czepa; U. Zdun","Faculty of Computer Science, Research Group Software Architecture, University of Vienna, Vienna, Vienna Austria (e-mail: christoph.czepa@univie.ac.at); Faculty of Computer Science, Research Group Software Architecture, University of Vienna, Vienna, Vienna Austria (e-mail: uwe.zdun@univie.ac.at)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Temporal properties are important in a wide variety of domains for different purposes. For example, they can be used to avoid architectural drift in software engineering or to support the regulatory compliance of business processes. In this work, we study the understandability of three major temporal property representations: (1) Linear Temporal Logic (LTL) is a formal and well-established logic that offers temporal operators to describe temporal properties; (2) Property Specification Patterns (PSP) are a collection of recurring temporal properties that abstract underlying formal and technical representations; (3) Event Processing Language (EPL) can be used for runtime monitoring of event streams using Complex Event Processing. We conducted two controlled experiments with 216 participants in total to study the understandability of those approaches using a completely randomized design with one alternative per experimental unit. We hypothesized that PSP, as a highly abstracting pattern language, is easier to understand than LTL and EPL, and that EPL, due to separation of concerns (as one or more queries can be used to explicitly define the truth value change that an observed event pattern causes), is easier to understand than LTL. We found evidence supporting our hypotheses which was statistically significant and reproducible.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2859926","Osterreichische Forschungsforderungsgesellschaft; Austrian Science Fund; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8419310","Controlled Experiment;Understandability;Temporal Property;Linear Temporal Logic;Property Specification Patterns;Complex Event Processing;Event Processing Language","Software;Computer science;Guidelines;Industries;Software architecture;Cognition","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Synthesis of mutual exclusion solutions based on binary semaphores","R. T. Jacob; I. P. Page","Dept. of Comput. Sci., North Texas State Univ., Denton, TX, USA; NA","IEEE Transactions on Software Engineering","","1989","15","5","560","568","A graphical form of the mutual exclusion problem is considered in which each vertex represents a process and each edge represents a mutual exclusion constraint between the critical sections of the processes associated with its endpoints. An edge semaphore solution for mutual exclusion problems is defined, and those graphs which are edge solvable are characterized in terms of both a forbidden subgraph and a graph grammar. Finally, an efficient algorithm is given which generates the entry and exit sections for all processes in an edge-solvable problem.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24705","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24705","","Jacobian matrices;Software engineering;System recovery;Time sharing computer systems;Computer science;Kernel;Programming environments","graph theory;operating systems (computers)","mutual exclusion solutions;binary semaphores;graphical form;mutual exclusion problem;vertex;mutual exclusion constraint;edge semaphore solution;edge solvable;forbidden subgraph;graph grammar;efficient algorithm;entry;exit sections","","1","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Reasoning About Probabilistic Behavior in Concurrent Systems","S. Purushothaman; P. A. Subrahmanyam","Department of Computer Science, Pennsylvania State University; NA","IEEE Transactions on Software Engineering","","1987","SE-13","6","740","745","Certain aspects of the behavior of concurrent systems are intrinsically probabilistic in nature, e.g., the behavior of imperfect communication media used in network protocols. We address the problem of expressing such behavior in an algebraic calculus for communicating systems. The introduction of probabilistic information in the calculus alleviates the problem of proving liveness, as proving liveness now amounts to proving that its probability is 1. A methodology for proving both safety and liveness is developed and used in proving the correctness of the Alternating Bit Protocol.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233478","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702278","Calculus for communicating systems;correctness;liveness;probability;protocol","Protocols;Calculus;Explosions;Automata;Intelligent networks;Probability;Safety;Contracts;Context modeling;Computational modeling","","Calculus for communicating systems;correctness;liveness;probability;protocol","","2","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Techniques for Classifying Executions of Deployed Software to Support Software Engineering Tasks","M. Haran; A. Karr; M. Last; A. Orso; A. A. Porter; A. Sanil; S. Fouche","Department of Statistics at Pennsylvania State University, University Park, PA 16802; National Institute of Statistical Sciences, Research Triangle Park, NC 27709-4006; National Institute of Statistical Sciences, Research Triangle Park, NC 27709-4006; College of Computing at the Georgia Institute of Technology, Atlanta, GA 30332-0765; Computer Science Department, University of Maryland, College Park, MD 20814; National Institute of Statistical Sciences, Research Triangle Park, NC 27709-4006; Computer Science Department, University of Maryland, College Park, MD 20814","IEEE Transactions on Software Engineering","","2007","33","5","287","304","There is an increasing interest in techniques that support analysis and measurement of fielded software systems. These techniques typically deploy numerous instrumented instances of a software system, collect execution data when the instances run in the field, and analyze the remotely collected data to better understand the system's in-the-field behavior. One common need for these techniques is the ability to distinguish execution outcomes (e.g., to collect only data corresponding to some behavior or to determine how often and under which condition a specific behavior occurs). Most current approaches, however, do not perform any kind of classification of remote executions and either focus on easily observable behaviors (e.g., crashes) or assume that outcomes' classifications are externally provided (e.g., by the users). To address the limitations of existing approaches, we have developed three techniques for automatically classifying execution data as belonging to one of several classes. In this paper, we introduce our techniques and apply them to the binary classification of passing and failing behaviors. Our three techniques impose different overheads on program instances and, thus, each is appropriate for different application scenarios. We performed several empirical studies to evaluate and refine our techniques and to investigate the trade-offs among them. Our results show that 1) the first technique can build very accurate models, but requires a complete set of execution data; 2) the second technique produces slightly less accurate models, but needs only a small fraction of the total execution data; and 3) the third technique allows for even further cost reductions by building the models incrementally, but requires some sequential ordering of the software instances' instrumentation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1004","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160968","Execution classification;remote analysis/measurement.","Software engineering;Instruments;Software systems;Software measurement;Computer crashes;Debugging;Performance evaluation;Costs;Data analysis;Failure analysis","software metrics;software performance evaluation","software engineering;binary classification;program instances;execution classification","","22","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Conceptual Modeling in the Context of Development","C. H. Kung","Department of Computer Science, University of Iowa. Iowa city, IA 52242.","IEEE Transactions on Software Engineering","","1989","15","10","1176","1187","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1989.559766","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=559766","","Context modeling;Prototypes;Software prototyping;Programming;Design optimization;System testing;Information systems;Software maintenance;Software quality;Costs","","Conceptual modeling;consistency checking;executable specification;formal analysis of model;requirements specification","","25","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient discrete-event simulation of colored Petri nets","R. Gaeta","Dipartimento di Inf., Torino Univ., Italy","IEEE Transactions on Software Engineering","","1996","22","9","629","639","Colored Petri nets are a powerful formalism for the description of complex, asynchronous distributed systems. They can express in a very concise way the behavior of very large systems, especially in case these systems are composed of many replications of a few basic components that individually behave in a similar way. The simulation of such models is, however, difficult to perform in a computationally efficient way. For the specific class of stochastic well-formed nets (SWNs), we present a set of optimizations that allow a very efficient implementation of the event-driven simulation technique. Three approaches are followed to improve simulation efficiency: first, an efficient algorithm for the computation of the occurrences of a transition in a given marking; second, reduction of the amount of work needed to schedule or preempt the occurrence of a transition as a consequence of a marking change, taking into account the restrictions on color functions for the SWN formalism; third, reduction of the average length of the event list in the case of symmetric models where the so-called symbolic simulation technique applies. The approach is validated by performance measurements on several large SWN models taken from the literature.","0098-5589;1939-3520;2326-3881","","10.1109/32.541434","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=541434","","Discrete event simulation;Petri nets;Computational modeling;Analytical models;Engines;Stochastic processes;Virtual prototyping;Power system modeling;Scheduling algorithm;Processor scheduling","Petri nets;graph colouring;discrete event simulation;optimisation;symmetry;stochastic systems;symbol manipulation;mathematics computing","discrete-event simulation;colored Petri nets;asynchronous distributed systems;very large systems;stochastic well-formed nets;optimizations;event-driven simulation technique;simulation efficiency;transition occurrences;marking change;work reduction;scheduling;event list length reduction;symmetric models;symbolic simulation technique;performance measurements;high-level Petri nets","","25","","18","","","","","","IEEE","IEEE Journals & Magazines"
"The development and evaluation of three diverse techniques for object-oriented code inspection","A. Dunsmore; M. Roper; M. Wood","Abbey Mill Bus. Centre, Formedix, Paisley, UK; NA; NA","IEEE Transactions on Software Engineering","","2003","29","8","677","686","We describe the development and evaluation of a rigorous approach aimed at the effective and efficient inspection of object-oriented (OO) code. Since the time that inspections were developed they have been shown to be powerful defect detection strategies. However, little research has been done to investigate their application to OO systems, which have very different structural and execution models compared to procedural systems. This suggests that inspection techniques may not be currently being deployed to their best effect in the context of large-scale OO systems. Work to date has revealed three significant issues that need to be addressed - the identification of chunks of code to be inspected, the order in which the code is read, and the resolution of frequent nonlocal references. Three techniques are developed with the aim of addressing these issues: one based on a checklist, one focused on constructing abstract specifications, and the last centered on the route that a use case takes through a system. The three approaches are evaluated empirically and, in this instance, it is suggested that the checklist is the most effective approach, but that the other techniques also have potential strengths. For the best results in a practical situation, a combination of techniques is recommended, one of which should focus specifically on the characteristics of OO.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223643","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223643","","Inspection;Computer Society;Application software;Object oriented modeling;Power system modeling;Large-scale systems;Industrial control;Testing;Encapsulation;Libraries","inspection;software reviews;object-oriented programming;formal specification;software performance evaluation;software quality;program testing","software inspection technique;software quality evaluation;object-oriented code inspection;OO system;defect detection strategy;code chunk identification;nonlocal reference resolution;checklist technique;code review;abstract specification;use case technique;empirical study","","28","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Context-Aware Adaptive Applications: Fault Patterns and Their Automated Identification","M. Sama; S. Elbaum; F. Raimondi; D. S. Rosenblum; Z. Wang","University College London, UK; University of Nebraska, Lincoln, NE; University College London, UK; University College, London, UK; University of Nebraska, Lincoln, NE","IEEE Transactions on Software Engineering","","2010","36","5","644","661","Applications running on mobile devices are intensely context-aware and adaptive. Streams of context values continuously drive these applications, making them very powerful but, at the same time, susceptible to undesired configurations. Such configurations are not easily exposed by existing validation techniques, thereby leading to new analysis and testing challenges. In this paper, we address some of these challenges by defining and applying a new model of adaptive behavior called an Adaptation Finite-State Machine (A-FSM) to enable the detection of faults caused by both erroneous adaptation logic and asynchronous updating of context information, with the latter leading to inconsistencies between the external physical context and its internal representation within an application. We identify a number of adaptation fault patterns, each describing a class of faulty behaviors. Finally, we describe three classes of algorithms to detect such faults automatically via analysis of the A-FSM. We evaluate our approach and the trade-offs between the classes of algorithms on a set of synthetically generated Context-Aware Adaptive Applications (CAAAs) and on a simple but realistic application in which a cell phone's configuration profile changes automatically as a result of changes to the user's location, speed, and surrounding environment. Our evaluation describes the faults our algorithms are able to detect and compares the algorithms in terms of their performance and storage requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5432224","Adaptation;context-awareness;fault detection;mobile computing;model-based analysis;model checking;ordered binary decision diagrams;symbolic verification;ubiquitous computing.","Fault diagnosis;Fault detection;Context modeling;Algorithm design and analysis;Handheld computers;Personal digital assistants;Data structures;Global Positioning System;Computer science;Lead","finite state machines;formal logic;mobile computing;program verification;software fault tolerance","context-aware adaptive applications;mobile devices;validation techniques;adaptation finite-state machine;A-FSM analysis;asynchronous information updating;fault pattern adaptation;cell phone;fault detection","","52","","33","","","","","","IEEE","IEEE Journals & Magazines"
"PELAS-program error-locating assistant system","B. Korel","Dept. of Comput. Sci., Wayne State Univ., Detroit, MI, USA","IEEE Transactions on Software Engineering","","1988","14","9","1253","1260","Error localization in program debugging is the process of identifying program statements which cause incorrect behavior. A prototype of the error localization assistant system which guides a programmer during debugging of Pascal programs is described. The system is interactive: it queries the programmer for the correctness of the program behavior and uses answers to focus the programmer's attention on an erroneous part of the program (in particular, it can localize a faulty statement). The system differs from previous approaches in that it makes use of the knowledge of program structure, which is derived automatically. The knowledge of program structure is represented by the dependence network which is used by the error-locating reasoning mechanism to guide the construction, evaluation, and modification of hypothesis of possible causes of the error. Backtracking reasoning has been implemented in the reasoning mechanism.<<ETX>></ETX>","0098-5589;1939-3520;2326-3881","","10.1109/32.6169","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6169","","Programming profession;Debugging;Prototypes;Computer science;Software tools;Automatic programming;Software prototyping","interactive systems;knowledge based systems;knowledge representation;Pascal;program debugging;program verification;software tools","knowledge based system;knowledge representation;PELAS;program error-locating assistant system;program debugging;error localization assistant;Pascal programs;program structure;dependence network;reasoning","","53","","30","","","","","","IEEE","IEEE Journals & Magazines"
"The Interrogator: Protocol Secuity Analysis","J. K. Millen; S. C. Clark; S. B. Freedman","MITRE Corporation; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","2","274","288","The Interrogator is a Prolog program that searches for security vulnerabilities in network protocols for automatic cryptographic key distribution. Given a formal specification of the protocol, it looks for message modification attacks that defeat the protocol objective. It is still under developement, but is has been able to rediscover a known vulnerability in a published protocol. It is implemented in LM-Prolog on a Lisp Machine, with a graphical user interface.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233151","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702206","Active wiretapping;computer security;key distribution;network security;Prolog;protocol verification","Cryptographic protocols;Cryptography;Computer network management;Formal specifications;Computer security;Graphical user interfaces;User interfaces;Graphics;Mice;Computer displays","","Active wiretapping;computer security;key distribution;network security;Prolog;protocol verification","","35","","10","","","","","","IEEE","IEEE Journals & Magazines"
"A debugger for Ada tasking","A. F. Brindle; R. N. Taylor; D. F. Martin","Aerosp. Corp., El Segundo, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","3","293","304","The capabilities needed in an Ada debugger are discussed in light of the language's tasking constructs, and the design for a debugger is presented which operates in concert with a single-processor Ada interpreter. This debugger design demonstrates the extensions to sequential debugging techniques that are necessary to handle concurrency, and shows that significant debugging functionality can be provided even without the inclusion of automatic error diagnosis methods. The issues considered include isolation of effects and display of the full dynamic execution status, both of which are essential to diagnosis of concurrent programs.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21757","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21757","","Debugging;Concurrent computing;Aerodynamics;Testing;Information analysis;Program processors;Displays;Packaging;System recovery;Turning","Ada;parallel programming;program debugging;program interpreters","parallel programming;Ada tasking;Ada debugger;tasking constructs;single-processor Ada interpreter;sequential debugging;concurrency;dynamic execution status","","6","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Choosing Component Origins for Software Intensive Systems: In-House, COTS, OSS or Outsourcing?—A Case Survey","K. Petersen; D. Badampudi; S. M. A. Shah; K. Wnuk; T. Gorschek; E. Papatheocharous; J. Axelsson; S. Sentilles; I. Crnkovic; A. Cicchetti","Department of Software Engineering, Blekinge Institute of Technology, Campus Gräsvik, Karlskrona, Sweden; Blekinge Institute of Technology, Campus Gräsvik, Karlskrona, Sweden; SICS Swedish ICT AB, Kista, Sweden; Blekinge Institute of Technology, Campus Gräsvik, Karlskrona, Sweden; Blekinge Institute of Technology, Campus Gräsvik, Karlskrona, Sweden; SICS Swedish ICT AB, Kista, Sweden; SICS Swedish ICT AB, Kista, Sweden; Mälardalen University, Västerås, Sweden; Chalmers, Gothenberg, Sweden; Mälardalen University, Västerås, Sweden","IEEE Transactions on Software Engineering","","2018","44","3","237","261","The choice of which software component to use influences the success of a software system. Only a few empirical studies investigate how the choice of components is conducted in industrial practice. This is important to understand to tailor research solutions to the needs of the industry. Existing studies focus on the choice for off-the-shelf (OTS) components. It is, however, also important to understand the implications of the choice of alternative component sourcing options (CSOs), such as outsourcing versus the use of OTS. Previous research has shown that the choice has major implications on the development process as well as on the ability to evolve the system. The objective of this study is to explore how decision making took place in industry to choose among CSOs. Overall, 22 industrial cases have been studied through a case survey. The results show that the solutions specifically for CSO decisions are deterministic and based on optimization approaches. The non-deterministic solutions proposed for architectural group decision making appear to suit the CSO decision making in industry better. Interestingly, the final decision was perceived negatively in nine cases and positively in seven cases, while in the remaining cases it was perceived as neither positive nor negative.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2677909","ORION project; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7870688","Decision making;in-house;COTS;OSS;outsourcing","Decision making;Outsourcing;Companies;Computer architecture;Software;Industries","decision making;object-oriented programming;outsourcing;software architecture","CSO decision making;software intensive systems;COTS;OSS;Outsourcing;software component;software system;tailor research solutions;off-the-shelf components;OTS;alternative component sourcing options;CSOs;CSO decisions;nondeterministic solutions;architectural group decision;component origins;architectural group decision making","","3","","67","","","","","","IEEE","IEEE Journals & Magazines"
"A methodology for testing intrusion detection systems","N. J. Puketza; K. Zhang; M. Chung; B. Mukherjee; R. A. Olsson","Dept. of Comput. Sci., California Univ., Davis, CA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1996","22","10","719","729","Intrusion detection systems (IDSs) attempt to identify unauthorized use, misuse, and abuse of computer systems. In response to the growth in the use and development of IDSs, the authors have developed a methodology for testing IDSs. The methodology consists of techniques from the field of software testing which they have adapted for the specific purpose of testing IDSs. They identify a set of general IDS performance objectives which is the basis for the methodology. They present the details of the methodology, including strategies for test-case selection and specific testing procedures. They include quantitative results from testing experiments on the Network Security Monitor (NSM), an IDS developed at UC Davis. They present an overview of the software platform that has been used to create user-simulation scripts for testing experiments. The platform consists of the UNIX tool expect and enhancements that they have developed, including mechanisms for concurrent scripts and a record-and-replay feature. They also provide background information on intrusions and IDSs to motivate their work.","0098-5589;1939-3520;2326-3881","","10.1109/32.544350","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=544350","","System testing;Intrusion detection;Software testing;Computer security;Computerized monitoring;Computer networks;National security;Expert systems;Computational modeling;Computer simulation","testing;computer crime;security of data","intrusion detection system testing;unauthorized computer system use;computer system misuse;computer system abuse;software testing;test-case selection;testing procedures;Network Security Monitor;user-simulation scripts;UNIX tool expect;concurrent scripts;record-and-replay feature","","64","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Provable Protection against Web Application Vulnerabilities Related to Session Data Dependencies","L. Desmet; P. Verbaeten; W. Joosen; F. Piessens","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","50","64","Web applications are widely adopted and their correct functioning is mission critical for many businesses. At the same time, Web applications tend to be error prone and implementation vulnerabilities are readily and commonly exploited by attackers. The design of countermeasures that detect or prevent such vulnerabilities or protect against their exploitation is an important research challenge for the fields of software engineering and security engineering. In this paper, we focus on one specific type of implementation vulnerability, namely, broken dependencies on session data. This vulnerability can lead to a variety of erroneous behavior at runtime and can easily be triggered by a malicious user by applying attack techniques such as forceful browsing. This paper shows how to guarantee the absence of runtime errors due to broken dependencies on session data in Web applications. The proposed solution combines development-time program annotation, static verification, and runtime checking to provably protect against broken data dependencies. We have developed a prototype implementation of our approach, building on the JML annotation language and the existing static verification tool ESC/Java2, and we successfully applied our approach to a representative J2EE-based e-commerce application. We show that the annotation overhead is very small, that the performance of the fully automatic static verification is acceptable, and that the performance overhead of the runtime checking is limited.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70742","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359468","Software/Program Verification;Security;Security and Protection;Reliability;Data sharing;Web-based services;Web technologies;Software/Program Verification;Security;Security and Protection;Reliability;Data sharing;Web-based services;Web technologies","Protection;Runtime;Application software;Data security;Computer bugs;Computer errors;Software engineering;Databases;Computer crime;Information retrieval","programming languages;Web services","provable protection;Web application vulnerabilities;session data dependencies;software engineering;security engineering;JML annotation language;J2EE-based e-commerce application","","2","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Parametric graph drawing","P. Bertolazzi; G. Di Battista; G. Liotta","Istituto di Analisi dei Sistemi ed Inf., CNR, Rome, Italy; NA; NA","IEEE Transactions on Software Engineering","","1995","21","8","662","673","A diagram is a drawing on the plane that represents a graph like structure, where nodes are represented by symbols and edges are represented by curves connecting pairs of symbols. An automatic layout facility is a tool that receives as input a graph like structure and is able to produce a diagram that nicely represents such a structure. Many systems use diagrams in the interaction with the users; thus, automatic layout facilities and algorithms for graphs layout have been extensively studied in the last years. We present a new approach in designing an automatic layout facility. Our approach is based on a modular management of a large collection of algorithms and on a strategy that, given the requirements of an application, selects a suitable algorithm for such requirements. The proposed approach has been used for designing the automatic layout facility of Diagram Server, a network server that offers to its clients several facilities for managing diagrams.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.403790","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=403790","","Tree graphs;Computer aided software engineering;Layout;Joining processes;Algorithm design and analysis;Information systems;Information analysis;Flow production systems;Flow graphs;Software engineering","diagrams;computer graphics;graph theory;network servers;client-server systems","parametric graph drawing;graph like structure;automatic layout facility;graph layout;modular management;Diagram Server;network server;diagram","","8","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Single-site and distributed optimistic protocols for concurrency control","M. A. Bassiouni","Dept. of Comput. Sci., Central Florida Univ., Orlando, FL, USA","IEEE Transactions on Software Engineering","","1988","14","8","1071","1080","The authors consider that, in spite of their advantage in removing the overhead of lock maintenance and deadlock handling, optimistic concurrency control methods have been applied less in practice than locking schemes. Two complementary approaches are introduced that may help render the optimistic approach practically viable. For the high-level approach, integration schemes can be utilized so that the database management system is provided with a variety of synchronization methods each of which can be applied to the appropriate class of transactions. The low-level approach seeks to increase the concurrency of the original optimistic method and improve its performance. The author examines the low-level approach in depth, and presents algorithms that aim at reducing back-ups and improve throughput. Both the single-site and distributed networks are considered. Optimistic schemes using time-stamps for fully duplicated and partially duplicated database networks are presented, with emphasis on performance enhancement and on reducing the overall cost of implementation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.7617","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7617","","Protocols;Concurrency control;Optimization methods;Database systems;System recovery;Concurrent computing;Control systems;Transaction databases;Synchronization;Throughput","computer networks;distributed databases;protocols;system recovery","distributed protocols;single site protocols;single site networks;distributed databases;optimistic protocols;concurrency control;lock maintenance;deadlock handling;database management system;synchronization methods;distributed networks;time-stamps;performance enhancement","","2","","25","","","","","","IEEE","IEEE Journals & Magazines"
"An automatic physical designer for network model databases","P. Rullo; D. Sacca","CRAI, Rende-Santo Stefano, Italy; NA","IEEE Transactions on Software Engineering","","1988","14","9","1293","1306","Systems EROS is a physical design tool for CODASYL database systems which covers a large spectrum of decision variables, notably location mode, set implementation, set order, and search keys. System EROS is based on a model where the CODASYL physical database design problem is formulated as an extension of the index selection problem in the relational database environment. Optimization algorithms for index selection are extended to solve the more complex problem of selecting a good physical access path configuration for CODASYL databases. The proposed approach represents a unified solution to the physical database design problem for both CODASYL and relational systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6173","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6173","","Relational databases;Design optimization;Indexes;Database systems;Cost function;Process design;Design methodology;Transaction databases;Navigation","automatic programming;distributed databases;file organisation;relational databases;software tools","physical designer;network model databases;EROS;physical design tool;CODASYL;location mode;set implementation;set order;search keys;index selection;relational database;physical access path configuration","","2","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Test-suite reduction and prioritization for modified condition/decision coverage","J. A. Jones; M. J. Harrold","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Transactions on Software Engineering","","2003","29","3","195","209","Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software according to some criterion as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. This paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of empirical studies of these algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183927","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183927","","Software testing;Software algorithms;Costs;System testing;Software maintenance;Computer Society;Safety;FAA;Performance evaluation;Software performance","program testing;performance evaluation","critical software;software testing;commercial airborne systems;test suites;modified condition/decision coverage;test-suite prioritization","","152","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Experience with multiple processor versions of Concurrent C","R. F. Cmelik; N. H. Gehani; W. D. Roome","AT&T Bell Labs., Murray Hill, NJ, USA; AT&T Bell Labs., Murray Hill, NJ, USA; AT&T Bell Labs., Murray Hill, NJ, USA","IEEE Transactions on Software Engineering","","1989","15","3","335","344","Concurrent C, a superset of C providing parallel programming facilities, is considered. A uniprocessor version of Concurrent C was first implemented. After experience with this version, the Concurrent C implementation was extended to run on two types of multiple processor systems: a set of computers connected by a local area network (the distributed version) and a shared-memory multiprocessor (the multiprocessor version). Experience with implementing and using these versions of Concurrent C is described. Specifically, the language changes triggered by the multiple processor implementations, some sample programs, a comparison of the execution times on various systems, and the suitability of these multiple processor architectures are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21761","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21761","","Concurrent computing;Computer networks;Distributed computing;Local area networks;Parallel programming;Ethernet networks;Feedback;Operating systems;Computer architecture;Abortion","C language;local area networks;multiprocessing programs;multiprocessing systems;parallel programming","multiple processor versions;Concurrent C;parallel programming;uniprocessor version;local area network;shared-memory multiprocessor;execution times","","10","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers","K. Dejaeger; T. Verbraken; B. Baesens","Katholieke Universiteit Leuven, Leuven; Katholieke Universiteit Leuven, Leuven; Katholieke Universiteit Leuven, Leuven","IEEE Transactions on Software Engineering","","2013","39","2","237","257","Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network (BN) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to BN theory, is investigated. The results, both in terms of the AUC and the recently introduced H-measure, are rigorously tested using the statistical framework of Demšar. It is concluded that simple and comprehensible networks with less nodes can be constructed using BN classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6175912","Software fault prediction;Bayesian networks;classification;comprehensibility","Software;Predictive models;Bayesian methods;Measurement;Capability maturity model;Probability distribution;Machine learning","belief networks;feature extraction;learning (artificial intelligence);Markov processes;pattern classification;prediction theory;program testing;software fault tolerance;statistical analysis","software fault prediction models;Bayesian network classifiers;software testing;software development;faulty software code;machine learning literature;Naive Bayes classifier;citing predictive performance;BN classifiers;Markov blanket principle;feature selection;BN theory;AUC;introduced H-measure;statistical framework;Demsar;predictive performance;model selection","","72","","109","","","","","","IEEE","IEEE Journals & Magazines"
"Specification and analysis of parallel/distributed software and systems by Petri nets with transition enabling functions","Y. E. Papelis; T. L. Casavant","Dept. of Electr. & Comput. Eng., Iowa Univ., Iowa City, IA, USA; Dept. of Electr. & Comput. Eng., Iowa Univ., Iowa City, IA, USA","IEEE Transactions on Software Engineering","","1992","18","3","252","261","An approach for visually specifying parallel/distributed software using Petri nets (PNs) extend with transition enabling functions (TEFs) is investigated. The approach is demonstrated to be useful in the specification of decision-making activities that control distributed computing systems. PNs are employed because of their highly visual nature that can give insight into the nature of the controller of such a system and because of their analytical properties. In order to increase the expressive power of PNs, the extension of TEFs is used. The main focus is the specification and analysis of parallel/distributed software and systems. A key element of this approach is a set of rules derived to automatically transform such an extended net into a basic PN. Once the rules have been applied to transform the specification, analytical methods can be used to investigate characteristic properties of the system and validate correct operation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.126774","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=126774","","Software systems;Petri nets;Testing;Power system modeling;Control systems;Parallel processing;Decision making;Distributed control;Distributed computing;Concurrent computing","formal specification;parallel programming;Petri nets","parallel/distributed software;Petri nets;transition enabling functions;TEFs;specification;decision-making activities;distributed computing systems;analytical properties;expressive power;PNs;TEFs;parallel/distributed software","","20","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing Regulatory Rules for Privacy and Security Requirements","T. Breaux; A. Antón","NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","5","20","Information practices that use personal, financial, and health-related information are governed by US laws and regulations to prevent unauthorized use and disclosure. To ensure compliance under the law, the security and privacy requirements of relevant software systems must properly be aligned with these regulations. However, these regulations describe stakeholder rules, called rights and obligations, in complex and sometimes ambiguous legal language. These ""rules"" are often precursors to software requirements that must undergo considerable refinement and analysis before they become implementable. To support the software engineering effort to derive security requirements from regulations, we present a methodology for directly extracting access rights and obligations from regulation texts. The methodology provides statement-level coverage for an entire regulatory document to consistently identify and infer six types of data access constraints, handle complex cross references, resolve ambiguities, and assign required priorities between access rights and obligations to avoid unlawful information disclosures. We present results from applying this methodology to the entire regulation text of the US Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70746","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359472","Requirements/Specifications;Security and Privacy Protection;Legal Aspects of Computing;Requirements/Specifications;Security and Privacy Protection;Legal Aspects of Computing","Privacy;Law;Information security;Permission;Software systems;Legal factors;Software engineering;Data security;Data mining;Insurance","legislation;security of data;software engineering","software system security;software system privacy;software engineering;US Health Insurance Portability and Accountability Act Privacy Rule","","148","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Warm standby in hierarchically structured process-control programs","Ing-Ray Chen; F. B. Bastani","Inst. of Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; NA","IEEE Transactions on Software Engineering","","1994","20","8","658","663","We classify standby redundancy design space in process-control programs into the following three categories: cold standby, warm standby, and hot standby. Design parameters of warm standby are identified and the reliability of a system using warm standby is evaluated and compared with that of hot standby. Our analysis indicates that the warm standby scheme is particularly suitable for long-lived unmaintainable systems, especially those operating in harsh environments where burst hardware failures are possible. The feasibility of warm standby is demonstrated with a simulated chemical batch reactor system.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310674","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310674","","Process control;Control systems;Real time systems;Process design;Hardware;Fault tolerance;Costs;Circuit faults;Delay effects;Redundancy","process computer control;fault tolerant computing;software reliability;software maintenance;computer integrated manufacturing;redundancy","hierarchically structured process-control programs;standby redundancy design space;process-control programs;cold standby;warm standby;hot standby;system reliability;long-lived unmaintainable systems;burst hardware failures;simulated chemical batch reactor system","","3","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Approximate Oracles and Synergy in Software Energy Search Spaces","B. R. Bruce; J. Petke; M. Harman; E. T. Barr","Computer Science, University College London, London, London United Kingdom of Great Britain and Northern Ireland (e-mail: r.bruce@cs.ucl.ac.uk); Computer Science, University College London, London, London United Kingdom of Great Britain and Northern Ireland WC1E 6BT (e-mail: j.petke@ucl.ac.uk); CS, UCL, London, London United Kingdom of Great Britain and Northern Ireland WC1E 6BT (e-mail: mark.harman@ucl.ac.uk); Computer Science, University College London, London, London United Kingdom of Great Britain and Northern Ireland WC1E 6BT (e-mail: e.barr@ucl.ac.uk)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Reducing the energy consumption of software systems though optimisations techniques such as genetic improvement is gaining interest. However, efficient and effective improvement of software systems requires a better understanding of the code-change search space. One important choice practitioners have is whether to preserve the system&#x0027;s original output or permit approximation with each scenario having its own search space characteristics. When output preservation is a hard constraint, we report that the maximum energy reduction achievable by the modification operators is 2.69% (0.76% on average). By contrast, this figure increases dramatically to 95.60% (33.90% on average) when approximation is permitted, indicating the critical importance of approximate output quality assessment for code optimisation. We investigate synergy, a phenomenon that occurs when simultaneously applied source code modifications produce an effect greater than their individual sum. Our results reveal that 12.0% of all joint code modifications produced such a synergistic effect though 38.5% produce an antagonistic interaction in which simultaneously applied modifications are less effective than when applied individually. This highlights the need for more advanced search-based approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2827066","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8338061","search-based software engineering;search space;energy consumption;genetic improvement;synergy;antagonism;oracle;approximation","Energy consumption;Energy measurement;Software systems;Optimization;Aggregates;Genetics","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings","S. Lessmann; B. Baesens; C. Mues; S. Pietsch","University of Hamburg, Hamburg; K.U.Leuven, Leuven; University of Southampton, Southampton; University of Hamburg, Hamburg","IEEE Transactions on Software Engineering","","2008","34","4","485","496","Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527256","Complexity measures;Data mining;Formal methods;Statistical methods;Complexity measures;Data mining;Formal methods;Statistical methods","Predictive models;Software quality;Software testing;Statistical analysis;Large-scale systems;NASA;Software systems;Benchmark testing;Fault diagnosis;Convergence","benchmark testing;software quality;statistical testing","benchmarking classification models;software defect prediction;software quality;testing efficiency;predictive classification models;code attributes;fault-prone modules;metric-based classification;proprietary data sets;statistical testing procedures","","390","","67","","","","","","IEEE","IEEE Journals & Magazines"
"A Comparative Study of Software Model Checkers as Unit Testing Tools: An Industrial Case Study","M. Kim; Y. Kim; H. Kim","KAIST, Daejon; KAIST, Daejon; Samsung Electronics, Suwon","IEEE Transactions on Software Engineering","","2011","37","2","146","160","Conventional testing methods often fail to detect hidden flaws in complex embedded software such as device drivers or file systems. This deficiency incurs significant development and support/maintenance cost for the manufacturers. Model checking techniques have been proposed to compensate for the weaknesses of conventional testing methods through exhaustive analyses. Whereas conventional model checkers require manual effort to create an abstract target model, modern software model checkers remove this overhead by directly analyzing a target C program, and can be utilized as unit testing tools. However, since software model checkers are not fully mature yet, they have limitations according to the underlying technologies and tool implementations, potentially critical issues when applied in industrial projects. This paper reports our experience in applying Blast and CBMC to testing the components of a storage platform software for flash memory. Through this project, we analyzed the strong and weak points of two different software model checking technologies in the viewpoint of real-world industrial application-counterexample-guided abstraction refinement with predicate abstraction and SAT-based bounded analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5510242","Embedded software verification;software model checking;bounded model checking;CEGAR-based model checking;flash file systems.","Software tools;Software testing;Computer industry;Manufacturing industries;System testing;Embedded software;File systems;Costs;Flash memory;Refining","C language;program testing;program verification;storage management","software model checkers;unit testing tools;complex embedded software;model checking techniques;abstract target model;C program;Blast;CBMC;storage platform software;flash memory","","18","","54","","","","","","IEEE","IEEE Journals & Magazines"
"A visual language compiler for information retrieval by visual reasoning","S. -. Chang","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA","IEEE Transactions on Software Engineering","","1990","16","10","1136","1149","When a database increases in size, retrieving the data becomes a major problem. An approach based on data visualization and visual reasoning is described. The main idea is to transform the data objects and present sample data objects in a visual space. The user can use a visual language to incrementally formulate the information retrieval request in the visual space. A prototype system is described with the following features: (1) it is built on top of the SIL-ICON visual language compiler and therefore can be customized for different application domains; (2) it supports a fuzzy icon grammar to define reasonable visual sentences; (3) it incorporates a semantic model of the database for fuzzy visual query translation; and (4) it incorporates a VisualNet which stores the knowledge learned by the system in its interaction with the user so that the VisualReasoner can adapt its behavior.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60294","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60294","","Information retrieval;Visual databases;Data visualization;Fuzzy systems;Spatial databases;Information systems;Prototypes;User interfaces;Law;Legal factors","data structures;information retrieval;program compilers;visual programming","visual language compiler;information retrieval;visual reasoning;database;data visualization;data objects;visual language;prototype system;SIL-ICON;fuzzy icon grammar;semantic model;fuzzy visual query translation;VisualNet;VisualReasoner","","21","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Coordination Challenges in Large-Scale Software Development: A Case Study of Planning Misalignment in Hybrid Settings","S. Bick; K. Spohrer; R. Hoda; A. Scheerer; A. Heinzl","SAP SE, Walldorf, Germany; University of Mannheim, Mannheim, Germany; University of Auckland, Auckland, New Zealand; SAP SE, Walldorf, Germany; University of Mannheim, Mannheim, Germany","IEEE Transactions on Software Engineering","","2018","44","10","932","950","Achieving effective inter-team coordination is one of the most pressing challenges in large-scale software development. Hybrid approaches of traditional and agile development promise combining the overview and predictability of long-term planning on an inter-team level with the flexibility and adaptability of agile development on a team level. It is currently unclear, however, why such hybrids often fail. Our case study within a large software development unit of 13 teams at a global enterprise software company explores how and why a combination of traditional planning on an inter-team level and agile development on a team level can result in ineffective coordination. Based on a variety of data, including interviews with scrum masters, product owners, architects and senior management, and using Grounded Theory data analysis procedures, we identify a lack of dependency awareness across development teams as a key explanation of ineffective coordination. Our findings show how a lack of dependency awareness emerges from misaligned planning activities of specification, prioritization, estimation and allocation between agile team and traditional inter-team levels and ultimately prevents effective coordination. Knowing about these issues, large-scale hybrid projects in similar contexts can try to better align their planning activities across levels to improve dependency awareness and in turn achieve more effective coordination.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2730870","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7990187","Large-scale software development;agile;hybrid;inter-team coordination;dependency awareness;planning alignment;information systems development","Software;Planning;Agile software development;Companies;Task analysis;Interviews","data analysis;project management;software development management;software prototyping;team working","large-scale software development;agile development;long-term planning;inter-team level;software development unit;global enterprise software company;ineffective coordination;agile team;large-scale hybrid projects;coordination challenges;planning misalignment;hybrid settings;effective inter-team coordination;pressing challenges;scrum masters;product owners;architects;senior management;grounded theory data analysis procedures","","1","","97","","","","","","IEEE","IEEE Journals & Magazines"
"A language and system for the construction and tuning of parallel programs","K. Schwan; R. Ramnath; S. Vasudevan; D. Ogle","Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA","IEEE Transactions on Software Engineering","","1988","14","4","455","471","The programming of efficient parallel software typically requires extensive experimentation with program prototypes. To facilitate such experimentation, any programming system that supports rapid prototyping of parallel programs should provide high-level language primitives with which programs can be explicitly, statically, or dynamically tuned with respect to performance and reliability. Such language primitives should be able to refer conveniently to the information about the executing program and the parallel hardware required for tuning. Such information may include monitoring data about the current or previous program or even hints regarding appropriate tuning decisions. Language primitives and an associated programming system for program tuning are presented. The primitives and system have been implemented, and have been tested with several parallel applications on a network of Unix workstations.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4669","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4669","","Parallel programming;Operating systems;Manipulator dynamics;Software prototyping;Prototypes;Hardware;Monitoring;Runtime;Resource management;Dynamic programming","high level languages;parallel programming;program testing;programming environments","parallel programming;programming environments;parallel software;rapid prototyping;high-level language primitives;program tuning","","17","","60","","","","","","IEEE","IEEE Journals & Magazines"
"A Comparison of Six UML-Based Languages for Software Process Modeling","R. Bendraou; J. Jezequel; M. Gervais; X. Blanc","University of Pierre and Marie Curie (UPMC), Paris; IRISA, INRIA-Rennes Bretagne Atlantique, Rennes; University of Paris Ouest Nanterre La Défense, Paris; University of Pierre and Marie Curie (UPMC), Paris","IEEE Transactions on Software Engineering","","2010","36","5","662","675","Describing and managing activities, resources, and constraints of software development processes is a challenging goal for many organizations. A first generation of Software Process Modeling Languages (SPMLs) appeared in the 1990s but failed to gain broad industrial support. Recently, however, a second generation of SPMLs has appeared, leveraging the strong industrial interest for modeling languages such as UML. In this paper, we propose a comparison of these UML-based SPMLs. While not exhaustive, this comparison concentrates on SPMLs most representative of the various alternative approaches, ranging from UML-based framework specializations to full-blown executable metamodeling approaches. To support the comparison of these various approaches, we propose a frame gathering a set of requirements for process modeling, such as semantic richness, modularity, executability, conformity to the UML standard, and formality. Beyond discussing the relative merits of these approaches, we also evaluate the overall suitability of these UML-based SPMLs for software process modeling. Finally, we discuss the impact of these approaches on the current state of the practice, and conclude with lessons we have learned in doing this comparison.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.85","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5593045","Metamodeling;process modeling and execution;software process modeling languages;UML.","Unified modeling language;Software;Object oriented modeling;Analytical models;Semantics;Programming;Computational modeling","software engineering;Unified Modeling Language","UML based language;software development process;software process modeling language;UML based SPML;metamodeling approach","","37","","78","","","","","","IEEE","IEEE Journals & Magazines"
"Linearization of nonlinear recursive rules","D. J. Troy; C. T. Yu; W. Zhang","Dept. of Math. Sci., Purdue Univ., Hammond, IN, USA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","9","1109","1119","The problem of converting a simple nonlinear recursive logic query into an equivalent linear one is considered. A general method is given to transform a nonlinear rule into a sequence of linear ones. For efficient processing it is necessary to convert a nonlinear rule into a single linear rule. For such a conversion, a necessary and sufficient condition is provided for a type of doubly recursive rule to be equivalent to the resulting linear rule. It is also shown that a restricted type of higher order recursive rule is equivalent to the linear rule obtained by its conversion.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31368","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31368","","Logic;Deductive databases;Sufficient conditions;Polynomials;Mathematics;Terminology","recursive functions","linearization;nonlinear recursive rules;nonlinear recursive logic query;necessary and sufficient condition;type;doubly recursive rule;equivalent","","13","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Time-by-example query language for historical databases","A. U. Tansel; M. E. Arkun; G. Ozsoyoglu","Dept. of Stat. & Comput. Inf. Syst., City Univ. of New York, NY, USA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","4","464","478","The authors propose a graphical query language, Time-by-Example (TBE), which has suitable constructs for interacting with historical relational databases in a natural way. TBE is user-friendly. It follows the graphical, two-dimensional approach of such previous languages as Query-by-Example (QBE), Aggregation-by-Example (ABE), and Summary-Table-by-Example (STBE). TBE also uses the hierarchical window (subquery) concept of ABE and STBE. TBE manipulates triple-valued (set-triple-valued) attributes and historical relations. Set-theoretic expressions are followed to deal with time intervals. The BNF specification for TBE is given.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.16597","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=16597","","Database languages;Relational databases;Data models;History;Algebra;Upper bound;Query processing;Statistics;Maintenance engineering;Marine vehicles","query languages;relational databases","triple-valued attributes;graphical query language;Time-by-Example;historical relational databases;Summary-Table-by-Example;historical relations","","32","","41","","","","","","IEEE","IEEE Journals & Magazines"
"On the relationships among the all-uses, all-DU-paths, and all-edges testing criteria","A. S. Parrish; S. H. Zweben","Dept. of Comput. Sci., Alabama Univ., Tuscaloosa, AL, USA; NA","IEEE Transactions on Software Engineering","","1995","21","12","1006","1009","The all-du-paths data flow testing criterion was designed to be more demanding than the all-uses criterion, which itself was designed to be more demanding than the all-edges criterion. However, formal comparison metrics developed within the testing community have failed to validate these relationships, without requiring restrictive or undecidable assumptions regarding the universe of programs to which the criteria apply. We show that the formal relationships among these criteria can be made consistent with their intended relative strengths, without making restrictive or undecidable assumptions.","0098-5589;1939-3520;2326-3881","","10.1109/32.489075","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=489075","","Testing;Computer science;Information science;Logic","data flow analysis;data flow analysis;data flow analysis;program diagnostics;program diagnostics;program diagnostics;program testing;program testing;program testing;software metrics;software metrics;software metrics","all-DU-paths testing criteria;all-edges testing criteria;all-uses testing criteria;all-du-paths data flow testing criterion;formal comparison metrics","","6","","11","","","","","","IEEE","IEEE Journals & Magazines"
"An Observe-Model-Exercise* Paradigm to Test Event-Driven Systems with Undetermined Input Spaces","B. N. Nguyen; A. M. Memon","Department of Computer Science, University of Maryland, College Park; Department of Computer Science, University of Maryland, College Park","IEEE Transactions on Software Engineering","","2014","40","3","216","234","System testing of software applications with a graphical-user interface (GUI) front-end requires that sequences of GUI events, that sample the application's input space, be generated and executed as test cases on the GUI. However, the context-sensitive behavior of the GUI of most of today's non-trivial software applications makes it practically impossible to fully determine the software's input space. Consequently, GUI testers-both automated and manual-working with undetermined input spaces are, in some sense, blindly navigating the GUI, unknowingly missing allowable event sequences, and failing to realize that the GUI implementation may allow the execution of some disallowed sequences. In this paper, we develop a new paradigm for GUI testing, one that we call Observe-Model-Exercise* (OME*) to tackle the challenges of testing context-sensitive GUIs with undetermined input spaces. Starting with an incomplete model of the GUI's input space, a set of coverage elements to test, and test cases, OME* iteratively observes the existence of new events during execution of the test cases, expands the model of the GUI's input space, computes new coverage elements, and obtains new test cases to exercise the new elements. Our experiment with 8 open-source software subjects, more than 500,000 test cases running for almost 1,100 machine-days, shows that OME* is able to expand the test space on average by 464.11 percent; it detected 34 faults that had never been detected before.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2300857","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6714448","Test generation;user interfaces;quality concepts","Graphical user interfaces;Computational modeling;Blogs;Testing;Software;Context;Layout","graphical user interfaces;program testing;public domain software","observe-model-exercise paradigm;test event-driven system;undetermined input spaces;software system testing;graphical-user interface front-end;GUI context-sensitive behavior;open-source software subjects;test generation","","17","","44","","","","","","IEEE","IEEE Journals & Magazines"
"A model of visibility control","A. L. Wolf; L. A. Clarke; J. C. Wileden","Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA; Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA; Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA","IEEE Transactions on Software Engineering","","1988","14","4","512","520","A formal model for describing and evaluating visibility control mechanisms is introduced. The model reflects a general view of visibility in which the concepts of requisition of access and provision of access are distinguished. This model provides a means for characterizing and reasoning about the various properties of visibility control mechanisms. Specifically, the notion of preciseness is defined. The utility of the model is illustrated by using it to evaluate and compare the relative strengths and weaknesses, with respect to preciseness, of the visibility control mechanisms found in Algol 60, Ada, Gypsy, and an approach called PIC, which specifically addresses the concerns of visibility control in large software systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4673","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4673","","Software systems;Control systems;Mechanical factors;Application software;Computer science;Programming;Laboratories;Information science;Trademarks;Computer languages","Ada;ALGOL;directed graphs;high level languages;programming theory","directed graphs;programming theory;visibility control;requisition of access;provision of access;Algol 60;Ada;Gypsy;PIC","","7","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal selection of secondary indexes","E. Barcucci; R. Pinzani","Dipartimento di Sistemi e Inf., Firenze Univ., Italy; Dipartimento di Sistemi e Inf., Firenze Univ., Italy","IEEE Transactions on Software Engineering","","1990","16","1","32","38","When planning a database, the problem of index selection is of particular interest. The authors examine a transaction model that includes queries, updates, insertions, and deletions, and they define a function that calculates the transaction's total cost when an index set is used. Their aim is to minimize the function cost in order to identify the optimal set. The algorithms proposed in other studies require an exponential time in the number of attributes in order to solve the problem. The authors propose a heuristic algorithm based on some properties of the cost function that produces an almost optimal set in polynomial time. In many cases, the cost function properties make it possible to prove that the solution obtained is the optimal one.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44361","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44361","","Cost function;Indexes;Transaction databases;Heuristic algorithms;Optimization;Relational databases;Algorithm design and analysis;Testing;Read-write memory","database management systems;heuristic programming;indexing;transaction processing","optimal selection;secondary indexes;database;transaction model;queries;updates;insertions;deletions;heuristic algorithm;polynomial time;cost function properties","","12","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing Semi-Automated Clustering Methods for Persona Development","J. Brickey; S. Walczak; T. Burgess","US Army, Combating Terrorism Center, West Point; University of Colorado Denver, Denver; United States Military Academy, West Point","IEEE Transactions on Software Engineering","","2012","38","3","537","546","Current and future information systems require a better understanding of the interactions between users and systems in order to improve system use and, ultimately, success. The use of personas as design tools is becoming more widespread as researchers and practitioners discover its benefits. This paper presents an empirical study comparing the performance of existing qualitative and quantitative clustering techniques for the task of identifying personas and grouping system users into those personas. A method based on Factor (Principal Components) Analysis performs better than two other methods which use Latent Semantic Analysis and Cluster Analysis as measured by similarity to expert manually defined clusters.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928355","Clustering;interaction styles;personas;user-centered design;user interfaces.","Manuals;Clustering methods;Principal component analysis;Software;Interviews;Humans;Organizations","information systems;pattern clustering;principal component analysis","semi-automated clustering methods;persona development;information systems;design tools;quantitative clustering techniques;qualitative clustering techniques;principal components analysis;latent semantic analysis","","4","","47","","","","","","IEEE","IEEE Journals & Magazines"
"A formal method for composing a network command language","B. Meandzija","Department of Computer Science, School of Engineering and Applied Science, Southern Methodist University, Dallas, TX 75275","IEEE Transactions on Software Engineering","","1986","SE-12","8","860","865","A formal method is introduced for the development and definition of command languages for heterogeneous computer networks. The network command languages are developed from the command languages of the systems constituting the network. This is done by defining a common presentation model for the system command languages and constructing the network command language by applying a composition principle to the commonly represented languages. The common presentation model is defined as a Vienna Development Method (Meta IV) abstract processor for command languages. System command languages are represented by means of predicate functions which are defined on the abstract domains of the abstract processor. This allows a straightforward formulation of the composition principle as a function for the logical combination of predicate functions. Two sample network command languages are composed out of two hypothetical command languages. The results are related to the International Organization for Standardization Open Systems Interconnection model (ISO OSI).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312988","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312988","Computer networks;design methodology;development system;formal semantics;network command languages","Command languages;Abstracts;Open systems;Semantics;ISO;Computers;Computer networks","computer communications software;computer networks;programming languages","network command language;heterogeneous computer networks;common presentation model;Vienna Development Method;Meta IV;abstract processor;predicate functions;ISO OSI","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Spawn: a distributed computational economy","C. A. Waldspurger; T. Hogg; B. A. Huberman; J. O. Kephart; W. S. Stornetta","Xerox Palo Alto Res. Center, CA, USA; Xerox Palo Alto Res. Center, CA, USA; Xerox Palo Alto Res. Center, CA, USA; Xerox Palo Alto Res. Center, CA, USA; Xerox Palo Alto Res. Center, CA, USA","IEEE Transactions on Software Engineering","","1992","18","2","103","117","The authors have designed and implemented an open, market-based computational system called Spawn. The Spawn system utilizes idle computational resources in a distributed network of heterogeneous computer workstations. It supports both coarse-grain concurrent applications and the remote execution of many independent tasks. Using concurrent Monte Carlo simulations as prototypical applications, the authors explore issues of fairness in resource distribution, currency as a form of priority, price equilibria, the dynamics of transients, and scaling to large systems. In addition to serving the practical goal of harnessing idle processor time in a computer network, Spawn has proven to be a valuable experimental workbench for studying computational markets and their dynamics.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.121753","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=121753","","Distributed computing;Computer networks;Humans;Resource management;Application software;Computer science;Biology computing;Workstations;Virtual prototyping;Microeconomics","computer networks;Monte Carlo methods;parallel programming","distributed computational economy;market-based computational system;Spawn system;idle computational resources;distributed network;heterogeneous computer workstations;coarse-grain concurrent applications;remote execution;concurrent Monte Carlo simulations;fairness;resource distribution;price equilibria;idle processor time;experimental workbench;computational markets","","282","","38","","","","","","IEEE","IEEE Journals & Magazines"
"A three-view model for performance engineering of concurrent software","C. M. Woodside","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada","IEEE Transactions on Software Engineering","","1995","21","9","754","767","This paper describes a multiview characterization of concurrent software and systems suitable for displaying and analyzing performance information. The views draw from well-known descriptions, and are compatible with established techniques and tools such as execution graphs, Petri Nets, State-Charts, structured design or object-oriented design, and various models for performance. The views are connected by means of a ""Core model"" and are used together to extract information relating to system integration, such as interprocess overheads, and the delay behavior of separate software components in complex systems. The integration of the views in the Core assists by converting results in one view (such as scheduling delay for resources) to parameters in another (such as delays along a path). The ultimate goal of the views is to support designers in making tradeoffs which involve performance, and to provide early assessment of the performance potential of software designs.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.464545","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=464545","","Software performance;Object oriented modeling;Delay;Software design;Hardware;Process planning;Software systems;Information analysis;Performance analysis;Petri nets","software performance evaluation;parallel programming;systems analysis;graph theory","three-view model;performance engineering;concurrent software;multiview characterization;performance information;execution graphs;Petri Nets;State-Charts;structured design;object-oriented design;Core model;system integration;interprocess overheads;scheduling delay;software designs","","28","","37","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient distributed deadlock avoidance algorithm for the AND model","Hui Wu; Wei-Ngan Chin; J. Jaffar","Sch. of Comput., Nat. Univ. of Singapore, Singapore; NA; NA","IEEE Transactions on Software Engineering","","2002","28","1","18","29","A new rank-based distributed deadlock avoidance algorithm for the AND resource request model is presented. Deadlocks are avoided by dynamically maintaining an invariant Con(WFG): For each pair of processes p/sub i/ and p/sub j/, p/sub i/ is allowed to wait for process p/sub j/ iff the rank of p/sub j/ is greater than that of p/sub i/ for the WFG (Wait-For Graph). Our algorithm neither restricts the order of resource requests nor needs a priori information about resource requests nor causes unnecessary abortion of processes. Multidimensional ranks, which are partially ordered and dynamically modified are used to drastically reduce the cost of maintaining Con(WFG). Our simulation results show that the performance of our algorithm is better than that of existing algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/32.979987","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979987","","System recovery","concurrency control;resource allocation;processor scheduling;distributed algorithms;graph theory","distributed deadlock avoidance algorithm;partially ordered rank;AND resource request model;invariant;WFG;Wait-For Graph;unnecessary process abortion;multidimensional ranks;partially ordered ranks;dynamically modified ranks;concurrency control","","8","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Omega—A Data Flow Analysis Tool for the C Programming Language","C. Wilson; L. J. Osterweil","AT&T Information Systems; NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","832","838","This paper describes Omega, a prototype system designed to analyze data flow in C programs. Omega is capable of detecting certain types of common programming errors, or assuring their absence. Omega also addresses the problems of analyzing pointer variables.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232542","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702102","Anomaly;data flow;error detection;flowgraph;static analysis","Data analysis;Computer languages;Documentation;Testing;Debugging;Computer errors;Event detection;Prototypes;Error analysis;Information systems","","Anomaly;data flow;error detection;flowgraph;static analysis","","6","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Hierarchical modeling of availability in distributed systems","S. Hariri; H. Mutlu","Dept. of Electr. & Comput. Eng., Syracuse Univ., NY, USA; NA","IEEE Transactions on Software Engineering","","1995","21","1","50","56","Distributed computing systems are attractive due to the potential improvement in availability, fault-tolerance, performance, and resource sharing. Modeling and evaluation of such computing systems is an important step in the design process of distributed systems. We present a two-level hierarchical model to analyze the availability of distributed systems. At the higher level (user level), the availability of the tasks (processes) is analyzed using a graph-based approach. At the lower level (component level), detailed Markov models are developed to analyze the component availabilities. These models take into account the hardware/software failures, congestion and collisions in communication links, allocation of resources, and the redundancy level. A systematic approach is developed to apply the two-level hierarchical model to evaluate the availability of the processes and the services provided by a distributed computing environment. This approach is then applied to analyze some of the distributed processes of a real distributed system, Unified Workstation Environment (UWE), that is currently being implemented at AT&T Bell Laboratories.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.341847","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=341847","","Availability;Distributed computing;Resource management;Fault tolerant systems;Time sharing computer systems;Throughput;Reliability engineering;Design engineering;Steady-state;Fault trees","distributed processing;redundancy;fault tolerant computing;reliability;Markov processes;computer network reliability","hierarchical modeling;distributed system availability;availability;fault-tolerance;resource sharing;two-level hierarchical model;graph-based approach;Markov models;congestion;communication links;redundancy level;distributed computing environment;Unified Workstation Environment","","19","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Stack and queue integrity on hostile platforms","P. T. Devanbu; S. G. Stubblebine","Dept. of Comput. Sci., California Univ., Davis, CA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","1","100","108","When computationally intensive tasks have to be carried out on trusted, but limited platforms such as smart cards, it becomes necessary to compensate for the limited resources (memory, CPU speed) by off-loading implementations of data structures onto an available (but insecure, untrusted) fast coprocessor. However, data structures such as stacks, queues, RAMs, and hash tables can be corrupted (and made to behave incorrectly) by a potentially hostile implementation platform or by an adversary knowing or choosing data structure operations. The paper examines approaches that can detect violations of data structure invariants, while placing limited demands on the resources of the secure computing platform.","0098-5589;1939-3520;2326-3881","","10.1109/32.979991","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979991","","Data structures;Smart cards;Random access memory;Coprocessors","data integrity;data structures;security of data","queue integrity;stack integrity;computationally intensive tasks;smart cards;data structures;fast coprocessor;hash tables;hostile implementation platform;secure computing platform;security;software protection","","7","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Software reliability modeling and analysis","F. -. Scholz","Boeing Computer Services, 565 Andover Park West, Tukwila, WA 98188","IEEE Transactions on Software Engineering","","1986","SE-12","1","25","31","A discrete and, as approximation to it, a continuous model for the software reliability growth process are examined. The discrete model is based on independent multinomial trials and concerns itself with the joint distribution of the first occurrence time of its underlying events (bugs). The continuous model is based on the order statistics of <i>N</i> independent nonidentically distributed exponential random variables. It is shown that the spacings between bugs are not necessarily independent, or exponentially (geometrically) distributed. However, there is a statistical rationale for viewing them so conditionally. Some identifiability problems are pointed out and resolved. In particular, it appears that the number of bugs in a program is not identifiable.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312916","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312916","Conditional inference;confidence bounds;exponential order statistics (non-i.i.d.);identifiability;multinomial trials;order restricted maximum likelihood estimates;spacings","Computer bugs;Random variables;Debugging;Software reliability;Software;Zinc","reliability theory;software reliability","software reliability growth;discrete model;multinomial trials;nonidentically distributed exponential random variables;bugs","","4","","","","","","","","IEEE","IEEE Journals & Magazines"
"Verifying a logic-synthesis algorithm and implementation: a case study in software verification","M. Aagaard; M. Leeser","Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada; NA","IEEE Transactions on Software Engineering","","1995","21","10","822","833","We describe the verification of a logic synthesis tool with the Nuprl proof development system. The logic synthesis tool, Pbs, implements the weak division algorithm. Pbs consists of approximately 1000 lines of code implemented in a functional subset of Standard ML. It is a proven and usable implementation and is an integral part of the Bedroc high level synthesis system. The program was verified by embedding the subset of Standard ML in Nuprl and then verifying the correctness of the implementation of Pbs in the Nuprl logic. The proof required approximately 500 theorems. In the process of verifying Pbs we developed a consistent approach for using a proof development system to reason about functional programs. The approach hides implementation details and uses higher order theorems to structure proofs and aid in abstract reasoning. Our approach is quite general, should be applicable to any higher order proof system, and can aid in the future verification of large software implementations.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.469458","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=469458","","Software algorithms;Computer aided software engineering;Logic;Hardware;Equations;Circuit synthesis;Software tools;Code standards;High level synthesis;Software libraries","high level synthesis;logic design;program verification;theorem proving;formal logic;functional programming","logic synthesis algorithm verification;logic-synthesis algorithm;software verification;logic synthesis tool;Nuprl proof development system;Pbs;weak division algorithm;functional subset;Standard ML;Bedroc high level synthesis system;correctness verification;Nuprl logic;functional programs;higher order theorems;abstract reasoning;higher order proof system;large software implementations","","3","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Approximate Structural Context Matching: An Approach to Recommend Relevant Examples","R. Holmes; R. J. Walker; G. C. Murphy","Department of Computer Science, University of Calgary, 2500 University Dr. NW, Calgary, Alberta, Canada T2N 1N4; Department of Computer Science, University of Calgary, 2500 University Dr. NW, Calgary, Alberta, Canada T2N 1N4; Department of Computer Science, University of British Columbia, 201-2366 Main Mall, Vancouver, British Columbia, Canada V6T 1Z4","IEEE Transactions on Software Engineering","","2006","32","12","952","970","When coding to an application programming interface (API), developers often encounter difficulties, unsure of which class to subclass, which objects to instantiate, and which methods to call. Example source code that demonstrates the use of the API can help developers make progress on their task. This paper describes an approach to provide such examples in which the structure of the source code that the developer is writing is matched heuristically to a repository of source code that uses the API. The structural context needed to query the repository is extracted automatically from the code, freeing the developer from learning a query language or from writing their code in a particular style. The repository is generated automatically from existing applications, avoiding the need for handcrafted examples. We demonstrate that the approach is effective, efficient, and more reliable than traditional alternatives through four empirical studies","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.117","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016572","API usage;structural context;heuristic search;Strathcona;example recommendation.","Application software;Database languages;Writing;Computer Society;Software libraries;Protocols;Documentation;Investments;Code standards;Standards development","application program interfaces;pattern matching;query languages","approximate structural context matching;application programming interface;source code structure;query language","","66","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal scheduling of cooperative tasks in a distributed system using an enumerative method","D. -. Peng; K. G. Shin","Dept. of Electr. Eng. & Comput. Sci., Michigan Univ., Ann Arbor, MI, USA; Dept. of Electr. Eng. & Comput. Sci., Michigan Univ., Ann Arbor, MI, USA","IEEE Transactions on Software Engineering","","1993","19","3","253","267","Preemptive (resume) scheduling of cooperative tasks that have been preassigned to a set of processing nodes in a distributed system, when each task is assumed to consist of several modules is discussed. During the course of their execution, the tasks communicate with each other to collectively accomplish a common goal. Such intertask communications lead to precedence constraints between the modules of different tasks. The objective of this scheduling is to minimize the maximum normalized task response time, called the system hazard. Real-time tasks and the precedence constraints among them are expressed in a PERT/CPM form with activity on arc (AOA), called the task graph (TG), in which the dominance relationship between simultaneously schedulable modules is derived and used to reduce the size of the set of active schedules to be searched for an optimal schedule. Lower-bound costs are estimated, and are used to bound the search. An example of the task scheduling problem and some computational experiences are presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.221134","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=221134","","Optimal scheduling;Hazards;Processor scheduling;Real time systems;Resumes;Costs;Delay effects;Microelectronics;Aerospace engineering","distributed processing;PERT;real-time systems;scheduling","cooperative tasks;enumerative method;processing nodes;distributed system;common goal;intertask communications;precedence constraints;normalized task response time;system hazard;PERT/CPM form;activity on arc;AOA;task graph;dominance relationship;simultaneously schedulable modules;active schedules;optimal schedule;task scheduling problem;computational experiences","","21","","18","","","","","","IEEE","IEEE Journals & Magazines"
"DECOR: A Method for the Specification and Detection of Code and Design Smells","N. Moha; Y. Gueheneuc; L. Duchien; A. Le Meur","INRIA, IRISA, Universit&#x0E9; de Rennes 1, France; &#x0C9;cole Polytechnique de Montr&#x0E9;al, Qu&#x0E9;bec; LIFL, INRIA Lille-Nord Europe, Universit&#x0E9; de Lille, France; LIFL, INRIA Lille-Nord Europe, Universit&#x0E9; de Lille, France","IEEE Transactions on Software Engineering","","2010","36","1","20","36","Code and design smells are poor solutions to recurring implementation and design problems. They may hinder the evolution of a system by making it hard for software engineers to carry out changes. We propose three contributions to the research field related to code and design smells: (1) DECOR, a method that embodies and defines all the steps necessary for the specification and detection of code and design smells, (2) DETEX, a detection technique that instantiates this method, and (3) an empirical validation in terms of precision and recall of DETEX. The originality of DETEX stems from the ability for software engineers to specify smells at a high level of abstraction using a consistent vocabulary and domain-specific language for automatically generating detection algorithms. Using DETEX, we specify four well-known design smells: the antipatterns Blob, Functional Decomposition, Spaghetti Code, and Swiss Army Knife, and their 15 underlying code smells, and we automatically generate their detection algorithms. We apply and validate the detection algorithms in terms of precision and recall on XERCES v2.7.0, and discuss the precision of these algorithms on 11 open-source systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5196681","Antipatterns;design smells;code smells;specification;metamodeling;detection;Java.","Detection algorithms;Vocabulary;Domain specific languages;Algorithm design and analysis;Metamodeling;Java;Design engineering;Object oriented programming;Phase detection;Costs","formal specification;program verification;software quality","code specification;code detection;design smells;DECOR;DETEX;antipatterns Blob;functional decomposition;Spaghetti code;Swiss army knife;empirical validation;domain-specific language;open-source systems","","246","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Developing interpretable models with optimized set reduction for identifying high-risk software components","L. C. Briand; V. R. Brasili; C. J. Hetmanski","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1993","19","11","1028","1044","Applying equal testing and verification effort to all parts of a software system is not very efficient, especially when resources are tight. Therefore, one needs to low/high fault frequency components so that testing/verification effort can be concentrated where needed. Such a strategy is expected to detect more faults and thus improve the resulting reliability of the overall system. The authors present the optimized set reduction approach for constructing such models, which is intended to fulfill specific software engineering needs. The approach to classification is to measure the software system and build multivariate stochastic models for predicting high-risk system components. Experimental results obtained by classifying Ada components into two classes (is, or is not likely to generate faults during system and acceptance rest) are presented. The accuracy of the model and the insights it provides into the error-making process are evaluated.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.256851","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=256851","","Software engineering;System testing;Software systems;Predictive models;Data analysis;Logistics;Classification tree analysis;Machine learning;Software testing;Frequency","program testing;program verification;software reliability","high-risk software components;testing effort;verification effort;optimized set reduction approach;multivariate stochastic model;classifying Ada components;error-making process","","89","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Proving Liveness and Termination of Systolic Arrays Using Communicating Finite State Machines","M. G. Gouda; Hui-Seng Lee","Department of Computer Sciences, University of Texas; NA","IEEE Transactions on Software Engineering","","1985","SE-11","10","1240","1251","We model a systolic array as a network of, mostly identical, communicating finite state machines that exchange messages over one-to-one, unbounded, FIFO channels. Each machine has a cyclic behavior; in each cycle, a machine first receives one message from each of its input channels, then sends one message to each of its output channels. If in a cycle a machine does not have any data message to send to one of its output channels, it sends a null message instead; thus, machines exchange two types of messages, data and null. We characterize the liveness and termination properties for such networks, and discuss two algorithms that can be used to decide these properties for any given network. We apply these algorithms to establish the liveness and termination properties of four systolic array examples. These examples include a linear matrix-vector multiplier, a linear priority queue, and a search tree.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231871","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701939","Communicating finite state machines;communication progress;liveness;systolic array;termination;verification;VLSI","Systolic arrays;Automata;History;Protocols;Very large scale integration;Computer networks;Safety;Joining processes;Wires;Costs","","Communicating finite state machines;communication progress;liveness;systolic array;termination;verification;VLSI","","","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ScottKnottESD in response to ""An empirical comparison of model validation techniques for defect prediction models""","S. Herbold","Institute of Computer Science, University of Goettingen, Goettingen, Germany","IEEE Transactions on Software Engineering","","2017","43","11","1091","1094","In this article, we discuss the ScottKnottESD test, which was proposed in a recent paper “An Empirical Comparison of Model Validation Techniques for Defect Prediction Models” that was published in this journal. We discuss the implications and the empirical impact of the proposed normality correction of ScottKnottESD and come to the conclusion that this correction does not necessarily lead to the fulfillment of the assumptions of the original Scott-Knott test and may cause problems with the statistical analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2748129","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8024011","Scott-knott test, log transformation, statistics","Analysis of variance;Measurement;Distributed databases;Predictive models;Sociology","program testing;software metrics;statistical analysis","model validation techniques;defect prediction models;ScottKnottESD test;empirical impact;statistical analysis","","","","23","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Atomic actions for fault-tolerance using CSP","P. Jalote; R. H. Campbell","Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801; Department of Computer Science, University of Maryland, College Park, MD 20742; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana. IL 61801","IEEE Transactions on Software Engineering","","1986","SE-12","1","59","68","Two complementary techniques have evolved for providing fault-tolerance in software: forward error recovery and backward error recovery. Few implementations permit both approaches to be combined within a particular application. Fewer techniques are available for the construction of fault-tolerant software for systems involving concurrent processes and multiple processors. Many schemes for supporting forward or backward recovery are based on some concept of an atomic action. The authors propose a mechanism for supporting an atomic action in a system of communicating sequential processes (CSP). The atomic action is used as the basic unit for providing fault-tolerance. The atomic action is called an FT-action, and both forward and backward error recovery are performed in the context of an FT-action.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312920","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312920","Atomic actions;backward recovery;communicating sequential processes;forward recovery;software fault-tolerance","Fault tolerance;Fault tolerant systems;Software;Computer languages;Process control;Synchronization","fault tolerant computing;software reliability","software reliability;fault-tolerance;CSP;forward error recovery;backward error recovery;concurrent processes;multiple processors;atomic action;communicating sequential processes","","11","","","","","","","","IEEE","IEEE Journals & Magazines"
"A distributed algorithm for performance improvement through file replication, file migration, and process migration","A. Hac","AT&T Bell Lab., Naperville, IL, USA","IEEE Transactions on Software Engineering","","1989","15","11","1459","1470","The author presents a distributed algorithm that considers the number of read and write accesses to files for every process type, the number of processes and their demands on system resources, the utilization of bottlenecks on all machines, and file sizes. Performance improvement obtained with the algorithm is discussed and proved. A number of experiments executed in a distributed system in order to predict the impact on performance of various algorithm strategies are examined. The experiments show changes in system performance due to file and process placement, file replication, and file and process migration.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41337","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41337","","Distributed algorithms;System performance;Resource management;Cost function;Load management;File systems;Databases;Availability;Reliability;Routing","distributed processing;programming theory;system recovery","distributed algorithm;performance improvement;file replication;file migration;process migration;read and write accesses;system resources;bottlenecks;file sizes;distributed system","","30","","11","","","","","","IEEE","IEEE Journals & Magazines"
"A Game-Theoretic Foundation for the Maximum Software Resilience against Dense Errors","C. Huang; D. A. Peled; S. Schewe; F. Wang","Graduate Institute of Electronic Engineering, National Taiwan University, Taiwan, ROC; Department of Computer Science, Bar Ilan University, Ramat Gan, Israel; Department of Computer Science, University of Liverpool, Liverpool, United Kingdom; Department of Electrical Engineering, National Taiwan University, Taiwan, ROC","IEEE Transactions on Software Engineering","","2016","42","7","605","622","Safety-critical systems need to maintain their functionality in the presence of multiple errors caused by component failures or disastrous environment events. We propose a game-theoretic foundation for synthesizing control strategies that maximize the resilience of a software system in defense against a realistic error model. The new control objective of such a game is called<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq1-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>-resilience. In order to be<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq2-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>-resilient, a system needs to rapidly recover from infinitely many waves of a small number of up to<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq3-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>close errors provided that the blocks of up to<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq4-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>errors are separated by short time intervals, which can be used by the system to recover. We first argue why we believe this to be the right level of abstraction for safety critical systems when local faults are few and far between. We then show how the analysis of<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq5-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>-resilience problems can be formulated as a model-checking problem of a mild extension to the alternating-time<inline-formula><tex-math notation=""LaTeX"">$\mu$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq6-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>-calculus (AMC). The witness for<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq7-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>resilience, which can be provided by the model checker, can be used for providing control strategies that are optimal with respect to resilience. We show that the computational complexity of constructing such optimal control strategies is low and demonstrate the feasibility of our approach through an implementation and experimental results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2510001","ISF; Efficient Synthesis Method of Control for Concurrent Systems; Engineering and Physical Science Research Council (EPSRC); MOST; Research Center for Information Technology Innovation (CITI); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7360234","Fault tolerance;resilience;formal verification;model-checking;game;strategy;complexity","Resilience;Games;Software systems;Safety;Game theory;Computer science","","","","3","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Design and evaluation of a support service for mobile, wireless publish/subscribe applications","M. Caporuscio; A. Carzaniga; A. L. Wolf","Dipt. di Informatica, Universita degli Studi dell'Aquila, L'Aquila, Italy; NA; NA","IEEE Transactions on Software Engineering","","2003","29","12","1059","1071","This paper presents the design and evaluation of a support service for mobile, wireless clients of a distributed publish/subscribe system. A distributed publish/subscribe system is a networked communication infrastructure where messages are published by senders and then delivered to the receivers whose subscriptions match the messages. Communication therefore does not involve the use of explicit addresses, but rather emerges from the dynamic arrangement of publishers and subscribers. Such a communication mechanism is an ideal platform for a variety of Internet applications, including multiparty messaging, personal information management, information sharing, online news distribution, service discovery, and electronic auctions. Our goal is to support such applications on mobile, wireless host devices in such a way that the applications can, if they chose, be oblivious to the mobility and intermittent connectivity of their hosts as they move from one publish/subscribe access point to another. In this paper, we describe a generic, value-added service that can be used in conjunction with publish/subscribe systems to achieve these goals. We detail the implementation of the service and present the results of our evaluation of the service in both wireline and emulated wireless environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265521","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265521","","Application software;Mobile communication;Mobile computing;Subscriptions;Web and internet services;Information management;Network servers;Switches;Personal digital assistants;Wireless networks","middleware;message passing;mobile computing;wireless LAN;cellular radio;synchronisation","support service;mobile wireless publish/subscribe applications;distributed publish/subscribe system;networked communication infrastructure;Internet applications;multiparty messaging;personal information management;information sharing;online news distribution;service discovery;electronic auctions;mobile wireless host devices;wireline environments;emulated wireless environments","","70","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Dynamic Software Update Safety Using Systematic Testing","C. M. Hayden; E. K. Smith; E. A. Hardisty; M. Hicks; J. S. Foster","University of Maryland, College Park, College Park; University of Maryland, College Park, College Park; University of Maryland, College Park, College Park; University of Maryland, College Park, College Park; University of Maryland, College Park, College Park","IEEE Transactions on Software Engineering","","2012","38","6","1340","1354","Dynamic software updating (DSU) systems patch programs on the fly without incurring downtime. To avoid failures due to the updating process itself, many DSU systems employ timing restrictions. However, timing restrictions are theoretically imperfect, and their practical effectiveness is an open question. This paper presents the first significant empirical evaluation of three popular timing restrictions: activeness safety (AS), which prevents updates to active functions, con-freeness safety (CFS), which only allows modifications to active functions when doing so is provably type-safe, and manual identification of the event-handling loops during which an update may occur. We evaluated these timing restrictions using a series of DSU patches to three programs: OpenSSH, vsftpd, and ngIRCd. We systematically applied updates at each distinct update point reached during execution of a suite of system tests for these programs to determine which updates pass and which fail. We found that all three timing restrictions prevented most failures, but only manual identification allowed none. Further, although CFS and AS allowed many more update points, manual identification still supported updates with minimal delay. Finally, we found that manual identification required the least developer effort. Overall, we conclude that manual identification is most effective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.101","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6035725","Dynamic software updating (DSU);hot-swapping;software reliability;testing;program tracing","Software reliability;Software testing;Servers","program testing;safety-critical software;software fault tolerance;software maintenance","dynamic software updating safety evaluation;systematic testing;DSU systems;timing restrictions;activeness safety;AS;active functions;con-freeness safety;CFS;event-handling loop identification;OpenSSH;vsftpd;ngIRCd;manual identification;failure prevention","","10","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Instance Generator and Problem Representation to Improve Object Oriented Code Coverage","A. Sakti; G. Pesant; Y. Guéhéneuc","Department of Computer and Software Engineering, École Polytechnique de Montral, Montral, QC, Canada; Department of Computer and Software Engineering, École Polytechnique de Montral, Montral, QC, Canada; Department of Computer and Software Engineering, École Polytechnique de Montral, Montral, QC, Canada","IEEE Transactions on Software Engineering","","2015","41","3","294","313","Search-based approaches have been extensively applied to solve the problem of software test-data generation. Yet, test-data generation for object-oriented programming (OOP) is challenging due to the features of OOP, e.g., abstraction, encapsulation, and visibility that prevent direct access to some parts of the source code. To address this problem we present a new automated search-based software test-data generation approach that achieves high code coverage for unit-class testing. We first describe how we structure the test-data generation problem for unit-class testing to generate relevant sequences of method calls. Through a static analysis, we consider only methods or constructors changing the state of the class-under-test or that may reach a test target. Then we introduce a generator of instances of classes that is based on a family of means-of-instantiation including subclasses and external factory methods. It also uses a seeding strategy and a diversification strategy to increase the likelihood to reach a test target. Using a search heuristic to reach all test targets at the same time, we implement our approach in a tool, JTExpert, that we evaluate on more than a hundred Java classes from different open-source libraries. JTExpert gives better results in terms of search time and code coverage than the state of the art, EvoSuite, which uses traditional techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2363479","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6926828","Automatic Test Data Generation;Search Based Software Testing;Unit Class Testing;Seeding Strategy;Diversification Strategy;Java Testing;Automatic test data generation;search based software testing;unit class testing;seeding strategy;diversification strategy;Java testing","Testing;Complexity theory;Generators;Search problems;Java;Production facilities;Libraries","Java;object-oriented programming;program diagnostics;program testing;public domain software","instance generator;problem representation;object oriented code coverage;search-based approach;object-oriented programming;OOP;abstraction;encapsulation;visibility;source code;automated search-based software test-data generation approach;unit-class testing;method call sequences;static analysis;class-under-test;means-of-instantiation;seeding strategy;diversification strategy;search heuristic;JTExpert;Java class evaluation;open-source libraries;search time;EvoSuite","","25","","53","","","","","","IEEE","IEEE Journals & Magazines"
"On rigorous transaction scheduling","Y. Breitbart; D. Georgakopoulos; M. Rusinkiewicz; A. Silberschatz","Dept. of Comput. Sci., Kentucky Univ., Lexington, KY, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1991","17","9","954","960","The class of transaction scheduling mechanisms in which the transaction serialization order can be determined by controlling their commitment order, is defined. This class of transaction management mechanisms is important, because it simplifies transaction management in a multidatabase system environment. The notion of analogous execution and serialization orders of transactions is defined and the concept of strongly recoverable and rigorous execution schedules is introduced. It is then proven that rigorous schedulers always produce analogous execution and serialization orders. It is shown that the systems using the rigorous scheduling can be naturally incorporated in hierarchical transaction management mechanisms. It is proven that several previously proposed multidatabase transaction management mechanisms guarantee global serializability only if all participating databases systems produce rigorous schedules.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92915","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92915","","Scheduling;Database systems;Control systems;Transaction databases;Environmental management;Concurrency control;Helium;Delay;Image databases","concurrency control;database theory;distributed databases;scheduling;transaction processing","rigorous transaction scheduling;transaction scheduling mechanisms;transaction serialization order;commitment order;multidatabase system environment;analogous execution;serialization orders;rigorous schedulers;hierarchical transaction management mechanisms;global serializability","","65","","16","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Educational Background on the Effectiveness of Requirements Inspections: An Empirical Study","J. C. Carver; N. Nagappan; A. Page","The University of Alabama, Tuscaloosa; Microsoft Research, Redmond; Microsoft Research, Redmond","IEEE Transactions on Software Engineering","","2008","34","6","800","812","While the inspection of various software artifacts increases the quality of the end product, the effectiveness of an inspection depends largely on the individual inspectors involved. To address that issue, a large-scale controlled inspection experiment with over 70 professionals was conducted at Microsoft Corporation that focused on the relationship between an inspector's background and their effectiveness during a requirements inspection. The results of the study showed that inspectors with university degrees in majors not related to computer science found significantly more defects than those with degrees in computer science majors. We also observed that level of education (Masters, PhD), prior industrial experience or other job related experiences did not significantly impact the effectiveness of an inspector. The only other type of experience that had a significant impact on effectiveness was experience in writing requirements, i.e. professionals with prior experience writing requirements found statistically significant more defects than their counterparts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4564472","Requirements/Specifications;Software Quality/SQA;Metrics/Measurement;Requirements/Specifications;Software Quality/SQA;Metrics/Measurement","Inspection;Software quality;Computer science;Writing;Bioreactors;Large-scale systems;Computer science education;Software measurement;Programming","formal specification;software quality","software quality;software artifacts;educational background;requirements inspections","","18","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A Formal and Tool-Equipped Approach for the Integration of State Diagrams and Formal Datatypes","C. Attiogbe; P. Poizat; G. Salaun","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","3","157","170","Separation of concerns or aspects is a way to deal with the increasing complexity of systems. The separate design of models for different aspects also promotes a better reusability level. However, an important issue is then to define means to integrate them into a global model. We present a formal and tool-equipped approach for the integration of dynamic models (behaviors expressed using state diagrams) and static models (formal data types) with the benefit to share advantages of both: graphical user-friendly models for behaviors, formal and abstract models for data types. Integration is achieved in a generic way so that it can deal with both different static specification languages (algebraic specifications, Z, B) and different dynamic specification semantics","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084134","Formal methods;languages;integrated environments;state diagrams;specification techniques;operational semantics;tools.","Specification languages;Proposals;Unified modeling language;Software architecture;Concurrent computing;Communication system control;Software engineering;Formal languages","algebraic specification;formal languages;formal verification;programming language semantics;specification languages;visual languages","formal approach;tool-equipped approach;state diagram;formal datatypes;reusability level;graphical user-friendly model;static specification language;algebraic specification;dynamic specification semantics","","4","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Guided Mutation Testing for JavaScript Web Applications","S. Mirshokraie; A. Mesbah; K. Pattabiraman","Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC, Canada","IEEE Transactions on Software Engineering","","2015","41","5","429","444","Mutation testing is an effective test adequacy assessment technique. However, there is a high computational cost in executing the test suite against a potentially large pool of generated mutants. Moreover, there is much effort involved in filtering out equivalent mutants. Prior work has mainly focused on detecting equivalent mutants after the mutation generation phase, which is computationally expensive and has limited efficiency. We propose an algorithm to select variables and branches for mutation as well as a metric, called FunctionRank, to rank functions according to their relative importance from the application's behaviour point of view. We present a technique that leverages static and dynamic analysis to guide the mutation generation process towards parts of the code that are more likely to influence the program's output. Further, we focus on the JavaScript language, and propose a set of mutation operators that are specific to Web applications. We implement our approach in a tool called MUTANDIS. The results of our empirical evaluation show that (1) more than 93 percent of generated mutants are non-equivalent, and (2) more than 75 percent of the surviving non-equivalent mutants are in the top 30 percent of the ranked functions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2371458","NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6960094","mutation testing;JavaScript;equivalent mutants;guided mutation generation;web applications;Mutation testing;JavaScript;equivalent mutants;guided mutation generation;web applications","Testing;Measurement;Heuristic algorithms;Complexity theory;Performance analysis;Instruments;IEEE Computer Society","Java;program diagnostics;program testing","guided mutation testing;JavaScript Web applications;test adequacy assessment technique;computational cost;test suite execution;equivalent mutants;mutation generation phase;variable selection;FunctionRank;function ranking;relative function importance;application behaviour;static analysis;dynamic analysis;program output;mutation operators;nonequivalent mutants;empirical evaluation;MUTANDIS tool;Web applications","","5","","50","","","","","","IEEE","IEEE Journals & Magazines"
"On the Use of an Extended Relational Model to Handle Changing Incomplete Information","A. M. Keller; M. W. Wilkins","Department of Computer Science, Stanford University; NA","IEEE Transactions on Software Engineering","","1985","SE-11","7","620","633","In this paper we consider approaches to updating databases containing null values and incomplete information. Our approach distinguishes between modeling incompletely known worlds and modeling changes in these worlds. As an alternative to the open and closed world assumptions, we propose the expanded closed world assumption. Under this assumption, we discuss how to perform updates on databases containing set nulls, marked nulls, and simple conditional tuples, and address some issues of refining incompletely specified information.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232506","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702066","Databases;incomplete information;null values;relational databases;updates","Relational databases;Economic forecasting;Contracts;Indexes;Algorithm design and analysis;Scholarships;Computer science;Privacy;Data security","","Databases;incomplete information;null values;relational databases;updates","","7","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Timestamp-based orphan elimination","M. P. Herlihy; M. S. McKendry","Dept. of Comput. Sci., Carnegie-Mellon Univ., Pittsburgh, PA, USA; NA","IEEE Transactions on Software Engineering","","1989","15","7","825","831","An orphan in a distributed transaction system is an activity executing on behalf of an aborted transaction. A method is proposed for managing orphans created by crashes and by aborts that ensures that orphans are detected and eliminated in a timely manner, and also prevents them from observing inconsistent states. The method uses timestamps generated at each site. Transactions are assigned timeouts at different sites. These timeouts are related by a global invariant, and they may be adjusted by simple two-phase protocols. The principal advantage of this method is simplicity: it is easy to understand, and to implement, and it can be proved correct. An 'eager' version of this method uses approximately synchronized real-time clocks to ensure that orphans are eliminated within a fixed duration, and a 'lazy' version uses logical clocks to ensure that orphans are eventually eliminated as information propagates through the system. The method is fail-safe: unsynchronized clocks and lost messages may affect performance, but they cannot produce inconsistencies or protect orphans from eventual elimination. Although the method is informally described in terms of two-phase locking, the formal argument shows it is applicable to any concurrency control method that preserved atomicity.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.29482","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=29482","","Computer crashes;Clocks;Synchronization;Real time systems;Protection;Computer networks;Distributed computing;Concurrent computing;Banking;Books","concurrency control;database management systems;distributed processing;transaction processing","timestamp based orphan elimination;distributed transaction system;aborted transaction;two-phase protocols;real-time clocks;concurrency control method","","5","","31","","","","","","IEEE","IEEE Journals & Magazines"
"VT-Revolution: Interactive Programming Video Tutorial Authoring and Watching System","L. Bao; Z. Xing; X. Xia; D. Lo","College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang China (e-mail: lingfengbao@zju.edu.cn); Research School of Computer Science, Australian National University, Canberra, Australian Capital Territory Australia (e-mail: zhenchang.xing@anu.edu.au); Faculty of Information Tehnoology, Monash University, Melbourne, Victoria Australia (e-mail: xxkidd@zju.edu.cn); School of Information Systems, Singapore Management University, Singapore, Singapore Singapore 17890 (e-mail: davidlo@smu.edu.sg)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Procedural knowledge describes actions and manipulations that are carried out to complete programming tasks. An effective way to document procedural knowledge is programming video tutorials. Existing solutions to adding interactive workflow and elements to programming videos have a dilemma between the level of desired interaction and the efforts required for authoring tutorials. In this work, we tackle this dilemma by designing and building a programming video tutorial authoring system that leverages operating system level instrumentation to log workflow history while tutorial authors are creating programming videos, and the corresponding tutorial watching system that enhances the learning experience of video tutorials by providing programming-specific workflow history and timeline-based browsing interactions. Our tutorial authoring system does not incur any additional burden on tutorial authors to make programming videos interactive. Given a programming video accompanied by synchronously-logged workflow history, our tutorial watching system allows tutorial watchers to freely explore the captured workflows and interact with files, code and program output in the tutorial. We conduct a user study of 135 developers to evaluate the design and effectiveness of our system in helping developers learn programming knowledge in video tutorials","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2802916","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8283605","Program Comprehension;Human-Computer Interaction;Workflow","Tutorials;Programming;Streaming media;Tools;Task analysis;History;Software","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Modeling Human-in-the-Loop Security Analysis and Decision-Making Processes","M. A. Schumann; D. Drusinsky; J. B. Michael; D. Wijesekera","KEYW Corporation, 7740 Milestone Pkwy, Suite 400, Hanover; Department of Computer Science , Naval Postgraduate School, Monterey; Departments of Computer Science and Electrical & Computer Engineering, Naval Postgraduate School, 900 N Glebe Road, Arlington; Department of Computer Science , George Mason University, Fairfax","IEEE Transactions on Software Engineering","","2014","40","2","154","166","This paper presents a novel application of computer-assisted formal methods for systematically specifying, documenting, statically and dynamically checking, and maintaining human-centered workflow processes. This approach provides for end-to-end verification and validation of process workflows, which is needed for process workflows that are intended for use in developing and maintaining high-integrity systems. We demonstrate the technical feasibility of our approach by applying it on the development of the US government's process workflow for implementing, certifying, and accrediting cross-domain computer security solutions. Our approach involves identifying human-in-the-loop decision points in the process activities and then modeling these via statechart assertions. We developed techniques to specify and enforce workflow hierarchies, which was a challenge due to the existence of concurrent activities within complex workflow processes. Some of the key advantages of our approach are: it results in development of a model that is executable, supporting both upfront and runtime checking of process-workflow requirements; aids comprehension and communication among stakeholders and process engineers; and provides for incorporating accountability and risk management into the engineering of process workflows.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2302433","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6727512","Formal methods;information assurance;process modeling;software engineering;statechart assertions;verification and validation","Unified modeling language;Object oriented modeling;Software;Runtime;Formal specifications;Analytical models;Business","decision making;formal specification;formal verification;government data processing;security of data;workflow management software","human-in-the-loop security analysis;decision-making process;computer-assisted formal methods;human-centered workflow process;process specification;process documentation;process statically checking;process dynamically checking;process maintenance;end-to-end verification;end-to-end validation;high-integrity systems;US government process workflow;United States;cross-domain computer security solutions;human-in-the-loop decision points;process activities;statechart assertions;workflow hierarchies;accountability;risk management;process workflows engineering","","1","","44","","","","","","IEEE","IEEE Journals & Magazines"
"A deep learning model for estimating story points","M. Choetkiertikul; H. K. Dam; T. Tran; T. T. M. Pham; A. Ghose; T. Menzies","School of Computing and Information Technology, University of Wollongong, 8691 Wollongong, New South Wales Australia 2522 (e-mail: morakotch@gmail.com); School of Computer Science and Software Engineering, University of Wollongong, Wollongong, New South Wales Australia 2522 (e-mail: hoa@uow.edu.au); Centre for Pattern Recognition and Data Analytics, Deakin University, Waurn Ponds, Victoria Australia 3216 (e-mail: truyen.tran@deakin.edu.au); 434468, Deakin University, Waurn Ponds, Victoria Australia 3216 (e-mail: phtra@deakin.edu.au); School of Computing and Information Technology, University of Wollongong, 8691 Wollongong, New South Wales Australia (e-mail: aditya@uow.edu.au); Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, West Virginia United States 26501 (e-mail: tim@menzies.us)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Although there has been substantial research in software analytics for effort estimation in traditional software projects, little work has been done for estimation in agile projects, especially estimating the effort required for completing user stories or issues. Story points are the most common unit of measure used for estimating the effort involved in completing a user story or resolving an issue. In this paper, we propose a prediction model for estimating story points based on a novel combination of two powerful deep learning architectures: long short-term memory and recurrent highway network. Our prediction system is end-to-end trainable from raw input data to prediction outcomes without any manual feature engineering. We offer a comprehensive dataset for story points-based estimation that contains 23,313 issues from 16 open source projects. An empirical evaluation demonstrates that our approach consistently outperforms three common baselines (Random Guessing, Mean, and Median methods) and six alternatives (e.g. using Doc2Vec and Random Forests) in Mean Absolute Error, Median Absolute Error, and the Standardized Accuracy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2792473","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8255666","software analytics;effort estimation;story point estimation;deep learning","Estimation;Software;Planning;Machine learning;Predictive models;Electronic mail;Springs","","","","4","","","","","","","","IEEE","IEEE Early Access Articles"
"Comparison and Evaluation of Clone Detection Tools","S. Bellon; R. Koschke; G. Antoniol; J. Krinke; E. Merlo","Axivion GmbH, Nobelstr. 15, 70569 Stuttgart, Germany; Universitat Bremen, Fachbereich 03, Postfach 33 04 40, 28334 Bremen, Germany; Departement de Genie Informatique, Ecole Polytechnique de Montreal. Pavillons Lassonde, Mac-Lassonde, 2500, chemin de Polytechnique, Montreal (Quebec), Canada, H3T 1J4; Fern-Universitat in Hagen, Universitatsstr. 27, 58097 Hagen, Germany; Department of Computer Engineering, Ecole Polytechnique of Montreal, PO Box 6079, Station Downtown, Montreal (Quebec), Canada, H3C 3A7","IEEE Transactions on Software Engineering","","2007","33","9","577","591","Many techniques for detecting duplicated source code (software clones) have been proposed in the past. However, it is not yet clear how these techniques compare in terms of recall and precision as well as space and time requirements. This paper presents an experiment that evaluates six clone detectors based on eight large C and Java programs (altogether almost 850 KLOC). Their clone candidates were evaluated by one of the authors as an independent third party. The selected techniques cover the whole spectrum of the state-of-the-art in clone detection. The techniques work on text, lexical and syntactic information, software metrics, and program dependency graphs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70725","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288192","Redundant code;duplicated code;software clones","Cloning;Computer Society;Detectors;Java;Software metrics;Concrete;Software tools;Visualization;Fingerprint recognition;Abstracts","program diagnostics;program verification;software metrics","clone detection tool;duplicated source code detection;text information;lexical information;syntactic information;software metrics;program dependency graphs","","263","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Black-Box String Test Case Generation through a Multi-Objective Optimization","A. Shahbazi; J. Miller","Department of Electrical and Computer Engineering, Edmonton, AB, Canada; Department of Electrical and Computer Engineering, Edmonton, AB, Canada","IEEE Transactions on Software Engineering","","2016","42","4","361","378","String test cases are required by many real-world applications to identify defects and security risks. Random Testing (RT) is a low cost and easy to implement testing approach to generate strings. However, its effectiveness is not satisfactory. In this research, black-box string test case generation methods are investigated. Two objective functions are introduced to produce effective test cases. The diversity of the test cases is the first objective, where it can be measured through string distance functions. The second objective is guiding the string length distribution into a Benford distribution based on the hypothesis that the population of strings is right-skewed within its range. When both objectives are applied via a multi-objective optimization algorithm, superior string test sets are produced. An empirical study is performed with several real-world programs indicating that the generated string test cases outperform test cases generated by other methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2487958","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7293669","Adaptive random testing;automated test case generation;black-box testing;mutation;random testing;software testing;string distance;string test cases;Adaptive random testing;automated test case generation;black-box testing;mutation;random testing;software testing;string distance;string test cases","Sociology;Statistics;Biological cells;Subspace constraints;Testing;Power capacitors;Genetic algorithms","optimisation;program testing;security of data","black-box string test case generation;security risks;random testing;RT;objective functions;string distance functions;Benford distribution;multiobjective optimization algorithm","","10","","76","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing the evolutionary history of the logical design of object-oriented software","Z. Xing; E. Stroulia","Dept. of Comput. Sci., Alberta Univ., Edmonton, Alta., Canada; Dept. of Comput. Sci., Alberta Univ., Edmonton, Alta., Canada","IEEE Transactions on Software Engineering","","2005","31","10","850","868","Today, most object-oriented software systems are developed using an evolutionary process model. Therefore, understanding the phases that the system's logical design has gone through and the style of their evolution can provide valuable insights in support of consistently maintaining and evolving the system, without compromising the integrity and stability of its architecture. In this paper, we present a method for analyzing the evolution of object-oriented software systems from the point of view of their logical design. This method relies on UMLDiff, a UML-structure differencing algorithm, which, given a sequence of UML class models corresponding to the logical design of a sequence of system code releases, produces a sequence of ""change records"" that describe the design-level changes between subsequent system releases. This change-records sequence is subsequently analyzed from the perspective of each individual system class, to produce the class-evolution profile, i.e., a class-specific change-records' sequence. Three types of longitudinal analyses - phasic, gamma, and optimal matching analysis - are applied to the class-evolution profiles to recover a high-level abstraction of distinct evolutionary phases and their corresponding styles and to identify class clusters with similar evolution trajectories. The recovered knowledge facilitates the overall understanding of system evolution and the planning of future maintenance activities. We report on one real-world case study evaluating our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.106","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542067","Index Terms- Restructuring;reverse engineering;and reengineering.","History;Software design;Software systems;Object oriented modeling;Algorithm design and analysis;Surges;Quality management;Project management;Visualization;Documentation","object-oriented programming;object-oriented methods;Unified Modeling Language;software maintenance;software architecture;reverse engineering","object-oriented software system;evolutionary process model;system logical design;UMLDiff;UML-structure differencing algorithm;phasic analysis;gamma analysis;optimal matching analysis;software maintenance","","39","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic storage fragmentation and file deterioration","C. H. C. Leung","Department of Computer Science, University College, London University, London WC1E 6BT, England","IEEE Transactions on Software Engineering","","1986","SE-12","3","436","441","As a result of insertions and deletions, a file tends to be cluttered with deleted records which are physically present. These unwanted records cause fragmentation within the file and give rise to additional access overhead because they have to be skipped over during processing. A connection between the dynamic fragmentation characteristics and the pattern of record insertions and deletions over time is presented, and performance degradation is studied in terms of the number of record accesses per reference. Deterioration characteristics are obtained for nonhomogeneous Poisson insertion and general deletion processes. For constant insertion rate, it is found that the deterioration over time is asymptotically linear, with the rate of decline governed by the record deletion rate. An expression for the optimum compaction interval is also given for files subject to a constant insertion rate and an exponentially distributed record lifetime.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312884","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312884","File compaction;fragmentation;performance deterioration;reorganization;volatile files","Compaction;Writing;Degradation;Linear approximation;File systems;Steady-state","storage management","software engineering;storage fragmentation;file deterioration;insertions;deletions;access overhead;dynamic fragmentation;record insertions;performance degradation;nonhomogeneous Poisson insertion;optimum compaction interval;exponentially distributed record lifetime","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Software productivity measurement using multiple size measures","B. Kitchenham; E. Mendes","Nat. ICT Australia, Alexandria, NSW, Australia; NA","IEEE Transactions on Software Engineering","","2004","30","12","1023","1035","Productivity measures based on a simple ratio of product size to project effort assume that size can be determined as a single measure. If there are many possible size measures in a data set and no obvious model for aggregating the measures into a single measure, we propose using the expression AdjustedSize/Effort to measure productivity. AdjustedSize is defined as the most appropriate regression-based effort estimation model, where all the size measures selected for inclusion in the estimation model have a regression parameter significantly different from zero (p<0.05). This productivity measurement method ensures that each project has an expected productivity value of one. Values between zero and one indicate lower than expected productivity, values greater than one indicate higher than expected productivity. We discuss the assumptions underlying this productivity measurement method and present an example of its use for Web application projects. We also explain the relationship between effort prediction models and productivity models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.104","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377195","Index Terms- Software productivity measurement;software cost estimation.","Size measurement;Software measurement;Productivity;Predictive models;Costs;Equations;Computer science;Computer Society;Application software;Production","software cost estimation;software metrics;productivity;project management;regression analysis","software productivity measurement;software cost estimation;product size;project effort;regression-based effort estimation;parameter estimation","","63","","27","","","","","","IEEE","IEEE Journals & Magazines"
"LISPACK-a methodology and tool for the performance analysis of parallel systems and algorithms","G. Iazeolla; F. Marinuzzi","Dept. of Elecron. Eng., Rome Univ., Italy; NA","IEEE Transactions on Software Engineering","","1993","19","5","486","502","The performance analysis of parallel algorithms and systems is considered. For these, numerical solutions methods quickly show their limits because of the enormous state-space growth. The proposed methodology and software tool, list-manipulation parallel-modeling package (LISPACK) uses string manipulation, lumping, and recursive elimination to define the large Markovian process, its restructuring, and efficient solution. The analysis of a typical parallel system and algorithm model is developed as a case study, to discuss the features of the method. The paper has two contributions. The first is the symbolic-approach methodology proposed for the performance analysis of parallel algorithms and systems. The second is a tool that exploits the capabilities of the symbolic approach in the solution of parallel models, where the numerical techniques reveal their limits.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232014","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232014","","Performance analysis;Parallel algorithms;Software tools;Algorithm design and analysis;Software packages;Packaging;Gaussian processes;Costs;Numerical models;Supercomputers","Markov processes;parallel algorithms;parallel processing;performance evaluation;software tools","parallel algorithms;LISPACK;performance analysis;parallel systems;software tool;list-manipulation parallel-modeling package;string manipulation;lumping;recursive elimination;large Markovian process;symbolic-approach methodology","","","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Systematic reliability analysis of a class of application-specific embedded software framework","Sung Kim; F. B. Bastani; I-Ling Yen; I. -. Chen","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; NA","IEEE Transactions on Software Engineering","","2004","30","4","218","230","Dramatic advances in computer and communication technologies have made it economically feasible to extend the use of embedded computer systems to more and more critical applications. At the same time, these embedded computer systems are becoming more complex and distributed. As the bulk of the complex application-specific logic of these systems is realized by software, the need for certifying software systems has grown substantially. While relatively mature techniques exist for certifying hardware systems, methods of rigorously certifying software systems are still being actively researched. Possible certification methods for embedded software systems range from formal verification to statistical testing. These methods have different strengths and weaknesses and can be used to complement each other. One potentially useful approach is to decompose the specification into distinct aspects that can be independently certified using the method that is most effective for it. Even though substantial-research has been carried out to reduce the complexity of the software system through decomposition, one major hurdle is the need to certify the overall system on the basis of the aspect properties. One way to address this issue is to focus on architectures in which the aspects are relatively independent of each other. However, complex embedded systems are typically comprised of multiple architectures. We present an alternative approach based on the use of application-oriented-frameworks for implementing embedded systems. We show that it is possible to design such frameworks for embedded applications and derive expressions for determining the system reliability from the reliabilities of the framework and the aspects. The method is illustrated using a distributed multimedia collaboration system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274042","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274042","","Embedded software;Embedded computing;Software systems;Embedded system;Communications technology;Application software;Distributed computing;Logic;Hardware;Certification","embedded systems;formal verification;formal specification;software reliability;software metrics;statistical testing;software architecture;object-oriented methods","software reliability analysis;application-specific embedded software framework;software systems;hardware systems;formal verification;statistical testing;software complexity;software architectures;distributed multimedia collaboration system;software composition","","3","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal transfer trees and distinguishing trees for testing observable nondeterministic finite-state machines","Fan Zhang; To-yat Cheung","Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China; NA","IEEE Transactions on Software Engineering","","2003","29","1","1","14","The fault-state detection approach for blackbox testing consists of two phases. The first is to bring the system under test (SUT) from its initial state to a targeted state t and the second is to check various specified properties of the SUT at t. This paper investigates the first phase for testing systems specified as observable nondeterministic finite-state machines with probabilistic and weighted transitions. This phase involves two steps. The first step transfers the SUT to some state t' and the second step identifies whether t' is indeed the targeted state t or not. State transfer is achieved by moving the SUT along one of the paths of a transfer tree (TT) and state identification is realized by using diagnosis trees (DT). A theoretical foundation for the existence and characterization of TT and DT with minimum weighted height or minimum average weight is presented. Algorithms for their computation are proposed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166585","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166585","","System testing;Software testing;Fault detection;Software systems;Computer Society;Design methodology;Automata;Nonhomogeneous media","finite state machines;program testing;optimisation;trees (mathematics)","optimal transfer trees;distinguishing trees;observable nondeterministic finite-state machine testing;fault-state detection;blackbox testing;probabilistic transitions;weighted transitions;TT;DT;diagnosis trees","","22","","24","","","","","","IEEE","IEEE Journals & Magazines"
"A development environment for horizontal microcode","A. Aiken; A. Nicolau","Dept. of Comput. Sci., Cornell Univ., Ithaca, NY, USA; Dept. of Comput. Sci., Cornell Univ., Ithaca, NY, USA","IEEE Transactions on Software Engineering","","1988","14","5","584","594","A development environment for horizontal microcode is described that uses percolation scheduling-a transformational system for parallelism extraction-and an interactive profiling system to give the user control over the microcode compaction process while reducing the burdensome details of architecture, correctness preservation, and synchronization. Through a graphical interface, the user suggests what can be executed in parallel, while the system performs the actual changes using semantics-preserving transformations. If a request cannot be satisfied, the system reports the problem causing the failure. The user can then help eliminate the problem by supplying guidance or information not explicit in the code.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6136","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6136","","Parallel processing;Humans;Compaction;High level languages;Control systems;Graphical user interfaces;Information analysis;Program processors;Computer science;Reduced instruction set computing","microprogramming;parallel programming;programming environments;scheduling;synchronisation;user interfaces","development environment;horizontal microcode;percolation scheduling;parallelism extraction;interactive profiling system;microcode compaction process;architecture;correctness preservation;synchronization;graphical interface;semantics-preserving transformations","","38","","23","","","","","","IEEE","IEEE Journals & Magazines"
"A Test Case Prioritization Genetic Algorithm guided by the Hypervolume Indicator","D. Di Nucci; A. Panichella; A. Zaidman; A. De Lucia","Department of Computer Science, Vrije Universiteit Brussel, 70493 Brussels, Brussels Belgium (e-mail: dario.di.nucci@vub.be); Software Engineering Research Group, Technische Universiteit Delft, 2860 Delft, Zuid-Holland Netherlands (e-mail: anni.panico@gmail.com); Software Engineering Research Group, Delft University of Technology, Delft, Zuid Holland Netherlands (e-mail: a.e.zaidman@tudelft.nl); Department of Computer Science, University of Salerno, Fisciano, Salerno Italy (e-mail: adelucia@unisa.it)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Regression testing is performed during maintenance activities to assess whether the unchanged parts of a software behave as intended. To reduce its cost, test case prioritization techniques can be used to schedule the execution of the available test cases to increase their ability to reveal regression faults earlier. Optimal test ordering can be determined using various techniques, such as greedy algorithms and meta-heuristics, and optimizing multiple fitness functions, such as the average percentage of statement and branch coverage. These fitness functions condense the cumulative coverage scores achieved when incrementally running test cases in a given ordering using Area Under Curve (AUC) metrics. In this paper, we notice that AUC metrics represent a bi-dimensional (simplified) version of the hypervolume metric, which is widely used in many-objective optimization. Thus, we propose a Hypervolume-based Genetic Algorithm, namely HGA, to solve the Test Case Prioritization problem when using multiple test coverage criteria. An empirical study conducted with respect to five state-of-the-art techniques shows that (i) HGA is more cost-effective, (ii) HGA improves the efficiency of Test Case Prioritization, (iii) HGA has a stronger selective pressure when dealing with more than three criteria.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2868082","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8453036","Test Case Prioritization;Genetic Algorithm;Hypervolume","Measurement;Greedy algorithms;Genetic algorithms;Testing;Software systems;Fault detection","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A pessimistic consistency control algorithm for replicated files which achieves high availability","S. Jajodia; D. Mutchler","US Naval Res. Lab., Washington, DC, USA; US Naval Res. Lab., Washington, DC, USA","IEEE Transactions on Software Engineering","","1989","15","1","39","46","A consistency control algorithm is described for managing replicated files in the face of network partitioning due to node or communication link failures. It adopts a pessimistic approach in that mutual consistency among copies of a file is maintained by permitting files to be accessed only in a single partition at any given time. The algorithm simplifies the Davcev-Burkhard dynamic voting algorithm (1985) and also improves its availability by adding the notion of linearly ordered copies. A proof that any pessimistic algorithm with fresh reads is one-copy serializable is given.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21724","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21724","","Availability;Partitioning algorithms;Communication system control;Voting;Heuristic algorithms;Laboratories;Distributed databases;Protocols;Information systems;Computer science","concurrency control;data integrity;distributed databases","pessimistic consistency control algorithm;replicated files;high availability;network partitioning;communication link failures;mutual consistency;dynamic voting algorithm;fresh reads;one-copy serializable","","16","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Region scheduling: an approach for detecting and redistributing parallelism","R. Gupta; M. L. Soffa","Philips Lab., Briarcliff Manor, NY, USA; NA","IEEE Transactions on Software Engineering","","1990","16","4","421","431","Region scheduling, a technique applicable to both fine-grain and coarse-grain parallelism, uses a program representation that divides a program into regions consisting of source and intermediate level statements and permits the expression of both data and control dependencies. Guided by estimates of the parallelism present in regions, the region scheduler redistributes code, thus providing opportunities for parallelism in those regions containing insufficient parallelism compared to the capabilities of the executing architecture. The program representation and the transformations are applicable to both structured and unstructured programs, making region scheduling useful for a wide range of applications. The results of experiments conducted using the technique in the generation of code for a reconfigurable long instruction word architecture are presented. The advantages of region scheduling over trace scheduling are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.54294","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=54294","","Program processors;Processor scheduling","parallel programming;program compilers;scheduling","region scheduling;code generation;redistributing parallelism;fine-grain;coarse-grain parallelism;program representation;reconfigurable long instruction word architecture;trace scheduling","","50","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Effective analysis for engineering real-time fixed priority schedulers","A. Burns; K. Tindell; A. Wellings","Dept. of Comput. Sci., York Univ., UK; NA; NA","IEEE Transactions on Software Engineering","","1995","21","5","475","480","There has been considerable activity in recent years in developing analytical techniques for hard real-time systems. Inevitably these techniques make simplifying assumptions so as to reduce the complexity of the problem to be solved. Unfortunately this leads to a gap between theory and engineering practice. The paper presents new analysis that enables the costs of the scheduler (clock overheads, queue manipulations and release delays) to be factored into the standard equations for calculating worst-case response times. As well as predicting the true behavior of realistic systems, the analysis also allows free parameters, such as clock interrupt rate, to be determined.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.387477","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=387477","","Distributed computing;Queueing analysis;Real time systems;Delay;Application software;Information science;Clocks;Aerospace engineering;Timing;Traffic control","real-time systems;processor scheduling;scheduling;operating system kernels;system monitoring;systems re-engineering;systems engineering;systems analysis","real-time fixed priority scheduler engineering;effective analysis;hard real-time systems;scheduler costs;clock overheads;queue manipulations;release delays;standard equations;worst-case response times;true behavior;realistic systems;free parameters;clock interrupt rate","","81","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Validation of Three Software Metrics Suites to Predict Fault-Proneness of Object-Oriented Classes Developed Using Highly Iterative or Agile Software Development Processes","H. M. Olague; L. H. Etzkorn; S. Gholston; S. Quattlebaum","US Army Space and Missile Defense Command, SMDC-RDTI-S, Huntsville, AL; Computer Science Department, University of Alabama in Huntsville, Huntsville, AL; Industrial and Systems Engineering and Engineering Management Department, University of Alabama in Huntsville, Huntsville, AL; Dragonfly Athletics, Hartselle, AL","IEEE Transactions on Software Engineering","","2007","33","6","402","419","Empirical validation of software metrics suites to predict fault proneness in object-oriented (OO) components is essential to ensure their practical use in industrial settings. In this paper, we empirically validate three OO metrics suites for their ability to predict software quality in terms of fault-proneness: the Chidamber and Kemerer (CK) metrics, Abreu's Metrics for Object-Oriented Design (MOOD), and Bansiya and Davis' Quality Metrics for Object-Oriented Design (QMOOD). Some CK class metrics have previously been shown to be good predictors of initial OO software quality. However, the other two suites have not been heavily validated except by their original proposers. Here, we explore the ability of these three metrics suites to predict fault-prone classes using defect data for six versions of Rhino, an open-source implementation of JavaScript written in Java. We conclude that the CK and QMOOD suites contain similar components and produce statistical models that are effective in detecting error-prone classes. We also conclude that the class components in the MOOD metrics suite are not good class fault-proneness predictors. Analyzing multivariate binary logistic regression models across six Rhino versions indicates these models may be useful in assessing quality in OO classes produced using modern highly iterative or agile software development processes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1015","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181709","Object-oriented software metrics;object-oriented metrics;software quality metrics;software maintenance programming;software reuse.","Software metrics;Programming;Software quality;Object oriented modeling;Mood;Java;Software maintenance;Computer industry;Open source software;Logistics","Java;object-oriented programming;regression analysis;software metrics;software quality","software metrics;fault-prone prediction;object-oriented class;iterative software development process;agile software development process;software quality;object-oriented design;JavaScript;statistical analysis;multivariate binary logistic regression model","","130","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Exception handling in the spreadsheet paradigm","M. Burnett; A. Agrawal; P. van Zee","Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","10","923","942","Exception handling is widely regarded as a necessity in programming languages today and almost every programming language currently used for professional software development supports some form of it. However, spreadsheet systems, which may be the most widely used type of ""programming language"" today in terms of number of users using it to create ""programs"" (spreadsheets), have traditionally had only extremely limited support for exception handling. Spreadsheet system users range from end users to professional programmers and this wide range suggests that an approach to exception handling for spreadsheet systems needs to be compatible with the equational reasoning model of spreadsheet formulas, yet feature expressive power comparable to that found in other programming languages. We present an approach to exception handling for spreadsheet system users that is aimed at this goal. Some of the features of the approach are new; others are not new, but their effects on the programming language properties of spreadsheet systems have not been discussed before in the literature. We explore these properties, offer our solutions to problems that arise with these properties, and compare the functionality of the approach with that of exception handling approaches in other languages.","0098-5589;1939-3520;2326-3881","","10.1109/32.879817","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879817","","Programming profession;Computer languages;Equations;User interfaces;Software engineering;Power system modeling;Robustness;Logic;Control systems;Marketing and sales","exception handling;spreadsheet programs;software engineering","exception handling;programming language;professional software development;spreadsheet systems;end user programming;professional programmers;equational reasoning model","","4","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Computer-mediated group support, anonymity, and the software inspection process: an empirical investigation","P. Vitharana; K. Ramamurthy","Sch. of Manage., Syracuse Univ., NY, USA; NA","IEEE Transactions on Software Engineering","","2003","29","2","167","180","In software inspection, a key principle endorsed by Fagan (1986) is openness. However, scholars have recently questioned the efficacy of openness. For example, some argue that ego-involvement and personality conflicts that become more transparent due to openness might impede inspection. Still others point out that familiarity and (preexisting) relationships among inspection team members negatively affect the comprehensiveness in detection of defects. This brings up concerns if the openness as originally envisioned by Fagan may in fact lead to suboptimal outcomes. As the trend towards computer-based inspection continues, we believe that anonymity could play a positive role in overcoming some of the drawbacks noted in team-based inspection. Drawing upon the literature on software inspection and group support systems, this research proposes possible influences of group member anonymity on the outcome of computer-mediated software inspection and empirically examines the validity of the posited relationships in a set of controlled laboratory experiments. Two different inspection tasks with varying levels of software code complexity are employed. While both the control groups (i.e., teams without anonymity) and treatment groups (i.e., teams with support for anonymity) consume more or less the same time in performing the inspection tasks, the treatment groups are more effective in identifying the seeded errors in the more complex task. Treatment groups also express a more positive attitude toward both code inspection tasks. The findings of the study suggest a number of directions for future research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178054","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178054","","Collaborative software;Inspection;Software quality;Programming;Control systems;Impedance;Laboratories;Error correction;Software systems;Software tools","groupware;software quality;software development management;program testing;software process improvement;human factors","computer-mediated group support;anonymity;software inspection process;ego-involvement;personality conflicts;inspection team members;suboptimal outcomes;computer-based inspection;team-based inspection;software inspection;group support systems;group member anonymity;software code complexity;seeded errors;software quality assurance","","17","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Stochastic Petri net analysis of a replicated file system","J. Bechta Dugan; G. Ciardo","Dept. of Comput. Sci., Duke Univ., Durham, NC, USA; Dept. of Comput. Sci., Duke Univ., Durham, NC, USA","IEEE Transactions on Software Engineering","","1989","15","4","394","401","The authors present a stochastic Petri net model of a replicated file system in a distributed environment where replicated files reside on different hosts and a voting algorithm is used to maintain consistency. Witnesses, which simply record the status of the file but contain no data, can be used in addition to or in place of files to reduce overhead. A model sufficiently detailed to include file status (current or out-of-date) as well as failure and repair of hosts where copies or witnesses reside, is presented. The number of copies and witnesses is not fixed, but is a parameter of the model. Two different majority protocols are examined.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.16600","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=16600","","Stochastic systems;File systems;Voting;Availability;Protocols;Algorithm design and analysis;Protection;Databases;Testing;Costs","concurrency control;distributed databases;fault tolerant computing;Petri nets","performance reliability tradeoffs;stochastic Petri net model;replicated file system;distributed environment;voting algorithm;file status;witnesses;majority protocols","","50","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Software prototyping by relational techniques: experiences with program construction systems","S. Ceri; S. Crespi-Reghizzi; A. Di Maio; L. A. Lavazza","Dept. of Electron., Polytech. of Milan, Italy; Dept. of Electron., Polytech. of Milan, Italy; NA; NA","IEEE Transactions on Software Engineering","","1988","14","11","1597","1609","A method for designing and prototyping program construction systems using relational databases is presented. Relations are the only data structures used inside the systems and for interfaces; programs extensively use relational languages, in particular relational algebra. Two large projects are described. The Ada Relational Translator (ART) is an experimental compiler-interpreter for Ada in which all subsystems, including the parser, semantic analyzer, interpreter, kernel, and debugger, use relations as their only data structure; the relational approach has been pushed to the utmost to achieve fast prototyping in a student environment. Multi-Micro Line (MML) is a tool set for constructing programs for multimicroprocessors' targets, in which relations are used for allocation and configuration control. Both experiences validate the approach for managing teamwork in evolving projects, identify areas where this approach is appropriate, and raise critical issues.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9048","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9048","","Software prototyping;Prototypes;Data structures;Design methodology;Relational databases;Algebra;Subspace constraints;Kernel;Project management;Teamwork","Ada;data structures;program compilers;program interpreters;programming environments;relational databases","programming environments;program construction systems;prototyping;relational databases;data structures;relational languages;relational algebra;Ada Relational Translator;ART;compiler-interpreter;parser;semantic analyzer;interpreter;kernel;debugger;Multi-Micro Line;MML;configuration control","","2","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Informal and formal requirements specification languages: bridging the gap","M. D. Fraser; K. Kumar; V. K. Vaishnavi","Georgia State Univ., Atlanta, GA, USA; Georgia State Univ., Atlanta, GA, USA; Georgia State Univ., Atlanta, GA, USA","IEEE Transactions on Software Engineering","","1991","17","5","454","466","The differences between informal and formal requirements specification languages are noted, and the issue of bridging the gap between them is discussed. Using structured analysis (SA) and the Vienna development method (VDM) as surrogates for informal and formal languages, respectively, two approaches are presented for integrating the two. The first approach uses the SA model of a system to guide the analyst's understanding of the system and the development of the VDM specifications. The second approach proposes a rule-based method for generating VDM specifications from a set of corresponding SA specifications. The two approaches are illustrated through a simplified payroll system case. The issues that emerge from the use of the two approaches are reported.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90448","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=90448","","Specification languages;Design engineering;Information systems;Information analysis;Mathematics;Computer science;Encoding;Process design;Software systems;Systems engineering and theory","formal languages;formal specification;payroll data processing;specification languages;systems analysis","informal languages;requirements specification languages;structured analysis;Vienna development method;VDM;formal languages;rule-based method;payroll system","","66","","46","","","","","","IEEE","IEEE Journals & Magazines"
"PICQUERY: a high level query language for pictorial database management","T. Joseph; A. F. Cardenas","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA; Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA","IEEE Transactions on Software Engineering","","1988","14","5","630","638","A reasonably comprehensive set of data accessing and manipulation operations that should be supported by a generalized pictorial database management system (PDBMS) is proposed. A corresponding high-level query language, PICQUERY, is presented and illustrated through examples. PICQUERY has been designed with a flavor similar to QBE as the highly nonprocedural and conservational language for the pictorial database management system PICDMS. PICQUERY and a relational QBE-like language would form the language by which a user could access conventional relational databases and at the same time pictorial databases managed by PICDMS or other robust PDBMS. This language interface is part of an architecture aimed toward data heterogeneity transparency over pictorial and nonpictorial databases.<<ETX>></ETX>","0098-5589;1939-3520;2326-3881","","10.1109/32.6140","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6140","","Database languages;Relational databases;Database systems;Robustness;Computer science;Operating systems;Information management;Reconnaissance;Medical services;Biomedical equipment","database management systems;query languages;user interfaces","PICQUERY;high level query language;pictorial database management;conservational language;PICDMS;relational QBE-like language;relational databases;language interface;data heterogeneity transparency","","69","","19","","","","","","IEEE","IEEE Journals & Magazines"
"The design and implementation of a secure auction service","M. K. Franklin; M. K. Reiter","AT&T Bell Labs., Murray Hill, NJ, USA; AT&T Bell Labs., Murray Hill, NJ, USA","IEEE Transactions on Software Engineering","","1996","22","5","302","312","We present the design and implementation of a distributed service for performing sealed bid auctions. This service provides an interface by which clients, or ""bidders"", can issue secret bids to the service for an advertised auction. Once the bidding period has ended, the auction service opens the bids, determines the winning bid, and provides the winning bidder with a ticket for claiming the item bid upon. Using novel cryptographic techniques, the service is constructed to provide strong protection for both the auction house and correct bidders, despite the malicious behavior of any number of bidders and fewer than one third of the servers comprising the auction service. Specifically, it is guaranteed that: bids of correct bidders are not revealed until after the bidding period has ended; the auction house collects payment for the winning bid; losing bidders forfeit no money; and only the winning bidder can collect the item bid upon. We also discuss techniques to enable anonymous bidding.","0098-5589;1939-3520;2326-3881","","10.1109/32.502223","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=502223","","Consumer electronics;Protection;Vehicles;Electronic commerce;Humans;Proposals;Cryptography;Cryptographic protocols;Data security;Ink","retail data processing;computer networks;cryptography","secure auction service;distributed service;sealed bid auctions;clients;secret bids;advertised auction;bidding period;cryptographic techniques;auction house;correct bidders;anonymous bidding;distributed systems;Byzantine failures;electronic commerce;verifiable signature sharing","","116","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Effects of Developer Experience on Learning and Applying Unit Test-Driven Development","R. Latorre","Universidad Autónoma de Madrid, Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","4","381","395","Unit test-driven development (UTDD) is a software development practice where unit test cases are specified iteratively and incrementally before production code. In the last years, researchers have conducted several studies within academia and industry on the effectiveness of this software development practice. They have investigated its utility as compared to other development techniques, focusing mainly on code quality and productivity. This quasi-experiment analyzes the influence of the developers' experience level on the ability to learn and apply UTDD. The ability to apply UTDD is measured in terms of process conformance and development time. From the research point of view, our goal is to evaluate how difficult is learning UTDD by professionals without any prior experience in this technique. From the industrial point of view, the goal is to evaluate the possibility of using this software development practice as an effective solution to take into account in real projects. Our results suggest that skilled developers are able to quickly learn the UTDD concepts and, after practicing them for a short while, become as effective in performing small programming tasks as compared to more traditional test-last development techniques. Junior programmers differ only in their ability to discover the best design, and this translates into a performance penalty since they need to revise their design choices more frequently than senior programmers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.2295827","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6690135","Test-driven development;test-first design;software engineering process;software quality/SQA;software construction;process conformance;programmer productivity","Software;Testing;Training;Programming profession;Context;Production","learning (artificial intelligence);program testing;software quality","learning;unit test-driven development;UTDD;software development;code quality;productivity","","8","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Software complexity and its impact on software reliability","K. S. Lew; T. S. Dillon; K. E. Forward","L.M. Ericsson Pty. Ltd., Broadmeadows, Vic., Australia; NA; NA","IEEE Transactions on Software Engineering","","1988","14","11","1645","1655","To produce reliable software, its complexity must be controlled by suitably decomposing the software system into smaller subsystems. A software complexity metric is developed that includes both the internal and external complexity of a module. This allows analysis of a software system during its development and provides a guide to system decomposition. The basis of this complexity metric is in the development of an external complexity measure that characterizes module interaction.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9052","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9052","","Software reliability;Software systems;Hardware;Computer errors;Redundancy;Fault tolerance;Software measurement;Control systems;Real time systems;Computer science","software reliability","software reliability;software complexity metric;system decomposition","","37","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Generalized stochastic Petri nets: a definition at the net level and its implications","G. Chiola; M. A. Marsan; G. Balbo; G. Conte","Dipartimento di Inf., Torino Univ., Italy; NA; NA; NA","IEEE Transactions on Software Engineering","","1993","19","2","89","107","The class of Petri nets obtained by eliminating timing from generalized stochastic Petri net (GSPN) models while preserving the qualitative behavior is identified. Structural results for those nets are derived, obtaining the first structural analysis of Petri nets with priority and inhibitor arcs. A revision of the GSPN definition based on the structural properties of the models is presented. It is shown that for a (wide) class of nets, the definition of firing probabilities of conflicting immediate transitions does not require the information on reachable markings. Identification of the class of models for which the net-level specification is possible is also based on the structural analysis results. The procedure for the model specification is illustrated by means of an example. It is also shown that a net-level specification of the model associated with efficient structural analysis techniques can have a substantial impact on model analysis.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.214828","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=214828","","Stochastic processes;Petri nets;Power system modeling;Computational modeling;Concurrent computing;Performance analysis;Switches;Distributed computing;Senior members;Proposals","formal specification;performance evaluation;Petri nets;stochastic processes","generalized stochastic Petri net;qualitative behavior;structural analysis;structural properties;firing probabilities;net-level specification;model specification","","132","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Fluid Rewards for a Stochastic Process Algebra","M. Tribastone; J. Ding; S. Gilmore; J. Hillston","Ludwig-Maximilians-Universität, München; Yangzhou University, Yangzhou; Edinburgh University, Edinburgh; Edinburgh University, Edinburgh","IEEE Transactions on Software Engineering","","2012","38","4","861","874","Reasoning about the performance of models of software systems typically entails the derivation of metrics such as throughput, utilization, and response time. If the model is a Markov chain, these are expressed as real functions of the chain, called reward models. The computational complexity of reward-based metrics is of the same order as the solution of the Markov chain, making the analysis infeasible when evaluating large-scale systems. In the context of the stochastic process algebra PEPA, the underlying continuous-time Markov chain has been shown to admit a deterministic (fluid) approximation as a solution of an ordinary differential equation, which effectively circumvents state-space explosion. This paper is concerned with approximating Markovian reward models for PEPA with fluid rewards, i.e., functions of the solution of the differential equation problem. It shows that (1) the Markovian reward models for typical metrics of performance enjoy asymptotic convergence to their fluid analogues, and that (2) via numerical tests, the approximation yields satisfactory accuracy in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.81","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975178","Modeling and prediction;ordinary differential equations;Markov processes","Convergence;Markov processes;Approximation methods;Computational modeling;Mathematical model;Servers","computational complexity;Markov processes;mathematics computing;process algebra;stochastic processes","fluid rewards;stochastic process algebra;software systems;metric derivation;Markov chain;computational complexity;ordinary differential equation;state-space explosion","","15","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Using Genetic Search for Reverse Engineering of Parametric Behavior Models for Performance Prediction","K. Krogmann; M. Kuperberg; R. Reussner","Karlsruhe Institute of Technology (KIT), Karlsruhe; Karlsruhe Institute of Technology (KIT), Karlsruhe; Karlsruhe Institute of Technology (KIT), Karlsruhe","IEEE Transactions on Software Engineering","","2010","36","6","865","877","In component-based software engineering, existing components are often reused in new applications. Correspondingly, the response time of an entire component-based application can be predicted from the execution durations of individual component services. These execution durations depend on the runtime behavior of a component which itself is influenced by three factors: the execution platform, the usage profile, and the component wiring. To cover all relevant combinations of these influencing factors, conventional prediction of response times requires repeated deployment and measurements of component services for all such combinations, incurring a substantial effort. This paper presents a novel comprehensive approach for reverse engineering and performance prediction of components. In it, genetic programming is utilized for reconstructing a behavior model from monitoring data, runtime bytecode counts, and static bytecode analysis. The resulting behavior model is parameterized over all three performance-influencing factors, which are specified separately. This results in significantly fewer measurements: The behavior model is reconstructed only once per component service, and one application-independent bytecode benchmark run is sufficient to characterize an execution platform. To predict the execution durations for a concrete platform, our approach combines the behavior model with platform-specific benchmarking results. We validate our approach by predicting the performance of a file sharing application.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5530323","Genetic search;genetic programming;reverse engineering;performance prediction;bytecode benchmarking.","Reverse engineering;Predictive models;Application software;Delay;Runtime;Software engineering;Wiring;Genetic programming;Monitoring;Concrete","genetic algorithms;object-oriented programming;reverse engineering;search problems;software performance evaluation","genetic search;reverse engineering;parametric behavior model;component based software engineering;genetic programming;runtime bytecode count;static bytecode analysis;application independent bytecode benchmark","","21","","38","","","","","","IEEE","IEEE Journals & Magazines"
"SPARE: a development environment for program analysis algorithms","G. A. Venkatesh; C. N. Fischer","Bellcore Morristown, NJ, USA; NA","IEEE Transactions on Software Engineering","","1992","18","4","304","318","A tool that bridges the gap between the theory and practice of program analysis specifications is described. The tool supports a high-level specification language that enables clear and concise expression of analysis algorithms. The denotational nature of the specifications eases the derivation of formal proofs of correctness for the analysis algorithm. SPARE (structured program analysis refinement environment) is based on a hybrid approach that combines the positive aspects of both the operational and the semantics-driven approach. An extended denotational framework is used to provide specifications in a modular fashion. Several extensions to the traditional denotational specification language have been designed to allow analysis algorithms to be expressed in a clear and concise fashion. This extended framework eases the design of analysis algorithms as well as the derivation of correctness proofs. The tool provides automatic implementation for testing purposes.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.129219","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=129219","","Algorithm design and analysis;Information analysis;Programming environments;Formal verification;Bridges;Specification languages;Automatic testing;Program processors;Data analysis;Programming profession","formal specification;program testing;programming environments;software tools;specification languages","software tools;development environment;program analysis specifications;high-level specification language;SPARE;structured program analysis refinement environment;denotational specification;correctness proofs;testing","","12","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Rapid transaction-undo recovery using twin-page storage management","K. -. Wu; W. K. Fuchs","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA","IEEE Transactions on Software Engineering","","1993","19","2","155","164","A twin-page storage method, which is an alternative to the TWIST (twin slot) approach by A. Reuter","0098-5589;1939-3520;2326-3881","","10.1109/32.214832","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=214832","","Transaction databases;Shadow mapping;Performance analysis;Contracts;NASA;Database systems;Computer applications;Application software;Instruments;Laboratories","database management systems;storage management;transaction processing","rapid transaction-undo recovery;twin-page storage management;TWIST;aborted transaction identifiers;disk I/O;CPU overhead","","1","","16","","","","","","IEEE","IEEE Journals & Magazines"
"An algebra for data flow diagram process decomposition","M. Adler","Control Data Corp., Bloomington, MN, USA","IEEE Transactions on Software Engineering","","1988","14","2","169","183","Data flow diagram process decomposition, as applied in the analysis phase of software engineering, is a top-down method that takes a process, and its input and output data flows, and logically implements the process as a network of smaller processes. The decomposition is generally performed in an ad hoc manner by an analyst applying heuristics, expertise, and knowledge to the problem. An algebra that formalizes process decomposition is presented using the De Marco representation scheme. In this algebra, the analyst relates the disjoint input and output sets of a single process by specifying the elements of an input/output connectivity matrix. A directed acyclic graph is constructed from the matrix and is the decomposition of the process. The graph basis, grammar matrix, and graph interpretations, and the operators of the algebra are discussed. A decomposition procedure for applying the algebra, prototype, and production tools and outlook are also discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4636","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4636","","Algebra;Matrix decomposition;Software engineering;Design for disassembly;Software prototyping;Performance analysis;Prototypes;Production;Flow graphs;Data engineering","directed graphs;program verification;programming theory;software engineering","data flow diagram process decomposition;software engineering;top-down method;De Marco representation scheme;input/output connectivity matrix;directed acyclic graph;grammar","","30","","11","","","","","","IEEE","IEEE Journals & Magazines"
"The Design of the Saguaro Distributed Operating System","G. R. Andrews; R. D. Schlichting; R. Hayes; T. D. M. Purdin","Department of Computer Science, University of Arizona; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","1","104","118","This paper describes the design of the Saguaro operating system for computers connected by a local-area network. Systems constructed on such an architecture have the potential advantages of concurrency and robustness. In Saguaro, these advantages are made available to the user through several mechanisms. One is channels, an interprocess communication and synchronization facility that allows the input and output of different commands to be connected to form general graphs of communicating processes. Two additional mechanisms are provided to support semitransparent file replication and access: reproduction sets and metafiles. A reproduction set is a collection of files that the system attempts to keep identical on a ""best effort"" basis. A metafile is a special file that contains symbolic pathnames of other files; when a metafile is opened, the system selects an available constituent file and opens it instead. The advantages of concurrency and robustness are also realized at the system level by the use of pools of server processes and decentralized allocation protocols. Saguaro also makes extensive use of a type system to describe user data such as files and to specify the types of arguments to commands and procedures. This enables the system to assist in type checking and leads to a user interface in which command-specific templates are available to facilitate command invocation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232839","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702137","Distributed computing;distributed operating system;file systems;type systems;user interfaces","Operating systems;File systems;Concurrent computing;User interfaces;Robustness;Computer networks;Local area networks;Computer architecture;Protocols;Computer interfaces","","Distributed computing;distributed operating system;file systems;type systems;user interfaces","","7","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Cecil: a sequencing constraint language for automatic static analysis generation","K. M. Olender; L. J. Osterweil","Dept. of Comput. Sci., Colorado Univ., Boulder, CO, USA; Dept. of Comput. Sci., Colorado Univ., Boulder, CO, USA","IEEE Transactions on Software Engineering","","1990","16","3","268","280","A flexible and general mechanism for specifying problems relating to the sequencing of events and mechanically translating them into dataflow analysis algorithms capable of solving those problems is presented. Dataflow analysis has been used for quite some time in compiler code optimization. Most static analyzers have been custom-built to search for fixed and often quite limited classes of dataflow conditions. It is shown that the range of sequences for which it is interesting and worthwhile to search in actually quite broad and diverse. A formalism for specifying this diversity of conditions is created. It is shown that these conditions can be modeled essentially as dataflow analysis problems for which effective solutions are known. It is also shown how these solutions can be exploited to serve as the basis for mechanical creation of analyzers for these conditions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.48935","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=48935","","Software quality;Computer science;Data analysis;Software tools;Software testing;Security;Software engineering;Algorithm design and analysis;Optimizing compilers;Automata","automatic programming;parallel programming;program compilers;specification languages","Cecil;sequencing constraint language;automatic static analysis generation;general mechanism;dataflow analysis algorithms;compiler code optimization;custom-built;dataflow conditions;dataflow analysis problems","","41","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Update transport: a new technique for update synchronization in replicated database systems","M. Singhal","Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA","IEEE Transactions on Software Engineering","","1990","16","12","1325","1336","A fully distributed approach to update synchronization is presented where each site completely executes every update. This approach has several features-higher resiliency to different kinds of failures, higher parallelism, improved response to user requests, and low communication overhead. A fully distributed algorithm for concurrency control obtained by rehashing a previously published semidistributed algorithm into the fully distributed model of update execution is presented. A performance model of replicated database systems is presented and used to study the performance of the proposed algorithm and its semidistributed version. The results of the performance study reveal that the proposed approach can substantially improve the performance at the cost of moderate input/output overhead.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.62441","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=62441","","Database systems;Concurrency control;Distributed algorithms;Costs;System recovery;Throughput;Communication system control;Maintenance;Delay;NASA","concurrency control;distributed databases;redundancy","update transport;update synchronization;replicated database systems;fully distributed approach;parallelism;user requests;low communication overhead;fully distributed algorithm;concurrency control;semidistributed algorithm;fully distributed model;update execution;performance model;performance study;moderate input/output overhead","","9","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Loop Summarization via Path Dependency Analysis","X. Xie; B. Chen; L. Zou; Y. Liu; W. Le; X. Li","School of Computer Science and Technology, Tianjin University, Tianjin, Tianjin China (e-mail: xiexiaofei@tju.edu.cn); School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, 12478 Shanghai, Shanghai China (e-mail: chenbihuan@gmail.com); School of Computer Science and Engineering, Nanyang Technological University, 54761 Singapore, Singapore Singapore (e-mail: liang.d.zou@gmail.com); School of Computer Engineering, Nanyang Technological University, Singapore, Singapore Singapore 639798 (e-mail: yangliu@ntu.edu.sg); Department of Computer Science, Iowa State University, 1177 Ames, Iowa United States (e-mail: weile@iastate.edu); School of Computer Science and Technology, Tianjin University, Tianjin, Tianjin China (e-mail: xiaohongli@tju.edu.cn)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Analyzing loops is very important for various software engineering tasks such as bug detection, test case generation and program optimization. However, loops are very challenging structures for program analysis, especially when (nested) loops contain multiple paths that have complex interleaving relationships. In this paper, we propose the path dependency automaton (PDA) to capture the dependencies among the multiple paths in a loop. Based on the PDA, we first propose a loop classification to understand the complexity of loop summarization. Then, we propose a loop analysis framework, which summarizes path-sensitive loop effects on the variables of interest. A DFS-based algorithm is proposed to traverse the PDA to summarize the effect for all possible executions in the loop. We have evaluated Proteus using loops from five open-source projects and two well-known benchmarks and applying the disjunctive loop summary to three applications: loop bound analysis, program verification and test case generation. The evaluation results have demonstrated that Proteus can compute a more precise bound than the existing loop bound analysis techniques; Proteus can significantly outperform the state-of-the-art tools for loop program verification; and Proteus can help generate test cases for deep loops within one second.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2788018","National Research Foundation Singapore; National Natural Science Foundation of China; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8241837","Disjunctive Loop Summary;Path Dependency Automaton;Path Interleaving","Handheld computers;Automata;Benchmark testing;Computer science;Electronic mail;Open source software","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Using Traceability Links to Recommend Adaptive Changes for Documentation Evolution","B. Dagenais; M. P. Robillard","Resulto Inc., Montreal, QC, Canada; School of Computer Science, McGill University, 3480 University Street, McConnell Engineering Building, Office 114N, Montreal, QC, Canada","IEEE Transactions on Software Engineering","","2014","40","11","1126","1146","Developer documentation helps developers learn frameworks and libraries, yet developing and maintaining accurate documentation requires considerable effort and resources. Contributors who work on developer documentation often need to manually track all changes in the code, determine which changes are significant enough to document, and then, adapt the documentation. We propose AdDoc, a technique that automatically discovers documentation patterns, i.e., coherent sets of code elements that are documented together, and that reports violations of these patterns as the code and the documentation evolves. We evaluated our approach in a retrospective analysis of four Java open source projects and found that at least 50 percent of all the changes in the documentation were related to existing documentation patterns. Our technique allows contributors to quickly adapt existing documentation, so that they can focus their documentation effort on the new features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2347969","NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6878435","Documentation;maintainability;frameworks","Documentation;Java;Manuals;Libraries;Sections;Joining processes;Concrete","data mining;Java;program diagnostics;public domain software;system documentation","traceability links;adaptive changes;documentation evolution;developer documentation;AdDoc;automatic documentation pattern discovery;code elements;Java open source projects","","5","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Property-based software engineering measurement","L. C. Briand; S. Morasca; V. R. Basili","Centre de Recherche Inf. de Montreal, Montreal, Que., Canada; NA; NA","IEEE Transactions on Software Engineering","","1996","22","1","68","86","Little theory exists in the field of software system measurement. Concepts such as complexity, coupling, cohesion or even size are very often subject to interpretation and appear to have inconsistent definitions in the literature. As a consequence, there is little guidance provided to the analyst attempting to define proper measures for specific problems. Many controversies in the literature are simply misunderstandings and stem from the fact that some people talk about different measurement concepts under the same label (complexity is the most common case). There is a need to define unambiguously the most important measurement concepts used in the measurement of software products. One way of doing so is to define precisely what mathematical properties characterize these concepts, regardless of the specific software artifacts to which these concepts are applied. Such a mathematical framework could generate a consensus in the software engineering community and provide a means for better communication among researchers, better guidelines for analysts, and better evaluation methods for commercial static analyzers for practitioners. We propose a mathematical framework which is generic, because it is not specific to any particular software artifact, and rigorous, because it is based on precise mathematical concepts. We use this framework to propose definitions of several important measurement concepts (size, length, complexity, cohesion, coupling). It does not intend to be complete or fully objective; other frameworks could have been proposed and different choices could have been made. However, we believe that the formalisms and properties we introduce are convenient and intuitive. This framework contributes constructively to a firmer theoretical ground of software measurement.","0098-5589;1939-3520;2326-3881","","10.1109/32.481535","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=481535","","Software engineering;Software measurement;Size measurement;Software quality;Proposals;Software design;Costs;Virtual reality;Computer science;Job shop scheduling","software metrics;computational complexity;system monitoring","property-based software engineering measurement;software system measurement;complexity;coupling;cohesion;software products;mathematical properties;evaluation methods;commercial static analyzers","","320","","28","","","","","","IEEE","IEEE Journals & Magazines"
"DEVS formalism: a framework for hierarchical model development","A. I. Concepcion; B. P. Zeigler","Dept. of Comput. Sci., Michigan State Univ., East Lansing, MI, USA; NA","IEEE Transactions on Software Engineering","","1988","14","2","228","241","A methodology is being developed to map hierarchical, modular discrete event models onto distributed simulator architectures. Concept developed for the first step of the methodology concerning model representation are discussed. The DEVS (Discrete Event System Specification) is extended to facilitate modular, hierarchical model specification. Procedures for top-down model development are expressed with the extended formalism and illustrated with a computer system model design.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4640","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4640","","Computational modeling;Discrete event simulation;Application software;Microcomputers;Distributed processing;Power system modeling;Computer architecture;System recovery;Discrete event systems;Software design","distributed processing;programming theory;software engineering","DEVS;hierarchical model development;discrete event models;distributed simulator architectures;model representation;Discrete Event System Specification;hierarchical model specification;top-down model development","","60","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Semi-distributed load balancing for massively parallel multicomputer systems","I. Ahmad; A. Ghafoor","Sch. of Comput. & Inf. Sci., Syracuse Univ., NY, USA; NA","IEEE Transactions on Software Engineering","","1991","17","10","987","1004","A semidistributed approach is given for load balancing in large parallel and distributed systems which is different from the conventional centralized and fully distributed approaches. The proposed strategy uses a two-level hierarchical control by partitioning the interconnection structure of a distributed or multiprocessor system into independent symmetric regions (spheres) centered at some control points. The central points, called schedulers, optimally schedule tasks within their spheres and maintain state information with low overhead. The authors consider interconnection structures belonging to a number of families of distance transitive graphs for evaluation, and, using their algebraic characteristics, show that identification of spheres and their scheduling points is in general an NP-complete problem. An efficient solution for this problem is presented by making exclusive use of a combinatorial structure known as the Hadamard matrix. The performance of the proposed strategy has been evaluated and compared with an efficient fully distributed strategy through an extensive simulation study. The proposed strategy yielded much better results.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.99188","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=99188","","Load management;Concurrent computing;Delay;Power engineering computing;Control systems;Resource management;Multiprocessor interconnection networks;Processor scheduling;Partitioning algorithms;Scheduling algorithm","computational complexity;multiprocessor interconnection networks;parallel architectures;parallel machines;scheduling","massively parallel multicomputer systems;semidistributed approach;load balancing;distributed systems;fully distributed approaches;two-level hierarchical control;interconnection structure;multiprocessor system;independent symmetric regions;state information;interconnection structures;distance transitive graphs;scheduling points;NP-complete problem;combinatorial structure;Hadamard matrix;fully distributed strategy;simulation study","","54","","45","","","","","","IEEE","IEEE Journals & Magazines"
"Trends in the Quality of Human-Centric Software Engineering Experiments--A Quasi-Experiment","B. Kitchenham; D. I. K. Sjøberg; T. Dybå; O. P. Brereton; D. Budgen; M. Höst; P. Runeson","Keele University, Keele; University of Oslo, Oslo; University of Oslo, Oslo and SINTEF, Trondheim; Keele University, Keele; Durham University, Durham; Lund University, Lund; Lund University, Lund","IEEE Transactions on Software Engineering","","2013","39","7","1002","1017","Context: Several text books and papers published between 2000 and 2002 have attempted to introduce experimental design and statistical methods to software engineers undertaking empirical studies. Objective: This paper investigates whether there has been an increase in the quality of human-centric experimental and quasi-experimental journal papers over the time period 1993 to 2010. Method: Seventy experimental and quasi-experimental papers published in four general software engineering journals in the years 1992-2002 and 2006-2010 were each assessed for quality by three empirical software engineering researchers using two quality assessment methods (a questionnaire-based method and a subjective overall assessment). Regression analysis was used to assess the relationship between paper quality and the year of publication, publication date group (before 2003 and after 2005), source journal, average coauthor experience, citation of statistical text books and papers, and paper length. The results were validated both by removing papers for which the quality score appeared unreliable and using an alternative quality measure. Results: Paper quality was significantly associated with year, citing general statistical texts, and paper length (p <; 0.05). Paper length did not reach significance when quality was measured using an overall subjective assessment. Conclusions: The quality of experimental and quasi-experimental software engineering papers appears to have improved gradually since 1993.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6374196","Quality evaluation;empirical studies;human-centric experiments;experimentation;software engineering","Software engineering;Guidelines;Correlation;Manuals;Educational institutions;Humans;Materials","citation analysis;design of experiments;publishing;regression analysis;software quality;text analysis","human-centric software engineering experiments;experimental design;statistical methods;human-centric experimental journal papers;quasi-experimental journal papers;quality assessment methods;questionnaire-based method;subjective overall assessment;regression analysis;paper quality;publication year;publication date group;source journal;average coauthor experience;statistical text books citation;statistical papers citation;paper length","","5","","28","","","","","","IEEE","IEEE Journals & Magazines"
"The automatic inversion of attribute grammars","D. M. Yellin; E. M. Mueckstein","Department of Computer Science, Columbia University, New York, NY 10027; IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598; IBM Information Services, Bethesda, MD 20817","IEEE Transactions on Software Engineering","","1986","SE-12","5","590","599","Attribute grammars constitute a formal mechanism for specifying translations between languages; from a formal description of the translation, a translator can be automatically constructed. This process is taken one step further; given an attribute grammar specifying the translation from language <i>L</i><sub>1</sub> to language <i>L</i><sub>2</sub>, the question of whether the inverse attribute grammar specifying the inverse translation from <i>L</i><sub>2</sub> to <i>L</i><sub>1</sub> can be automatically generated is addressed. It is shown how to solve this problem for a restricted subset of attribute grammars. This inversion process allows compatible two-way translators to be generated from a single description. To show the practical feasibility of attribute grammar inversion, experience in inverting an attribute grammar used as an interface for a formal database accessing language, SQL, is related. The attributed grammar is used to paraphrase SQL database queries in English.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312955","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312955","Automatic software generation;bidirectional translators;formal specifications;inversion of attribute grammars;natural language interfaces to databases","Grammar;Production;Semantics;Context;Program processors;Databases","context-free grammars;database theory;program interpreters;query languages","automatic inversion;attribute grammars;specifying translations;formal description;inverse translation;formal database accessing language;SQL;database queries","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Improving Fault Detection Capability by Selectively Retaining Test Cases during Test Suite Reduction","D. Jeffrey; N. Gupta","NA; NA","IEEE Transactions on Software Engineering","","2007","33","2","108","123","Software testing is a critical part of software development. As new test cases are generated over time due to software modifications, test suite sizes may grow significantly. Because of time and resource constraints for testing, test suite minimization techniques are needed to remove those test cases from a suite that, due to code modifications over time, have become redundant with respect to the coverage of testing requirements for which they were generated. Prior work has shown that test suite minimization with respect to a given testing criterion can significantly diminish the fault detection effectiveness (FDE) of suites. We present a new approach for test suite reduction that attempts to use additional coverage information of test cases to selectively keep some additional test cases in the reduced suites that are redundant with respect to the testing criteria used for suite minimization, with the goal of improving the FDE retention of the reduced suites. We implemented our approach by modifying an existing heuristic for test suite minimization. Our experiments show that our approach can significantly improve the FDE of reduced test suites without severely affecting the extent of suite size reduction","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052586","Software testing;testing criteria;test suite minimization;test suite reduction;fault detection effectiveness.","Fault detection;Software testing;Programming;Time factors;Life testing;Resource management;Software development management;Polynomials","program testing","test suite reduction;software testing;software development;test cases;fault detection effectiveness","","72","","31","","","","","","IEEE","IEEE Journals & Magazines"
"MIDAS: integrated design and simulation of distributed systems","R. L. Bagrodia; C. -. Shen","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA; Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA","IEEE Transactions on Software Engineering","","1991","17","10","1042","1058","An approach called MIDAS is described that supports the design of distributed systems via iterative refinement of hybrid models. A hybrid model is a partially implemented design where some components exist as simulation models and others as operational subsystems. It is an executable model and may be used to determine the stochastic performance characteristics of a partially elaborated design. MIDAS enhances the applicability of hybrid models in system design with its support for interrupts and its inclusion of distributed components in the partially implemented design. The authors describe how an existing simulation language may be extended to program hybrid models, and show how simulation algorithms may be adapted to execute hybrid models. A prototype MIDAS implementation is operational and was used to develop a set of applications. The experimental results of the exercise are also described.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.99192","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=99192","","Analytical models;Iterative methods;Performance analysis;Stochastic processes;Prototypes;Helium;Timing;Hardware;Humans;Aircraft","distributed processing;interrupts;simulation languages;virtual machines","MIDAS;distributed systems;iterative refinement;hybrid models;partially implemented design;simulation models;operational subsystems;executable model;stochastic performance characteristics;partially elaborated design;system design;interrupts;distributed components;simulation language;simulation algorithms","","35","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing staffing needs for a software maintenance project through queuing simulation","G. Antoniol; A. Cimitile; G. A. Di Lucca; M. Di Penta","Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy","IEEE Transactions on Software Engineering","","2004","30","1","43","58","We present an approach based on queuing theory and stochastic simulation to help planning, managing, and controlling the project staffing and the resulting service level in distributed multiphase maintenance processes. Data from a Y2K massive maintenance intervention on a large COBOL/JCL financial software system were used to simulate and study different service center configurations for a geographically distributed software maintenance project. In particular, a monolithic configuration corresponding to the customer's point-of-view and more fine-grained configurations, accounting for different process phases as well as for rework, were studied. The queuing theory and stochastic simulation provided a means to assess staffing, evaluate service level, and assess the likelihood to meet the project deadline while executing the project. It turned out to be an effective staffing tool for managers, provided that it is complemented with other project-management tools, in order to prioritize activities, avoid conflicts, and check the availability of resources.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265735","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265735","","Software maintenance;Queueing analysis;Computational modeling;Stochastic processes;Project management;Costs;Computer simulation;Software systems;Counting circuits;Computer Society","software maintenance;queueing theory;discrete event simulation;personnel;project management;stochastic processes;program testing","queuing theory;stochastic simulation;distributed multiphase maintenance process;Y2K massive maintenance;financial software system;distributed software maintenance project;project-management tool;software maintenance staffing;discrete-event simulation;process simulation;schedule estimation","","43","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Managing Technical Debt in Enterprise Software Packages","N. Ramasubbu; C. F. Kemerer","Joseph M. Katz Graduate School of Business, University of Pittsburgh, Pittsburgh, PA; Joseph M. Katz Graduate School of Business, University of Pittsburgh, Pittsburgh, PA","IEEE Transactions on Software Engineering","","2014","40","8","758","772","We develop an evolutionary model and theory of software technical debt accumulation to facilitate a rigorous and balanced analysis of its benefits and costs in the context of a large commercial enterprise software package. Our theory focuses on the optimization problem involved in managing technical debt, and illustrates the different tradeoff patterns between software quality and customer satisfaction under early and late adopter scenarios at different lifecycle stages of the software package. We empirically verify our theory utilizing a ten year longitudinal data set drawn from 69 customer installations of the software package. We then utilize the empirical results to develop actionable policies for managing technical debt in enterprise software product adoption.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2327027","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6824267","Technical debt;enterprise software;software platforms;customer satisfaction;software quality;technology adoption;COTS;software evolution;software maintenance;software management;longitudinal data","Software packages;Business;Software quality;Measurement;Maintenance engineering;Context","cost-benefit analysis;evolutionary computation;software development management;software maintenance;software packages;software quality","technical debt management;evolutionary model;software technical debt theory;commercial enterprise software package;optimization problem;software quality;customer satisfaction;early adopter scenario;late adopter scenario;software package lifecycle stage;enterprise software product adoption","","10","","57","","","","","","IEEE","IEEE Journals & Magazines"
"An analytic/empirical study of distributed sorting on a local area network","W. S. Luk; F. Ling","Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada; Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada","IEEE Transactions on Software Engineering","","1989","15","5","575","586","A model for distributed sorting on a local area network (LAN) is presented. This model, contrary to the conventional model, takes into account both local processing time and communication time. This model is intended to provide a framework within which the performances of various distributed sorting algorithms are analyzed and implemented on Ethernet-connected Sun workstations. The empirical results by and large agree with the predictions derivable from the model. They show that local processing, particularly sorting of local subfiles, dominates the whole process, as far as response time is concerned. All algorithms examined have similar asymptotic behavior for large files. For medium-sized files, the degree of communication parallelism has a great impact on algorithm performance.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24707","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24707","","Sorting;Local area networks;Algorithm design and analysis;Parallel processing;Workstations;Distributed algorithms;Costs;Predictive models;Performance analysis;Sun","distributed processing;local area networks;sorting","local area network;LAN;local processing time;communication time;distributed sorting algorithms;Ethernet-connected Sun workstations;local processing;local subfiles;asymptotic behavior;large files;communication parallelism;algorithm performance","","4","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Task Scheduling on the PASM Parallel Processing System","D. L. Tuomenoksa; H. J. Siegel","AT&amp;T Information Systems; NA","IEEE Transactions on Software Engineering","","1985","SE-11","2","145","157","PASM is a proposed large-scale distributed/parallel processing system which can be partitioned into independent SIMD/MIMD machines of various sizes. One design problem for systems such as PASM is task scheduling. The use of multiple FIFO queues for nonpreemptive task scheduling is described. Four multiple-queue scheduling algorithms with different placement policies are presented and applied to the PASM parallel processing system. Simulation of a queueing network model is used to compare the performance of the algorithms. Their performance is also considered in the case where there are faulty control units and processors. The multiple-queue scheduling algorithms can be adapted for inclusion in other multiple-SIMD and partitionable SIMD/MIMD systems that use similar types of interconnection networks to those being considered for PASM.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232189","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701982","Distributed processing;multimicroprocessor systems;multiple-SIMD systems;parallel processing;partitionable SIMD/MIMD systems;PASM;performance evaluation;reconfigurable computer systems;scheduling","Parallel processing;Scheduling algorithm;Processor scheduling;Large-scale systems;Switches;Laboratories;Partitioning algorithms;Process control;Multiprocessor interconnection networks;Concurrent computing","","Distributed processing;multimicroprocessor systems;multiple-SIMD systems;parallel processing;partitionable SIMD/MIMD systems;PASM;performance evaluation;reconfigurable computer systems;scheduling","","9","","30","","","","","","IEEE","IEEE Journals & Magazines"
"On Scheduling Constraint Abstraction for Multi-Threaded Program Verification","L. Yin; W. Dong; W. Liu; J. Wang","School of Computer, National University of Defense Technology, 58294 Changsha, Hunan China (e-mail: yinliangze@163.com); School of Computer, National University of Defense Technology, 58294 Changsha, Hunan China (e-mail: wdong@nudt.edu.cn); School of Computer, National University of Defense Technology, 58294 Changsha, Hunan China (e-mail: wwliu@nudt.edu.cn); School of Computer, National University of Defense Technology, Changsha, Hunan China (e-mail: wj@nudt.edu.cn)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Bounded model checking is among the most efficient techniques for the automatic verification of concurrent programs. However, due to the nondeterministic thread interleavings, a huge and complex formula is usually required to give an exact encoding of all possible behaviors, which significantly limits the salability. Observing that the huge formula is usually dominated by the exact encoding of the scheduling constraint, this paper proposes a novel scheduling constraint based abstraction refinement method for multi-threaded C program verification. Our method is both efficient in practice and complete in theory, which is challenging for existing techniques. To achieve this, we first proposed an effective and powerful technique which works well for nearly all examples in practice. We have proposed the notion of Event Order Graph (EOG), and have devised two graph-based algorithms over EOG for counterexample validation and refinement generation, which can always obtain a small yet effective refinement constraint. Then, to ensure the completeness, our method was enhanced with two constraint-based algorithms for counterexample validation and refinement generation. Experimental results on SV-COMP 2017 benchmarks and two real-world server systems indicate that our method is promising and significantly outperforms the existing state-of-the-art tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2864122","National key RD program of China; National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8428438","Multi-Threaded Program;Bounded Model Checking;Scheduling Constraint;Event Order Graph","Instruction sets;Electrooculography;Encoding;Concurrent computing;Programming;Model checking;Tools","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Researcher Bias: The Use of Machine Learning in Software Defect Prediction","M. Shepperd; D. Bowes; T. Hall","Brunel University, Uxbridge, Middlesex, United Kingdom; Science and Technology Research Institute, University of Hertfordshire, Hatfield, Hertfordshire, United Kingdom; Brunel University, Uxbridge, Middlesex, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","6","603","616","Background. The ability to predict defect-prone software components would be valuable. Consequently, there have been many empirical studies to evaluate the performance of different techniques endeavouring to accomplish this effectively. However no one technique dominates and so designing a reliable defect prediction model remains problematic. Objective. We seek to make sense of the many conflicting experimental results and understand which factors have the largest effect on predictive performance. Method. We conduct a meta-analysis of all relevant, high quality primary studies of defect prediction to determine what factors influence predictive performance. This is based on 42 primary studies that satisfy our inclusion criteria that collectively report 600 sets of empirical prediction results. By reverse engineering a common response variable we build a random effects ANOVA model to examine the relative contribution of four model building factors (classifier, data set, input metrics and researcher group) to model prediction performance. Results. Surprisingly we find that the choice of classifier has little impact upon performance (1.3 percent) and in contrast the major (31 percent) explanatory factor is the researcher group. It matters more who does the work than what is done. Conclusion. To overcome this high level of researcher bias, defect prediction researchers should (i) conduct blind analysis, (ii) improve reporting protocols and (iii) conduct more intergroup studies in order to alleviate expertise issues. Lastly, research is required to determine whether this bias is prevalent in other applications domains.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2322358","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6824804","Software defect prediction;meta-analysis;researcher bias","Software;Predictive models;Correlation;Data models;Buildings;Software engineering;Measurement","learning (artificial intelligence);object-oriented programming;reverse engineering;software metrics;software performance evaluation;statistical analysis","machine learning;software defect prediction;defect-prone software component prediction;performance evaluation;reverse engineering;common response variable;random effects ANOVA model;model building factors;classifier factor;data set factor;input metrics factor;researcher group factor;researcher bias;blind analysis;reporting protocol improvement;intergroup studies","","69","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Scale Economies in New Software Development","R. D. Banker; C. F. Kemerer","NA; NA","IEEE Transactions on Software Engineering","","1989","15","10","1199","1205","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1989.559768","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=559768","","Programming;Economies of scale;Production;Productivity;Data envelopment analysis;Costs;Software measurement;Software engineering;Industrial economics;Assembly","","Data envelopment analysis;function points;productivity measurement;scale economies;software development;source lines of code","","110","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Approximate mean value analysis for stochastic marked graphs","M. Sereno","Dipartimento di Inf., Torino Univ., Italy","IEEE Transactions on Software Engineering","","1996","22","9","654","664","An iterative technique for the computation of approximate performance indices of a class of stochastic Petri net models is presented. The proposed technique is derived from the mean value analysis algorithm for product-form solution stochastic Petri nets. In this paper, we apply the approximation technique to stochastic marked graphs. In principle, the proposed technique can be used for other stochastic Petri net subclasses. In this paper, some of these possible applications are presented. Several examples are presented in order to validate the approximate results.","0098-5589;1939-3520;2326-3881","","10.1109/32.541436","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=541436","","Stochastic processes;Petri nets;Performance analysis;Algorithm design and analysis;Approximation algorithms;Equations;Iterative algorithms;Concurrent computing;Steady-state;Linear systems","stochastic systems;approximation theory;Petri nets;graph colouring;iterative methods;performance index","approximate mean value analysis;stochastic marked graphs;iterative technique;approximate performance indices;stochastic Petri nets;product-form solution;computational algorithms;approximation techniques","","7","","31","","","","","","IEEE","IEEE Journals & Magazines"
"On inspection and verification of software with timing requirements","J. Xu","Dept. of Comput. Sci., York Univ., North York, Ont., Canada","IEEE Transactions on Software Engineering","","2003","29","8","705","720","Software with hard timing requirements should be designed using a systematic approach to make its timing properties easier to inspect and verify; otherwise, it may be practically impossible to determine whether the software satisfies the timing requirements. Pre-runtime scheduling provides such an approach by placing restrictions on software structures to reduce complexity. A major benefit of using a pre-runtime scheduling approach is that it makes it easier to systematically inspect and verify the timing properties of the actual software code, not just various high-level abstractions of the code.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223645","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223645","","Inspection;Timing;Job shop scheduling;Software tools;Real time systems;Algorithm design and analysis;Protocols;Processor scheduling;Transportation;Finance","program verification;timing;inspection;real-time systems;scheduling;formal specification;program control structures;software reliability;software metrics","software verification;timing requirement;preruntime scheduling;software structure;software inspection;timing property;software code;real-time software;software complexity","","19","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Operating System Models in a Concurrent Pascal Environment: Complexity and Performance Considerations","A. Pashtan","Gould Research Center","IEEE Transactions on Software Engineering","","1985","SE-11","1","136","141","Empirical observations of computer operating systems have shown that operating systems are designed with one of two object oriented strategies: a process or a monitor oriented approach. This paper compares the two design approaches in a Concurrent Pascal environment. Resource manager programs that are implemented in conformity with each model are evaluated using software complexity measures and program performance measures. The average complexity of resource manager processes is 94 percent larger than the average complexity of resource manager monitors. The runtime synchronization overhead of the process model program is two-eight times higher than that of its counterpart.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231538","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701906","Concurrent Pascal;monitor model;operating system;process model;program effort;program performance;resource manager object;software complexity","Operating systems;Object oriented modeling;Resource management;Data structures;Software performance;Software measurement;Computer architecture;Computerized monitoring;High level languages;Runtime","","Concurrent Pascal;monitor model;operating system;process model;program effort;program performance;resource manager object;software complexity","","1","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Improving quicksort performance with a codeword data structure","J. -. Baer; Y. -. Lin","Dept. of Comput. Sci., Washington Univ., Seattle, WA, USA; Dept. of Comput. Sci., Washington Univ., Seattle, WA, USA","IEEE Transactions on Software Engineering","","1989","15","5","622","631","The problem is discussed of how the use of a new data structure, the codeword structure, can help improve the performance of quicksort when the records to be sorted are long and the keys are alphanumeric sequences of bytes. The codeword is a compact representation of a key with respect to some codeword generator. It consists of a byte for a character count of equal bytes, a byte for the first nonequal byte, and a pointer to the record. It is shown how the ordering of keys is preserved by an adequate choice of the code generator and how this can be applied to the quicksort algorithm. An analysis of the potential saving son various architectures and actual measurements shows the improvements that can be attained by using codewords rather than pointers. Architecturally independent parameters, such as the number of bytes to be compared, the number of swaps, architecture-dependent parameters such as caches and their write policies, and compiler optimizations such as in-line expansion and register allocation are considered.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24711","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24711","","Data structures;Optimizing compilers;Memory management;Computer science;Sorting;Indexing","data structures;sorting","performance improvement;codeword data structure;records;long;keys;alphanumeric sequences;bytes;codeword generator;character count;first nonequal byte;pointer;ordering;quicksort algorithm;swaps;architecture-dependent parameters;caches;write policies;compiler optimizations;in-line expansion;register allocation","","5","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Statistical relational tables for statistical database management","S. P. Ghosh","Department of Computer Science, IBM Research Laboratory, San Jose, CA 94193","IEEE Transactions on Software Engineering","","1986","SE-12","12","1106","1116","E.F. Codd's (1970) relational view is extended to represent statistical data and to achieve its analysis. A new view called a statistical relational table is presented to meet the needs of statisticians, and some of Codd's relational operators are extended to statistical relational tables. New operators based on these tables are introduced for communicating requests for statistical analysis. A new query language called the query-by-statistical-relational-table (which has some similarities to query-by-example) is introduced. Extensions of the SQL language for processing the commands of the new query language are also discussed. Creation and storage of metadata for fast statistical analysis are considered. Some problems related to privacy in statistical databases are also examined.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313006","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313006","Codd's relational model;metadata;statistical data;statistical relational table","Databases;Remuneration;Database languages;Data models;Statistical analysis;Computational modeling","database theory;mathematics computing;query languages;relational databases;statistics","mathematics computing;statistical database management;statistical data;statistical relational table;relational operators;query language;query-by-statistical-relational-table;SQL language;privacy","","17","","","","","","","","IEEE","IEEE Journals & Magazines"
"Semantic Slicing of Software Version Histories","Y. Li; C. Zhu; J. Rubin; M. Chechik","Department of Computer Science, University of Toronto, Toronto, ON, Canada; Department of Computer Science, University of Toronto, Toronto, ON, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Computer Science, University of Toronto, Toronto, ON, Canada","IEEE Transactions on Software Engineering","","2018","44","2","182","201","Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of the change history, “inheriting” additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and propose techniques to minimize the produced slice. We then instantiate the overall approach, CSlicer, in a specific implementation for Java projects managed in Git and evaluate its correctness and effectiveness on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2664824","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7843626","Software changes;version control;dependency;program analysis","History;Semantics;Software;Minimization;Context;Computer bugs;Java","configuration management;Java;program slicing;public domain software","CSlicer;configuration management system;software developers;software version histories;open-source software repositories;semantic slicing problem;semantically-related commits","","3","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Self-Supervising BPEL Processes","L. Baresi; S. Guinea","Politecnico di Milano, Milano, Italy; Politecnico di Milano, Milano, Italy","IEEE Transactions on Software Engineering","","2011","37","2","247","263","Service compositions suffer changes in their partner services. Even if the composition does not change, its behavior may evolve over time and become incorrect. Such changes cannot be fully foreseen through prerelease validation, but impose a shift in the quality assessment activities. Provided functionality and quality of service must be continuously probed while the application executes, and the application itself must be able to take corrective actions to preserve its dependability and robustness. We propose the idea of self-supervising BPEL processes, that is, special-purpose compositions that assess their behavior and react through user-defined rules. Supervision consists of monitoring and recovery. The former checks the system's execution to see whether everything is proceeding as planned, while the latter attempts to fix any anomalies. The paper introduces two languages for defining monitoring and recovery and explains how to use them to enrich BPEL processes with self-supervision capabilities. Supervision is treated as a cross-cutting concern that is only blended at runtime, allowing different stakeholders to adopt different strategies with no impact on the actual business logic. The paper also presents a supervision-aware runtime framework for executing the enriched processes, and briefly discusses the results of in-lab experiments and of a first evaluation with industrial partners.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5432226","Software engineering;software/program verification;assertion checkers;assertion languages;performance;design tools and techniques;distributed/Internet-based software engineering tools and techniques.","Runtime;Monitoring;Robustness;Software engineering;Application software;Quality assessment;Quality of service;Logic;Software performance;Software tools","business process re-engineering;program verification;service-oriented architecture;Web services","self supervising BPEL process;quality assessment;stakeholder;business logic;supervision aware runtime framework;industrial partner;business process execution language","","57","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Service combinators for Web computing","L. Cardelli; R. Davies","Microsoft Res., Cambridge, UK; NA","IEEE Transactions on Software Engineering","","1999","25","3","309","316","The World Wide Web is rich in content and services, but access to these resources must be obtained mostly through manual browsers. We would like to be able to write programs that reproduce human browsing behavior, including reactions to slow transmission-rates and failures on many simultaneous links. We thus introduce a concurrent model that directly incorporates the notions of failure and rate of communication, and then describe programming constructs based on this model.","0098-5589;1939-3520;2326-3881","","10.1109/32.798321","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=798321","","Web sites;Computational modeling;Uniform resource locators;Distributed databases;Humans;LAN interconnection;Computer architecture;Counting circuits;Protection","information resources;information retrieval","service combinators;Web computing;World Wide Web;human browsing behavior;slow transmission rate reaction;failure reaction;simultaneous links;concurrent model;programming constructs","","25","","","","","","","","IEEE","IEEE Journals & Magazines"
"Modular algebraic nets to specify concurrent systems","E. Battiston; F. De Cindio; G. Mauri","Dipartimento di Sci. dell'Inf., Milan Univ., Italy; Dipartimento di Sci. dell'Inf., Milan Univ., Italy; Dipartimento di Sci. dell'Inf., Milan Univ., Italy","IEEE Transactions on Software Engineering","","1996","22","10","689","705","The authors present the basic features of a specification language for concurrent distributed systems, developed at the Department of Information Sciences of the University of Milan, Italy. The language is based on a class of modular algebraic high-level nets, OBJSA nets, which result from the synthesis of superposed automata (SA) nets and of the algebraic specification language OBJ. It is supported by the OBJSA Net Environment (ONE). OBJSA nets stress the possibility of building the system model by composing its components and encourage the incremental development of the specification and its reusability. An OBJSA net consists of an SA net inscribed with terms of an OBJ module. The ONE environment supports the user in producing and executing a specification, hiding from her/him, as much as possible, the technical details of the algebraic part of the specification. The paper provides a complete presentation of OBJSA nets, including a user-oriented introduction, the definition of OBJSA nets (as subclass of SPEC-inscribed nets), of their occurrence rule (the semantics) and of the composition operation. In addition it presents the kernel of the support environment.","0098-5589;1939-3520;2326-3881","","10.1109/32.544348","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=544348","","Petri nets;Specification languages;Concurrent computing;Automata;Stress;Kernel;Carbon capture and storage;Algebra","algebraic specification;formal specification;specification languages;parallel processing;Petri nets;software reusability","modular algebraic nets;concurrent system specification;specification language;concurrent distributed systems;modular algebraic high-level nets;OBJSA nets;superposed automata net synthesis;OBJ algebraic specification language;OBJSA Net Environment;system model;incremental specification development;specification reusability;OBJ module;composition operation;occurrence rule;support environment kernel","","12","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Complexity measure evaluation and selection","J. Tian; M. V. Zelkowitz","Software Solutions Toronto Lab., IBM, North York, Ont., Canada; NA","IEEE Transactions on Software Engineering","","1995","21","8","641","650","A formal model of program complexity developed earlier by the authors is used to derive evaluation criteria for program complexity measures. This is then used to determine which measures are appropriate within a particular application domain. A set of rules for determining feasible measures for a particular application domain are given, and an evaluation model for choosing among alternative feasible measures is presented. This model is used to select measures from the classification trees produced by the empirically guided software development environment of R.W. Selby and A.A. Porter, and early experiments show it to be an effective process.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.403788","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=403788","","Software measurement;Particle measurements;Application software;Programming;Boundary conditions;Classification tree analysis;Software engineering;Software quality;Maintenance;Fluid flow measurement","software metrics;software quality;software selection;software performance evaluation","complexity measure evaluation;program complexity measures;evaluation criteria;application domain;feasible measures;evaluation model;alternative feasible measures;classification trees;empirically guided software development environment","","27","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Covert Channels in LAN's","C. G. Girling","Topexpress Ltd.","IEEE Transactions on Software Engineering","","1987","SE-13","2","292","296","An information transfer path that allows information to be transferred in a manner that violates the security policy of a trusted network is called a covert channel.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233153","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702208","Covert channels;encipherment;local area computer networks;protocols;traffic stereotyping;wiretapping","Local area networks;Bandwidth;Protocols;Telecommunication traffic;Working environment noise;Information security;Timing;Intelligent networks;Design methodology;Computer networks","","Covert channels;encipherment;local area computer networks;protocols;traffic stereotyping;wiretapping","","64","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Detection of Ada static deadlocks using Petri net invariants","T. Murata; B. Shenker; S. M. Shatz","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA","IEEE Transactions on Software Engineering","","1989","15","3","314","326","A method is presented for detecting deadlocks in Ada tasking programs using structural; and dynamic analysis of Petri nets. Algorithmic translation of the Ada programs into Petri nets which preserve control-flow and message-flow properties is described. Properties of these Petri nets are discussed, and algorithms are given to analyze the nets to obtain information about static deadlocks that can occur in the original programs. Petri net invariants are used by the algorithms to reduce the time and space complexities associated with dynamic Petri net analysis (i.e. reachability graph generation).<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21759","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21759","","System recovery;Petri nets;Algorithm design and analysis;Information analysis;Programming profession;Monitoring;Dynamic scheduling;Computer science;Terminology","Ada;computational complexity;concurrency control;Petri nets;program testing;system recovery","Ada static deadlocks;Petri net invariants;Ada tasking programs;control-flow;message-flow;complexities","","100","","11","","","","","","IEEE","IEEE Journals & Magazines"
"A validation of object-oriented design metrics as quality indicators","V. R. Basili; L. C. Briand; W. L. Melo","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","1996","22","10","751","761","This paper presents the results of a study in which we empirically investigated the suite of object-oriented (OO) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known OO analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of these OO metrics are discussed. Several of Chidamber and Kemerer's OO metrics appear to be useful to predict class fault-proneness during the early phases of the life-cycle. Also, on our data set, they are better predictors than ""traditional"" code metrics, which can only be collected at a later phase of the software development processes.","0098-5589;1939-3520;2326-3881","","10.1109/32.544352","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=544352","","Programming;Object oriented modeling;Software systems;Software maintenance;Design methodology;Computer languages;Predictive models;Resource management;System testing;Costs","object-oriented methods;software metrics;software quality;software maintenance;information systems;object-oriented languages;C language","object-oriented design metrics;software quality indicators;fault-prone classes;class maintenance changes;metric validation;information management systems;sequential life cycle model;object oriented analysis;C++ programming language;data set;software development","","746","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Test Oracle Strategies for Model-Based Testing","N. Li; J. Offutt","Research and Development Division, Medidata Solutions, New York, NY; George Mason University, Fairfax, VA","IEEE Transactions on Software Engineering","","2017","43","4","372","395","Testers use model-based testing to design abstract tests from models of the system's behavior. Testers instantiate the abstract tests into concrete tests with test input values and test oracles that check the results. Given the same test inputs, more elaborate test oracles have the potential to reveal more failures, but may also be more costly. This research investigates the ability for test oracles to reveal failures. We define ten new test oracle strategies that vary in amount and frequency of program state checked. We empirically compared them with two baseline test oracle strategies. The paper presents several main findings. (1) Test oracles must check more than runtime exceptions because checking exceptions alone is not effective at revealing failures. (2) Test oracles do not need to check the entire output state because checking partial states reveals nearly as many failures as checking entire states. (3) Test oracles do not need to check program states multiple times because checking states less frequently is as effective as checking states more frequently. In general, when state machine diagrams are used to generate tests, checking state invariants is a reasonably effective low cost approach to creating test oracles.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2597136","George Mason University; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7529115","Test oracle;RIPR model;test oracle strategy;test automation;subsumption;model-based testing","Unified modeling language;Software;Context;Concrete;System testing;Observability","data flow analysis;diagrams;finite state machines;program testing;software fault tolerance","test oracle strategy;model-based testing;abstract test design;failure revelation;runtime exception;partial state checking;state machine diagram","","6","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Model-based performance risk analysis","V. Cortellessa; K. Goseva-Popstojanova; Kalaivani Appukkutty; A. R. Guedem; A. Hassan; R. Elnaggar; W. Abdelmoez; H. H. Ammar","Dept. of Comput. Sci., L'Aquila Univ., Italy; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","1","3","20","Performance is a nonfunctional software attribute that plays a crucial role in wide application domains spreading from safety-critical systems to e-commerce applications. Software risk can be quantified as a combination of the probability that a software system may fail and the severity of the damages caused by the failure. In this paper, we devise a methodology for estimation of performance-based risk factor, which originates from violations, of performance requirements, (namely, performance failures). The methodology elaborates annotated UML diagrams to estimate the performance failure probability and combines it with the failure severity estimate which is obtained using the functional failure analysis. We are thus able to determine risky scenarios as well as risky software components, and the analysis feedback can be used to improve the software design. We illustrate the methodology on an e-commerce case study using step-by step approach, and then provide a brief description of a case study based on large real system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392717","Index Terms- Nonfunctional requirements;software risk;software performance;UML;performance failure;Functional Failure Analysis.","Risk analysis;Software systems;Unified modeling language;Software performance;Software safety;Application software;Failure analysis;Computer Society;Feedback;Software design","software performance evaluation;risk analysis;electronic commerce;software fault tolerance;Unified Modeling Language;safety-critical software;formal specification","nonfunctional software;safety-critical systems;e-commerce applications;annotated UML diagrams;functional failure analysis;risky software components;model-based performance risk analysis","","30","","32","","","","","","IEEE","IEEE Journals & Magazines"
"ConPredictor: Concurrency Defect Prediction in Real-World Applications","T. Yu; W. Wen; X. Han; J. Hayes","Computer Science, University of Kentucky, Lexington, Kentucky United States 40506 (e-mail: tyu@cs.uky.edu); Computer Science, University of Kentucky, Lexington, Kentucky United States (e-mail: wei.wen0@uky.edu); Computer Science, University of Kentucky, Lexington, Kentucky United States (e-mail: xha225@g.uky.edu); Computer Science, University of Kentucky, Lexington, Kentucky United States 40506-0495 (e-mail: hayes@cs.uky.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Concurrent programs are difficult to test due to their inherent non-determinism. To address this problem, testing often requires the exploration of thread schedules of a program; this can be time-consuming when applied to real-world programs. Software defect prediction has been used to help developers find faults and prioritize their testing efforts. Prior studies have used machine learning to build such predicting models based on designed features that encode the characteristics of programs. However, research has focused on sequential programs; to date, no work has considered defect prediction for concurrent programs, with program characteristics distinguished from sequential programs. In this paper, we present ConPredictor, an approach to predict defects specific to concurrent programs by combining both static and dynamic program metrics. Specifically, we propose a set of novel static code metrics based on the unique properties of concurrent programs. We also leverage additional guidance from dynamic metrics constructed based on mutation analysis. Our evaluation on four large open source projects shows that ConPredictor improved both within-project defect prediction and cross-project defect prediction compared to traditional features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2791521","Directorate for Computer and Information Science and Engineering; Division of Computing and Communication Foundations; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8252721","","Concurrent computing;Predictive models;Software;Programming;Testing;Synchronization","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Exact analysis of Bernoulli superposition of streams into a least recently used cache","H. Levy; R. J. T. Morris","RUTCOR, Rutgers Univ., New Brunswick, NJ, USA; NA","IEEE Transactions on Software Engineering","","1995","21","8","682","688","We present an exact analysis of the superposition of address streams into a cache buffer which is managed according to a least recently used (LRU) replacement policy. Each of the streams is characterized by a stack depth distribution, and we seek the cache hit ratio for each stream, when the combined, or superposed, stream is applied to a shared LRU cache. The combining process is taken to be a Bernoulli switching process. This problem arises in a number of branches of computer science, particularly in database systems and processor architecture. Previously, a number of approximation techniques of various complexities have been proposed for the solution of this problem. The main contribution of the paper is the description of an exact technique. We evaluate the performance of the exact and an approximate technique on realistic data, both in a lab environment and a large database installation. The results allow comparisons of the techniques, and provide insight into the validity of the Bernoulli switching assumption.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.403792","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=403792","","Computer science;Database systems;Cache memory;Memory management;Predictive models;Information analysis;Approximation methods;Computational complexity","cache storage;storage allocation;storage management","Bernoulli superposition;streams;least recently used cache;exact analysis;cache buffer;LRU replacement policy;cache hit ratio;shared LRU cache;Bernoulli switching process;approximation techniques;realistic data;large database installation;Bernoulli switching assumption","","6","","9","","","","","","IEEE","IEEE Journals & Magazines"
"A distributed specification model and its prototyping","Y. Wang","GTE Labs. Inc., Waltham, MA, USA","IEEE Transactions on Software Engineering","","1988","14","8","1090","1097","A specification model is described that is based on the finite-state machine but is distributed. The model allows the user to decompose a large system into separate views. Each view is a complete system in itself, and reveals how the whole system would behave as seen from a certain angle. Put together, the combined views present a complete picture of the whole system. The complexity of a large centralized system is thus distributed and subdued. The author offers a simple execution scheme for the model. Using a high-level state-transition language called SXL, constructs in the model are expressed as preconditions and postconditions of transitions. The execution scheme allows all the views in the model to proceed in a parallel but harmonious way, producing a working prototype for the modeled system.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.7619","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7619","","Prototypes;Automata;Switching systems;Computer errors;Feedback;Vehicles;Software prototyping;Design engineering;Proposals;Programming","distributed processing;finite automata;high level languages;software engineering;specification languages","Boolean expressions;switching systems;software engineering;distributed specification model;prototyping;finite-state machine;complexity;execution scheme;high-level state-transition language;SXL;preconditions;postconditions","","8","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing join queries in distributed databases","S. Pramanik; D. Vineyard","Dept. of Comput. Sci., Michigan State Univ., East Lansing, MI, USA; NA","IEEE Transactions on Software Engineering","","1988","14","9","1319","1326","A reduced cover set of the set of full reducer semijoin programs for an acyclic query graph for a distributed database system is given. An algorithm is presented that determines the minimum cost full reducer program. The computational complexity of finding the optimal full reducer for a single relation is of the same order as that of finding the optimal full reducer for all relations. The optimization algorithm is able to handle query graphs where more than one attribute is common between the relations. A method for determining the optimum profitable semijoin program is presented. A low-cost algorithm which determines a near-optimal profitable semijoin program is outlined. This is done by converting a semijoin program into a partial order graph. This graph also allows one to maximize the concurrent processing of the semijoins. It is shown that the minimum response time is given by the largest cost path of the partial order graph. This reducibility is used as a post optimizer for the SSD-1 query optimization algorithm. It is shown that the least upper bound on the length of any profitable semijoin program is N(N-1) for a query graph of N nodes.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6175","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6175","","Distributed databases;Cost function;Query processing;Data communication;Database systems;Computational complexity;Delay;Computer science;Upper bound;Greedy algorithms","computational complexity;database theory;distributed databases;graph theory;optimisation","join queries;distributed databases;reduced cover set;acyclic query graph;distributed database;computational complexity;optimization;partial order graph;concurrent processing;minimum response time","","33","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Locating features in source code","T. Eisenbarth; R. Koschke; D. Simon","Inst. of Comput. Sci., Stuttgart Univ., Germany; Inst. of Comput. Sci., Stuttgart Univ., Germany; Inst. of Comput. Sci., Stuttgart Univ., Germany","IEEE Transactions on Software Engineering","","2003","29","3","210","224","Understanding the implementation of a certain feature of a system requires identification of the computational units of the system that contribute to this feature. In many cases, the mapping of features to the source code is poorly documented. In this paper, we present a semiautomatic technique that reconstructs the mapping for features that are triggered by the user and exhibit an observable behavior. The mapping is in general not injective; that is, a computational unit may contribute to several features. Our technique allows for the distinction between general and specific computational units with respect to a given set of features. For a set of features, it also identifies jointly and distinctly required computational units. The presented technique combines dynamic and static analyses to rapidly focus on the system's parts that relate to a specific set of features. Dynamic information is gathered based on a set of scenarios invoking the features. Rather than assuming a one-to-one correspondence between features and scenarios as in earlier work, we can now handle scenarios that invoke many features. Furthermore, we show how our method allows incremental exploration of features while preserving the ""mental map"" the analyst has gained through the analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183929","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183929","","Computer Society;Software architecture;Instruction sets;Documentation;Degradation;Reverse engineering;Terminology","reverse engineering;program diagnostics;software architecture","formal concept analysis;feature location;program analysis;software architecture recovery;identification;program comprehension","","270","","46","","","","","","IEEE","IEEE Journals & Magazines"
"The class blueprint: visually supporting the understanding of glasses","S. Ducasse; M. Lanza","Software Composition Group, Bern Univ., Switzerland; NA","IEEE Transactions on Software Engineering","","2005","31","1","75","90","Understanding source code is an important task in the maintenance of software systems. Legacy systems are not only limited to procedural languages, but are also written in object-oriented languages. In such a context, understanding classes is a key activity as they are the cornerstone of the object-oriented paradigm and the primary abstraction from which applications are built. Such an understanding is however difficult to obtain because of reasons such as the presence of late binding and inheritance. A first level of class understanding consists of the understanding of its overall structure, the control flow among its methods, and the accesses on its attributes. We propose a novel visualization of classes called class blueprint that is based on a semantically enriched visualization of the internal structure of classes. This visualization allows a software engineer to build a first mental model of a class that he validates via opportunistic code-reading. Furthermore, we have identified visual patterns that represent recurrent situations and as such convey additional, information to the viewer. The contributions of this article are the class blueprint, a novel visualization of the internal structure of classes, the identification of visual patterns, and the definition of a vocabulary based on these visual patterns. We have performed several case studies of which one is presented in depth, and validated the usefulness of the approach in a controlled experiment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392721","Index Terms- Object-oriented programming;software visualization;reverse engineering;visual patterns;smalltalk.","Visualization;Software maintenance;Software systems;Application software;Cognitive science;Vocabulary;Programming profession;Reverse engineering;Phase measurement;Software measurement","object-oriented programming;Smalltalk;reverse engineering;program visualisation;software maintenance;inheritance;vocabulary;formal verification","software system maintenance;legacy systems;procedural languages;object-oriented languages;late binding;class understanding;class blueprint;opportunistic code-reading;software visualization;reverse engineering;visual patterns;Smalltalk","","43","","44","","","","","","IEEE","IEEE Journals & Magazines"
"WASP: Protecting Web Applications Using Positive Tainting and Syntax-Aware Evaluation","W. Halfond; A. Orso; P. Manolios","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","65","81","Many software systems have evolved to include a Web-based component that makes them available to the public via the Internet and can expose them to a variety of Web-based attacks. One of these attacks is SQL injection, which can give attackers unrestricted access to the databases that underlie Web applications and has become increasingly frequent and serious. This paper presents a new highly automated approach for protecting Web applications against SQL injection that has both conceptual and practical advantages over most existing techniques. From a conceptual standpoint, the approach is based on the novel idea of positive tainting and on the concept of syntax-aware evaluation. From a practical standpoint, our technique is precise and efficient, has minimal deployment requirements, and incurs a negligible performance overhead in most cases. We have implemented our techniques in the Web application SQL-injection preventer (WASP) tool, which we used to perform an empirical evaluation on a wide range of Web applications that we subjected to a large and varied set of attacks and legitimate accesses. WASP was able to stop all of the otherwise successful attacks and did not generate any false positives.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70748","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359474","Protection mechanisms;Security and Protection;Protection mechanisms;Security and Protection","Protection;Databases;Application software;Computer Society;Internet;Runtime;Data security;Information security;Software systems;Performance evaluation","Internet;security of data;SQL","WASP;positive tainting;syntax-aware evaluation;software systems;Internet;Web-based attacks;Web application SQL-injection preventer","","59","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Towards complexity metrics for Ada tasking","S. M. Shatz","Dept. of Elect. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA","IEEE Transactions on Software Engineering","","1988","14","8","1122","1127","Using Ada as a representative distributed programming language, the author discusses some ideas on complexity metrics that focus on Ada tasking and rendezvous. Concurrently active rendezvous are claimed to be an important aspect of communication complexity. A Petri net graph model of Ada rendezvous is used to introduce a rendezvous graph, an abstraction that can be useful in viewing and computing effective communication complexity.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.7623","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7623","","Distributed computing;Software maintenance;Software measurement;Software testing;Complexity theory;Petri nets;Programming;Software design;Guidelines;Software metrics","Ada;computational complexity;directed graphs;distributed processing;software engineering","concurrently active rendezvous;software engineering;software complexity;complexity metrics;Ada tasking;representative distributed programming language;communication complexity;Petri net graph model;Ada rendezvous;rendezvous graph","","22","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Are Slice-Based Cohesion Metrics Actually Useful in Effort-Aware Post-Release Fault-Proneness Prediction? An Empirical Study","Y. Yang; Y. Zhou; H. Lu; L. Chen; Z. Chen; B. Xu; H. Leung; Z. Zhang","State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China; State Key Laboratory for Novel Software Technology, School of Software, Nanjing, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong, China; State Key Laboratory of Computer Science, Institute of Software, Beijing, China","IEEE Transactions on Software Engineering","","2015","41","4","331","357","Background. Slice-based cohesion metrics leverage program slices with respect to the output variables of a module to quantify the strength of functional relatedness of the elements within the module. Although slice-based cohesion metrics have been proposed for many years, few empirical studies have been conducted to examine their actual usefulness in predicting fault-proneness. Objective. We aim to provide an in-depth understanding of the ability of slice-based cohesion metrics in effort-aware post-release fault-proneness prediction, i.e. their effectiveness in helping practitioners find post-release faults when taking into account the effort needed to test or inspect the code. Method. We use the most commonly used code and process metrics, including size, structural complexity, Halstead's software science, and code churn metrics, as the baseline metrics. First, we employ principal component analysis to analyze the relationships between slice-based cohesion metrics and the baseline metrics. Then, we use univariate prediction models to investigate the correlations between slice-based cohesion metrics and post-release fault-proneness. Finally, we build multivariate prediction models to examine the effectiveness of slice-based cohesion metrics in effort-aware post-release fault-proneness prediction when used alone or used together with the baseline code and process metrics. Results. Based on open-source software systems, our results show that: 1) slice-based cohesion metrics are not redundant with respect to the baseline code and process metrics; 2) most slice-based cohesion metrics are significantly negatively related to post-release fault-proneness; 3) slice-based cohesion metrics in general do not outperform the baseline metrics when predicting post-release fault-proneness; and 4) when used with the baseline metrics together, however, slice-based cohesion metrics can produce a statistically significant and practically important improvement of the effectiveness in effort-aware post-release fault-proneness prediction. Conclusion. Slice-based cohesion metrics are complementary to the most commonly used code and process metrics and are of practical value in the context of effort-aware post-release fault-proneness prediction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2370048","National Key Basic Research and Development Program of China; National Natural Science Foundation of China; National Natural Science Foundation of Jiangsu Province; National Science and Technology Major Project of China; Hong Kong Competitive Earmarked Research; PolyU; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6954519","Cohesion;metrics;slice-based;fault-proneness;prediction;effort-aware","Measurement;Software;Predictive models;Context;Complexity theory;Correlation;Laboratories","principal component analysis;public domain software;software metrics","effort aware post-release fault proneness prediction;slice-based cohesion metrics leverage program slices;structural complexity;Halstead's software science;code churn metrics;baseline metrics;principal component analysis;univariate prediction models;multivariate prediction models;baseline code;process metrics;open source software systems","","16","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Visual knowledge engineering","M. Eisenstadt; J. Domingue; T. Rajan; E. Motta","Human Cognition Res. Lab., Open Univ., Milton Keynes, UK; Human Cognition Res. Lab., Open Univ., Milton Keynes, UK; Human Cognition Res. Lab., Open Univ., Milton Keynes, UK; Human Cognition Res. Lab., Open Univ., Milton Keynes, UK","IEEE Transactions on Software Engineering","","1990","16","10","1164","1177","The knowledge engineer is only weakly supported at three critical stages in the knowledge engineering life cycle: (1) knowledge acquisition during which problem conceptualization must largely be tackled with paper and pencil; (2) knowledge encoding, during which it is frequently necessary to be able to navigate across a variety of knowledge representation formalisms; and (3) large-scale debugging, in which the graphical rule traces cannot cope with enormous rule sets involving hundreds or thousands of rules. The research described attempts to provide just such support through complementary visual programming (VP) and program visualization (PV) techniques embedded in a fully implemented software environment called KEATS: the knowledge engineer's assistant. Several novel visual programming and program visualization techniques aimed at knowledge engineers have been developed, which include (1) a hypertext transcript analyzer from which conceptual models can be generated, (2) a direct graph manipulation sketchpad which allows the knowledge engineer to sketch out objects and relations (including control flow and rule dependencies) from which code can be generated, and (3) dependency viewers which allow the knowledge engineer to examine and manipulate temporal and logical rule dependencies at different levels of granularity. How these facilities are incorporated into KEATS and the key themes that emerge from this approach to visual knowledge engineering are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60296","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60296","","Knowledge engineering;Visualization;Knowledge acquisition;Encoding;Navigation;Knowledge representation;Large-scale systems;Debugging;Embedded software;Logic programming","knowledge engineering;programming environments;visual programming","visual knowledge engineering;knowledge acquisition;knowledge encoding;knowledge representation formalisms;large-scale debugging;graphical rule traces;complementary visual programming;program visualization;software environment;KEATS;hypertext transcript analyzer;direct graph manipulation sketchpad;dependency viewers;logical rule dependencies","","30","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Reasoning about places, times, and actions in the presence of mobility","C. D. Wilcox; G. -. Roman","6746 W. Robin Lane, Glendale, AZ, USA; NA","IEEE Transactions on Software Engineering","","1996","22","4","225","247","The current trend toward portable computing systems (e.g., cellular phones, laptop computers) brings with it the need for a new paradigm to facilitate thinking about and designing distributed applications. We use the term mobile to refer to distributed systems that include moving, autonomous agents which loosely cooperate to accomplish a task. The fluid nature of the interconnections among components of a mobile system provides new challenges and opportunities for the research community. While we do not claim to have fully grasped all the issues involved in specifying and modeling such systems, we believe that the notions of place, time, and action will play a central role in any model that is developed. We show that these concepts can be expressed and reasoned about in the UNITY logic with a minimal amount of additional notation. The formal derivation of a control system for a radio-dispatched elevator is used to show how considerations involving place, time, and actions impact the design process, be it formal or semiformal.","0098-5589;1939-3520;2326-3881","","10.1109/32.491647","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=491647","","Portable computers;Distributed computing;Cellular phones;Application software;Autonomous agents;Logic;Radio control;Control systems;Elevators;Process design","mobile radio;mobile communication;formal specification;distributed processing;portable computers;program verification","portable computing systems;mobility;distributed applications;distributed systems;moving autonomous agents;agent cooperation;component interconnections;specification;modeling;UNITY logic;formal control system derivation;radio-dispatched elevator;place reasoning;time reasoning;action reasoning","","3","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Static Specification Mining Using Automata-Based Abstractions","S. Shoham; E. Yahav; S. J. Fink; M. Pistoia","Technion, Haifa; IBM Research, Hawthorne; IBM Research, Hawthorne; IBM Research, Hawthorne","IEEE Transactions on Software Engineering","","2008","34","5","651","666","We present a novel approach to client-side mining of temporal API specifications based on static analysis. Specifically, we present an interprocedural analysis over a combined domain that abstracts both aliasing and event sequences for individual objects. The analysis uses a new family of automata-based abstractions to represent unbounded event sequences, designed to disambiguate distinct usage patterns and merge similar usage patterns. Additionally, our approach includes an algorithm that summarizes abstract traces based on automata clusters, and effectively rules out spurious behaviors. We show experimental results mining specifications from a number of Java clients and APIs. The results indicate that effective static analysis for client-side mining requires fairly precise treatment of aliasing and abstract event sequences. Based on the results, we conclude that static client-side specification mining shows promise as a complement or alternative to dynamic approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.63","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4589215","Specification;Formal methods;Validation;Software/Program Verification;Specifying and Verifying and Reasoning about Programs;Specification;Formal methods;Validation;Software/Program Verification;Specifying and Verifying and Reasoning about Programs","Pattern analysis;Libraries;Abstracts;Clustering algorithms;Automata;Java;Software engineering;Formal specifications;Internet;Search engines","application program interfaces;data mining;finite automata;formal specification;Java;program diagnostics","static specification mining;automata-based abstractions;temporal API specifications;static analysis;distinct usage patterns;Java clients;client-side mining;abstract event sequences","","26","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Monitor-Based Instant Software Refactoring","H. Liu; X. Guo; W. Shao","Beijing Institute of Technology, Beijing; Beijing Institute of Technology, Beijing; Peking University, Beijing","IEEE Transactions on Software Engineering","","2013","39","8","1112","1126","Software refactoring is an effective method for improvement of software quality while software external behavior remains unchanged. To facilitate software refactoring, a number of tools have been proposed for code smell detection and/or for automatic or semi-automatic refactoring. However, these tools are passive and human driven, thus making software refactoring dependent on developers' spontaneity. As a result, software engineers with little experience in software refactoring might miss a number of potential refactorings or may conduct refactorings later than expected. Few refactorings might result in poor software quality, and delayed refactorings may incur higher refactoring cost. To this end, we propose a monitor-based instant refactoring framework to drive inexperienced software engineers to conduct more refactorings promptly. Changes in the source code are instantly analyzed by a monitor running in the background. If these changes have the potential to introduce code smells, i.e., signs of potential problems in the code that might require refactorings, the monitor invokes corresponding smell detection tools and warns developers to resolve detected smells promptly. Feedback from developers, i.e., whether detected smells have been acknowledged and resolved, is consequently used to optimize smell detection algorithms. The proposed framework has been implemented, evaluated, and compared with the traditional human-driven refactoring tools. Evaluation results suggest that the proposed framework could drive inexperienced engineers to resolve more code smells (by an increase of 140 percent) promptly. The average lifespan of resolved smells was reduced by 92 percent. Results also suggest that the proposed framework could help developers to avoid similar code smells through timely warnings at the early stages of software development, thus reducing the total number of code smells by 51 percent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6409360","Software refactoring;code smell detection;monitor;instant refactoring","Software;Monitoring;Detection algorithms;Cloning;Detectors;Algorithm design and analysis;Inspection","software maintenance;software quality","monitor-based instant software refactoring;software quality;software external behavior;code smell detection;smell detection algorithm","","19","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Perceived influences on implementing data warehousing","R. G. Little; M. L. Gibson","Dept. of Inf. Syst./Decision Sci., Auburn Univ., Montgomery, AL, USA; NA","IEEE Transactions on Software Engineering","","2003","29","4","290","296","This study surveyed data warehousing implementation project participants to determine what aspects they perceived should contribute to the implementation process. The respondents included: functional managers/staff, IS managers/staff, and consultants. The study identified eight significant factors that participants perceived should impact data warehouse implementation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1191794","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1191794","","Warehousing;Data warehouses;Decision support systems;Databases;Data mining;Decision making;Performance analysis;Instruments;Vehicles","data warehouses;decision support systems;data mining","data warehousing;data mining;decision support systems;online analytical processing;expert interviews;survey instrument;factor analysis;Cronbach's coefficient alpha;latent factor names","","16","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Formal specification and design time testing","C. P. Gerrard; D. Coleman; R. M. Gallimore","Gerrard Software, Macclesfield, UK; NA; NA","IEEE Transactions on Software Engineering","","1990","16","1","1","12","It is shown how design time testing can be used in conjunction with formal specification. Emphasis is placed on the benefits of using an executable specification language OBJ, of having a design controlled by requirements specification, and of adherence to the regularity and uniformity hypotheses in dynamic validation. It is shown that such an approach offers positive benefits by providing early design validation and a controlled, disciplined design process.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44359","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44359","","Formal specifications;Software testing;Equations;Specification languages;Process design;Costs;Process control;Software design;Computer languages;System testing","formal specification;specification languages","formal specification;design time testing;executable specification language OBJ;regularity;uniformity","","6","","17","","","","","","IEEE","IEEE Journals & Magazines"
"The Link between Dependency and Cochange: Empirical Evidence","M. M. Geipel; F. Schweitzer","ETH Zurich, Zurich; ETH Zurich, Zurich","IEEE Transactions on Software Engineering","","2012","38","6","1432","1444","We investigate the relationship between class dependency and change propagation (cochange) in software written in Java. On the one hand, we find a strong correlation between dependency and cochange. Furthermore, we provide empirical evidence for the propagation of change along paths of dependency. These findings support the often alleged role of dependencies as propagators of change. On the other hand, we find that approximately half of all dependencies are never involved in cochanges and that the vast majority of cochanges pertain to only a small percentage of dependencies. This means that inferring the cochange characteristics of a software architecture solely from its dependency structure results in a severely distorted approximation of cochange characteristics. Any metric which uses dependencies alone to pass judgment on the evolvability of a piece of Java software is thus unreliable. As a consequence, we suggest to always take both the change characteristics and the dependency structure into account when evaluating software architecture.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363462","Modularity;class dependency;open source","Software development;Java;Open source software","Java;software architecture","class dependency;change propagation;Java software;cochange characteristic;software architecture","","10","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Stochastic Petri net representation of discrete event simulations","P. J. Haas; G. S. Shedler","IBM Almaden Res. Center, San Jose, CA, USA; IBM Almaden Res. Center, San Jose, CA, USA","IEEE Transactions on Software Engineering","","1989","15","4","381","393","In the context of discrete event simulation, the marking of a stochastic Petri net (SPN) corresponds to the state of the underlying stochastic process of the simulation and the firing of a transition corresponds to the occurrence of an event. A study is made of the modeling power of SPNs with timed and immediate transitions, showing that such Petri nets provide a general framework for simulation. The principle result is that for any (finite or) countable state GSMP (generalized semi-Markov process) there exists an SPN having a marking process that mimics the GSMP in the sense that the two processes (and their underlying general state-space Markov chains) have the same finite dimensional distributions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.16599","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=16599","","Stochastic processes;Discrete event simulation;State-space methods;Context modeling;Petri nets;Bipartite graph;Fires;Markov processes;Steady-state;State estimation","formal specification;Markov processes;Petri nets;simulation","transition firings;timed transitions;Petri net simulation;discrete event simulation;stochastic Petri net;countable state GSMP;generalized semi-Markov process","","27","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing software prediction techniques using simulation","M. Shepperd; G. Kadoda","Sch. of Design, Eng. & Comput., Bournemouth Univ., Poole, UK; NA","IEEE Transactions on Software Engineering","","2001","27","11","1014","1022","The need for accurate software prediction systems increases as software becomes much larger and more complex. We believe that the underlying characteristics: size, number of features, type of distribution, etc., of the data set influence the choice of the prediction system to be used. For this reason, we would like to control the characteristics of such data sets in order to systematically explore the relationship between accuracy, choice of prediction system, and data set characteristic. It would also be useful to have a large validation data set. Our solution is to simulate data allowing both control and the possibility of large (1000) validation cases. The authors compare four prediction techniques: regression, rule induction, nearest neighbor (a form of case-based reasoning), and neural nets. The results suggest that there are significant differences depending upon the characteristics of the data set. Consequently, researchers should consider prediction context when evaluating competing prediction systems. We observed that the more ""messy"" the data and the more complex the relationship with the dependent variable, the more variability in the results. In the more complex cases, we observed significantly different results depending upon the particular training set that has been sampled from the underlying data set. However, our most important result is that it is more fruitful to ask which is the best prediction system in a particular context rather than which is the ""best"" prediction system.","0098-5589;1939-3520;2326-3881","","10.1109/32.965341","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=965341","","Predictive models;Software systems;Machine learning;Uncertainty;Control systems;Accuracy;Nearest neighbor searches;Neural networks;Helium;Data engineering","software metrics;learning (artificial intelligence);case-based reasoning;neural nets;virtual machines","software prediction technique comparison;simulation;software prediction systems;prediction problem;data set characteristics;regression;rule induction;nearest neighbor;case-based reasoning;neural nets;small data sets;training set;machine learning","","128","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A performance comparison of multimicro and mainframe database architectures","P. Heidelberger; M. S. Lakshmi","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","1988","14","4","522","531","The performance of a hypothetical multiprocessor back-end database machine is compared to that of a mainframe database system. The hypothetical database machine uses standard microprocessors and disks as well as processors and disks projected to be available in the future. The class of workloads considered is high-volume transaction processing. Analytic queueing models of the two architectures are constructed and used in parametric performance studies. The performance sensitivities with respect to transactions complexity, the amount of overhead required to implement the distributed database function, the amount of skew in the data access pattern, and the buffer miss ratio are determined.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4675","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4675","","Database machines;Transaction databases;Relational databases;Microprocessors;Performance analysis;Costs;Queueing analysis;Delay;Database systems;Distributed databases","database management systems;mainframes;microcomputers;multiprocessing systems;performance evaluation;queueing theory;special purpose computers","multimicro database machines;mainframe database machines;performance comparison;database architectures;transaction processing;queueing models;distributed database;data access","","9","","","","","","","","IEEE","IEEE Journals & Magazines"
"Multidimensional Timestamp Protocols for Concurrency Control","Pei-Jyun Leu; B. Bhargava","Department of Computer Sciences, Purdue University; NA","IEEE Transactions on Software Engineering","","1987","SE-13","12","1238","1253","We propose multidimensional timestamp protocols for concurrency control in database systems where each transaction is assigned a timestamp vector containing multiple elements. The timestamp vectors for two transactions can be equal if timestamp elements are assigned the same values. The serializability order among the transactions is determined by a topological sort of the corresponding timestamp vectors. The timestamp in our protocols is assigned dynamically and is not just based on the starting/finishing time as in conservative and optimistic timestamp methods. The concurrency control can be enforced based on more precise dependency information derived dynamically from the operations of the transactions. Several classes of logs have been identified based on the degree of concurrency or the number of logs accepted by a concurrency controller. The class recognized by our protocols is within D-serializable (DSR), and is different from all previously known classes such as two phase locking (2PL), strictly serializable (SSR), timestamp ordering (TO), which have been defined in literature. The protocols have been analyzed to study the complexity of recognition of logs. We briefly discuss the implementation of the concurrency control algorithm for the new class, and give a timestamp vector processing mechanism. The extension of the protocols for nested transaction and distributed database models has also been included.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232878","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702176","Concurrency control algorithms;database systems;degree of concurrency;k-dimensional timestamp ordering;logs;parallel processing;serializability;transactions","Multidimensional systems;Protocols;Concurrency control;Concurrent computing;Database systems;Finishing;Optimization methods;Distributed databases;Transaction databases;Control systems","","Concurrency control algorithms;database systems;degree of concurrency;k-dimensional timestamp ordering;logs;parallel processing;serializability;transactions","","4","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Verifying general safety properties of Ada tasking programs","L. K. Dillon","Dept. of Comput. Sci., California Univ., Santa Barbara, CA, USA","IEEE Transactions on Software Engineering","","1990","16","1","51","63","The isolation approach to symbolic execution of Ada tasking programs provides a basis for automating partial correctness proofs. The strength of this approach lies in its isolation nature; tasks are symbolically executed and verified independently, and then checked for cooperation where interference can occur. This keeps the verification task computationally feasible and enhances its compositionality. Safety, however, is a more appropriate notion of correctness for concurrent programs than partial correctness. The author shows how the isolation approach to symbolic execution of Ada tasking program supports the verification of general safety properties. Specific safety properties that are considered include mutual exclusion, freedom from deadlock, and absence of communication failure. The techniques are illustrated using a solution to the readers and writers problem.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44363","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44363","","Interference;System recovery;Interleaved codes;Mechanical factors;Logic;Computer science;Concurrent computing;Software safety","Ada;multiprocessing programs;program verification","safety properties verification;Ada tasking programs;isolation approach;symbolic execution;automating partial correctness proofs;concurrent programs;mutual exclusion;deadlock","","5","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Adaptation of Service Protocols Using Process Algebra and On-the-Fly Reduction Techniques","R. Mateescu; P. Poizat; G. Salaün","Inria Grenoble-Rhône-Alpes/CONVECS, Montbonnot Saint-Martin; Université d'Evry Val d'Essonne, Paris and LRI UMR CNRS, Université Paris Sud, Orsay; Grenoble INP and Inria Grenoble-Rhône-Alpes/CONVECS, Montbonnot Saint-Martin","IEEE Transactions on Software Engineering","","2012","38","4","755","777","Reuse and composition are increasingly advocated and put into practice in modern software engineering. However, the software entities that are to be reused to build an application, e.g., services, have seldom been developed to integrate and to cope with the application requirements. As a consequence, they present mismatch, which directly hampers their reusability and the possibility of composing them. Software Adaptation has become a hot topic as a nonintrusive solution to work mismatch out using corrective pieces named adaptors. However, adaptation is a complex issue, especially when behavioral interfaces, or conversations, are taken into account. In this paper, we present state-of-the-art techniques to generate adaptors given the description of reused entities' conversations and an abstract specification of the way mismatch can be solved. We use a process algebra to encode the adaptation problem, and propose on-the-fly exploration and reduction techniques to compute adaptor protocols. Our approach follows the model-driven engineering paradigm, applied to service-oriented computing as a representative field of composition-based software engineering. We take service description languages as inputs of the adaptation process and we implement adaptors as centralized service compositions, i.e., orchestrations. Our approach is completely tool supported.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.62","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928357","Service composition;software adaptation;interfaces;protocols;mismatch;adaptation contracts;process algebra;on--the-fly generation;verification;tools","Adaptation model;Protocols;Contracts;Algebra;Semantics;Encoding;Computational modeling","formal specification;process algebra;protocols;service-oriented architecture;software reusability;specification languages","service protocol adaptation;process algebra;on-the-fly reduction techniques;composition;software entities;reusability;software adaptation;adaptors;reused entity conversations;abstract specification;on-the-fly exploration;model-driven engineering paradigm;service-oriented computing;composition-based software engineering;service description languages;centralized service compositions","","23","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic Update of Discrete Event Controllers","L. Nahabedian; V. Braberman; N. D'Ippolito; S. Honiden; J. Kramer; K. Tei; S. Uchitel","Departamento de Computación, Universidad de Buenos Aires Facultad de Ciencias Exactas y Naturales, 117139 Ciudad Autónoma de Buenos Aires, Ciudad Autónoma de Buenos Aires Argentina (e-mail: lnahabedian@dc.uba.ar); Departamento de Computación, FCEyN, Universidad de Buenos Aires, Ciudad Autónoma de Buenos Aires, Ciudad Autónoma de Buenos Aires Argentina (e-mail: vbraber@dc.uba.ar); Departamento de Computación, Universidad de Buenos Aires Facultad de Ciencias Exactas y Naturales, 117139 Ciudad Autónoma de Buenos Aires, Ciudad Autónoma de Buenos Aires Argentina (e-mail: ndippolito@dc.uba.ar); Research Division, NII, Tokyo, Chiyoda-ku Japan (e-mail: honiden@nii.ac.jp); Computing, Imperial College, London, London United Kingdom of Great Britain and Northern Ireland (e-mail: j.kramer@imperial.ac.uk); Research Division, NII, Tokyo, Chiyoda-ku Japan (e-mail: tei@nii.ac.jp); Departamento de Computación, Universidad de Buenos Aires Facultad de Ciencias Exactas y Naturales, 117139 Ciudad Autónoma de Buenos Aires, Ciudad Autónoma de Buenos Aires Argentina (e-mail: suchitel@dc.uba.ar)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Discrete event controllers are at the heart of many software systems that require continuous operation. Changing these controllers at runtime to cope with changes in its execution environment or system requirements change is a challenging open problem. In this paper we address the problem of dynamic update of controllers in reactive systems. We present a general approach to specifying correctness criteria for dynamic update and a technique for automatically computing a controller that handles the transition from the old to the new specification, assuring that the system will reach a state in which such a transition can correctly occur and in which the underlying system architecture can reconfigure. Our solution uses discrete event controller synthesis to automatically build a controller that guarantees both progress towards update and safe update.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2876843","Universidad de Buenos Aires; Fondo para la Investigacion Cientefica y Tecnologica; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8500345","Controller Synthesis;Dynamic Update;Adaptive Systems","Tools;Runtime;Paints;Control systems;Business;Safety","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A Generalized Timed Petri Net Model for Performance Analysis","M. A. Holliday; M. K. Vernon","Department of Computer Science, Duke University; NA","IEEE Transactions on Software Engineering","","1987","SE-13","12","1297","1310","We have developed a Generalized Timed Petri Net (GTPN) model for evaluating the performance of computer systems. Our model is a generalization of the TPN model proposed by Zuberek [1] and extended by Razouk and Phelps [2]. In this paper, we define the GTPN model and present how performance estimates are obtained from the GTPN. We demonstrate the use of our automated GTPN analysis techniques on the dining philosophers example. This example violates restrictions made in the earlier TPN models. Finally, we compare the GTPN to the stochastic Petri net (SPN) models. We show that the GTPN model has capabilities for modeling and analyzing parallel systems lacking in existing SPN models. The GTPN provides an efficient, easily used method of obtaining accurate performance estimates for models of computer systems which include both deterministic and geometric holding times.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233141","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702180","Deterministic delays;dining philosophers;embedded Markov chain;Markov models;performance analysis;Petri nets","Performance analysis;Petri nets;Stochastic processes;Solid modeling;Delay;Frequency;Interference;Computational modeling;System performance;Reachability analysis","","Deterministic delays;dining philosophers;embedded Markov chain;Markov models;performance analysis;Petri nets","","112","","24","","","","","","IEEE","IEEE Journals & Magazines"
"X-ware reliability and availability modeling","J. -. Laprie; K. Kanoun","LAAS-CNRS, Toulouse, France; LAAS-CNRS, Toulouse, France","IEEE Transactions on Software Engineering","","1992","18","2","130","147","The problem of modeling a system's reliability and availability with respect to the various classes of faults (physical and design, internal and external) which may affect the service delivered to its users is addressed. Hardware and software models are currently exceptions in spite of the user's requirements; these requirements are expressed in terms of failures independently of their sources, i.e., the various classes of faults. The causes of this situation are analyzed; it is shown that there is no theoretical impediment to deriving such models, and that the classical reliability theory can be generalized in order to cover both hardware and software viewpoints that are X-Ware.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.121755","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=121755","","Availability;Hardware;Electronic switching systems;Maintenance;Software safety;Software performance;Performance evaluation;Impedance;Reliability theory;Software reliability","fault tolerant computing;performance evaluation;reliability theory;software reliability","availability modeling;faults;software models;classical reliability theory;software viewpoints;X-Ware","","70","","78","","","","","","IEEE","IEEE Journals & Magazines"
"The Programmers' Playground: I/O abstraction for user-configurable distributed applications","K. J. Goldman; B. Swaminathan; P. McCartney; M. D. Anderson; R. Sethuraman","Dept. of Comput. Sci., Washington Univ., St. Louis, MO, USA; Dept. of Comput. Sci., Washington Univ., St. Louis, MO, USA; Dept. of Comput. Sci., Washington Univ., St. Louis, MO, USA; NA; NA","IEEE Transactions on Software Engineering","","1995","21","9","735","746","I/O abstraction is offered as a new high-level approach to interprocess communication. Functional components of a distributed system are written as encapsulated modules that act upon local data structures, some of which may be published for external use. Relationships among modules are specified by logical connections among their published data structures. Whenever a module updates published data, I/O takes place implicitly according to the configuration of logical connections. The Programmers' Playground, a software library and runtime system supporting I/O abstraction, is described. Design goals include the separation of communication from computation, dynamic reconfiguration of the communication structure, and the uniform treatment of discrete and continuous data types. Support for end-user configuration of distributed multimedia applications is the motivation for the work.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.464547","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=464547","","Programming profession;Application software;Data structures;Software libraries;Programming environments;Dynamic programming;Asynchronous transfer mode;Streaming media;Bandwidth;Collaborative work","program compilers;software libraries;distributed processing;multimedia computing;programming environments;software tools;data structures","Programmers' Playground;I/O abstraction;user-configurable distributed applications;interprocess communication;distributed system;encapsulated modules;local data structures;logical connections;software library;runtime system;continuous data types;discrete data types;end-user configuration;distributed multimedia applications;input/output abstraction;programming environments","","29","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic synthesis of SARA design models from system requirements","K. -. E. Lor; D. M. Berry","AT&T Bell Labs., Middletown, NJ, USA; NA","IEEE Transactions on Software Engineering","","1991","17","12","1229","1240","In this research in design automation, two views are employed as the requirements of a system-namely, the functional requirements and the operations concept. A requirement analyst uses data flow diagrams and system verification diagrams (SVDs) to represent the functional requirements and the operations concept, respectively. System Architect's Apprentice (SARA) is an environment-supported method for designing hardware and software systems. A knowledge-based system, called the design assistant, was built to help the system designer to transform requirements stated in one particular collection of design languages. The SVD requirement specification features and the SARA design models are reviewed. The knowledge-based tool for synthesizing a particular domain of SARA design from the requirements is described, and an example is given to illustrate this synthesis process. This example shows the rules used and how they are applied. An evaluation of the approach is given.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.106984","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=106984","","Design automation;Process design;Humans;Hardware;Design methodology;Software design;Computer science;Systems engineering and theory;Buildings;Software systems","diagrams;knowledge based systems;software tools;systems analysis","computer aided software engineering;software design automation;SARA design models;functional requirements;requirement analyst;data flow diagrams;system verification diagrams;System Architect's Apprentice;hardware;software;knowledge-based system;design assistant;design languages;SVD requirement specification;knowledge-based tool","","8","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A formal specification and verification framework for Time Warp-based parallel simulation","P. Frey; R. Radhakrishnan; H. W. Carter; P. A. Wilsey; P. Alexander","Cadence Design Syst. Inc., San Jose, CA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","1","58","78","The paper describes a formal framework developed using the Prototype Verification System (PVS) to model and verify distributed simulation kernels based on the Time Warp paradigm. The intent is to provide a common formal base from which domain specific simulators can be modeled, verified, and developed. PVS constructs are developed to represent basic Time Warp constructs. Correctness conditions for Time Warp simulation are identified, describing causal ordering of event processing and correct rollback processing. The PVS theorem prover and type-check condition system are then used to verify all correctness conditions. In addition, the paper discusses the framework's reusability and extensibility properties in support of specification and verification of Time Warp extensions and optimizations.","0098-5589;1939-3520;2326-3881","","10.1109/32.979989","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979989","","Formal specifications;Time warp simulation","time warp simulation;formal specification;program verification;theorem proving;parallel programming","formal specification;formal verification framework;Time Warp-based parallel simulation;Prototype Verification System;distributed simulation kernels;Time Warp paradigm;common formal base;domain specific simulators;correctness conditions;causal ordering;event processing;correct rollback processing;PVS theorem prover;type-check condition system;parallel discrete event simulation","","5","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional Control of IP Media","P. Zave; E. Cheung","AT&T Laboratories-Research, Florham Park; AT&T Laboratories-Research, Florham Park","IEEE Transactions on Software Engineering","","2009","35","1","46","66","In many IP media services, the media channels are point-to-point, dynamic, and set up with the participation of one or more application servers, even thou the media packets themselves travel directly between media endpoints. The application servers must be programmed so that media behavior is globally correct, even though the servers may attempt to manipulate the same media channels concurrently and without knowledge of each other. Our proposed solution to this problem of compositional media control includes an architecture-independent descriptive model, a set of high-level programming primitives, a formal specification of their compositional semantics, a signaling protocol, an implementation, and partial verification of correctness. The paper includes performance analysis, comparison to related work, and principles for making other networked applications more compositional.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4569853","distributed applications;domain-specific architectures;protocol verification;protocol design;software/program verification;networks;streaming media;multimedia services;telecommunications;feature interaction;distributed applications;domain-specific architectures;protocol verification;protocol design;software/program verification;networks;streaming media;multimedia services;telecommunications;feature interaction","Application software;Network servers;Streaming media;Protocols;Internet telephony;Web server;Performance analysis;Computer architecture;Computer networks;Home computing","formal specification;IP networks;multimedia communication;signalling protocols","compositional control;IP media services;media channels;media packets;media endpoints;architecture-independent descriptive model;high-level programming primitives;formal specification;signaling protocol","","5","","16","","","","","","IEEE","IEEE Journals & Magazines"
"The gains from computer communication","H. Kameda","Department of Computer Science, University of Electro-Communications, 1-5-1 Chofugaoka, Chofu-shi, Tokyo 182, Japan","IEEE Transactions on Software Engineering","","1986","SE-12","11","1049","1055","The effects of providing communication channels among computer systems located at different sites are studied. It is shown that the maximum utility at every site with computer communication can be greater (or at least not less) than that without communication. This can be achieved in such a way that, with communication, each computer system is more specialized in processing transactions of the types in which the system has comparative advantage and mutually exchanges the processed transactions through communication channels; the utility here is assumed to depend on performance. It is shown, however, that growth in computing resources at a site toward a higher degree of specialization may sometimes lead to a decrease in the maximum utility at the site.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312994","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312994","Comparative advantage;computer communication;computer network;computer system performance;immiserizing growth;processing capacity;processing rate;specialization;transaction exchanging;utility","Computers;Exchange rates;Communities;Production;Communication channels;Computer networks","computer networks;telecommunication channels","transactions processing;computer communication;communication channels","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Identification of Move Method Refactoring Opportunities","N. Tsantalis; A. Chatzigeorgiou","University of Macedonia, Thessaloniki; University of Macedonia, Thessaloniki","IEEE Transactions on Software Engineering","","2009","35","3","347","367","Placement of attributes/methods within classes in an object-oriented system is usually guided by conceptual criteria and aided by appropriate metrics. Moving state and behavior between classes can help reduce coupling and increase cohesion, but it is nontrivial to identify where such refactorings should be applied. In this paper, we propose a methodology for the identification of Move Method refactoring opportunities that constitute a way for solving many common feature envy bad smells. An algorithm that employs the notion of distance between system entities (attributes/methods) and classes extracts a list of behavior-preserving refactorings based on the examination of a set of preconditions. In practice, a software system may exhibit such problems in many different places. Therefore, our approach measures the effect of all refactoring suggestions based on a novel entity placement metric that quantifies how well entities have been placed in system classes. The proposed methodology can be regarded as a semi-automatic approach since the designer will eventually decide whether a suggested refactoring should be applied or not based on conceptual or other design quality criteria. The evaluation of the proposed approach has been performed considering qualitative, metric, conceptual, and efficiency aspects of the suggested refactorings in a number of open-source projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4752842","Move Method refactoring;Feature Envy;object-oriented design;Jaccard distance;design quality.","Software systems;Performance evaluation;Open source software;Runtime;Productivity;Data mining","object-oriented programming;software maintenance;software metrics","Move Method refactoring opportunity identification;object-oriented system;conceptual criteria;software metrics;feature envy","","139","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Semantics-Preserving Design of Embedded Control Software from Synchronous Models","L. Mangeruca; M. Baleani; A. Ferrari; A. Sangiovanni-Vincentelli","PARADES GEIE, via San Pantaleo 66, 00186 Rome, Italy; PARADES GEIE, via San Pantaleo 66, 00186 Rome, Italy; PARADES GEIE, via San Pantaleo 66, 00186 Rome, Italy; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CA 94720","IEEE Transactions on Software Engineering","","2007","33","8","497","509","The design of embedded controllers is experiencing a growth in complexity as embedded systems increase their functionality while they become ubiquitous in electronic appliances, cars, airplanes, etc. As requirements become more challenging, mathematical models gain importance for mastering complexity. Among the different computational models proposed, synchronous models have proved to be the most widely used for control dominated applications. While synchronous models simplify the way of dealing with concurrency by decoupling functional and timing aspects, their software implementation on multitasking and multiprocessor platforms is far from straightforward, because of the asynchronous nature of most industrial software platforms. Known solutions in the literature either restrict the solution space or focus on special cases. We present a method for preserving the synchronous semantics through buffer-based intertask communication mechanisms, grounded on an abstraction of the target platform. This allows us to deal with any task set and, most importantly, being independent of the implementation, to explore the design space effectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70718","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267022","Software design methodologies;protection mechanisms;embedded software design.","Embedded software;Space exploration;Control systems;Embedded system;Home appliances;Airplanes;Mathematical model;Pervasive computing;Computational modeling;Application software","embedded systems;multiprocessing programs;software engineering;systems analysis","semantics-preserving design;embedded control software;embedded systems;software implementation;multitasking;multiprocessor","","4","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Design Stability Measures for Software Maintenance","S. S. Yau; J. S. Collofello","Department of Electrical Engineering and Computer Science, Northwestern University; NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","849","856","The high cost of software during its life cycle can be attributer largely to software maintenance activities, and a major portion of these activities is to deal with the modifications of the software. In this paper, design stability measures which indicate the potential ripple effect characteristics due to modifications of the program at the design level are presented. These measures can be generated at any point in the design phase of the software life cycle which enables early maintainability feedback to the software developers. The validation of these measures and future research efforts involving the development of a user-oriented maintainability measure, which incorporates the design stability measures as well as other design measures, are discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232544","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702104","Design stability measures;program modifications;software maintenance","Stability;Software measurement;Software maintenance;Costs;Phase measurement;Military computing;Hardware;Productivity;Application software;Software quality","","Design stability measures;program modifications;software maintenance","","69","","15","","","","","","IEEE","IEEE Journals & Magazines"
"The Work Life of Developers: Activities, Switches and Perceived Productivity","A. N. Meyer; L. E. Barton; G. C. Murphy; T. Zimmermann; T. Fritz","University of Zurich, Zürich, Switzerland; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; Microsoft Research, Redmond, WA; University of Zurich, Zürich, Switzerland","IEEE Transactions on Software Engineering","","2017","43","12","1178","1193","Many software development organizations strive to enhance the productivity of their developers. All too often, efforts aimed at improving developer productivity are undertaken without knowledge about how developers spend their time at work and how it influences their own perception of productivity. To fill in this gap, we deployed a monitoring application at 20 computers of professional software developers from four companies for an average of 11 full work day in situ. Corroborating earlier findings, we found that developers spend their time on a wide variety of activities and switch regularly between them, resulting in highly fragmented work. Our findings extend beyond existing research in that we correlate developers' work habits with perceived productivity and also show productivity is a personal matter. Although productivity is personal, developers can be roughly grouped into morning, low-at-lunch and afternoon people. A stepwise linear regression per participant revealed that more user input is most often associated with a positive, and emails, planned meetings and work unrelated websites with a negative perception of productivity. We discuss opportunities of our findings, the potential to predict high and low productivity and suggest design approaches to create better tool support for planning developers' work day and improving their personal productivity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2656886","ABB; SNF; NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7829407","Productivity;developer activity;work fragmentation;interruptions;human factors;user studies","Productivity;Software development;Encoding;Human factors;Monitoring","regression analysis;software development management;software engineering","developer productivity;software development organizations;personal productivity;work unrelated websites;perceived productivity;highly fragmented work;professional software developers","","4","","64","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Theory of fault-based predicate testing for computer programs","Kuo-Chung Tai","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","IEEE Transactions on Software Engineering","","1996","22","8","552","562","Predicates appear in both the specification and implementation of a program. One approach to software testing, referred to as predicate testing, is to require certain types of tests for a predicate. In this paper, three fault-based testing criteria are defined for compound predicates, which are predicates with one or more AND/OR operators. BOR (boolean operator) testing requires a set of tests to guarantee the detection of (single or multiple) boolean operator faults, including incorrect AND/OR operators and missing/extra NOT operators. BRO (boolean and relational operator) testing requires a set of tests to guarantee the detection of boolean operator faults and relational operator faults (i.e., incorrect relational operators). BRE (boolean and relational expression) testing requires a set of tests to guarantee the detection of boolean operator faults, relational operator faults, and a type of fault involving arithmetical expressions. It is shown that for a compound predicate with n, n>0, AND/OR operators, at most n+2 constraints are needed for BOR testing and at most 2*n+3 constraints for BRO or BRE testing, where each constraint specifies a restriction on the value of each boolean variable or relational expression in the predicate. Algorithms for generating a minimum set of constraints for BOR, BRO, and BRE testing of a compound predicate are given, and the feasibility problem for the generated constraints is discussed. For boolean expressions that contain multiple occurrences of some boolean variables, how to combine BOR testing with the meaningful impact strategy (Weyuker et al., 1994) is described.","0098-5589;1939-3520;2326-3881","","10.1109/32.536956","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=536956","","Software testing;Fault detection;Materials testing;Software engineering;Computer science;Digital arithmetic","programming theory;program testing;Boolean functions;program debugging","fault-based predicate testing;program testing;compound predicates;formal specification;program implementation;AND/OR operators;BOR testing;boolean operator testing;NOT operators;BRO testing;boolean relational operator testing;relational operator faults;boolean operator faults;BRE testing;boolean relational expression testing;arithmetical expressions;boolean variable;feasibility problem;boolean expressions;meaningful impact strategy","","56","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Distributed program reliability analysis","V. K. P. Kumar; S. Hariri; C. S. Raghavendra","Department of Electrical Engineering - Systems, University of Southern California, Los Angeles, CA 90089; Department of Electrical Engineering - Systems, University of Southern California, Los Angeles, CA 90089; Department of Electrical Engineering - Systems, University of Southern California, Los Angeles, CA 90089","IEEE Transactions on Software Engineering","","1986","SE-12","1","42","50","The reliability of distributed processing systems can be expressed in terms of the reliability of the processing elements that run the programs, the reliability of the processing elements holding the required files, and the reliability of the communication links used in file transfers. The authors introduce two reliability measures, namely distributed program reliability and distributed system reliability, to accurately model the reliability of distributed systems. The first measure describes the probability of successful execution of a distributed program which runs on some processing elements and needs to communicate with other processing elements for remote files, while the second measure describes the probability that all the programs of a given set can run successfully. The notion of minimal file spanning trees is introduced to efficiently evaluate these reliability measures. Graph theory techniques are used to systematically generate file spanning trees that provide all the required connections. The technique is general and can be used in a dynamic environment for efficient reliability evaluation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312918","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312918","Distributed program;distributed system;graph theory;reliability;spanning tree","Computer network reliability;Software reliability;Distributed processing;Reliability theory;Indexes;Reliability engineering","distributed processing;software reliability;trees (mathematics)","distributed processing systems;reliability;files;communication links;file transfers;reliability measures;distributed program reliability;distributed system reliability;probability;minimal file spanning trees","","43","","","","","","","","IEEE","IEEE Journals & Magazines"
"Modular Pluggable Analyses for Data Structure Consistency","V. Kuncak; P. Lam; K. Zee; M. C. Rinard","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","12","988","1005","Hob is a program analysis system that enables the focused application of multiple analyses to different modules in the same program. In our approach, each module encapsulates one or more data structures and uses membership in abstract sets to characterize how objects participate in data structures. Each analysis verifies that the implementation of the module 1) preserves important internal data structure consistency properties and 2) correctly implements a set algebra interface that characterizes the effects of operations on the data structure. Collectively, the analyses use the set algebra to 1) characterize how objects participate in multiple data structures and to 2) enable the interanalysis communication required to verify properties that depend on multiple modules analyzed by different analyses. We implemented our system and deployed several pluggable analyses, including a flag analysis plug-in for modules in which abstract set membership is determined by a flag field in each object, a PALE shape analysis plug-in, and a theorem proving plug-in for analyzing arbitrarily complicated data structures. Our experience shows that our system can effectively 1) verify the consistency of data structures encapsulated within a single module and 2) combine analysis results from different analysis plug-ins to verify properties involving objects shared by multiple modules analyzed by different analyses","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.125","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016574","Typestate;data structure;invariant;program analysis;program verification;shape analysis;formal methods;programming language design.","Data analysis;Data structures;Algorithm design and analysis;Shape;Algebra;Computer languages;Design methodology;Computer crashes;Scalability;Failure analysis","data structures;formal specification;program diagnostics;program verification","modular pluggable analyses;data structure consistency;program analysis system;set algebra interface;flag analysis plug-in;abstract set membership;PALE shape analysis plug-in;theorem proving plug-in;Hob analysis approach;program verification","","18","","80","","","","","","IEEE","IEEE Journals & Magazines"
"Support algorithms for incremental attribute evaluation of asynchronous subtree replacements","J. Micallef; G. E. Kaiser","Bellcore, Morristown, NJ, USA; NA","IEEE Transactions on Software Engineering","","1993","19","3","231","252","A solution to the problem of incremental attribute evaluation for multiple asynchronous subtree replacements that is applicable to arbitrary noncircular attribute grammars is discussed. The algorithm supports multiple independent editing cursors. Concurrent evaluation processes proceed independently as long as they cover disjoint regions of the derivation tree. Evaluation processes are merged when they overlap, to prevent unnecessary attribute evaluations. The complexity of these three parts of the algorithm is discussed. The algorithm ensures that when evaluation terminates, the tree is consistently attributed. The results solve two open problems that arose in connection with the original algorithm for asynchronous subtree replacements reported by S.M. Kaplan and G.F. Kaiser (1986).<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.221136","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=221136","","Programming profession;User interfaces;Computer languages;Collaborative work;Law;Legal factors;Writing;Error correction;Sun;Telecommunication computing","attribute grammars;computational complexity;text editing;tree data structures;trees (mathematics)","incremental attribute evaluation;multiple asynchronous subtree replacements;arbitrary noncircular attribute grammars;multiple independent editing cursors;disjoint regions;derivation tree;complexity;open problems","","1","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling Product Line Software Assets Using Domain-Specific Kits","N. I. Altintas; S. Cetin; A. H. Dogru; H. Oguztuzun","Cybersoft Information Technologies, Istanbul; Cybersoft Information Technologies, Istanbul; Middle East Technical University, Ankara; Middle East Technical University, Ankara","IEEE Transactions on Software Engineering","","2012","38","6","1376","1402","Software Product Line Engineering (SPLE) is a prominent paradigm for the assembly of a family of products using product line core assets. The modeling of software assets that together form the actual products is critical for achieving the strategic benefits of Software Product Lines (SPLs). We propose a feature-based approach to software asset modeling based on abstractions provided by Domain-Specific Kits (DSKs). This approach involves a software Asset Metamodel (AMM) used to derive Asset Modeling Languages (AMLs) that define reusable software assets in domain-specific terms. The approach also prescribes a roadmap for modeling these software assets in conjunction with the product line reference architecture. Asset capabilities can be modeled using feature diagrams as the external views of the software assets. Internal views can be expressed in terms of Domain-Specific Artifacts (DSAs) with Variability Points (VPs), where the domain-specific artifacts are created using Domain-Specific Kits. This approach produces loosely coupled and highly cohesive software assets that are reusable for multiple product lines. The approach is validated by assessing software asset reuse in two different product lines in the finance domain. We also evaluated the productivity gains in large-scale complex projects, and found that the approach yielded a significant reduction in the total project effort.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.109","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6065739","Asset modeling;domain-specific kits;feature models;reuse;software asset;software product lines","Software reliability;Computer architecture;Productivity;Programming;Complexity theory;Systematics","financial data processing;product development;project management;simulation languages;software reusability","product line software asset modelling;domain-specific kits;software product line engineering;SPLE;product line core assets;feature-based approach;DSK;software asset metamodel;AMM;asset modeling languages;AML;software asset reusability;domain-specific terms;product line reference architecture;feature diagrams;internal views;domain-specific artifacts;DSA;variability points;VP;finance domain;productivity gains;large-scale complex projects","","5","","86","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying Code of Individual Features in Client-Side Web Applications","J. Maras; M. Stula; J. Carlson; I. Crnkovic","University of Split, Split; University of Split, Split; Mälardalen University, Västerås; Mälardalen University, Västerås","IEEE Transactions on Software Engineering","","2013","39","12","1680","1697","Web applications are one of today's fastest growing software systems. Structurally, they are composed of two parts: the server side, used for data access and business logic, and the client side, used as a user interface. In recent years, with developers building complex interfaces, the client side is playing an increasingly important role. Unfortunately, the techniques and tools used to support development are not as advanced as in other disciplines. From the user's perspective, the client side offers a number of features that are relatively easy to distinguish. However, the same cannot be said for their implementation details. This makes the understanding, maintenance, and reuse of code difficult. The goal of the work presented in this paper is to improve reusability, maintainability, and performance of client-side web applications by identifying the code that implements a particular feature. We have evaluated the approach based on three different experiments: extracting features, extracting library functionalities, and page optimization. The evaluation shows that the method is able to identify the implementation details of individual features, and that by extracting the identified code, a considerable reduction in code size and increase in performance can be achieved.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6576749","Web applications;feature location;program slicing;code extraction","Feature extraction;HTML;Cascading style sheets;Browsers;Web and internet services;Optimization;Codes","feature extraction;Internet;user interfaces","individual features code identification;client-side Web application reusability;software systems;server side;data access;business logic;user interface;client-side Web application maintainability;client-side Web application performance;feature extraction;library functionality extraction;page optimization","","5","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Network-Clustered Multi-Modal Bug Localization","T. V. Hoang; R. J. Oentaryo; T. B. Le; D. Lo","School of Information System, Singapore Management University, 54756 Singapore, Singapore Singapore 188065 (e-mail: vdthoang.2016@smu.edu.sg); Living Analytics Research Centre, Singapore Management University, Singapore, SG Singapore (e-mail: roentaryo@smu.edu.sg); School of Information Systems, Singapore Management University, 54756 Singapore, Singapore Singapore 188065 (e-mail: btdle.2012@smu.edu.sg); School of Information Systems, Singapore Management University, Singapore, Singapore Singapore 17890 (e-mail: davidlo@smu.edu.sg)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Developers often spend much effort and resources to debug a program. To help the developers debug, numerous information retrieval (IR)-based and spectrum-based bug localization techniques have been devised. IR-based techniques process textual information in bug reports, while spectrum-based techniques process program spectra. While both techniques ultimately generate a ranked list of program elements that likely contain a bug, they only consider one source of information&#x2014;either bug reports or program spectra&#x2014;which is not optimal. In light of this deficiency, this paper presents a new approach dubbed Network-clustered Multi-modal Bug Localization (NetML), which utilizes multi-modal information from both bug reports and program spectra to localize bugs. NetML facilitates an effective bug localization by carrying out a joint optimization of bug localization error and clustering of both bug reports and program elements. Extensive experiments on 355 real bugs from seven software systems have been conducted to evaluate NetML against various state-of-the-art localization methods. The results show that NetML surpasses the best-performing baseline by 31.82%, 22.35%, 19.72%, and 19.24%, in terms of the number of bugs successfully localized when a developer inspects the top 1, 5, and 10 methods and Mean Average Precision (MAP), respectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2810892","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8306117","","Computer bugs;Adaptation models;Optimization;Debugging;Task analysis;Computational modeling;Information retrieval","","","","2","","","","","","","","IEEE","IEEE Early Access Articles"
"In-process evaluation for software inspection and test","J. K. Chaar; M. J. Halliday; I. S. Bhandari; R. Chillarege","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","1993","19","11","1055","1070","The goal of software inspection and test is to reduce the expected cost of software failure over the life of a product. The authors extend the use of defect triggers, the events that cause defects to be discovered, to help evaluate the effectiveness of inspections and test scenarios. In the case of inspections, the defect trigger is defined as a set of values that associate the skills of the inspector with the discovered defect. Similarly, for test scenarios, the defect trigger values embody the deferring strategies being used in creating these scenarios. The usefulness of triggers in evaluating the effectiveness of software inspections and tests is demonstrated by evaluating the inspection and test activities of some software products. These evaluations are used to point to deficiencies in inspection and test strategies, and to progress made in improving such strategies. The trigger distribution of the entire inspection or test series may then be used to highlight areas for further investigation, with the aim of improving the design, implementation, and test processes.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.256853","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=256853","","Inspection;Software testing;Software quality;Life testing;Costs;Programming;Senior members;Software reliability;Software development management;Software maintenance","program testing;software quality;software reliability","in-process evaluation;software inspection;software testing;expected cost;software failure;defect triggers;test scenarios","","34","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Modular redundancy in a message passing system","L. Mancini","Computing Laboratory, University of Newcastle-upon-Tyne, Newcastle-upon-Tyne, NE1 7RU, England","IEEE Transactions on Software Engineering","","1986","SE-12","1","79","86","Modular redundancy in the form of replicated computations in a concurrent programming model consisting of communicating sequential processes is investigated. Some conditions are given which must always be verified to ensure correctness in the presence of nondeterminism. Then some implementations which satisfy the given conditions are proposed. This approach permits redundant systems to be robust with respect to failures in redundant processors, and also permits the use of software fault tolerance techniques such as <i>N</i>-version programming. The concurrent programming model which has been chosen is based on a set of active entities, i.e. processes, each running in a local protected environment. The processes interact using message passing only.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312922","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312922","Agreement;communicating sequential processes;fault tolerance;guarded commands;nondeterminism;replicated processing;voting","Redundancy;Semantics;Receivers;Kernel;Program processors;Programming;Message passing","fault tolerant computing;parallel processing;software reliability","modular redundancy;parallel programming;software reliability;message passing system;concurrent programming model;communicating sequential processes;correctness;nondeterminism;software fault tolerance techniques;N-version programming;active entities","","14","","","","","","","","IEEE","IEEE Journals & Magazines"
"The consistent comparison problem in N-version software","S. S. Brilliant; J. C. Knight; N. G. Leveson","Dept. of Math. Sci., Virginia Commonwealth Univ., Richmond, VA, USA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","11","1481","1485","The authors have identified a difficulty in the implementation of N-version programming. The problem, called the consistent comparison problem, arises for applications in which decisions are based on the results of comparing finite-precision numbers. It is shown that when versions make comparisons involving the results of finite-precision calculations, it is impossible to guarantee the consistency of their results. It is therefore possible that correct versions may arrive at completely different outputs for an application that does not apparently have multiple correct solutions. If this problem is not dealt with explicitly, an N-version system may be unable to reach consensus even when none of its component versions falls.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41339","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41339","","Application software;Temperature sensors;Arithmetic;Fault tolerance;Computer science;Pressure measurement;Software reliability;Large-scale systems;NASA;Aircraft","fault tolerant computing;programming","consistent comparison problem;N-version software;N-version programming;decisions;finite-precision numbers;finite-precision calculations","","36","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Chaff from the Wheat: Characterizing and Determining Valid Bug Reports","Y. Fan; X. Xia; D. Lo; A. E. Hassan","College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang China (e-mail: yrfan@zju.edu.cn); Faculty of Information Technology, Monash University, Melbourne, Victoria Australia (e-mail: xxkidd@zju.edu.cn); School of Information Systems, Singapore Management University, Singapore, Singapore Singapore 17890 (e-mail: davidlo@smu.edu.sg); School of Computing, Queen's University, Kingston, Ontario Canada K7L 3N6 (e-mail: ahmed@cs.queensu.ca)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Developers use bug reports to triage and fix bugs. When triaging a bug report, developers must decide whether the bug report is valid (i.e., a real bug). A large amount of bug reports are submitted every day, with many of them end up being invalid reports. Manually determining valid bug report is a difficult and tedious task. Thus, an approach that can automatically analyze the validity of a bug report and determine whether a report is valid can help developers prioritize their triaging tasks and avoid wasting time and effort on invalid bug reports.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2864217","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8428477","Bug Report;Feature Generation;Machine Learning","Computer bugs;Feature extraction;Collaboration;Forestry;Support vector machines;Task analysis;Software","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Context-aware middleware for resource management in the wireless Internet","P. Bellavista; A. Corradi; R. Montanari; C. Stefanelli","Dept. of Electron., Comput. Sci., & Syst. (DEIS), Bologna Univ., Italy; Dept. of Electron., Comput. Sci., & Syst. (DEIS), Bologna Univ., Italy; Dept. of Electron., Comput. Sci., & Syst. (DEIS), Bologna Univ., Italy; NA","IEEE Transactions on Software Engineering","","2003","29","12","1086","1099","The provisioning of Web services over the wireless Internet introduces novel challenging issues for service design and implementation: from user/terminal mobility during service execution, to wide heterogeneity of portable access devices and unpredictable modifications in accessible resources. In this scenario, there are frequent provision-time changes in the context, defined as the logical set of accessible resources depending on client location, access terminal capabilities, and system/service management policies. The development of context-dependent services requires novel middlewares with full context visibility. We propose a middleware for context-aware resource management, called CARMEN, capable of supporting the automatic reconfiguration of wireless Internet services in response to context changes without any intervention on the service logic. CARMEN determines the context on the basis of metadata, which include declarative management policies and profiles for user preferences, terminal capabilities, and resource characteristics. In addition, CARMEN exploits the mobile agent technology to implement mobile middleware components that follow the provision-time movement of clients to support locally their customized service access. The proposed middleware shows how metadata and mobile agents can favor component reusability and automatic service reconfiguration, by reducing the development/ deployment complexity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265523","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265523","","Middleware;Resource management;Web and internet services;Context-aware services;Personal digital assistants;Mobile computing;Mobile agents;Computer Society;Web services;Automatic logic units","Internet;middleware;mobile computing;mobile radio;mobile agents","context-aware middleware;resource management;wireless Internet;Web services;user/terminal mobility;portable access devices;provision-time changes;CARMEN;automatic services reconfiguration;metadata;declarative management policies;user preferences;terminal capabilities;resource characteristics;mobile agent technology;mobile middleware components","","101","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Modular operational test plans for inferences on software reliability based on a Markov model","J. Rajgopal; M. Mazumdar","Dept. of Ind. Eng., Pittsburgh Univ., PA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","4","358","363","This paper considers the problem of assessing the reliability of a software system that can be decomposed into a finite number of modules. It uses a Markovian model for the transfer of control between modules in order to develop the system reliability expression in terms of the module reliabilities. An operational test procedure is considered in which only the individual modules are tested and the system is considered acceptable if, and only if, no failures are observed. The minimum number of tests required of each module is determined such that the probability of accepting a system whose reliability falls below a specified value R/sub 0/ is less than a specified small fraction /spl beta/. This sample size determination problem is formulated as a two-stage mathematical program and an algorithm is developed for solving this problem. Two examples from the literature are considered to demonstrate the procedure.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995424","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995424","","Software testing;Software reliability","software reliability;program testing;Markov processes;probability","modular operational test plans;software reliability;Markov model;system reliability expression;probability;software testing;sample size determination problem;two-stage mathematical programming","","16","","23","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Literature Review on Fault Prediction Performance in Software Engineering","T. Hall; S. Beecham; D. Bowes; D. Gray; S. Counsell","Brunel University, Uxbridge; University of Limerick, Limerick; University of Hertfordshire, Hatfield; University of Hertfordshire, Hatfield; Brunel University, Uxbridge","IEEE Transactions on Software Engineering","","2012","38","6","1276","1304","Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.103","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6035727","Systematic literature review;software fault prediction","Predictive models;Context modeling;Software testing;Data models;Systematics;Analytical models;Fault diagnosis","Bayes methods;regression analysis;software fault tolerance;software quality","systematic literature review;fault prediction performance;software engineering;cost reduction;software quality;independent variables;fault prediction models;contextual information;methodological information;simple modeling techniques;naive Bayes;logistic regression;feature selection;predictive performance;fault prediction study;reliable methodology","","259","","241","","","","","","IEEE","IEEE Journals & Magazines"
"Precise documentation of well-structured programs","D. Lorge Parnas; J. Madey; M. Iglewski","Telecommun. Res. Inst., McMaster Univ., Hamilton, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","1994","20","12","948","976","Describes a new form of program documentation that is precise, systematic and readable. This documentation comprises a set of displays supplemented by a lexicon and an index. Each display presents a program fragment in such a way that its correctness can be examined without looking at any other display. Each display has three parts: (1) the specification of the program presented in the display, (2) the program itself, and (3) the specifications of programs invoked by this program. The displays are intended to be used by software engineers as a reference document during inspection and maintenance. This paper also introduces a specification technique that is a refinement of H.D. Mills's (1975) functional approach to program documentation and verification; programs are specified and described in tabular form.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.368133","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=368133","","Documentation;Displays;Programming profession;Software maintenance;Inspection;Heart;Councils;Informatics;Humans","structured programming;system documentation;program verification;formal specification;software maintenance","precise documentation;well-structured programs;displays;lexicon;index;program fragments;program correctness;specification;software engineering;reference document;software inspection;software maintenance;functional approach;program documentation;program verification;tabular form","","52","","29","","","","","","IEEE","IEEE Journals & Magazines"
"A 15 Year Perspective on Automatic Programming","R. Balzer","Information Sciences Institute, University of Southern California","IEEE Transactions on Software Engineering","","1985","SE-11","11","1257","1268","Automatic programming consists not only of an automatic compiler, but also some means of acquiring the high-level specification to be compiled, some means of determining that it is the intended specification, and some (interactive) means of translating this high-level specification into a lower-level one which can be automatically compiled.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231877","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701945","Automatic programming;evolution;explanation;knowledge base;maintenance;prototyping;specification;transformation","Automatic programming;Hardware;Program processors;Prototypes;Costs;Software development management;System testing;Vehicles;Genetic programming","","Automatic programming;evolution;explanation;knowledge base;maintenance;prototyping;specification;transformation","","183","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Safety analysis of timing properties in real-time systems","F. Jahanian; A. K. Mok","Department of Computer Sciences, University of Texas at Austin, Austin, TX 78712; Department of Computer Sciences, University of Texas at Austin, Austin, TX 78712","IEEE Transactions on Software Engineering","","1986","SE-12","9","890","904","The authors formalize the safety analysis of timing properties in real-time systems. The analysis is based on a formal logic, RTL (real-time logic), which is especially suitable for reasoning about the timing behavior of systems. Given the formal specification of a system and a safety assertion to be analyzed, the goal is to relate the safety assertion to the systems specification. There are three distinct cases: (1) the safety assertion is a theorem derivable from the systems specification; (2) the safety assertion is unsatisfiable with respect to the systems specification; or (3) the negation of the safety assertion is satisfiable under certain conditions. A systematic method for performing safety analysis is presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313045","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313045","Real time;real-time logic;safety analysis;systems specification;time-critical system;verification","Real-time systems;Timing;Safety;Clocks;Syntactics;Computational modeling;Time factors","formal logic;real-time systems;safety;systems analysis","timing properties;real-time systems;safety analysis;formal logic;RTL;real-time logic;formal specification;systems specification;safety assertion","","210","","","","","","","","IEEE","IEEE Journals & Magazines"
"SLA-Driven Clustering of QoS-Aware Application Servers","G. Lodi; F. Panzieri; D. Rossi; E. Turrini","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","3","186","197","In this paper, we discuss the design, implementation, and experimental evaluation of a middleware architecture for enabling service level agreement (SLA)-driven clustering of QoS-aware application servers. Our middleware architecture supports application server technologies with dynamic resource management: application servers can dynamically change the amount of clustered resources assigned to hosted applications on-demand so as to meet application-level quality of service (QoS) requirements. These requirements can include timeliness, availability, and high throughput and are specified in SLAs. A prototype of our architecture has been implemented using the open-source J2EE application server JBoss. The evaluation of this prototype shows that our approach makes possible JBoss' resource usage optimization and allows JBoss to effectively meet the QoS requirements of the applications it hosts, i.e., to honor the SLAs of those applications","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084136","Service Level Agreement;Quality of Service;QoS-aware application server;QoS-aware cluster;dynamic cluster configuration;monitoring;load balancing.","Quality of service;Load management;Middleware;Resource management;Availability;Monitoring;Runtime;Application software;Prototypes;Scalability","Java;middleware;quality of service;resource allocation;system monitoring","service level agreement-driven clustering;QoS-aware application server technology;middleware architecture;dynamic resource management;JBoss open-source J2EE application server;JBoss resource usage optimization;dynamic cluster configuration;runtime monitoring;load balancing","","45","","50","","","","","","IEEE","IEEE Journals & Magazines"
"A Second Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems","T. Galinac Grbac; P. Runeson; D. Huljenić","Univesity of Rijeka, Rijeka; Lund University, Lund; Ericsson Nikola Tesla, Zagreb","IEEE Transactions on Software Engineering","","2013","39","4","462","476","Background: Software engineering is searching for general principles that apply across contexts, for example, to help guide software quality assurance. Fenton and Ohlsson presented such observations on fault distributions, which have been replicated once. Objectives: We aimed to replicate their study again to assess the robustness of the findings in a new environment, five years later. Method: We conducted a literal replication, collecting defect data from five consecutive releases of a large software system in the telecommunications domain, and conducted the same analysis as in the original study. Results: The replication confirms results on unevenly distributed faults over modules, and that fault proneness distributions persist over test phases. Size measures are not useful as predictors of fault proneness, while fault densities are of the same order of magnitude across releases and contexts. Conclusions: This replication confirms that the uneven distribution of defects motivates uneven distribution of quality assurance efforts, although predictors for such distribution of efforts are not sufficiently precise.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6235962","Software fault distributions;software metrics;empirical research;replication","Testing;Measurement;Context;Software;Software engineering;Complexity theory;Telecommunications","software fault tolerance;software quality","second replicated quantitative analysis;fault distribution;complex software system;software engineering;literal replication;telecommunications domain;data analysis;data collection;fault proneness distribution;size measure;fault density;quality assurance effort","","14","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Coordination Breakdowns and Their Impact on Development Productivity and Software Failures","M. Cataldo; J. D. Herbsleb","Robert Bosch LLC; Carnegie Mellon University, Pittsburgh","IEEE Transactions on Software Engineering","","2013","39","3","343","360","The success of software development projects depends on carefully coordinating the effort of many individuals across the multiple stages of the development process. In software engineering, modularization is the traditional technique intended to reduce the interdependencies among modules that constitute a system. Reducing technical dependencies, the theory argues, results in a reduction of work dependencies between teams developing interdependent modules. Although that research stream has been quite influential, it considers a static view of the problem of coordination in engineering activities. Building on a dynamic view of coordination, we studied the relationship between socio-technical congruence and software quality and development productivity. In order to investigate the generality of our findings, our analyses were performed on two large-scale projects from two companies with distinct characteristics in terms of product and process maturity. Our results revealed that the gaps between coordination requirements and the actual coordination activities carried out by the developers significantly increased software failures. Our analyses also showed that higher levels of congruence are associated with improved development productivity. Finally, our results showed the congruence between dependencies and coordinative actions is critical both in mature development settings as well as in novel and dynamic development contexts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6205767","Metrics/measurement;productivity;organizational management and coordination;quality analysis and evaluation","Productivity;Programming;Software quality;Context;Complexity theory;Organizations","software metrics;software quality;software reliability","coordination breakdowns;development productivity;software failures;software development projects;development process;software engineering;modularization;technical dependency;work dependency;interdependent modules;engineering activity;socio-technical congruence;software quality;large-scale projects;product maturity;process maturity;coordination requirements;coordination activity;dynamic development contexts","","40","","78","","","","","","IEEE","IEEE Journals & Magazines"
"Describing software architecture styles using graph grammars","D. Le Metayer","IRISA, Rennes, France","IEEE Transactions on Software Engineering","","1998","24","7","521","533","We believe that software architectures should provide an appropriate basis for the proof of properties of large software. This goal can be achieved through a clearcut separation between computation and communication and a formal definition of the interactions between individual components. We present a formalism for the definition of software architectures in terms of graphs. Nodes represent the individual agents and edges define their interconnection. Individual agents can communicate only along the links specified by the architecture. The dynamic evolution of an architecture is defined independently by a ""coordinator"". An architecture style is a class of architectures specified by a graph grammar. The class characterizes a set of architectures sharing a common communication pattern. The rules of the coordinator are statically checked to ensure that they preserve the constraints imposed by the architecture style.","0098-5589;1939-3520;2326-3881","","10.1109/32.708567","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=708567","","Software architecture;Computer architecture;Computer languages;Application software;Engineering drawings;Costs;Organizing;Software standards;Software design;Mathematical model","formal verification;graph grammars","software architecture styles;graph grammars;formal definition;individual agents;interconnection","","119","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Representing Roles in Universal Scheme Interfaces","D. Maier; D. Rozenshtein; J. Stein","Oregon Graduate Center; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","7","644","652","Users of a relational database must explicitly navigate between relations in order to establish a connection among a set of attributes spanning several relation schemes. While a universal scheme interface to a relational database provides users with automatic navigation, it usually imposes on the database a unique role assumption. This assumption requires every attribute name to represent a unique role in the database, so that connections among sets of attributes are unambiguous.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232508","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702068","Access path;automatic database navigation;database query language;generalization hierarchy;relational database;unique role assumption;universal scheme interface","Relational databases;Navigation;Jacobian matrices;Database languages;Automatic logic units","","Access path;automatic database navigation;database query language;generalization hierarchy;relational database;unique role assumption;universal scheme interface","","","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A Dissection of the Test-Driven Development Process: Does It Really Matter to Test-First or to Test-Last?","D. Fucci; H. Erdogmus; B. Turhan; M. Oivo; N. Juristo","Department of Information Processing Science, University of Oulu, Oulu, Finland; Silicon Valley Campus, Carnegie Mellon University, Pittsburgh, PA; Department of Information Processing Science, University of Oulu, Oulu, Finland; Department of Information Processing Science, University of Oulu, Oulu, Finland; Department of Information Processing Science, University of Oulu, Oulu, Finland","IEEE Transactions on Software Engineering","","2017","43","7","597","614","Background: Test-driven development (TDD) is a technique that repeats short coding cycles interleaved with testing. The developer first writes a unit test for the desired functionality, followed by the necessary production code, and refactors the code. Many empirical studies neglect unique process characteristics related to TDD iterative nature. Aim: We formulate four process characteristic: sequencing, granularity, uniformity, and refactoring effort. We investigate how these characteristics impact quality and productivity in TDD and related variations. Method: We analyzed 82 data points collected from 39 professionals, each capturing the process used while performing a specific development task. We built regression models to assess the impact of process characteristics on quality and productivity. Quality was measured by functional correctness. Result: Quality and productivity improvements were primarily positively associated with the granularity and uniformity. Sequencing, the order in which test and production code are written, had no important influence. Refactoring effort was negatively associated with both outcomes. We explain the unexpected negative correlation with quality by possible prevalence of mixed refactoring. Conclusion: The claimed benefits of TDD may not be due to its distinctive test-first dynamic, but rather due to the fact that TDD-like processes encourage fine-grained, steady steps that improve focus and flow.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2616877","Academy of Finland; TEKES; FiDiPro; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7592412","Test-driven development;empirical investigation;process dimensions;external quality;productivity","Testing;Productivity;Context;Companies;Sequential analysis;Conferences","program testing;regression analysis;software engineering","test-driven development process;TDD technique;TDD iterative nature;sequencing characteristic;granularity characteristic;uniformity characteristic;refactoring effort characteristic;regression model","","2","","46","","","","","","IEEE","IEEE Journals & Magazines"
"The accuracy of the clock synchronization achieved by TEMPO in Berkeley UNIX 4.3BSD","R. Gusella; S. Zatti","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; NA","IEEE Transactions on Software Engineering","","1989","15","7","847","853","The authors discuss the upper and lower bounds on the accuracy of the time synchronization achieved by the algorithm implemented in TEMPO, the distributed service that synchronizes the clocks of the University of California, Berkeley, UNIX 4.3BSD systems. The accuracy is shown to be a function of the network transmission latency; it depends linearly upon the drift rate of the clocks and the interval between synchronizations. TEMPO keeps the clocks of the VAX computers in a local area network synchronized with an accuracy comparable to the resolution of single-machine clocks. Comparison with other clock synchronization algorithms shows that TEMPO, in an environment with no Byzantine faults, can achieve better synchronization at a lower cost.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.29484","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=29484","","Clocks;Synchronization;Master-slave;Delay;Computer science;Nominations and elections;Costs;Local area networks;Time measurement;Computer networks","fault tolerant computing;local area networks;synchronisation;Unix","upper bounds;clock synchronization;TEMPO;Berkeley UNIX 4.3BSD;lower bounds;distributed service;network transmission latency;VAX computers;local area network","","74","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Measuring the Discriminative Power of Object-Oriented Class Cohesion Metrics","J. Al Dallal","Kuwait University, Kuwait","IEEE Transactions on Software Engineering","","2011","37","6","788","804","Several object-oriented cohesion metrics have been proposed in the literature. These metrics aim to measure the relationship between class members, namely, methods and attributes. Different metrics use different models to represent the connectivity pattern of cohesive interactions (CPCI) between class members. Most of these metrics are normalized to allow for easy comparison of the cohesion of different classes. However, in some cases, these metrics obtain the same cohesion values for different classes that have the same number of methods and attributes but different CPCIs. This leads to incorrectly considering the classes to be the same in terms of cohesion, even though their CPCIs clearly indicate that the degrees of cohesion are different. We refer to this as a lack of discrimination anomaly (LDA) problem. In this paper, we list and discuss cases in which the LDA problem exists, as expressed through the use of 16 cohesion metrics. In addition, we empirically study the frequent occurrence of the LDA problem when the considered metrics are applied to classes in five open source Java systems. Finally, we propose a metric and a simulation-based methodology to measure the discriminative power of cohesion metrics. The discrimination metric measures the probability that a cohesion metric will produce distinct cohesion values for classes with the same number of attributes and methods but different CPCIs. A highly discriminating cohesion metric is more desirable because it exhibits a lower chance of incorrectly considering classes to be cohesively equal when they have different CPCIs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.97","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5639020","Cohesive interactions;connectivity pattern;discrimination metric;discriminative power;lack of discrimination anomaly;object-oriented class cohesion.","Power measurement;Object oriented modeling;Software measurement;Phase measurement","Java;laser Doppler anemometry;object-oriented methods;public domain software","discriminative power measurement;object oriented class cohesion metrics;cohesive interactions;discrimination anomaly;open source Java systems;simulation based methodology","","23","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Documentation of Android Apps","E. Aghajani; G. Bavota; M. Linares-Vásquez; M. Lanza","Informatics, Universita della Svizzera Italiana, 27216 Lugano, Ticino Switzerland (e-mail: emad.aghajani@usi.ch); Faculty of Informatics, Universita della Svizzera Italiana, 27216 Lugano, Lugano Switzerland 6904 (e-mail: gabriele.bavota@usi.ch); Systems Engineering and Computing, Universidad de los Andes, 27991 Bogota, Bogota Colombia (e-mail: m.linaresv@uniandes.edu.co); Faculty of Informatics, University of Lugano, Lugano, TI Switzerland 6904 (e-mail: michele.lanza@usi.ch)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Developers do not always have the knowledge needed to understand source code and must refer to different resources (e.g., teammates, documentation, the web). This non-trivial process, called program comprehension, is very time-consuming. While many approaches support the comprehension of a given code at hand, they are mostly focused on defining extractive summaries from the code (i.e., on selecting from a given piece of code the most important statements/comments to comprehend it). However, if the information needed to comprehend the code is not there, their usefulness is limited. We present ADANA, an approach to automatically inject comments describing a given piece of Android code. ADANA reuses the descriptions of similar and well-documented code snippets retrieved from various online resources. Our evaluation has shown that ADANA is able to aid the program comprehension process.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2890652","Schweizerischer Nationalfonds zur Forderung der Wissenschaftlichen Forschung; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8598894","Program Comprehension;Documentation;Android","Knowledge based systems;Documentation;Java;Cloning;Asia;Task analysis;Data mining","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A Model of Data Warehousing Process Maturity","A. Sen; K. Ramamurthy; A. P. Sinha","Texas A&M University, College Station; University of Wisconsin-Milwaukee, Milwaukee; University of Wisconsin-Milwaukee, Milwaukee","IEEE Transactions on Software Engineering","","2012","38","2","336","353","Even though data warehousing (DW) requires huge investments, the data warehouse market is experiencing incredible growth. However, a large number of DW initiatives end up as failures. In this paper, we argue that the maturity of a data warehousing process (DWP) could significantly mitigate such large-scale failures and ensure the delivery of consistent, high quality, “single-version of truth” data in a timely manner. However, unlike software development, the assessment of DWP maturity has not yet been tackled in a systematic way. In light of the critical importance of data as a corporate resource, we believe that the need for a maturity model for DWP could not be greater. In this paper, we describe the design and development of a five-level DWP maturity model (DWP-M) over a period of three years. A unique aspect of this model is that it covers processes in both data warehouse development and operations. Over 20 key DW executives from 13 different corporations were involved in the model development process. The final model was evaluated by a panel of experts; the results strongly validate the functionality, productivity, and usability of the model. We present the initial and final DWP-M model versions, along with illustrations of several key process areas at different levels of maturity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.2","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680911","Data warehousing process;design-science research;model validation;software maturity models.","Data warehouses;Business;Warehousing;Software;Programming;Data mining;Standards organizations","data warehouses","data warehousing process maturity;data warehouse market;large-scale failures;DWP-M model","","19","","75","","","","","","IEEE","IEEE Journals & Magazines"
"The effectiveness of software development technical reviews: a behaviorally motivated program of research","C. Sauer; D. R. Jeffery; L. Land; P. Yetton","Templeton Coll., Oxford Univ., UK; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","1","1","14","Software engineers use a number of different types of software development technical review (SDTR) for the purpose of detecting defects in software products. This paper applies the behavioral theory of group performance to explain the outcomes of software reviews. A program of empirical research is developed, including propositions to both explain review performance and identify ways of improving review performance based on the specific strengths of individuals and groups. Its contributions are to clarify our understanding of what drives defect detection performance in SDTRs and to set an agenda for future research. In identifying individuals' task expertise as the primary driver of review performance, the research program suggests specific points of leverage for substantially improving review performance. It points to the importance of understanding software reading expertise and implies the need for a reconsideration of existing approaches to managing reviews.","0098-5589;1939-3520;2326-3881","","10.1109/32.825763","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=825763","","Programming;Software quality;Inspection;Management training;Engineering management;Software safety;Costs;Humans;Technology management;Australia","software quality;program testing;program debugging","software development technical reviews;software defect detection;behavioral theory;group performance;research;software reading;software quality","","100","","88","","","","","","IEEE","IEEE Journals & Magazines"
"Management of performance requirements for information systems","B. A. Nixon","Dept. of Comput. Sci., Toronto Univ., Ont., Canada","IEEE Transactions on Software Engineering","","2000","26","12","1122","1146","The management of performance requirements is a major challenge for information systems as well as other software systems. This is because performance requirements can have a global impact on the target system. In addition, there are interactions and trade-offs among performance requirements, other nonfunctional requirements (NFRs), and the numerous alternatives for the target system. To provide a systematic approach to managing performance requirements, this paper presents a performance requirements framework (PeRF). It integrates and catalogues a variety of kinds of knowledge of information systems and performance. These include: performance concepts, software performance engineering principles for building performance into systems, and information systems development knowledge. In addition, layered structures organize performance knowledge and the development process. All this knowledge is represented using an existing goal-oriented approach, the ""NFR framework"", which offers a developer-directed graphical treatment for stating NFRs, analyzing and interrelating them, and determining the impact of decisions upon NFRs. This approach allows customized solutions to be built, taking into account the characteristics of the particular domain. The use of PeRF in managing performance requirements is illustrated in a study of performance requirements and other NFRs for a university student record system. This paper concludes with a summary of other studies of information systems, tool support and directions for future work.","0098-5589;1939-3520;2326-3881","","10.1109/32.888627","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888627","","Management information systems;Knowledge engineering;Delay;Software quality;Software performance;Information systems;Software systems;Design engineering;Data engineering;Knowledge management","information systems;software performance evaluation;systems analysis;educational administrative data processing;software quality;software management","performance requirements management;information systems;nonfunctional requirements;performance requirements framework;PeRF;performance concepts;software performance engineering principles;systems development knowledge;layered structures;goal-oriented approach;NFR framework;developer-directed graphical treatment;decision impact;customized solutions;university student record system;tool support;requirements engineering;semantic data models;catalogues;Year-2000 compliance","","21","","65","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study on the Relationship Between Software Design Quality, Development Effort and Governance in Open Source Projects","E. Capra; C. Francalanci; F. Merlo","Politecnico di Milano, Milano; Politecnico di Milano, Milan; Politecnico di Milano, Milan","IEEE Transactions on Software Engineering","","2008","34","6","765","782","The relationship among software design quality, development effort, and governance practices is a traditional research problem. However, the extent to which consolidated results on this relationship remain valid for open source (OS) projects is an open research problem. An emerging body of literature contrasts the view of open source as an alternative to proprietary software and explains that there exists a continuum between closed and open source projects. This paper hypothesizes that as projects approach the OS end of the continuum, governance becomes less formal. In turn a less formal governance is hypothesized to require a higher-quality code as a means to facilitate coordination among developers by making the structure of code explicit and facilitate quality by removing the pressure of deadlines from contributors. However, a less formal governance is also hypothesized to increase development effort due to a more cumbersome coordination overhead. The verification of research hypotheses is based on empirical data from a sample of 75 major OS projects. Empirical evidence supports our hypotheses and suggests that software quality, mainly measured as coupling and inheritance, does not increase development effort, but represents an important managerial variable to implement the more open governance approach that characterizes OS projects which, in turn, increases development effort.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4599582","Cost estimation;Qualitative process analysis;Organizational management and coordination;Management;Complexity measures;Process metrics;Metrics/Measurement;Software Engineering;Software/Software Engineering;Cost estimation;Qualitative process analysis;Organizational management and coordination;Management;Complexity measures;Process metrics;Metrics/Measurement;Software Engineering;Software/Software Engineering","Software design;Open source software;Software quality;Costs;Project management;Software measurement;Quality management;Companies;Software development management;Equations","project management;public domain software;software development management;software quality","software design quality;open source projects;qualitative process analysis;structural equation modeling;software cost estimation;software development effort","","41","","111","","","","","","IEEE","IEEE Journals & Magazines"
"Finding Clones with Dup: Analysis of an Experiment","B. S. Baker","140 North Road, Berkeley Heights, NJ 07922","IEEE Transactions on Software Engineering","","2007","33","9","608","621","An experiment was carried out by a group of scientists to compare different tools and techniques for detecting duplicated or near-duplicated source code. The overall comparative results are presented elsewhere. This paper takes a closer look at the results for one tool, Dup, which finds code sections that are textually the same or the same except for systematic substitution of parameters such as identifiers and constants. Various factors that influenced the results are identified and their impact on the results is assessed via rerunning Dup with changed options and modifications. These improve the performance of Dup with regard to the experiment and could be incorporated into a postprocessor to be used with other tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70720","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288194","Redundant code;duplicated code;softwareclones","Cloning;Java;Computer bugs;Plagiarism;Data structures;Design methodology;Software tools;Tree graphs;Particle measurements","software reusability;software tools","Dup;source code;code sections;systematic substitution;postprocessor","","31","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Test Data Selection and Quality Estimation Based on the Concept of Essential Branches for Path Testing","T. Chusho","Systems Development Laboratory, Hitachi Ltd.","IEEE Transactions on Software Engineering","","1987","SE-13","5","509","517","A new coverage measure is proposed for efficient and effective software testing. The conventional coverage measure for branch testing has such defects as overestimation of software quality and redundant test data selection because all branches are treated equally. These problems can be avoided by paying attention to only those branches essential for path testing. That is, if one branch is executed whenever another particular branch is executed, the former branch is nonessential for path testing. This is because a path covering the latter branch also covers the former branch. Branches other than such nonessential branches will be referred to as essential branches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233196","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702251","Algorithm;branch testing;control flow graph;coverage measure;path testing;program testing;quality estimation;test data selection","Software testing;Software measurement;Logic testing;Software quality;Flow graphs;Software tools;Linearity;Monitoring;Redundancy;Fluid flow measurement","","Algorithm;branch testing;control flow graph;coverage measure;path testing;program testing;quality estimation;test data selection","","33","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Studying the fault-detection effectiveness of GUI test cases for rapidly evolving software","A. M. Memon; Q. Xie","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","2005","31","10","884","896","Software is increasingly being developed/maintained by multiple, often geographically distributed developers working concurrently. Consequently, rapid-feedback-based quality assurance mechanisms such as daily builds and smoke regression tests, which help to detect and eliminate defects early during software development and maintenance, have become important. This paper addresses a major weakness of current smoke regression testing techniques, i.e., their inability to automatically (re)test graphical user interfaces (GUIs). Several contributions are made to the area of GUI smoke testing. First, the requirements for GUI smoke testing are identified and a GUI smoke test is formally defined as a specialized sequence of events. Second, a GUI smoke regression testing process called daily automated regression tester (DART) that automates GUI smoke testing is presented. Third, the interplay between several characteristics of GUI smoke test suites including their size, fault detection ability, and test oracles is empirically studied. The results show that: 1) the entire smoke testing process is feasible in terms of execution time, storage space, and manual effort, 2) smoke tests cannot cover certain parts of the application code, 3) having comprehensive test oracles may make up for not having long smoke test cases, and 4) using certain oracles can make up for not having large smoke test suites.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.117","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542069","Index Terms- Smoke testing;GUI testing;test oracles;empirical studies;regression testing.","Graphical user interfaces;Software testing;Computer aided software engineering;Automatic testing;Application software;Software maintenance;Fault detection;Quality assurance;Smoke detectors;System testing","software prototyping;program testing;fault diagnosis;software maintenance;software quality;software fault tolerance;graphical user interfaces;formal specification;formal verification","fault-detection;GUI test cases;graphical user interfaces;rapidly evolving software;quality assurance mechanism;software development;software maintenance;smoke regression testing technique;daily automated regression tester;test oracles","","124","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Approximate analysis of open networks of queues with blocking: Tandem configurations","H. G. Perros; T. Altiok","Department of Computer Science, North Carolina State University, Raleigh, NC 27695; Department of Industrial Engineering, Rutgers University, Piscataway, NJ 08854","IEEE Transactions on Software Engineering","","1986","SE-12","3","450","461","An approximation procedure is developed for the analysis of tandem configurations consisting of single server finite queues linked in series. External arrivals occur at the first queue which may be either finite or infinite. Departures from the queuing network may only occur from the last queue. All service times and interarrival times are assumed to be exponentially distributed. The approximation algorithm gives results in the form of the marginal probability distribution of the number of units in each queue of the tandem configuration. Other performance measures, such as mean queue-length and throughput, can be readily obtained. The approximation procedure was validated using exact and simulation data. The approximate results seem to have an acceptable error level.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312886","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312886","Approximations;blocking;Coxian queues;exponential distribution;finite queues;queues in tandem","Servers;Approximation methods;Approximation algorithms;Queueing analysis;Throughput;Algorithm design and analysis;Delay","programming theory;queueing theory","software engineering;open networks;queues;blocking;Tandem configurations;single server finite queues;service times;interarrival times;approximation algorithm;marginal probability distribution;performance measures;mean queue-length;throughput","","16","","","","","","","","IEEE","IEEE Journals & Magazines"
"A reflective implementation of Java multi-methods","R. Forax; E. Duris; G. Roussel","Inst. Gaspard-Monge, Univ. de Marne-la-Vallee, France; Inst. Gaspard-Monge, Univ. de Marne-la-Vallee, France; Inst. Gaspard-Monge, Univ. de Marne-la-Vallee, France","IEEE Transactions on Software Engineering","","2004","30","12","1055","1071","In Java, method implementations are chosen at runtime by late-binding with respect to the runtime class of just the receiver argument. However, in order to simplify many programming designs, late-binding with respect to the dynamic type of all arguments is sometimes desirable. This behavior, usually provided by multimethods, is known as multipolymorphism. This work presents a new multimethod implementation based on the standard Java reflection mechanism. Provided as a package, it does not require any language extension or any virtual machine modification. The design issues of this reflective implementation are presented together with a new and simple multimethod dispatch algorithm that efficiently supports class loading at runtime. This implementation provides a practicable and fully portable multimethod solution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377197","Index Terms- Java;polymorphism;reflection;dynamic class loading.","Java;Runtime;Reflection;Dynamic programming;Algorithm design and analysis;Packaging machines;Virtual machining;Software maintenance;Libraries","Java;object-oriented programming","Java multimethods;late-binding;multipolymorphism;Java reflection mechanism;dynamic class loading","","5","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A learning agent that assists the browsing of software libraries","C. G. Drummond; D. Ionescu; R. C. Holte","Sch. of Inf. Technol. & Eng., Ottawa Univ., Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2000","26","12","1179","1196","Locating software items is difficult, even for knowledgeable software designers, when searching in large, complex and continuously growing libraries. This paper describes a technique we term ""active browsing"". An active browser suggests to the designer items it estimates to be close to the target of the search. The novel aspect of active browsing is that it is entirely unobtrusive: it infers its similarity measure from the designer's normal browsing actions, without any special input. Experiments are presented in which the active browsing system succeeds 40% of the time in identifying the target before the designer has found it. An additional experiment indicates that this approach does, indeed, speed up searches.","0098-5589;1939-3520;2326-3881","","10.1109/32.888631","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888631","","Software libraries;Software design;Programming;Humans;Documentation;Costs;Information retrieval;Power system reliability;Organizing","software libraries;online front-ends;learning (artificial intelligence);software agents;software reusability;user modelling;utility programs","learning agent;software library browsing assistant;software items location;active browsing;similarity measure;search speed;software reuse;software library searching;simulated human users","","17","","47","","","","","","IEEE","IEEE Journals & Magazines"
"A comparison of function point counting techniques","D. R. Jeffery; G. C. Low; M. Barnes","Sch. of Inf. Syst., New Sotuh Wales Univ., Kensington, NSW, Australia; Sch. of Inf. Syst., New Sotuh Wales Univ., Kensington, NSW, Australia; Sch. of Inf. Syst., New Sotuh Wales Univ., Kensington, NSW, Australia","IEEE Transactions on Software Engineering","","1993","19","5","529","532","Effective management of the software development process requires that management be able to estimate total development effort and cost. One of the fundamental problems associated with effort and cost estimation is the a priori estimation of software size. Function point analysis has emerged over the last decade as a popular tool for this task. Criticisms of the method that relate to the way in which function counts are calculated and the impact of the processing complexity adjustment on the function point count have arisen. SPQR/20 function points among others are claimed to overcome some of these criticisms. The SPQR/20 function point method is compared to traditional function point analysis as a measure of software size in an empirical study of MIS environments. In a study of 64 projects in one organization it was found that both methods would appear equally satisfactory. However consistent use of one method should occur since the individual counts differ considerably.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232016","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232016","","Size measurement;Software measurement;Software development management;Costs;Productivity;Performance analysis;Programming;Application software;Appropriate technology;Information systems","DP management;project management;software engineering","function point counting techniques;software development process;a priori estimation;software size;processing complexity adjustment;SPQR/20 function points","","54","","16","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of open-source and closed-source software products","J. W. Paulson; G. Succi; A. Eberlein","Gen. Dynamics Canada Ltd., Calgary, Alta., Canada; NA; NA","IEEE Transactions on Software Engineering","","2004","30","4","246","256","We describe an empirical study of open-source and closed-source software projects. The motivation for this research is to quantitatively investigate common perceptions about open-source projects, and to validate these perceptions through an empirical study. We investigate the hypothesis that open-source software grows more quickly, but does not find evidence to support this. The project growth is similar for all the projects in the analysis, indicating that other factors may limit growth. The hypothesis that creativity is more prevalent in open-source software is also examined, and evidence to support this hypothesis is found using the metric of functions added over time. The concept of open-source projects succeeding because of their simplicity is not supported by the analysis, nor is the hypothesis of open-source projects being more modular. However, the belief that defects are found and fixed more rapidly in open-source projects is supported by an analysis of the functions modified. We find support for two of the five common beliefs and conclude that, when implementing or switching to the open-source development model, practitioners should ensure that an appropriate metrics collection strategy is in place to verify the perceived benefits.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274044","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274044","","Open source software;Computer Society;Programming;Software metrics;Computer industry;Scalability;Data security;Costs;Software systems","public domain software;software metrics;formal verification","open-source software project;closed-source software projects;open-source development model;software metrics;software growth models","","106","","14","","","","","","IEEE","IEEE Journals & Magazines"
"A formal architectural model for logical agent mobility","Dianxiang Xu; Jianwen Yin; Yi Deng; Junhua Ding","Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","1","31","45","The process of agent migration is the major difference between logical code mobility of software agents and physical mobility of mobile nodes in ad hoc networks. Without considering agent transfer, it would make little sense to mention the modeling of strong code mobility, which aims to make a migrated agent restarted exactly from the state when it was stopped before migration. From the perspective of system's architecture, this paper proposes a two-layer approach for the formal modeling of logical agent mobility (LAM) using predicate/transition (PrT) nets. We view a mobile agent system as a set of agent spaces and agents could migrate from one space to another. Each agent space is explicitly abstracted to be a component, consisting of an environmental part and an internal connector dynamically binding agents with their environment. We use a system net, agent nets, and a connector net to model the environment, agents, and the connector, respectively. In particular, agent nets are packed up as parts of tokens in system nets, so that agent transfer and location change are naturally captured by transition firing (token game) in Petri nets. Agent nets themselves are active only at specific places and disabled at all the other places in a system net. The semantics of such a two-layer LAM model is defined by transforming it into a PrT net. This facilitates the analysis of several properties about location, state, and connection. In addition, this paper also presents a case study of modeling and analyzing an information retrieval system with mobile agents.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166587","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166587","","Mobile agents;Connectors;Software agents;Ad hoc networks;Computer Society;Mobile computing;Petri nets;Information analysis;Information retrieval;Software architecture","mobile agents;Petri nets;reachability analysis;software architecture;ad hoc networks","agent migration;logical code mobility;software agents;physical mobility;ad hoc networks;strong code mobility;two-layer approach;logical agent mobility;Petri nets;information retrieval system;software architecture","","36","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Using Stochastic State Classes in Quantitative Evaluation of Dense-Time Reactive Systems","E. Vicario; L. Sassoli; L. Carnevali","Universit&#x0E0; di Firenze, Firenze; Universit&#x0E0; di Firenze, Firenze; Universit&#x0E0; di Firenze, Firenze","IEEE Transactions on Software Engineering","","2009","35","5","703","719","In the verification of reactive systems with nondeterministic densely valued temporal parameters, the state-space can be covered through equivalence classes, each composed of a discrete logical location and a dense variety of clock valuations encoded as a difference bounds matrix (DBM). The reachability relation among such classes enables qualitative verification of properties pertaining events ordering and stimulus/response deadlines, but it does not provide any measure of probability for feasible behaviors. We extend DBM equivalence classes with a density-function which provides a measure for the probability of individual states. To this end, we extend time Petri nets by associating a probability density-function to the static firing interval of each nondeterministic transition. We then explain how this stochastic information induces a probability distribution for the states contained within a DBM class and how this probability evolves in the enumeration of the reachability relation among classes. This enables the construction of a stochastic transition system which supports correctness verification based on the theory of TPNs, provides a measure of probability for each feasible run, enables steady-state analysis based on Markov renewal theory. In so doing, we provide a means to identify feasible behaviors and to associate them with a measure of probability in models with multiple concurrent generally distributed nondeterministic timers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5010440","Correctness verification;performance and dependability evaluation;stochastic Time Petri nets;non-Markovian Stochastic Petri Nets;dense-time state-space analysis;Difference Bounds Matrix;Markov Renewal Theory.","Stochastic systems;Stochastic processes;Petri nets;Delay;Clocks;Timing;Density functional theory;Concurrent computing;Computer Society;Cost accounting","equivalence classes;Markov processes;matrix algebra;Petri nets;program verification;reachability analysis;statistical distributions","stochastic state-space class analysis;dense-time reactive system verification;quantitative evaluation;nondeterministic densely-valued temporal parameter;equivalence class;discrete logical location;difference bound matrix;DBM;reachability relation;qualitative correctness verification;event ordering;stimulus/response deadline;stochastic timed Petri net;probability density distribution function measure;static firing interval;nondeterministic transition;stochastic transition system;STPN theory;Markov renewal theory;nondeterministic timer","","27","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Naming and binding in a vertical migration environment","R. I. Winner","Inst. for Defense Anal., Alexandria, VA, USA","IEEE Transactions on Software Engineering","","1988","14","5","599","607","Achieving maximum performance through migration of functions from software to microcode requires rethinking the linkage editing process. An object-oriented model of naming and binding clarifies the alternative abstractions available in naming and linking across the macro-micro machine boundary. Alternative abstractions for sharing micro-objects and for dynamic use of micro-objects are presented and their implementations discussed. The conclusions are based on actual implementations.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6138","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6138","","Couplings;Compaction;Timing;Microprogramming;Processor scheduling;Production;Large Hadron Collider;Dynamic scheduling;Degradation;Programmable logic arrays","data structures;microprogramming;program compilers","compilers;vertical migration environment;maximum performance;microcode;linkage editing process;object-oriented model;naming;binding;abstractions;micro-objects","","","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic coupling measurement for object-oriented software","E. Arisholm; L. C. Briand; A. Foyen","Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; NA; NA","IEEE Transactions on Software Engineering","","2004","30","8","491","506","The relationships between coupling and external quality factors of object-oriented software have been studied extensively for the past few years. For example, several studies have identified clear empirical relationships between class-level coupling and class fault-proneness. A common way to define and measure coupling is through structural properties and static code analysis. However, because of polymorphism, dynamic binding, and the common presence of unused (""dead"") code in commercial software, the resulting coupling measures are imprecise as they do not perfectly reflect the actual coupling taking place among classes at runtime. For example, when using static analysis to measure coupling, it is difficult and sometimes impossible to determine what actual methods can be invoked from a client class if those methods are overridden in the subclasses of the server classes. Coupling measurement has traditionally been performed using static code analysis, because most of the existing work was done on nonobject oriented code and because dynamic code analysis is more expensive and complex to perform. For modern software systems, however, this focus on static analysis can be problematic because although dynamic binding existed before the advent of object-orientation, its usage has increased significantly in the last decade. We describe how coupling can be defined and precisely measured based on dynamic analysis of systems. We refer to this type of coupling as dynamic coupling. An empirical evaluation of the proposed dynamic coupling measures is reported in which we study the relationship of these measures with the change proneness of classes. Data from maintenance releases of a large Java system are used for this purpose. Preliminary results suggest that some dynamic coupling measures are significant indicators of change proneness and that they complement existing coupling measures based on static analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316867","Index Terms- Coupling measurement;change predictions;quality modeling;maintenance.","Software measurement;Object oriented modeling;Software quality;Predictive models;Performance evaluation;Performance analysis;Q factor;Fault diagnosis;Runtime;Software systems","object-oriented programming;program diagnostics;software quality;software maintenance;inheritance","dynamic coupling;object-oriented software;class-level coupling;class fault-proneness;static code analysis;polymorphism;dynamic binding;software quality modeling;software maintenance","","142","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Are Fix-Inducing Changes a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Prediction","S. McIntosh; Y. Kamei","Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Principles of Software Languages Group (POSL), Kyushu University, Fukuoka, Japan","IEEE Transactions on Software Engineering","","2018","44","5","412","428","Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2693980","Natural Sciences and Engineering Research Council of Canada (NSERC); JSPS KAKENHI; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7898457","Just-In-Time prediction;defect prediction;mining software repositories","Predictive models;Data models;Software;Complexity theory;Market research;Context modeling;Calibration","data mining;just-in-time;learning (artificial intelligence);public domain software;software management;software quality;source code (software)","Just-In-Time models;fix-inducing code changes;code change properties;target moving;just-in-time defect prediction;fix-inducing changes;JIT models;OpenStack systems;Qt systems;mining software repositories","","2","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Deduction graphs: an algorithm and applications","Chao-Chih Yang","Dept. of Comput. Sci., North Texas Univ., Denton, TX, USA","IEEE Transactions on Software Engineering","","1989","15","1","60","67","A deduction graph (DG) for logically deducing a new functional dependency (FD) or function-free Horn formula (extended from Horn clauses) from a subset of a given FDs or function-free headed Horn clauses in a relational database or rule-based expert systems is defined. An algorithm with a polynomial time complexity for constructing a DG based on a number of rules is designed. Applications of DGs to relational databases, rule-based expert systems, logic programming, and artificial intelligence are investigated. In addition to graphically solving the inference problem by DGs, many logic queries can be answered by DGs with substitutions for unifying expressions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21726","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21726","","Relational databases;Expert systems;Artificial intelligence;Inference algorithms;Polynomials;Algorithm design and analysis;Deductive databases;Control systems;Logic programming;Terminology","database theory;expert systems;inference mechanisms;logic programming;relational databases","deduction graph;functional dependency;function-free Horn formula;Horn clauses;relational database;rule-based expert systems;polynomial time complexity;logic programming;artificial intelligence;inference problem;logic queries","","5","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling of hierarchical distributed systems with fault-tolerance","Y. -. Shieh; D. Ghosal; P. R. Chintamaneni; S. K. Tripathi","IBM Research Triangle Park, NC, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1990","16","4","444","457","Since each of the levels in a hierarchical system could have various characteristics, different fault-tolerant schemes could be appropriate at different levels. A stochastic Petri net (SPN) is used to investigate various fault-tolerant schemes in this context. The basic SPN is augmented by parameterized subnet primitives to model the fault-tolerant schemes. Both centralized and distributed fault-tolerant schemes are considered. The two schemes are investigated by considering the individual levels in a hierarchical system independently. In the case of distributed fault tolerance, two different checkpointing strategies are considered. The first scheme is called the arbitrary checkpointing strategy. Each process in this scheme does its checkpointing independently; thus, the domino effect may occur. The second scheme is called the planned strategy. Here, process checkpointing is constrained to ensure no domino effect. The results show that, under certain conditions, an arbitrary checkpointing strategy can perform better than a planned strategy. The effect of integration on the fault-tolerant strategies of the various levels of a hierarchy are studied.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.54296","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=54296","","Fault tolerant systems;Fault tolerance;Checkpointing;Application software;Costs;LAN interconnection;Manufacturing automation;Stochastic processes;Petri nets;Hierarchical systems","distributed processing;fault tolerant computing;Petri nets","hierarchical distributed systems modelling;fault-tolerance;stochastic Petri net;parameterized subnet primitives;centralized;checkpointing strategies;arbitrary checkpointing strategy;planned strategy","","6","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Formal methods for protocol testing: a detailed study","D. P. Sidhu; T. -. Leung","Dept. of Comput. Sci., Maryland Univ., Baltimore, MD, USA; NA","IEEE Transactions on Software Engineering","","1989","15","4","413","426","The authors present a detailed study of four formal methods (T-, U-, D-, and W-methods) for generating test sequences for protocols. Applications of these methods to the NBS Class 4 Transport Protocol are discussed. An estimation of fault coverage of four protocol-test-sequence generation techniques using Monte Carlo simulation is also presented. The ability of a test sequence to decide whether a protocol implementation conforms to its specification heavily relies on the range of faults that it can capture. Conformance is defined at two levels, namely, weak and strong conformance. This study shows that a test sequence produced by T-method has a poor fault detection capability, whereas test sequences produced by U-, D-, and W-methods have comparable (superior to that for T-method) fault coverage on several classes of randomly generated machines used in this study. Also, some problems with a straightforward application of the four protocol-test-sequence generation methods to real-world communication protocols are pointed out.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.16602","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=16602","","Automatic testing;Computer networks;Software testing;NIST;Communication networks;ISO standards;Transport protocols;Fault detection;Modems","conformance testing;failure analysis;Monte Carlo methods;protocols","protocol testing;test sequences;NBS Class 4 Transport Protocol;fault coverage;protocol-test-sequence generation techniques;Monte Carlo simulation;protocol implementation;fault detection;fault coverage;real-world communication protocols","","192","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Trace Analysis of Discrete-Event System Models","P. Kemper; C. Tepper","College of William and Mary, Williamsburg; ITGAIN Consulting, Hanover","IEEE Transactions on Software Engineering","","2009","35","2","195","208","In this paper, we describe a novel technique that helps a modeler gain insight into the dynamic behavior of a complex stochastic discrete event simulation model based on trace analysis. We propose algorithms to distinguish progressive from repetitive behavior in a trace and to extract a minimal progressive fragment of a trace. The implied combinatorial optimization problem for trace reduction is solved in linear time with dynamic programming. We present and compare several approximate and one exact solution method. Information on the reduction operation as well as the reduced trace itself helps a modeler to recognize the presence of certain errors and to identify their cause. We track down a subtle modeling error in a dependability model of a multi-class server system to illustrate the effectiveness of our approach in revealing the cause of an observed effect. The proposed technique has been implemented and integrated in Traviando, a trace analyzer to debug stochastic simulation models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.75","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4620122","Testing and Debugging;Simulation;Trace analysis;Cycle reduction;Testing and Debugging;Simulation;Trace analysis;Cycle reduction","Discrete event systems;Discrete event simulation;Context modeling;Communication system software;Software performance;Automatic control;Control systems;Stochastic systems;Debugging;Data mining","combinatorial mathematics;discrete event simulation;dynamic programming;program diagnostics","automated trace analysis;discrete event system models;dynamic behavior;complex stochastic discrete event simulation model;minimal progressive fragment;combinatorial optimization problem;trace reduction;linear time;dynamic programming;reduction operation;modeling error;dependability model;multiclass server system;Traviando;trace analyzer;stochastic simulation model","","13","","22","","","","","","IEEE","IEEE Journals & Magazines"
"An analysis of test data selection criteria using the RELAY model of fault detection","D. J. Richardson; M. C. Thompson","Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA; NA","IEEE Transactions on Software Engineering","","1993","19","6","533","553","RELAY is a model of faults and failures that defines failure conditions, which describe test data for which execution will guarantee that a fault originates erroneous behavior that also transfers through computations and information flow until a failure is revealed. This model of fault detection provides a framework within which other testing criteria's capabilities can be evaluated. Three test data selection criteria that detect faults in six fault classes are analyzed. This analysis shows that none of these criteria is capable of guaranteeing detection for these fault classes and points out two major weaknesses of these criteria. The first weakness is that the criteria do not consider the potential unsatisfiability of their rules. Each criterion includes rules that are sufficient to cause potential failures for some fault classes, yet when such rules are unsatisfiable, many faults may remain undetected. Their second weakness is failure to integrate their proposed rules.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232020","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232020","","Data analysis;Relays;Fault detection;Software testing;Failure analysis;Computer science;Data flow computing;Software measurement;Aircraft manufacture;Computer errors","program debugging;program testing;software reliability","test data selection criteria;fault detection;RELAY;failure conditions;erroneous behavior;information flow","","36","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Exception Handling: Formal Specification and Systematic Program Construction","M. Bidoit; B. Biebow; M. -. Gaudel; C. Gresse; G. D. Guiho","Laboratoire de Recherche en Informatique, University of Paris-Sud; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","3","242","252","We present an algebraic specification language (PLUSS) and a program construction method. Programs are built systematically from an algebraic specification of the data they deal with. The method was tested on a realistic problem (part of a telephone switching system). In these experiments, it turned out that error handling was the difficult part to specify and to program. This paper shows how to cope with this problem at the specification level and during the program development process.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232207","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702000","Abstract data types;algebraic specification;decomposition schemes;error handling;industrial experiment;program construction","Formal specifications;Specification languages;System testing;Telephony;Switching systems;Programming profession;Contracts;Construction industry;Context;Manufacturing","","Abstract data types;algebraic specification;decomposition schemes;error handling;industrial experiment;program construction","","2","","20","","","","","","IEEE","IEEE Journals & Magazines"
"A new method of image compression using irreducible covers of maximal rectangles","Y. Cheng; S. S. Iyengara; R. L. Kashyap","Louisiana State Univ., Baton Rouge, LA, USA; Louisiana State Univ., Baton Rouge, LA, USA; NA","IEEE Transactions on Software Engineering","","1988","14","5","651","658","The binary-image-compression problem is analyzed using irreducible cover of maximal rectangles. A bound on the minimum-rectangular-cover problem for image compression is given under certain conditions that previously have not been analyzed. It is demonstrated that for a simply connected image, the irreducible cover proposed uses less than four times the number of the rectangles in a minimum cover. With n pixels in a square, the parallel algorithm for obtaining the irreducible cover uses (n/log n) concurrent-read-exclusive write (CREW) processors in O(log n) time.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6142","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6142","","Image coding;Image analysis;Pixel;Parallel algorithms;Image processing;Length measurement;Particle measurements;Mathematics;Computer science;NP-complete problem","computerised picture processing;data compression;parallel algorithms","data structures;image compression;irreducible covers;maximal rectangles;pixels;parallel algorithm;concurrent-read-exclusive write","","9","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic detection and masking of nonatomic exception handling","C. Fetzer; P. Felber; K. Hogstedt","Dept. of Comput. Sci., Dresden Univ. of Technol., Germany; NA; NA","IEEE Transactions on Software Engineering","","2004","30","8","547","560","The development of robust software is a difficult undertaking and is becoming increasingly more important as applications grow larger and more complex. Although modern programming languages such as C++ and Java provide sophisticated exception handling mechanisms to detect and correct runtime error conditions, exception handling code must still be programmed with care to preserve application consistency. In particular, exception handling is only effective if the premature termination of a method due to an exception does not leave an object in an inconsistent state. We address this issue by introducing the notion of failure atomicity in the context of exceptions. We propose practical techniques to automatically detect and mask the nonatomic exception handling situations encountered during program execution. These techniques can be applied to applications written in various programming languages that support exceptions. We perform experimental evaluation on both C++ and Java applications to demonstrate the effectiveness of our techniques and measure the overhead that they introduce.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316871","Index Terms- Software engineering;software/program verification;reliability;testing and debugging;error handling and recovery;reliability;testing tools.","Robustness;Application software;Computer languages;Programming profession;Java;Runtime;Software testing;Error correction codes;Performance evaluation;Reliability engineering","software reliability;exception handling;program testing;program debugging;program verification;C++ language;Java","nonatomic exception handling;robust software development;C++;Java;software verification;program verification;software reliability;program testing;program debugging;error handling;error recovery","","13","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Estimating Computational Requirements in Multi-Threaded Applications","J. F. Pérez; G. Casale; S. Pacheco-Sanchez","Department of Computing, Imperial College London, United Kingdom; Department of Computing, Imperial College London, United Kingdom; SAP HANA Cloud Computing, Systems Engineering, Belfast, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","3","264","278","Performance models provide effective support for managing quality-of-service (QoS) and costs of enterprise applications. However, expensive high-resolution monitoring would be needed to obtain key model parameters, such as the CPU consumption of individual requests, which are thus more commonly estimated from other measures. However, current estimators are often inaccurate in accounting for scheduling in multi-threaded application servers. To cope with this problem, we propose novel linear regression and maximum likelihood estimators. Our algorithms take as inputs response time and resource queue measurements and return estimates of CPU consumption for individual request types. Results on simulated and real application datasets indicate that our algorithms provide accurate estimates and can scale effectively with the threading levels.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2363472","Seventh Framework Programme; InvestNI/SAP VIRTEX; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6926798","Demand estimation;Multi-threaded application servers;Application performance management;Demand estimation;multi-threaded application servers;application performance management","Time factors;Servers;Instruction sets;Maximum likelihood estimation;Computational modeling;Time measurement","maximum likelihood estimation;multi-threading;quality of service;queueing theory;regression analysis;software performance evaluation;systems analysis","computational requirement estimation;expensive high-resolution monitoring;quality-of-service;QoS management;cost management;performance models;CPU consumption;multithreaded application server scheduling;linear regression;maximum likelihood estimators;input response time;resource queue measurements;enterprise applications","","12","","41","","","","","","IEEE","IEEE Journals & Magazines"
"A simplified framework for reduction in strength","K. J. Ottenstein","Dept. of Comput. Sci., Michigan Technol. Univ., Houghton, MI, USA","IEEE Transactions on Software Engineering","","1989","15","1","86","92","Reduction in strength is a traditional transformation for speeding up loop execution on sequential processors. The inverse transformation, induction variable substitution, can also speed up loops by decreasing register requirements, although it is typically a normalizing step in the detection of array dependences by parallelizing compilers. The author presents a simple framework for performing these transformations. In contrast to previous approaches to strength reduction, no unnecessary temporary variables or dead code fragments are introduced, only relevant intermediate language fragments are examined, iteration test replacement is not handled as a special case, and the execution time of the target code is never increased. The method is particularly easy to visualize, making it a useful teaching tool as well.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21730","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21730","","Error analysis;Predictive models;Software systems;Frequency;Equations;Large-scale systems;Testing;Fluctuations;Programming profession;Linear regression","program compilers","loop execution;sequential processors;inverse transformation;induction variable substitution;register requirements;array dependences;parallelizing compilers;language fragments;iteration test replacement;execution time;target code;teaching tool","","1","","20","","","","","","IEEE","IEEE Journals & Magazines"
"A network pump","M. H. Kang; I. S. Moskowitz; D. C. Lee","Naval Res. Lab., Washington, DC, USA; Naval Res. Lab., Washington, DC, USA; Naval Res. Lab., Washington, DC, USA","IEEE Transactions on Software Engineering","","1996","22","5","329","338","A designer of reliable multi level secure (MLS) networks must consider covert channels and denial of service attacks in addition to traditional network performance measures such as throughput, fairness, and reliability. We show how to extend the NRL data Pump to a certain MLS network architecture in order to balance the requirements of congestion control, fairness, good performance, and reliability against those of minimal threats from covert channels and denial of service attacks. We back up our claims with simulation results.","0098-5589;1939-3520;2326-3881","","10.1109/32.502225","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=502225","","Multilevel systems;Computer crime;Information security;Communication system security;B-ISDN;Asynchronous transfer mode;Switches;Throughput;Control systems;Information theory","multi-access systems;security of data","network pump;reliable multi level secure networks;covert channels;network performance measures;NRL data Pump;MLS network architecture;congestion control","","53","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Software Reliability Modeling with Software Metrics Data via Gaussian Processes","N. Torrado; M. P. Wiper; R. E. Lillo","Universidad Carlos III de Madrid, Madrid; Universidad Carlos III de Madrid, Madrid; Universidad Carlos III de Madrid, Madrid","IEEE Transactions on Software Engineering","","2013","39","8","1179","1186","In this paper, we describe statistical inference and prediction for software reliability models in the presence of covariate information. Specifically, we develop a semiparametric, Bayesian model using Gaussian processes to estimate the numbers of software failures over various time periods when it is assumed that the software is changed after each time period and that software metrics information is available after each update. Model comparison is also carried out using the deviance information criterion, and predictive inferences on future failures are shown. Real-life examples are presented to illustrate the approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6392172","Software metrics;software failures;reliability;statistical methods;Markov chain Monte Carlo method","Software;Gaussian processes;Software reliability;Software metrics;Predictive models;Bayesian methods","Bayes methods;Gaussian processes;inference mechanisms;software metrics;software reliability;statistical analysis;system recovery","software reliability modeling;statistical inference;covariate information;semiparametric Bayesian model;Gaussian process;software failure;software metrics information;deviance information criterion;predictive inference;future failure","","5","","30","","","","","","IEEE","IEEE Journals & Magazines"
"The join algorithms on a shared-memory multiprocessor database machine","G. Z. Qadah; K. B. Irani","Dept. of Electr. Eng. & Comput. Sci., Northwestern Univ., Evanston, IL, USA; NA","IEEE Transactions on Software Engineering","","1988","14","11","1668","1683","The authors develop and present a large set of parallel algorithms for implementing the join operation on a shared-memory multiprocessor database machine. The development of these algorithms follows a structured approach. The major steps involved in the processing of the join operation by the machine are first identified. Then, alternative join algorithms are constructed by concatenating the different ways of performing these steps. A study of the performance of the proposed algorithms is presented. This study shows, among other things, that for a given hardware configuration there is not just one overall best performing join algorithm, but rather different algorithms score the best performance, depending on the characteristics of the data participating in the join operation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9054","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9054","","Database machines;Parallel algorithms;Hardware;Database systems;Computer architecture;Parallel processing;Software systems;Relational databases;Concurrent computing;Data processing","parallel algorithms;parallel architectures;performance evaluation;relational databases;special purpose computers","parallel architectures;relational databases;performance evaluation;join algorithms;shared-memory multiprocessor database machine;parallel algorithms","","23","","25","","","","","","IEEE","IEEE Journals & Magazines"
"COVERT: Compositional Analysis of Android Inter-App Permission Leakage","H. Bagheri; A. Sadeghi; J. Garcia; S. Malek","Department of Computer Science, George Mason University, Fairfax, VA; Department of Computer Science, George Mason University, Fairfax, VA 22030; Computer Science and Artificial Intelligence Laboratory at MIT, Cambridge, MA; Computer Science and Artificial Intelligence Laboratory at MIT, Cambridge, MA","IEEE Transactions on Software Engineering","","2015","41","9","866","886","Android is the most popular platform for mobile devices. It facilitates sharing of data and services among applications using a rich inter-app communication system. While access to resources can be controlled by the Android permission system, enforcing permissions is not sufficient to prevent security violations, as permissions may be mismanaged, intentionally or unintentionally. Android's enforcement of the permissions is at the level of individual apps, allowing multiple malicious apps to collude and combine their permissions or to trick vulnerable apps to perform actions on their behalf that are beyond their individual privileges. In this paper, we present COVERT, a tool for compositional analysis of Android inter-app vulnerabilities. COVERT's analysis is modular to enable incremental analysis of applications as they are installed, updated, and removed. It statically analyzes the reverse engineered source code of each individual app, and extracts relevant security specifications in a format suitable for formal verification. Given a collection of specifications extracted in this way, a formal analysis engine (e.g., model checker) is then used to verify whether it is safe for a combination of applications-holding certain permissions and potentially interacting with each other-to be installed together. Our experience with using COVERT to examine over 500 real-world apps corroborates its ability to find inter-app vulnerabilities in bundles of some of the most popular apps on the market.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2419611","US Defense Advanced Research Projects Agency; US National Security Agency; US Department of Homeland Security; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7079508","Formal Verification;Static Analysis;Android;Inter-App Vulnerabilities;Formal verification;static analysis;Android;Inter-App vulnerabilities","Smart phones;Androids;Humanoid robots;Security;Analytical models;Data mining;Metals","Android (operating system);formal specification;formal verification;mobile computing;program diagnostics;security of data","COVERT tool;compositional analysis;formal analysis engine;security specification;formal verification;incremental analysis;Android inter-app vulnerabilities analysis;security violation;Android permission system;inter-app communication system;mobile devices;Android inter-app permission leakage","","34","","60","","","","","","IEEE","IEEE Journals & Magazines"
"WAM—The Weighted Average Method for Predicting the Performance of Systems with Bursts of Customer Sessions","D. Krishnamurthy; J. Rolia; M. Xu","University of Calgary, Calgary; Hewlett Packard Labs, Bristol; University of Calgary, Calgary","IEEE Transactions on Software Engineering","","2011","37","5","718","735","Predictive performance models are important tools that support system sizing, capacity planning, and systems management exercises. We introduce the Weighted Average Method (WAM) to improve the accuracy of analytic predictive performance models for systems with bursts of concurrent customers. WAM considers the customer population distribution at a system to reflect the impact of bursts. The WAM approach is robust with respect to distribution functions, including heavy-tail-like distributions, for workload parameters. We demonstrate the effectiveness of WAM using a case study involving a multitier TPC-W benchmark system. To demonstrate the utility of WAM with multiple performance modeling approaches, we developed both Queuing Network Models and Layered Queuing Models for the system. Results indicate that WAM improves prediction accuracy for bursty workloads for QNMs and LQMs by 10 and 12 percent, respectively, with respect to a Markov Chain approach reported in the literature.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.65","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5953602","Performance of systems;modeling techniques;queuing theory;operational analysis.","Markov processes;Analytical models;Predictive models;Accuracy;Queueing analysis;Time factors;Software","Markov processes;queueing theory;software performance evaluation;systems analysis","weighted average method;system performance prediction;customer session bursts;system sizing;capacity planning;systems management exercises;analytic predictive performance models;heavy tail like distributions;multitier TPC-W benchmark system;queuing network models;layered queuing models;bursty workloads;Markov chain approach","","9","","41","","","","","","IEEE","IEEE Journals & Magazines"
"MAHAKIL: Diversity Based Oversampling Approach to Alleviate the Class Imbalance Issue in Software Defect Prediction","K. E. Bennin; J. Keung; P. Phannachitta; A. Monden; S. Mensah","Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; College of Arts, Media and Technology, Chiang Mai University, Chiang Mai, Thailand; Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong","IEEE Transactions on Software Engineering","","2018","44","6","534","550","Highly imbalanced data typically make accurate predictions difficult. Unfortunately, software defect datasets tend to have fewer defective modules than non-defective modules. Synthetic oversampling approaches address this concern by creating new minority defective modules to balance the class distribution before a model is trained. Notwithstanding the successes achieved by these approaches, they mostly result in over-generalization (high rates of false alarms) and generate near-duplicated data instances (less diverse data). In this study, we introduce MAHAKIL, a novel and efficient synthetic oversampling approach for software defect datasets that is based on the chromosomal theory of inheritance. Exploiting this theory, MAHAKIL interprets two distinct sub-classes as parents and generates a new instance that inherits different traits from each parent and contributes to the diversity within the data distribution. We extensively compare MAHAKIL with SMOTE, Borderline-SMOTE, ADASYN, Random Oversampling and the No sampling approach using 20 releases of defect datasets from the PROMISE repository and five prediction models. Our experiments indicate that MAHAKIL improves the prediction performance for all the models and achieves better and more significant pf values than the other oversampling approaches, based on Brunner's statistical significance test and Cliff's effect sizes. Therefore, MAHAKIL is strongly recommended as an efficient alternative for defect prediction models built on highly imbalanced datasets.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2731766","General Research Fund of the Research Grants Council of Hong Kong; City University of Hong Kong; JSPS KAKENHI; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7990590","Software defect prediction;class imbalance learning;synthetic sample generation;data sampling methods;classification problems","Biological cells;Software;Predictive models;Animals;Electronic mail;Sampling methods","learning (artificial intelligence);pattern classification;sampling methods;software quality","MAHAKIL;software defect prediction;software defect datasets;nondefective modules;minority defective modules;class distribution;data instances;less diverse data;distinct sub-classes;data distribution;Random Oversampling;No sampling approach;prediction performance;defect prediction models;highly imbalanced datasets;defective modules;class imbalance issue;synthetic oversampling approaches;synthetic oversampling approach","","3","","80","","","","","","IEEE","IEEE Journals & Magazines"
"Linear complexity assertions for sorting","N. R. Saxena; E. J. McCluskey","HaL Comput. Syst., Campbell, CA, USA; NA","IEEE Transactions on Software Engineering","","1994","20","6","424","431","Correctness of the execution of sorting programs can be checked by two assertions: the order assertion and the permutation assertion. The order assertion checks if the sorted data is in ascending or descending order. The permutation assertion checks if the output data produced by sorting is a permutation of the original input data. Permutation and order assertions are sufficient for the detection of errors in the execution of sorting programs; however, in terms of execution time these assertions cost the same as sorting programs. An assertion, called the order-sum assertion, that has lower execution cost than sorting programs is derived from permutation and order assertions. The reduction in cost is achieved at the expense of incomplete checking. Some metrics are derived to quantify the effectiveness of order-sum assertion under various error models. A natural connection between the effectiveness of the order-sum assertion and the partition theory of numbers is shown. Asymptotic formulae for partition functions are derived.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.295891","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=295891","","Sorting;Computer errors;Error correction;Hardware;Costs;Change detection algorithms;Computer aided instruction;Testing;Counting circuits;Registers","sorting;program debugging;program diagnostics;program verification;programming theory","linear complexity assertions;sorting programs;programs correctness checking;program execution;order assertion;permutation assertion;sorted data;descending order;ascending order;output data;input data;error detection;execution time;order-sum assertion;partition theory;partition functions;error checking;watchdog checker","","4","","12","","","","","","IEEE","IEEE Journals & Magazines"
"On the expected number of failures detected by subdomain testing and random testing","T. Y. Chen; Y. T. Yu","Dept. of Comput. Sci., Melbourne Univ., Parkville, Vic., Australia; Dept. of Comput. Sci., Melbourne Univ., Parkville, Vic., Australia","IEEE Transactions on Software Engineering","","1996","22","2","109","119","We investigate the efficacy of subdomain testing and random testing using the expected number of failures detected (the E-measure) as a measure of effectiveness. Simple as it is, the E-measure does provide a great deal of useful information about the fault detecting capability of testing strategies. With the E-measure, we obtain new characterizations of subdomain testing, including several new conditions that determine whether subdomain testing is more or less effective than random testing. Previously, the efficacy of subdomain testing strategies has been analyzed using the probability of detecting at least one failure (the P-measure) for the special case of disjoint subdomains only. On the contrary, our analysis makes use of the E-measure and considers also the general case in which subdomains may or may not overlap. Furthermore, we discover important relations between the two different measures. From these relations, we also derive corresponding characterizations of subdomain testing in terms of the P-measure.","0098-5589;1939-3520;2326-3881","","10.1109/32.485221","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=485221","","Software testing;System testing;Failure analysis;Computer science;Electronic mail;Fault detection","program testing;software metrics;programming theory","subdomain testing;random testing;expected number;failure detection;E measure;E-measure;fault detecting capability;testing strategies;disjoint subdomains;partition testing;software testing","","81","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A Study of Causes and Consequences of Client-Side JavaScript Bugs","F. S. Ocariza; K. Bajaj; K. Pattabiraman; A. Mesbah","Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Software Engineering","","2017","43","2","128","144","Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2586066","Natural Sciences and Engineering Research Council of Canada (NSERC); Intel Corporation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7501855","Faults;JavaScript;Document Object Model (DOM);bug reports;empirical study","Computer bugs;Servers;Market research;HTML;Data mining;Reliability;Cascading style sheets","client-server systems;Internet;Java;program debugging;software fault tolerance","client-side JavaScript bugs;user-interactivity;client-server communications;JavaScript programmers;bug repositories;information extraction;DOM-related JavaScript faults;JavaScript code;document object model;Web application components;static analysis tools","","8","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Trust requirements and performance of a fast subtransport-level protocol for secure communication","P. V. Rangan","Dept. of Comput. Sci., California Univ., San Diego, CA, USA","IEEE Transactions on Software Engineering","","1993","19","2","181","186","A secure network protocol called the authenticated datagram protocol (ADP) that optimizes the performance of global networks by establishing host-to-host secure channels and building agent-to-agent channels on top of host-to-host channels is presented. The performance advantages of ADP come with an accompanying set of trust requirements that are stringent for a network spanning mutually distrustful organizations. The cause for this stringency is shown to be propagation of trust relationships in ADP. Methods of breaking their propagation and thereby accomplishing a significant reduction in ADP's trust requirements are presented. ADP, being a protocol for establishing host-to-host channels, can be handled at the subtransport level of the protocol hierarchy. A prototype of ADP implemented on Sun workstations connected by an Ethernet is described. Experimental measurements confirm that both the average latency of messages and the maximum throughput are substantially better than other secure protocols.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.214834","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=214834","","Protocols;Cryptography;Public key;Computer networks;Authentication;Privacy;Prototypes;Sun;Workstations;Ethernet networks","data integrity;protocols;security of data","trust requirements;performance;fast subtransport-level protocol;secure communication;authenticated datagram protocol;host-to-host secure channels;agent-to-agent channels;Sun workstations;Ethernet;average latency;maximum throughput","","1","","7","","","","","","IEEE","IEEE Journals & Magazines"
"An automated software design assistant","J. Karimi; B. R. Konsynsky","Coll. of Bus. & Adm., Colorado Univ., Denver, CO, USA; NA","IEEE Transactions on Software Engineering","","1988","14","2","194","210","An automated software design assistant was implemented as a part of a long-term project with the objectives of applying the computer-aided technique to the tools in a software engineering environment. A set of quantitative measures are derived based on the degree to which a particular design satisfied the attributes associated with a structured software design. The measure are then used as decision rules for a computer-aided methodology for structured design. The feasibility of the approach is also demonstrated by a case study using a small application system design problem.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4638","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4638","","Software design;Process design;Software tools;Software measurement;Design methodology;Software engineering;Particle measurements;Application software;Information management","automatic programming;software tools;structured programming","software tools;automated software design assistant;software engineering environment;quantitative measures;structured software design","","7","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Revisiting the Performance Evaluation of Automated Approaches for the Retrieval of Duplicate Issue Reports","M. S. Rakha; C. Bezemer; A. E. Hassan","Queen's University, Kingston, ON, Canada; Queen's University, Kingston, ON, Canada; Queen's University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2018","44","12","1245","1268","Issue tracking systems (ITSs), such as Bugzilla, are commonly used to track reported bugs, improvements and change requests for a software project. To avoid wasting developer resources on previously-reported (i.e., duplicate) issues, it is necessary to identify such duplicates as soon as they are reported. Several automated approaches have been proposed for retrieving duplicate reports, i.e., identifying the duplicate of a new issue report in a list of<inline-formula><tex-math notation=""LaTeX"">$n$</tex-math><alternatives><inline-graphic xlink:href=""rakha-ieq1-2755005.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>candidates. These approaches rely on leveraging the textual, categorical, and contextual information in previously-reported issues to decide whether a newly-reported issue has previously been reported. In general, these approaches are evaluated using data that spans a relatively short period of time (i.e., the classical evaluation). However, in this paper, we show that the classical evaluation tends to overestimate the performance of automated approaches for retrieving duplicate issue reports. Instead, we propose a realistic evaluation using all the reports that are available in the ITS of a software project. We conduct experiments in which we evaluate two popular approaches for retrieving duplicate issues (BM25F and REP) using the classical and realistic evaluations. We find that for the issue tracking data of the Mozilla foundation, the Eclipse foundation and OpenOffice, the realistic evaluation shows that previously proposed approaches perform considerably lower than previously reported using the classical evaluation. As a result, we conclude that the reported performance of approaches for retrieving duplicate issue reports is significantly overestimated in literature. In order to improve the performance of the automated retrieval of duplicate issue reports, we propose to leverage the resolution field of issue reports. Our experiments show that a relative improvement in the performance of a median of 7-21.5 percent and a maximum of 19-60 percent can be achieved by leveraging the resolution field of issue reports for the automated retrieval of duplicates.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2755005","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8048025","Text analysis;software engineering;performance evaluation","Text analysis;Computer bugs;Frequency measurement;Performance evaluation;Manuals;Ports (Computers)","","","","2","","36","","","","","","IEEE","IEEE Journals & Magazines"
"TuringTool: a user interface to aid in the software maintenance task","J. R. Cordy; N. L. Eliot; M. G. Robertson","Dept. of Comput. & Inf. Sci., Queen's Univ., Kingston, Ont., Canada; Dept. of Comput. & Inf. Sci., Queen's Univ., Kingston, Ont., Canada; NA","IEEE Transactions on Software Engineering","","1990","16","3","294","301","TuringTool is a source program viewing and editing system specifically designed to support the software maintenance task. TuringTool bases all of its views of the program on a single comprehensive viewing paradigm borrowed from program development environments: source text elision. It is shown how this paradigm can be used to represent several kinds of views appropriate to the maintenance of large source programs, including structural views and nonstructural views appropriate to the maintenance task and how it can be extended to allow dynamic creation of complex programmer-specified views using simple set theoretic operators to combine the effects of several views into one. The system exploits the highly structured nature of the Turing programming language to allow seamless viewing of programs consisting of many separately compiled source modules as one uniform source.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.48937","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=48937","","User interfaces;Software maintenance;Computer languages;Programming environments;Synthesizers;Programming profession;Councils;Tree graphs;Knowledge based systems;Large screen displays","high level languages;programming environments;software engineering;user interfaces","user interface;software maintenance task;source program;editing system;TuringTool;single comprehensive viewing paradigm;program development environments;source text elision;large source programs;structural views;nonstructural views;dynamic creation;complex programmer-specified views;simple set theoretic operators;Turing programming language;seamless viewing;separately compiled source modules;uniform source","","13","","17","","","","","","IEEE","IEEE Journals & Magazines"
"A Model-Driven Methodology for Developing Secure Data-Management Applications","D. Basin; M. Clavel; M. Egea; M. A. G. de Dios; C. Dania","ETH Zürich, Zürich, Switzerland; IMDEA Software, Campus de Montegancedo, s/n, Pozuelo de Alarcon, Madrid, Spain; ATOS Research & Innovation, Madrid, Spain; IMDEA Software, Campus de Montegancedo, s/n, Pozuelo de Alarcon, Madrid, Spain; IMDEA Software, Campus de Montegancedo, s/n, Pozuelo de Alarcon, Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","4","324","337","We present a novel model-driven methodology for developing secure data-management applications. System developers proceed by modeling three different views of the desired application: its data model, security model, and GUI model. These models formalize respectively the application's data domain, authorization policy, and its graphical interface together with the application's behavior. Afterwards a model-transformation function lifts the policy specified by the security model to the GUI model. This allows a separation of concerns where behavior and security are specified separately, and subsequently combined to generate a security-aware GUI model. Finally, a code generator generates a multi-tier application, along with all support for access control, from the security-aware GUI model. We report on applications built using our approach and the associated tool.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.2297116","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6698396","Model-driven development;model-driven security;access control;GUI models;model transformation","Data models;Graphical user interfaces;Unified modeling language;Authorization;Syntactics","authorisation;graphical user interfaces;software engineering","model-driven methodology;secure data-management applications;data model;security model;graphical user intefaces;authorization policy;model-transformation function;security-aware GUI model;code generator;multitier application;access control","","6","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental generation of parsers","J. Heering; P. Klint; J. Rekers","Dept. of Software Technol., Centre for Math. & Comput. Sci., Amsterdam, Netherlands; Dept. of Software Technol., Centre for Math. & Comput. Sci., Amsterdam, Netherlands; Dept. of Software Technol., Centre for Math. & Comput. Sci., Amsterdam, Netherlands","IEEE Transactions on Software Engineering","","1990","16","12","1344","1351","An LR-based parser generator for arbitrary context-free grammars that generates parsers by need and handles modifications to its input grammar by updating the parser it has generated so far is described. The need for these techniques is discussed in the context of interactive language definition environments. All required algorithms are presented. Measurements are given comparing their performance with that of conventional techniques.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.62443","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=62443","","Computer science;Mathematics;Delay","context-free grammars;program compilers","incremental parser generation;LR-based parser generator;arbitrary context-free grammars;input grammar;interactive language definition environments;algorithms;performance;conventional techniques","","13","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Improving Automated Bug Triaging with Specialized Topic Model","X. Xia; D. Lo; Y. Ding; J. M. Al-Kofahi; T. N. Nguyen; X. Wang","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Information Systems, Singapore Management University, Singapore, Singapore; School of Information Systems, Singapore Management University, Singapore, Singapore; Electrical and Computer Engineering Department, Iowa State University, Ames, IA, USA; Electrical and Computer Engineering Department, Iowa State University, Ames, IA, USA; College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Software Engineering","","2017","43","3","272","297","Bug triaging refers to the process of assigning a bug to the most appropriate developer to fix. It becomes more and more difficult and complicated as the size of software and the number of developers increase. In this paper, we propose a new framework for bug triaging, which maps the words in the bug reports (i.e., the term space) to their corresponding topics (i.e., the topic space). We propose a specialized topic modeling algorithm named<italic>multi-feature topic model (MTM)</italic>which extends Latent Dirichlet Allocation (LDA) for bug triaging.<italic>MTM</italic>considers product and component information of bug reports to map the term space to the topic space. Finally, we propose an incremental learning method named<italic>TopicMiner</italic>which considers the topic distribution of a new bug report to assign an appropriate fixer based on the affinity of the fixer to the topics. We pair<italic>TopicMiner</italic>with MTM (<italic>TopicMiner<inline-formula><tex-math notation=""LaTeX"">$^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=""xia-ieq1-2576454.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula></italic>). We have evaluated our solution on 5 large bug report datasets including GCC, OpenOffice, Mozilla, Netbeans, and Eclipse containing a total of 227,278 bug reports. We show that<italic>TopicMiner<inline-formula><tex-math notation=""LaTeX"">$^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=""xia-ieq2-2576454.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula></italic>can achieve top-1 and top-5 prediction accuracies of 0.4831-0.6868, and 0.7686-0.9084, respectively. We also compare<italic>TopicMiner<inline-formula><tex-math notation=""LaTeX"">$^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=""xia-ieq3-2576454.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula></italic>with Bugzie, LDA-KL, SVM-LDA, LDA-Activity, and Yang et al.'s approach. The results show that<italic>TopicMiner<inline-formula><tex-math notation=""LaTeX"">$^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=""xia-ieq4-2576454.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula></italic>on average improves top-1 and top-5 prediction accuracies of Bugzie by 128.48 and 53.22 percent, LDA-KL by 262.91 and 105.97 percent, SVM-LDA by 205.89 and 110.48 percent, LDA-Activity by 377.60 and 176.32 percent, and Yang et al.'s approach by 59.88 and 13.70 percent, respectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2576454","National Basic Research Program of China (the 973 Program); NSFC; National Key Technology R&D Program; Ministry of Science and Technology of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7484672","Developer;bug triaging;feature information;topic model","Software;Resource management;Software algorithms;Support vector machines;Learning systems;Indexes;Computer bugs","","","","11","","46","","","","","","IEEE","IEEE Journals & Magazines"
"GreenDroid: Automated Diagnosis of Energy Inefficiency for Smartphone Applications","Y. Liu; C. Xu; S. C. Cheung; J. Lü","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; State Key Laboratory for Novel Software Technology and Department of Computer Science and Technology, Nanjing University, 163 Xianlin Avenue, Nanjing, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; State Key Laboratory for Novel Software Technology and Department of Computer Science and Technology, Nanjing University, 163 Xianlin Avenue, Nanjing, China","IEEE Transactions on Software Engineering","","2014","40","9","911","940","Smartphone applications' energy efficiency is vital, but many Android applications suffer from serious energy inefficiency problems. Locating these problems is labor-intensive and automated diagnosis is highly desirable. However, a key challenge is the lack of a decidable criterion that facilitates automated judgment of such energy problems. Our work aims to address this challenge. We conducted an in-depth study of 173 open-source and 229 commercial Android applications, and observed two common causes of energy problems: missing deactivation of sensors or wake locks, and cost-ineffective use of sensory data. With these findings, wepropose an automated approach to diagnosing energy problems in Android applications. Our approach explores an application's state space by systematically executing the application using Java PathFinder (JPF). It monitors sensor and wake lock operations to detect missing deactivation of sensors and wake locks. It also tracks the transformation and usage of sensory data and judges whether they are effectively utilized by the application using our state-sensitive data utilization metric. In this way, our approach can generate detailed reports with actionable information to assist developers in validating detected energy problems. We built our approach as a tool, GreenDroid, on top of JPF. Technically, we addressed the challenges of generating user interaction events and scheduling event handlers in extending JPF for analyzing Android applications. We evaluated GreenDroid using 13 real-world popular Android applications. GreenDroid completed energy efficiency diagnosis for these applications in a few minutes. It successfully located real energy problems in these applications, and additionally found new unreported energy problems that were later confirmed by developers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2323982","Research Grants Council; National High-Tech Research & Development Program; National Natural Science Foundation; New Century Excellent Talents in University; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6815752","Smartphone application;energy inefficiency;automated diagnosis;sensory data utilization;green computing","Androids;Humanoid robots;Computer bugs;Sensors;Open source software;Green products;Google","Android (operating system);Java;power aware computing;program diagnostics;public domain software;smart phones","GreenDroid;automated energy inefficiency diagnosis;smartphone applications;labor-intensive diagnosis;automated diagnosis;open-source Android applications;commercial Android applications;Java PathFinder;JPF;wake lock operations;state-sensitive data utilization metric;user interaction events;scheduling event handlers","","25","","72","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Analysis of Business Process Execution Language Usage","M. Hertis; M. B. Juric","Laboratory for Integration of Information Systems, Faculty of Computer and Information Science, University of Ljubljana, Trzaska cesta 25, Ljubljana, Slovenia, and Seltron d.o.o., Trzaska cesta 85 a, Maribor; Faculty, of Computer and Information Science, Ljubljana, Slovenia","IEEE Transactions on Software Engineering","","2014","40","8","738","757","The current state of executable business process languages allows for and demands optimization of design practices and specifications. In this paper, we present the first empirical study that analyses Web Services Business Process Execution Language (WS-BPEL or BPEL) usage and characteristics of real world executable business processes. We have analysed 1,145 BPEL processes by measuring activity usage and process complexity. In addition, we investigated the occurrence of activity usage patterns. The results revealed that the usage frequency of BPEL activities varies and that some activities have a strong co-occurrence. BPEL activities often appear in activity patterns that are repeated in multiple processes. Furthermore, the current process complexity metrics have proved to be inadequate for measuring BPEL process complexity. The empirical results provide fundamental knowledge on how BPEL specification and process design practices can be improved. We propose BPEL design guidelines and BPEL language improvements for the design of more understandable and less complex processes. The results are of interest to business process language designers, business process tool developers, business process designers and developers, and software engineering researchers, and contribute to the general understanding of BPEL and service-oriented architecture.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2322618","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6812231","WS-BPEL Analysis;complexity measure;service composition;process patterns;process complexity;process comprehension;empirical study","Complexity theory;Business;Measurement;Semantics;XML;Syntactics;Guidelines","service-oriented architecture;Web Services Business Process Execution Language","empirical analysis;design practices;design specifications;Web services business process execution language;WS-BPEL;executable business processes;activity usage;process complexity;BPEL activities;BPEL design guidelines;BPEL language improvements;service-oriented architecture","","8","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Edmas: An Object-Oriented, Locally Distributed Mail System","G. T. Almes; C. L. Holman","Department of Computer Science, Rice University; NA","IEEE Transactions on Software Engineering","","1987","SE-13","9","1001","1009","The Eden Project conducts research in the design and implementation of a distributed computing environment for a local area network. A specific goal in designing Eden was to provide users the advantages of both physical distribution and logical integration. Edmas, the Eden mail system, provided an early test of Eden as a base for building distributed applications. This paper discusses Edmas, and shows how Eden's advanced functionality aided us in structuring a distributed mail system, particularly in the areas of replying and distribution lists.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233522","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702322","Capability;concurrent programming;deadlock avoidance;distributed program;Eden;electronic mail;local area network;object-oriented system;remote procedure call","Postal services;Local area networks;Page description languages;Buildings;Computer science;Operating systems;Application software;Testing;Computer languages;Data structures","","Capability;concurrent programming;deadlock avoidance;distributed program;Eden;electronic mail;local area network;object-oriented system;remote procedure call","","3","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Towards banishing the cut from Prolog","S. K. Debray; D. S. Warren","Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA; Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA","IEEE Transactions on Software Engineering","","1990","16","3","335","349","Logic programs can often be inefficient. The usual solution to this problem has been to return some control to the user in the form of impure language features like cut. The authors argue that it is not necessary to resort to such impure features for efficiency. This point is illustrated by considering how most of the common uses of cut can be eliminated from Prolog source programs, relying on static analysis to generate them at compile time. Three common situations where the cut is used are considered. Static analysis techniques are given to detect such situations, and applicable program transformations are described. Two language constructs, firstof and oneof, for situations involving don't-care nondeterminism, are suggested. These constructs have better declarative readings than the cut and extend better to parallel evaluation strategies. Together, these proposals result in a system where users need rely much less on cuts for efficiency, thereby promoting a purer programming style without sacrificing efficiency.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.48941","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=48941","","Logic programming;Computer science;Proposals;Programming profession;Costs","logic programming;PROLOG","logic programs;impure language features;impure features;Prolog source programs;static analysis;compile time;program transformations;language constructs;firstof;oneof;nondeterminism;declarative readings;cut;parallel evaluation strategies;purer programming style","","7","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Toward the Reverse Engineering of UML Sequence Diagrams for Distributed Java Software","L. C. Briand; Y. Labiche; J. Leduc","Software Quality Engineering Laboratory, Department of Systems and Computer Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, ON K1S5B6, Canada; Software Quality Engineering Laboratory, Department of Systems and Computer Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, ON K1S5B6, Canada; Siemens Corporate Research Inc., 755 College Road East, Princeton, NJ 08540","IEEE Transactions on Software Engineering","","2006","32","9","642","663","This paper proposes a methodology and instrumentation infrastructure toward the reverse engineering of UML (Unified Modeling Language) sequence diagrams from dynamic analysis. One motivation is, of course, to help people understand the behavior of systems with no (complete) documentation. However, such reverse-engineered dynamic models can also be used for quality assurance purposes. They can, for example, be compared with design sequence diagrams and the conformance of the implementation to the design can thus be verified. Furthermore, discrepancies can also suggest failures in meeting the specifications. Due to size constraints, this paper focuses on the distribution aspects of the methodology we propose. We formally define our approach using metamodels and consistency rules. The instrumentation is based on aspect-oriented programming in order to alleviate the effort overhead usually associated with source code instrumentation. A case study is discussed to demonstrate the applicability of the approach on a concrete example","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.96","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707665","UML;sequence diagram;reverse engineering;distribution;RMI;AspectJ;OCL.","Reverse engineering;Unified modeling language;Java;Object oriented modeling;Instruments;Testing;Runtime;Quality assurance;Information retrieval;Documentation","formal specification;formal verification;Java;object-oriented programming;program diagnostics;reverse engineering;software quality;Unified Modeling Language","reverse engineering;UML sequence diagram;distributed Java software;Unified Modeling Language;dynamic program analysis;quality assurance;formal verification;formal specification;metamodel;consistency rule;aspect-oriented programming;source code instrumentation","","94","","41","","","","","","IEEE","IEEE Journals & Magazines"
"The cloze procedure and software comprehensibility measurement","W. E. Hall; S. H. Zweben","AT&T Bell Laboratories, 6200 E. Broad St., Columbus, OH 43213; Department of Computer and Information Science, Ohio State University, Columbus, OH 43210","IEEE Transactions on Software Engineering","","1986","SE-12","5","608","623","Cloze tests (i.e. fill-in-missing-parts tests) have been a long-standing measure of prose comprehension. Through human-subject experimentation, evidence was gathered to support the practical advantages of using the cloze procedure for measuring software comprehension. Cloze tests were found to be easy to construct, administer, and score and to be capable of discriminating between programs of varying comprehensibility. However, discrepancies between multiple-choice comprehension quiz results and some cloze test results for the same software suggested that certain forms of software cloze tests may not be valid. A model of software cloze tests was developed to identify a software cloze test characteristic that may produce invalid results. The test characteristic was concerned with the relative proportion of `program-dependent' and `program-independent' cloze items within a test. The developed model was shown to be consistent with software cloze test results of another researcher and led to suggestions for improving software cloze testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312957","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312957","Cloze;human-subject experimentation;multiple-choice test;software comprehension;validity","Software;Software measurement;Materials;Testing;Guidelines;Syntactics;Programming","program testing;software reliability","software comprehensibility measurement;prose comprehension;cloze procedure;software cloze testing","","2","","","","","","","","IEEE","IEEE Journals & Magazines"
"PIE: a dynamic failure-based technique","J. M. Voas","Reliable Software Technologies Corp., Arlington, VA, USA","IEEE Transactions on Software Engineering","","1992","18","8","717","727","A dynamic technique called PIE (propagation, infection, and execution) is presented for statistically estimating three program characteristics that affect a program's computational behavior: (1) the probability that a particular section of a program is executed, (2) the probability that the particular section affects the data state, and (3) the probability that a data state produced by that section has an effect on program output. These three characteristics can be used to predict whether faults are likely to be uncovered by software testing.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.153381","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=153381","","Software testing;Genetic mutations;Frequency estimation;State estimation;Automatic testing;Information analysis;Failure analysis;Humans;Fault diagnosis;Councils","program testing","propagation;infection;execution;PIE;dynamic failure-based technique;statistically estimating;program characteristics;computational behavior;data state;software testing","","187","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Managing conflicts in goal-driven requirements engineering","A. van Lamsweerde; R. Darimont; E. Letier","Dept. d'Ingeniere Inf., Univ. Catholique de Louvain, Belgium; NA; NA","IEEE Transactions on Software Engineering","","1998","24","11","908","926","A wide range of inconsistencies can arise during requirements engineering as goals and requirements are elicited from multiple stakeholders. Resolving such inconsistencies sooner or later in the process is a necessary condition for successful development of the software implementing those requirements. The paper first reviews the main types of inconsistency that can arise during requirements elaboration, defining them in an integrated framework and exploring their interrelationships. It then concentrates on the specific case of conflicting formulations of goals and requirements among different stakeholder viewpoints or within a single viewpoint. A frequent, weaker form of conflict called divergence is introduced and studied in depth. Formal techniques and heuristics are proposed for detecting conflicts and divergences from specifications of goals/requirements and of domain properties. Various techniques are then discussed for resolving conflicts and divergences systematically by the introduction of new goals or by transforming the specifications of goals/objects toward conflict-free versions. Numerous examples are given throughout the paper to illustrate the practical relevance of the concepts and techniques presented. The latter are discussed in the framework of the KAOS methodology for goal-driven requirements engineering.","0098-5589;1939-3520;2326-3881","","10.1109/32.730542","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=730542","","Engineering management;Humans;Labeling","formal specification;configuration management","conflict management;goal-driven requirements engineering;multiple stakeholders;inconsistencies;software development;requirements elaboration;integrated framework;requirements interrelationships;conflicting formulations;stakeholder viewpoints;divergence;formal techniques;heuristics;conflict detection;specification divergence;domain properties;conflict-free versions;KAOS methodology;specification transformation;lightweight formal methods","","235","","57","","","","","","IEEE","IEEE Journals & Magazines"
"A Controlled Experiment for Program Comprehension through Trace Visualization","B. Cornelissen; A. Zaidman; A. van Deursen","Software Improvement Group, Amsterdam; Delft University of Technology, Delft; Delft University of Technology, Delft","IEEE Transactions on Software Engineering","","2011","37","3","341","355","Software maintenance activities require a sufficient level of understanding of the software at hand that unfortunately is not always readily available. Execution trace visualization is a common approach in gaining this understanding, and among our own efforts in this context is Extravis, a tool for the visualization of large traces. While many such tools have been evaluated through case studies, there have been no quantitative evaluations to the present day. This paper reports on the first controlled experiment to quantitatively measure the added value of trace visualization for program comprehension. We designed eight typical tasks aimed at gaining an understanding of a representative subject system, and measured how a control group (using the Eclipse IDE) and an experimental group (using both Eclipse and Extravis) performed these tasks in terms of time spent and solution correctness. The results are statistically significant in both regards, showing a 22 percent decrease in time requirements and a 43 percent increase in correctness for the group using trace visualization.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5441291","Program comprehension;dynamic analysis;controlled experiment.","Visualization;Computer Society;Time measurement;Programming;Documentation;Scalability;Software maintenance;Gain measurement;Control systems;Performance evaluation","data visualisation;software maintenance","program comprehension;software maintenance;execution trace visualization","","50","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Asymptotic analysis of a heterogeneous multiprocessor system in a randomly changing environment","J. Sztrik; D. Kouvatsos","Dept. of Comput., Bradford Univ., UK; Dept. of Comput., Bradford Univ., UK","IEEE Transactions on Software Engineering","","1991","17","10","1069","1075","An asymptotic queuing theoretic approach is proposed to analyze the performance of an FCFS (first-come, first-served) heterogeneous multiprocessor computer system with a single bus operating in a randomly changing environment. All stochastic times in the system are considered to be exponentially distributed and independent of the random environment, while the access and service rates of the processors are subject to random fluctuations. It is shown under the assumption of 'fast' arrivals that the busy period length of the bus converges weakly, under appropriate normalization, to an exponentially distributed random variable. As a consequence, main steady-state performance measures such as system throughput, mean delay time, expected waiting time, and mean number of active processors can be approximately determined. The reliability of the proposed method is validated by comparing the new approximations with known exact results.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.99194","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=99194","","Multiprocessing systems;Queueing analysis;Performance analysis;Stochastic systems;Fluctuations;Random variables;Steady-state;Time measurement;Throughput;Delay systems","multiprocessing systems;performance evaluation;queueing theory;stochastic processes","asymptotic queuing theoretic approach;FCFS;heterogeneous multiprocessor computer system;randomly changing environment;stochastic times;random environment;service rates;busy period length;bus;exponentially distributed random variable;steady-state performance measures;system throughput;mean delay time;expected waiting time;reliability","","7","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Adaptive load sharing in homogeneous distributed systems","D. L. Eager; E. D. Lazowska; J. Zahorjan","Department of Computational Science, University of Saskatchewan, Sask. S7N 0W0, Canada; Department of Computer Science, University of Washington, Seattle, WA 98195; Department of Computer Science, University of Washington, Seattle, WA 98195","IEEE Transactions on Software Engineering","","1986","SE-12","5","662","675","Rather than proposing a specific load sharing policy for implementation, the authors address the more fundamental question of the appropriate level of complexity for load sharing policies. It is shown that extremely simple adaptive load sharing policies, which collect very small amounts of system state information and which use this information in very simple ways, yield dramatic performance improvements. These policies in fact yield performance close to that expected from more complex policies whose viability is questionable. It is concluded that simple policies offer the greatest promise in practice, because of their combination of nearly optimal performance and inherent stability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312961","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312961","Design;load sharing;local area networks;performance;queueing models;threshold policies","Load modeling;Adaptation models;Analytical models;Program processors;Time factors;Probes;Adaptive systems","distributed processing","load sharing;homogeneous distributed systems;system state information;performance improvements;optimal performance;inherent stability","","307","","","","","","","","IEEE","IEEE Journals & Magazines"
"Complete and Interpretable Conformance Checking of Business Processes","L. García-Bañuelos; N. R. T. P. van Beest; M. Dumas; M. L. Rosa; W. Mertens","University of Tartu, Tartu, Estonia; Data61, CSIRO, Brisbane, QLD, Australia; University of Tartu, Tartu, Estonia; Queensland University of Technology, Brisbane, QLD, Australia; Queensland University of Technology, Brisbane, QLD, Australia","IEEE Transactions on Software Engineering","","2018","44","3","262","290","This article presents a method for checking the conformance between an event log capturing the actual execution of a business process, and a model capturing its expected or normative execution. Given a process model and an event log, the method returns a set of statements in natural language describing the behavior allowed by the model but not observed in the log and vice versa. The method relies on a unified representation of process models and event logs based on a well-known model of concurrency, namely event structures. Specifically, the problem of conformance checking is approached by converting the event log into an event structure, converting the process model into another event structure, and aligning the two event structures via an error-correcting synchronized product. Each difference detected in the synchronized product is then verbalized as a natural language statement. An empirical evaluation shows that the proposed method can handle real datasets and produces more concise and higher-level difference descriptions than state-of-the-art conformance checking methods. In a survey designed according to the technology acceptance model, practitioners showed a preference towards the proposed method with respect to a state-of-the-art baseline.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2668418","Australian Research Council Discovery; Estonian Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7852436","Process mining;conformance checking;process model;event log;event structure","Business;Synchronization;Computational modeling;Data mining;Natural languages;Software systems;Context modeling","business data processing;data mining;natural languages","event log;process model;event structure;conformance checking;technology acceptance model;business process expected execution;business process normative execution;concurrency model;error-correcting synchronized product;natural language statement","","1","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Numerical operations on a relational database","S. P. Ghosh","IBM Almaden Res. Center, San Jose, CA, USA","IEEE Transactions on Software Engineering","","1989","15","5","600","610","The problem is discussed of defining numerical operations on a relational database to accommodate the statistical analyses associated with a bivariate frequency table. An attempt has been made to extend Codd's relational algebra to include simple bivariate statistical operations preserving the closure of Codd's algebra. This extended algebra is applied to bivariate relational data structures needed for real-time automatic statistical quality control of a manufacturing process. Also discussed are some new category-numeric operations on relational tables.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24709","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24709","","Relational databases;Algebra;Statistical analysis;Frequency;Data structures;Quality control;Set theory;Geometry;Database languages","database theory;relational databases;statistical analysis","Codd;relational database;numerical operations;statistical analyses;bivariate frequency table;relational algebra;bivariate statistical operations;bivariate relational data structures;real-time automatic statistical quality control;manufacturing process;category-numeric operations;relational tables","","3","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Applying Formal Specification to Software Development in Industry","I. J. Hayes","Programming Research Group, Computing Laboratory, Oxford University","IEEE Transactions on Software Engineering","","1985","SE-11","2","169","178","This paper reports experience gained in applying formal specification techniques to an existing transaction processing system. The system is the IBM Customer Information Control System (CICS) and the work has concentrated on specifying a number of modules of the CICS application programmer's interface.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232191","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701984","CICS;formal specification;large scale software","Formal specifications;Computer industry;Control systems;Application software;Electrical equipment industry;Programming profession;Large-scale systems;Communication system control;Transaction databases;Operating systems","","CICS;formal specification;large scale software","","28","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating testing methods by delivered reliability [software]","P. G. Frankl; R. G. Hamlet; B. Littlewood; L. Strigini","CIS Dept., Polytech.. Univ., Brooklyn, NY, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","8","586","601","There are two main goals in testing software: (1) to achieve adequate quality (debug testing), where the objective is to probe the software for defects so that these can be removed, and (2) to assess existing quality (operational testing), where the objective is to gain confidence that the software is reliable. Debug methods tend to ignore random selection of test data from an operational profile, while for operational methods this selection is all-important. Debug methods are thought to be good at uncovering defects so that these can be repaired, but having done so they do not provide a technically defensible assessment of the reliability that results. On the other hand, operational methods provide accurate assessment, but may not be as useful for achieving reliability. This paper examines the relationship between the two testing goals, using a probabilistic analysis. We define simple models of programs and their testing, and try to answer the question of how to attain program reliability: is it better to test by probing for defects as in debug testing, or to assess reliability directly as in operational testing? Testing methods are compared in a model where program failures are detected and the software changed to eliminate them. The ""better"" method delivers higher reliability after all test failures have been eliminated. Special cases are exhibited in which each kind of testing is superior. An analysis of the distribution of the delivered reliability indicates that even simple models have unusual statistical properties, suggesting caution in interpreting theoretical comparisons.","0098-5589;1939-3520;2326-3881","","10.1109/32.707695","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=707695","","Software testing;Software debugging;System testing;Computer bugs;Software quality;Reliability theory;Probes;Costs;Battery powered vehicles;Accidents","program testing;software reliability;program debugging;software quality;probability","software testing method evaluation;delivered software reliability;software quality;debug testing;software defects;operational testing;random test data selection;accurate assessment;probabilistic analysis;program failure detection;software modification;statistical properties;statistical testing theory","","47","","26","","","","","","IEEE","IEEE Journals & Magazines"
"How Effectively Does Metamorphic Testing Alleviate the Oracle Problem?","H. Liu; F. Kuo; D. Towey; T. Y. Chen","Australia-India Centre for Automation Software Engineering, RMIT University, Melbourne 3001 VIC, Australia; Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn 3122 VIC, Australia; Division of Computer Science, The University of Nottingham, Ningbo, China; Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn 3122 VIC, Australia","IEEE Transactions on Software Engineering","","2014","40","1","4","22","In software testing, something which can verify the correctness of test case execution results is called an oracle. The oracle problem occurs when either an oracle does not exist, or exists but is too expensive to be used. Metamorphic testing is a testing approach which uses metamorphic relations, properties of the software under test represented in the form of relations among inputs and outputs of multiple executions, to help verify the correctness of a program. This paper presents new empirical evidence to support this approach, which has been used to alleviate the oracle problem in various applications and to enhance several software analysis and testing techniques. It has been observed that identification of a sufficient number of appropriate metamorphic relations for testing, even by inexperienced testers, was possible with a very small amount of training. Furthermore, the cost-effectiveness of the approach could be enhanced through the use of more diverse metamorphic relations. The empirical studies presented in this paper clearly show that a small number of diverse metamorphic relations, even those identified in an ad hoc manner, had a similar fault-detection capability to a test oracle, and could thus effectively help alleviate the oracle problem.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6613484","Software testing;test oracle;oracle problem;metamorphic testing;metamorphic relation","Computer crashes;Software;Educational institutions;Software testing;Training;Benchmark testing","program diagnostics;program testing;software fault tolerance;software quality","metamorphic testing;oracle problem;software testing;metamorphic relations;software properties;program correctness;software analysis;fault-detection capability","","52","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient online schedulability tests for real-time systems","Tei-Wei Kuo; Li-Pin Chang; Yu-Hua Liu; Kwei-Jay Lin","Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan; NA; NA","IEEE Transactions on Software Engineering","","2003","29","8","734","751","Many computer systems, such as those for open system environments or multimedia services, need an efficient schedulability test for online admission control of new jobs. Although various polynomial time schedulability tests have been proposed, they often fail to decide the schedulability of the system precisely when the system is heavily loaded. On the other hand, most precise schedulability tests proposed to date have a high complexity and may not be suitable for online tests. We present new efficient online schedulability tests for both the periodic process model [C. L. Liu et al., (1973)] and the multiframe process model [A. K. Mok et al., (1997)] in uniprocessor environments. The schedulability tests are shown to be more precise and efficient than any existing polynomial-time schedulability tests. Moreover, the tests can be done incrementally as each new task arrives at the system. Our proposed tests can also be used for the multiframe model where a task may have different computation times in different periods. We show the performance of the proposed schedulability tests in several simulation experiments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223647","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223647","","System testing;Real time systems;Processor scheduling;Admission control;Open systems;Polynomials;Multimedia systems;Switches;Timing","scheduling;resource allocation;real-time systems;online operation;open systems;computational complexity;set theory;directed graphs","online schedulability test;real-time system;open system environment;online job admission control;polynomial time schedulability test;periodic process model;multiframe process model;uniprocessor environment;MPEG stream;division graph;directed graph;reduced set;time complexity","","16","","35","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient distributed protocol for finding shortest paths in networks with negative weights","K. B. Lakshmanan; K. Thulasiraman; M. A. Comeau","Dept. of Comput. Sci., Concordia Univ., Montreal, Que., Canada; Dept. of Comput. Sci., Concordia Univ., Montreal, Que., Canada; NA","IEEE Transactions on Software Engineering","","1989","15","5","639","644","The design is discussed of distributed algorithms for the single-source shortest-path problem to run on an asynchronous directed network in which some of the edges may be associated with negative weights, and thus in which a cycle of negative total weight may also exist. The only existing solution in the literature for this problem is due to K.M. Chandy and J. Misra (1982), and it has, in the worst case, an unbounded message complexity. A synchronous version of the Chandy-Misra algorithm is described and studied, and it is proved that for a network with m edges and n nodes, the worst case message and time complexities of this algorithm are O(mn) and O(n), respectively. This algorithm is then combined with an efficient synchronizer to yield an asynchronous protocol that retains the same message and time complexities.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24713","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24713","","Protocols;Intelligent networks;Distributed algorithms;Algorithm design and analysis;Computer networks;Councils;Computer science;Distributed computing","computational complexity;directed graphs;distributed processing;protocols","efficient distributed protocol;negative weights;distributed algorithms;single-source shortest-path problem;asynchronous directed network;edges;cycle;worst case;unbounded message complexity;synchronous version;Chandy-Misra algorithm;nodes;time complexities;efficient synchronizer;asynchronous protocol","","10","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Axiomatizing software test data adequacy","E. J. Weyuker","Department of Computer Science, Courant Institute of Mathematical Sciences, New York University, New York, NY 10012","IEEE Transactions on Software Engineering","","1986","SE-12","12","1128","1138","A test data adequacy criterion is a set of rules used to determine whether or not sufficient testing has been performed. A general axiomatic theory of test data adequacy is developed, and five previously proposed adequacy criteria are examined to see which of the axioms are satisfied. It is shown that the axioms are consistent, but that only two of the criteria satisfy all of the axioms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313008","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313008","Software testing;test data adequacy","Testing;Vectors;Syntactics;Shape;Positron emission tomography;Semantics;Software","program testing;programming theory","software test data;test data adequacy;axiomatic theory","","49","","","","","","","","IEEE","IEEE Journals & Magazines"
"Loupe: Verifying Publish-Subscribe Architectures with a Magnifying Lens","L. Baresi; C. Ghezzi; L. Mottola","Politecnico di Milano, Milano; Politecnico di Milano, Milano; Politecnico di Milano, Milano","IEEE Transactions on Software Engineering","","2011","37","2","228","246","The Publish-Subscribe (P/S) communication paradigm fosters high decoupling among distributed components. This facilitates the design of dynamic applications, but also impacts negatively on their verification, making it difficult to reason on the overall federation of components. In addition, existing P/S infrastructures offer radically different features to the applications, e.g., in terms of message reliability. This further complicates the verification as its outcome depends on the specific guarantees provided by the underlying P/S system. Although model checking has been proposed as a tool for the verification of P/S architectures, existing solutions overlook many characteristics of the underlying communication infrastructure to avoid state explosion problems. To overcome these limitations, the Loupe domain-specific model checker adopts a different approach. The P/S infrastructure is not modeled on top of a general-purpose model checker. Instead, it is embedded within the checking engine, and the traditional P/S operations become part of the modeling language. In this paper, we describe Loupe's design and the dedicated state abstractions that enable accurate verification without incurring state explosion problems. We also illustrate our use of state-of-the-art software verification tools to assess some key functionality in Loupe's current implementation. A complete case study shows how Loupe eases the verification of P/S architectures. Finally, we quantitatively compare Loupe's performance against alternative approaches. The results indicate that Loupe is effective and efficient in enabling accurate verification of P/S architectures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5432228","Publish-subscribe;verification;model-checking.","Publish-subscribe;Lenses;Application software;Explosions;Computer architecture;Context;Engines;Software tools;Software systems;Business communication","formal verification;message passing;middleware","publish-subscribe architectures;lens magnification;publish-subscribe communication paradigm;distributed component;P-S infrastructures;message reliability;P-S architectures verification;communication infrastructure;state explosion problem;Loupe domain-specific model checker;general purpose model checker;P-S operation;modeling language;Loupe design;dedicated state abstraction;state-of-the-art software verification tool","","7","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Mawl: a domain-specific language for form-based services","D. L. Atkins; T. Ball; G. Bruns; K. Cox","Lucent Technol., Bell Labs., Naperville, IL, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1999","25","3","334","346","A form-based service is one in which the flow of data between service and user is described by a sequence of query/response interactions, or forms. Mawl is a domain-specific language for programming form-based services in a device-independent manner. We focus on Mawl's form abstraction, which is the means for separating service logic from user interface description, and show how this simple abstraction addresses seven issues in service creation, analysis, and maintenance: compile-time guarantees, implementation flexibility, rapid prototyping, testing and validation, support for multiple devices, composition of services, and usage analysis.","0098-5589;1939-3520;2326-3881","","10.1109/32.798323","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=798323","","Domain specific languages;DSL;Web services;Telephony;User interfaces;HTML;Software engineering;Logic testing;Computer languages;Computer Society","high level languages;software prototyping;program testing;software maintenance;program verification;distributed programming;information resources","Mawl;domain-specific language;form-based services;data flow;query/response interaction sequence;device-independent programming;form abstraction;service logic;user interface description;service creation;service analysis;service maintenance;compile-time guarantees;implementation flexibility;rapid prototyping;testing;validation;multiple device support;usage analysis","","30","","23","","","","","","IEEE","IEEE Journals & Magazines"
"A progression model of software engineering goals, challenges, and practices in start-ups","E. Klotins; M. Unterkalmsteiner; P. Chatzipetrou; T. Gorschek; R. Prikladniki; N. Tripathi; L. Pompermaier","DIPT, Blekinge Tekniska Hogskola, 4206 Karlskrona, Blekinge Sweden 371 79 (e-mail: eriks.klotins@bth.se); School of Computing, Blekinge Institute of Technology, Karlskrona, Blekinge Sweden 37179 (e-mail: mun@bth.se); Software Engineering Research Lab, Blekinge Institute of Technology, Karlskrona, Blekinge Sweden (e-mail: panagiota.chatzipetrou@bth.se); Software Engineering, Blekinge Institue of Technology, Karlskrona, Blekinge Sweden (e-mail: tgo@bth.se); Software Engineering, PUCRS, Porto Alegre, Rio Grande do Sul Brazil (e-mail: rafael.prikladnicki@pucrs.br); m3s research unit, Oulun Yliopisto Luonnontieteellinen Tiedekunta, 101226 Oulu, Oulu Finland 90014 (e-mail: nirnaya.tripathi@oulu.fi); Software Engineering, PUCRS, Porto Alegre, Rio Grande do Sul Brazil (e-mail: leandro.pompermaier@pucrs.br)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Context: Software start-ups are emerging as suppliers of innovation and software-intensive products. However, traditional software engineering practices are not evaluated in the context, nor adopted to goals and challenges of start-ups. As a result, there is insufficient support for software engineering in the start-up context.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2900213","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8643804","software start-up;software engineering practices;progression model","Software;Software engineering;Companies;Market opportunities;Requirements engineering;Analytical models","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Heterogeneous Data Translations Based on Environment Grammars","M. Ruschitzka","Division of Computer Science, Department of Electrical Engineering and Computer Science, University of California. Davis, CA 95616.","IEEE Transactions on Software Engineering","","1989","15","10","1236","1251","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1989.559774","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=559774","","Software systems;Database systems;Hardware;Relational databases;Computer science;Software maintenance;Mars;Prototypes;Communications technology;Workstations","","design Methodology;environment grammars;heterogenous computer systems;parsing;proprietary data formats;relational database management systems;syntax-driven data translation","","4","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical analysis of safety-critical anomalies during operations","R. R. Lutz; I. C. Mikulski","Dept. of Comput. Sci., Iowa State Univ., Ames, IA, USA; NA","IEEE Transactions on Software Engineering","","2004","30","3","172","180","Analysis of anomalies that occur during operations is an important means of improving the quality of current and future software. Although the benefits of anomaly analysis of operational software are widely recognized, there has been relatively little research on anomaly analysis of safety-critical systems. In particular, patterns of software anomaly data for operational, safety-critical systems are not well understood. We present the results of a pilot study using orthogonal defect classification (ODC) to analyze nearly two hundred such anomalies on seven spacecraft systems. These data show several unexpected classification patterns such as the causal role of difficulties accessing or delivering data, of hardware degradation, and of rare events. The anomalies often revealed latent software requirements that were essential for robust, correct operation of the system. The anomalies also caused changes to documentation and to operational procedures to prevent the same anomalous situations from recurring. Feedback from operational anomaly reports helped measure the accuracy of assumptions about operational profiles, identified unexpected dependencies among embedded software and their systems and environment, and indicated needed improvements to the software, the development process, and the operational procedures. The results indicate that, for long-lived, critical systems, analysis of the most severe anomalies can be a useful mechanism both for maintaining safer, deployed systems and for building safer, similar systems in the future.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271171","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271171","","Software safety;Embedded software;Software quality;Space vehicles;Hardware;Degradation;Robustness;Documentation;Feedback;Software measurement","program diagnostics;software maintenance;software quality;safety-critical software;software metrics;formal specification;formal verification","anomaly analysis;safety-critical system;software pattern;software requirement;program diagnostics;software maintenance","","40","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Robustness testing of Java server applications","C. Fu; A. Milanova; B. G. Ryder; D. G. Wonnacott","Dept. of Comput. Sci., Rutgers Univ., Piscataway, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","4","292","311","This paper presents a new compile-time analysis that enables a testing methodology for white-box coverage testing of error recovery code (i.e., exception handlers) of server applications written in Java, using compiler-directed fault injection. The analysis allows compiler-generated instrumentation to guide the fault injection and to record the recovery code exercised. (An injected fault is experienced as a Java exception.) The analysis 1) identifies the exception-flow ""def-uses"" to be tested in this manner, 2) determines the kind of fault to be requested at a program point, and 3) finds appropriate locations for code instrumentation. The analysis incorporates refinements that establish sufficient context sensitivity to ensure relatively precise def-use links and to eliminate some spurious def-uses due to demonstrably infeasible control flow. A runtime test harness calculates test coverage of these links using an exception def-catch metric. Experiments with the methodology demonstrate the utility of the increased precision in obtaining good test coverage on a set of moderately sized server benchmarks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435351","Index Terms- Reliability;def-use testing;Java;exceptions;test coverage metrics.","Robustness;Java;Application software;Vehicle crash testing;System testing;Computer Society;Computer crashes;Instruments;Web server;Telephony","Java;program testing;error handling;program compilers;software fault tolerance;data flow analysis;file servers;program control structures","Java server applications testing;compile-time analysis;white-box coverage testing;error recovery code;exception handlers;compiler-directed fault injection;compiler-generated instrumentation;software reliability","","26","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Superviews: Virtual Integration of Multiple Databases","A. Motro","Department of Computer Science, University of Southern California","IEEE Transactions on Software Engineering","","1987","SE-13","7","785","798","An important advantage of a database system is that it provides each application with a custom view of the data. The issue addressed in this paper is how to provide such custom views to applications that access multiple databases. The paper describes a formal method that generates such superviews, in an interactive process of schema editing operations. A mapping of the superview into the individual databases is derived from the editing process, and is stored together with the superview as a virtual database. When this database is interrogated, the mapping is used to decompose each query into a set of queries against the individual databases, and recompose the answers to form an answer to the original query. As this process is transparent to the user, virtual databases may be regarded as a more general type of databases. A prototype database system, that allows users to construct virtual databases and interrogate them, has been developed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233490","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702290","Database;database integration;database view;multidatabase environment;query mapping;superview;virtual database","Relational databases;Database systems;Transaction databases;Virtual prototyping;Application software;Maintenance engineering;Computer science","","Database;database integration;database view;multidatabase environment;query mapping;superview;virtual database","","43","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Specification and validation of a security policy model","A. Boswell","Logica Cambridge Ltd., UK","IEEE Transactions on Software Engineering","","1995","21","2","63","68","The paper describes the development of a formal security policy model in Z for the NATO Air Command and Control System (ACCS): a large, distributed, multilevel-secure system. The model was subject to manual validation, and some of the issues and lessons in both writing and validating the model are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.345822","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=345822","","Information security;Access control;Computer security;Communication system security;Command and control systems;Writing;Certification;Costs;Production;Cryptography","command and control systems;aerospace control;aircraft computers;specification languages;formal specification;program verification;security of data","specification;validation;security policy model;formal security policy model;Z;NATO Air Command and Control System;multilevel-secure syste;manual validation","","11","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Completeness of Proof Systems for Equational Specifications","D. B. MacQueen; D. T. Sannella","AT&amp;amp;T Bell Laboratories; NA","IEEE Transactions on Software Engineering","","1985","SE-11","5","454","461","Contrary to popular belief, equational logic with induction is not complete for initial models of equational specifications. Indeed, under some regimes (the Clear specification language and most other algebraic specification languages) no proof system exists which is complete even with respect to ground equations. A collection of known results is presented along with some new observations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232484","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702035","Algebraic specifications;equational logic;proof systems","Equations;Algebra;Specification languages;Councils;Computer science;Automatic logic units;Logic functions;Artificial intelligence","","Algebraic specifications;equational logic;proof systems","","","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient branch-and-bound algorithms on a two-level memory system","C. -. Yu; B. W. Wah","Intel Corp., Santa Clara, CA, USA; NA","IEEE Transactions on Software Engineering","","1988","14","9","1342","1356","Branch-and-bound algorithms in a system with a two-level memory hierarchy were evaluated. An efficient implementation depends on the disparities in the numbers of subproblems expanded between the depth-first and best-first searches as well as the relative speeds of the main and secondary memories. A best-first search should be used when it expands a much smaller number of subproblems than that of a depth-first search, and the secondary memory is relatively slow. In contrast, a depth-first search should be used when the number of expanded subproblems is close to that of a best-first search. The choice is not as clear for cases in between these cases are studied. Two strategies are proposed and analyzed: a specialized virtual-memory system that matches the architectural design with the characteristics of the existing algorithm, and a modified branch-and-bound algorithm that can be tuned to the characteristic of the problem and the architecture. The latter strategy illustrates that designing a better algorithm is sometimes more effective that tuning the architecture alone.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6177","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6177","","Partitioning algorithms;Algorithm design and analysis;Iterative algorithms;Cost function;Guidelines;Artificial intelligence;Operations research;Search problems;Expert systems","storage allocation;storage management;virtual storage","branch-and-bound algorithms;two-level memory system;best-first search;depth-first search;virtual-memory","","9","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Visualizing Design Patterns in Their Applications and Compositions","J. Dong; S. Yang; K. Zhang","Department of Computer Science, University of Texas at Dallas, 2601 North Floyd Road, Richardson, TX 75083; Department of Computer Science, University of Texas at Dallas, 2601 North Floyd Road, Richardson, TX 75083; Department of Computer Science, University of Texas at Dallas, 2601 North Floyd Road, Richardson, TX 75083","IEEE Transactions on Software Engineering","","2007","33","7","433","453","Design patterns are generic design solutions that can be applied and composed in different applications where pattern-related information is generally implicit in the Unified Modeling Language (UML) diagrams of the applications. It is unclear in which pattern instances each modeling element, such as class, attribute, and operation, participates. It is hard for a designer to find the design patterns used in an application design. Consequently, the benefits of design patterns are compromised because designers cannot communicate with each other in terms of the design patterns they used and their design decisions and trade-offs. In this paper, we present a UML profile that defines new stereotypes, tagged values, and constraints for tracing design patterns in UML diagrams. These new stereotypes and tagged values are attached to a modeling element to explicitly represent the role the modeling element plays in a design pattern so that the user can identify the pattern in a UML diagram. Based on this profile, we also develop a Web service (tool) for explicitly visualizing design patterns in UML diagrams. With this service, users are able to visualize design patterns in their applications and compositions because pattern-related information can be dynamically displayed. A real-world case study and a comparative experiment with existing approaches are conducted to evaluate our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1012","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4227827","Design pattern;UML;model-driven architecture;Web service;visual tool.","Visualization;Unified modeling language;Object oriented modeling;Software systems;Application software;Software design;Web services;Service oriented architecture;Natural languages;Production facilities","software engineering;Unified Modeling Language;Web services","design patterns;Unified Modeling Language diagrams;UML profile;Web service","","34","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Support for reusability in Genesis","C. V. Ramamoorthy; V. Garg; A. Prakash","Div. of Comput. Sci., California Univ., Berkeley, CA, USA; Div. of Comput. Sci., California Univ., Berkeley, CA, USA; Div. of Comput. Sci., California Univ., Berkeley, CA, USA","IEEE Transactions on Software Engineering","","1988","14","8","1145","1154","Genesis is a software-engineering-based programming environment geared to support big software projects. The authors first discuss a reusability-driven development methodology that advocates software development based on reusability considerations. Then, they discuss the tools and techniques provided in Genesis to support this methodology. Techniques are suggested for improving the retrievability, composability, and understandability of software resources. Retrievability is improved by use of ESL (entity specification language) for tying resources through attributes and relations. Composability is improved through a mechanism called functional composition that provides considerably more generality than Unix pipes for composing programs. Understandability is improved by the use of program abstractors.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.7625","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7625","","Programming profession;Computer bugs;Specification languages;Software reusability;Writing;Programming environments;Productivity;Software systems;Computer interfaces;Software design","database management systems;file organisation;programming environments;query languages;software tools;specification languages","software reusability;software reuse;software tools;file organisation;databases;query languages;Genesis;programming environment;reusability-driven development methodology;software development;software resources;ESL;entity specification language;functional composition;program abstractors","","21","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Program Characterization Using Runtime Values and Its Application to Software Plagiarism Detection","Y. Jhi; X. Jia; X. Wang; S. Zhu; P. Liu; D. Wu","Samsung SDS R&D Center, Seoul, Korea; State Key Laboratory of Information Security, Institute of Information Engineering, Beijing, Haidian District, China; Shape Security, Mountain View, CA; Department of Computer Science and Engineering, Pennsylvania State University, University Park, PA; College of Information Sciences and Technology, Pennsylvania State University, University Park, PA; College of Information Sciences and Technology, Pennsylvania State University, University Park, PA","IEEE Transactions on Software Engineering","","2015","41","9","925","943","Illegal code reuse has become a serious threat to the software community. Identifying similar or identical code fragments becomes much more challenging in code theft cases where plagiarizers can use various automated code transformation or obfuscation techniques to hide stolen code from being detected. Previous works in this field are largely limited in that (i) most of them cannot handle advanced obfuscation techniques, and (ii) the methods based on source code analysis are not practical since the source code of suspicious programs typically cannot be obtained until strong evidences have been collected. Based on the observation that some critical runtime values of a program are hard to be replaced or eliminated by semantics-preserving transformation techniques, we introduce a novel approach to dynamic characterization of executable programs. Leveraging such invariant values, our technique is resilient to various control and data obfuscation techniques. We show how the values can be extracted and refined to expose the critical values and how we can apply this runtime property to help solve problems in software plagiarism detection. We have implemented a prototype with a dynamic taint analyzer atop a generic processor emulator. Our value-based plagiarism detection method (VaPD) uses the longest common subsequence based similarity measuring algorithms to check whether two code fragments belong to the same lineage. We evaluate our proposed method through a set of real-world automated obfuscators. Our experimental results show that the value-based method successfully discriminates 34 plagiarisms obfuscated by SandMark, plagiarisms heavily obfuscated by KlassMaster, programs obfuscated by Thicket, and executables obfuscated by Loco/Diablo.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2418777","US National Science Foundation (NSF); AFRL; National Natural Science Foundation of China (NSFC); National High-tech R&D Program of China; Strategic Priority Research Program of the Chinese Academy of Sciences; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7076635","Software plagiarism detection;dynamic code identification.;Software plagiarism detection;dynamic code identification","Plagiarism;Runtime;Optimization;Program processors;Semantics;Java","security of data;software engineering;source code (software)","program characterization;runtime values;software plagiarism detection;illegal code reuse;software community;identical code fragments;obfuscation techniques;source code;semantics-preserving transformation techniques;generic processor emulator;value-based plagiarism detection method;VaPD;SandMark;Thicket;Loco/Diablo","","10","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Adding roles to CORBA objects","C. Canal; L. Fuentes; E. Pimentel; J. M. Troya; A. Vallecillo","Dept. de Lenguajes y Ciencias de la Comput., Malaga Univ., Spain; Dept. de Lenguajes y Ciencias de la Comput., Malaga Univ., Spain; Dept. de Lenguajes y Ciencias de la Comput., Malaga Univ., Spain; Dept. de Lenguajes y Ciencias de la Comput., Malaga Univ., Spain; Dept. de Lenguajes y Ciencias de la Comput., Malaga Univ., Spain","IEEE Transactions on Software Engineering","","2003","29","3","242","260","Traditional IDLs were defined for describing the services that objects offer, but not those services they require from other objects, nor the relative order in which they expect their methods to be called. Some of the existing proposals try to add protocol information to object interfaces, but most of them fail to do so in a modular way. In this paper we propose an extension of the CORBA IDL that uses a sugared subset of the polyadic /spl pi/-calculus for describing object service protocols, based on the concept of roles. Roles allow the modular specification of the observable behavior of CORBA objects, reducing the complexity of the compatibility tests. Our main aim is the automated checking of protocol interoperability between CORBA objects in open component-based environments, using similar techniques to those used in software architecture description and analysis. In addition, our proposal permits the study of substitutability between CORBA objects, as well as the realization of dynamic compatibility tests during their runtime execution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183935","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183935","","Application software;Computer architecture;Protocols;Software architecture;Programming;Proposals;Testing;Software reusability;Irrigation;Runtime","distributed object management;object-oriented methods","CORBA objects;IDLs;protocol interoperability;automated checking;protocols;component-based software development;software components","","28","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Using Local Clocks to Reproduce Concurrency Bugs","Z. Wang; C. Wu; X. Yuan; Z. Wang; J. Li; P. Yew; J. Huang; X. Feng; Y. Lan; Y. Chen; Y. Lai; Y. Guan","Institute of Computing Technology, University of Chinese Academy of Sciences, Huairou, Beijing, P.R. China; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Huairou, Beijing, P.R. China; Huawei Technologies, Huairou, Beijing, P.R. China; Huawei Technologies, Huairou, Beijing, P.R. China; Horizon Robotics, Inc. Huairou, Beijing, P.R. China; Department of Computer Science and Engineering, University of Minnesota at Twin-Cities, Minnesota, MN; Department of Computer Science and Engineering, Texas A&M University, College Station, TX; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Huairou, Beijing, P.R. China; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Huairou, Beijing, P.R. China; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Huairou, Beijing, P.R. China; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Huairou, Beijing, P.R. China; Capital Normal University, Huairou, Beijing, P.R. China","IEEE Transactions on Software Engineering","","2018","44","11","1112","1128","Multi-threaded programs play an increasingly important role in current multi-core environments. Exposing concurrency bugs and debugging such multi-threaded programs are quite challenging due to their inherent non-determinism. In order to mitigate such non-determinism, many approaches such as record-and-replay have been proposed. However, those approaches often suffer significant performance degradation because they require a large amount of recorded information and/or long analysis and replay time. In this paper, we propose an efficient and effective approach, ReCBuLC (reproducing concurrency bugs using local clocks), to take advantage of the hardware clocks available on modern processors. The key idea is to reduce the recording overhead and the time to analyze events’ global order by recording timestamps in each thread. These timestamps are used to determine the global order of shared accesses. To avoid the large overhead in accessing system-wide global clock, we opt to use local per-core clocks that incur much less access overhead. We then propose techniques to resolve skews among local clocks and obtain an accurate global event order. By using per-core clocks, state-of-the-art bug reproducing systems such as PRES and CLAP can reduce their recording overheads by up to 85 percent, and the analysis time up to 84.66%<inline-formula><tex-math notation=""LaTeX"">$\sim$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq1-2752158.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>99.99%, respectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2752158","National High Technology Research and Development Program of China; National Natural Science Foundation of China (NSFC); Innovation Research Group of NSFC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8038023","Concurrency;bug reproducing;local clock","Clocks;Program processors;Computer bugs;Concurrent computing;Hardware;Debugging;Computer architecture","","","","","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Software Development Estimation Biases: The Role of Interdependence","M. Jorgensen; S. Grimstad","University of Oslo, Lysaker; University of Oslo, Lysaker","IEEE Transactions on Software Engineering","","2012","38","3","677","693","Software development effort estimates are frequently too low, which may lead to poor project plans and project failures. One reason for this bias seems to be that the effort estimates produced by software developers are affected by information that has no relevance for the actual use of effort. We attempted to acquire a better understanding of the underlying mechanisms and the robustness of this type of estimation bias. For this purpose, we hired 374 software developers working in outsourcing companies to participate in a set of three experiments. The experiments examined the connection between estimation bias and developer dimensions: self-construal (how one sees oneself), thinking style, nationality, experience, skill, education, sex, and organizational role. We found that estimation bias was present along most of the studied dimensions. The most interesting finding may be that the estimation bias increased significantly with higher levels of interdependence, i.e., with stronger emphasis on connectedness, social context, and relationships. We propose that this connection may be enabled by an activation of one's self-construal when engaging in effort estimation, and a connection between a more interdependent self-construal and increased search for indirect messages, lower ability to ignore irrelevant context, and a stronger emphasis on socially desirable responses.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6193066","Effort estimation;estimation bias;cultural differences;software engineering.","Estimation;Software;Companies;Context;Programming;Outsourcing;Instruments","estimation theory;software management","software development estimation;interdependence role;project failures;project plans;outsourcing companies","","6","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental scanning and parsing with galaxy","J. F. Beetem; A. F. Beetem","Dept. of Electr. & Comput. Eng., Wisconsin Univ., Madison, WI, USA; NA","IEEE Transactions on Software Engineering","","1991","17","7","641","651","The algorithms and techniques used in incremental scanning and parsing of the Galaxy language are presented. Incremental compilers, programming environments that feature instantaneous change processing as well as the execution time efficiency of compiled programs and code development using the Galaxy language are discussed. It is shown that the algorithms guarantee minimal rescanning and reparsing are space and time efficient and are easily adapted to any language of equivalent class, including such languages as C and Pascal.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.83901","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=83901","","Productivity;Programming environments;Software engineering;Programming profession;Software tools;Software algorithms;Environmental economics;National electric code;Application software;Sampling methods","high level languages;program compilers;programming environments","incremental scanning;parsing;Galaxy language;programming environments;instantaneous change processing;execution time efficiency;compiled programs;code development;algorithms;minimal rescanning;reparsing;time efficient;C;Pascal","","5","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Miro: visual specification of security","A. Heydon; M. W. Maimone; J. D. Tygar; J. M. Wing; A. M. Zaremski","Sch. of Comput. Sci., Carnegie-Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie-Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie-Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie-Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie-Mellon Univ., Pittsburgh, PA, USA","IEEE Transactions on Software Engineering","","1990","16","10","1185","1197","Miro is a set of languages and tools that support the visual specification of file system security. Two visual languages are presented: the instance language, which allows specification of file system access, and the constraint language, which allows specification of security policies. Miro visual languages and tools are used to specify security configurations. A visual language is one whose entities are graphical, such as boxes and arrows, specifying means stating independently of any implementation the desired properties of a system. Security means file system protection: ensuring that files are protected from unauthorized access and granting privileges to some users, but not others. Tools implemented and examples of how these languages can be applied to real security specification problems are described.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60298","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60298","","Security;File systems;Protection;Contracts;Visualization;Operating systems;Control systems;Data structures;Computer displays","security of data;specification languages;visual programming","Miro;visual specification of security;file system security;instance language;constraint language;tools;boxes;arrows;security specification problems","","17","","23","","","","","","IEEE","IEEE Journals & Magazines"
"The SL synchronous language","F. Boussinot; R. de Simone","CMA, Ecole des Mines de Paris, Valbonne, France; NA","IEEE Transactions on Software Engineering","","1996","22","4","256","266","We present SL, a new programming language of the synchronous reactive family in which hypotheses about signal presence/absence are disallowed. One can decide that a signal is absent during an instant only at the end of this instant, and so reaction to this absence is delayed to the next instant. Sources of causal circularities are avoided, while only weak preemption remains. A structural operational semantics is provided through rewrite rules, and an implementation is described. In addition to directly executing programs, this implementation can also be used to produce automata by symbolic evaluation.","0098-5589;1939-3520;2326-3881","","10.1109/32.491649","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=491649","","Automata;Computer languages;Resumes;TV broadcasting;Delay;Concurrent computing;Program processors;Equations;Radio control;Protocols","computational linguistics;rewriting systems;grammars;specification languages;parallel languages;parallel programming;program compilers","SL synchronous language;programming language;synchronous reactive languages;signal presence;signal absence;weak preemption;structural operational semantics;rewrite rules;direct program execution;automata;symbolic evaluation","","30","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting with sparse data","M. Shepperd; M. Cartwright","Sch. of Design, Eng. & Comput., Bournemouth Univ., Poole, UK; NA","IEEE Transactions on Software Engineering","","2001","27","11","987","998","It is well-known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. The authors describe our sparse data method (SDM) based upon a pairwise comparison technique and T.L. Saaty's (1980) Analytic Hierarchy Process (AHP). Our minimum data requirement is a single known point. The technique is supported by a software tool known as DataSalvage. We show, for data from two companies, how our approach, based upon expert judgement, adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that our approach is robust to pairwise comparison errors. We then describe the results of a small usability trial with a practicing project manager. From this empirical work, we conclude that the technique is promising and may help overcome some of the present barriers to effective project prediction.","0098-5589;1939-3520;2326-3881","","10.1109/32.965339","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=965339","","Costs;Programming;Software engineering;Machine learning;Calibration;Software tools;Sensitivity analysis;Robustness;Usability;Project management","software cost estimation;software reliability;data analysis;software tools","sparse data;project cost related factor prediction;software engineering;systematic historic data;sparse data method;SDM;pairwise comparison technique;Analytic Hierarchy Process;AHP;minimum data requirement;single known point;software tool;DataSalvage;expert judgement;sensitivity analysis;pairwise comparison errors;usability trial;practicing project manager;project prediction;software project effort","","49","","32","","","","","","IEEE","IEEE Journals & Magazines"
"A tool for analyzing and fine tuning the real-time properties of an embedded system","D. B. Stewart; G. Arora","EVP/CTO of Embedded Res. Solutions, Annapolis, MD, USA; NA","IEEE Transactions on Software Engineering","","2003","29","4","311","326","This paper describes a computer-aided software engineering (CASE) tool that helps designers analyze and fine-tune the timing properties of their embedded real-time software. Existing CASE tools focus on the software specification and design of embedded systems. However, they provide little, if any, support after the software has been implemented. Even if the developer used a CASE tool to design the system, their system most likely does not meet the specifications on the first try. This paper includes guidelines for implementing analyzable code, profiling a real-time system, filtering and extracting measured data, analyzing the data, and interactively predicting the effect of changes to the real-time system. The tool is a necessary first step towards automating the debugging and fine tuning of an embedded system's temporal properties.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1191796","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1191796","","Real time systems;Embedded system;Computer aided software engineering;Embedded software;Software tools;Timing;Software design;Guidelines;Filtering;Data mining","computer aided software engineering;real-time systems;embedded systems;interactive programming","embedded system real-time properties analysis tool;embedded system real-time properties fine tuning tool;computer-aided software engineering tool;CASE tool;embedded real-time software;analyzable code;real-time system profiling;data filtering;data extraction;data analysis;debugging","","14","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic symbolic verification of embedded systems","R. Alur; T. A. Henzinger; Pei-Hsin Ho","Comput. Syst. Res. Center, AT&T Bell Labs., Murray Hill, NJ, USA; NA; NA","IEEE Transactions on Software Engineering","","1996","22","3","181","201","Presents a model-checking procedure and its implementation for the automatic verification of embedded systems. The system components are described as hybrid automata-communicating machines with finite control and real-valued variables that represent continuous environment parameters such as time, pressure and temperature. The system requirements are specified in a temporal logic with stop-watches, and verified by symbolic fixpoint computation. The verification procedure-implemented in the Cornell Hybrid Technology tool, HyTech-applies to hybrid automata whose continuous dynamics is governed by linear constraints on the variables and their derivatives. We illustrate the method and the tool by checking safety, liveness, time-bounded and duration requirements of digital controllers, schedulers and distributed algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/32.489079","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=489079","","Embedded system;Automatic control;Pressure control;Temperature control;Control systems;Logic;Automata;Safety;Digital control;Distributed control","real-time systems;formal verification;temporal logic;finite state machines;software tools","automatic symbolic verification;real-time systems;embedded systems;model-checking procedure;hybrid automata;communicating machines;finite control;real-valued variables;continuous environment parameters;temporal logic;stop-watches;symbolic fixpoint computation;hybrid technology tool;HyTech;continuous dynamics;linear constraints;variable derivatives;safety checking;liveness;time-bounded requirements;duration requirements;digital controllers;schedulers;distributed algorithms","","253","","","","","","","","IEEE","IEEE Journals & Magazines"
"Apportioning: a technique for efficient reachability analysis of concurrent object-oriented programs","S. Iyer; S. Ramesh","Sch. of Inf. Technol., Indian Inst. of Technol., Mumbai, India; NA","IEEE Transactions on Software Engineering","","2001","27","11","1037","1056","The object-oriented paradigm in software engineering provides support for the construction of modular and reusable program components and is attractive for the design of large and complex distributed systems. Reachability analysis is an important and well-known tool for static analysis of critical properties in concurrent programs, such as deadlock freedom. It involves the systematic enumeration of all possible global states of program execution and provides the same level of assurance for properties of the synchronization structure in concurrent programs, such as formal verification. However, direct application of traditional reachability analysis to concurrent object-oriented programs has many problems, such as incomplete analysis for reusable classes (not safe) and increased computational complexity (not efficient). We propose a novel technique called apportioning, for safe and efficient reachability analysis of concurrent object-oriented programs, that is based upon a simple but powerful idea of classification of program analysis points as local (having influence within a class) and global (having possible influence outside a class). We have developed a number of apportioning-based algorithms, having different degrees of safety and efficiency. We present the details of one of these algorithms, formally show its safety for an appropriate class of programs, and present experimental results to demonstrate its efficiency for various examples.","0098-5589;1939-3520;2326-3881","","10.1109/32.965343","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=965343","","Reachability analysis;Modular construction;System recovery;Formal verification;Safety;Libraries;Software engineering;Computational complexity;Algorithm design and analysis;Object oriented programming","object-oriented programming;parallel programming;reachability analysis;program diagnostics;software cost estimation","apportioning technique;reachability analysis;concurrent object-oriented programs;object-oriented paradigm;software engineering;modular reusable program components;complex distributed systems;static analysis;concurrent programs;deadlock freedom;global states;synchronization structure;formal verification;direct application;reusable classes;computational complexity;program analysis points;apportioning-based algorithms;experimental results","","5","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Parallel discrete event simulation using shared memory","D. A. Reed; A. D. Malony; B. D. McCredie","Illinois Univ., Urbana, IL, USA; Illinois Univ., Urbana, IL, USA; Illinois Univ., Urbana, IL, USA","IEEE Transactions on Software Engineering","","1988","14","4","541","553","With traditional event-list techniques, evaluating a detailed discrete event simulation-model can often require hours or even days of computation time. By eliminating the event list and maintaining only sufficient synchronization to ensure causality, parallel simulation can potentially provide speedups that are linear in the numbers of processors. A set of shared-memory experiments using the Chandy-Misra distributed simulation algorithm, to simulate networks of queues is presented. Parameters of the study include queueing network topology and routing probabilities, number of processors, and assignment of network nodes to processors. These experiments show that Chandy-Misra distributed simulation is a questionable alternative to sequential simulation of most queuing network models.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4677","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4677","","Discrete event simulation;Computational modeling;Circuit simulation;Analytical models;Computer simulation;Network topology;Parallel processing;Computer networks;Concurrent computing;Routing","digital simulation;parallel processing;performance evaluation;queueing theory","deadlock recovery;parallel processing;performance evaluation;shared memory;discrete event simulation-model;synchronization;parallel simulation;Chandy-Misra distributed simulation algorithm;queueing network topology;routing probabilities","","41","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A Simulation Study of the Vertical-Migration Microprocessor Architecture","V. Milutinovic","School of Electrical Engineering, Purdue University","IEEE Transactions on Software Engineering","","1987","SE-13","12","1265","1277","Vertical-migration microprocessor architecture was introduced in [1], and results of its analytical study were presented in the same paper. This paper presents results of its simulation study which is based on selected production benchmarks. Vertical-migration architecture enables the constructs typical of HLL's to be mapped into the constructs typical of microcode. This mapping is provided only for selected types of HLL statements and for HLL statements with a relatively small number of operands and parameters, i.e., for-the most frequent HLL constructs. Using an extended subset of Fortran 77, one that matches the typical demands of the targeted application, i.e., dedicated microprocessing, it has been shown how the proposed architecture supports the mapping of HLL constructs into microinstructions. That was done through the description of a flexible register-transfer level simulator which was implemented to support this study. It was used to run benchmarks, typical of various applications, on various configurations of the architecture. Simulation study has shown that this approach is particularly suitable for the time-critical dedicated signal processing and robotics/control applications, as well as for the GaAs implementation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232880","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702178","Dedicated microprocessing;microprocessor architecture;vertical-migration architecture","Microprocessors;Computer architecture;Application software;Time factors;Signal processing;Hardware;Virtual manufacturing;Production;Computational modeling;Robots","","Dedicated microprocessing;microprocessor architecture;vertical-migration architecture","","1","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Comments analysis and programming errors","W. E. Howden","Dept. of Comput. Sci. & Eng., California Univ., La Jolla, CA, USA","IEEE Transactions on Software Engineering","","1990","16","1","72","81","Software validation is treated as the problem of detecting errors that programmers make during the software development process. This includes fault detection, in which the focus is on techniques for detecting the occurrence of local errors that result in well-defined classes of program statement faults. It also includes detecting other kinds of errors, such as decomposition errors. The main focus of the work is on a decomposition-error analysis technique called comments analysis. In this technique, errors are detected by analyzing special classes of program comments. Comments analysis has been applied to a variety of systems, including a data-processing program and an avionics real-time program. The use of comments analysis for sequential and concurrent systems is discussed, and the basic features of comments analysis tools are summarized. The relationship of comments analysis to other techniques, such as event sequence analysis, is discussed, and the differences between it and earlier work are explained.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44365","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44365","","Error analysis;Testing;Programming profession;Fault detection;Data processing;Real time systems;Genetic mutations;Aerospace electronics;Arithmetic;Computer science","software engineering","comments analysis;software validation;programming errors;software development process;fault detection;decomposition errors;data-processing program;avionics real-time program;event sequence analysis","","10","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Palantir: Early Detection of Development Conflicts Arising from Parallel Code Changes","A. Sarma; D. F. Redmiles; A. van der Hoek","University of Nebraska- Lincoln, Lincoln; University of California, Irvine, Irvine; University of California, Irvine, Irvine","IEEE Transactions on Software Engineering","","2012","38","4","889","908","The earlier a conflict is detected, the easier it is to resolve-this is the main precept of workspace awareness. Workspace awareness seeks to provide users with information of relevant ongoing parallel changes occurring in private workspaces, thereby enabling the early detection and resolution of potential conflicts. The key approach is to unobtrusively inform developers of potential conflicts arising because of concurrent changes to the same file and dependency violations in ongoing parallel work. This paper describes our research goals, approach, and implementation of workspace awareness through Palantír and includes a comprehensive evaluation involving two laboratory experiments. We present both quantitative and qualitative results from the experiments, which demonstrate that the use of Palantír, as compared to not using Palantír 1) leads to both earlier detection and earlier resolution of a larger number of conflicts, 2) leaves fewer conflicts unresolved in the code base that was ultimately checked in, and 3) involves reasonable overhead. Furthermore, we report on interesting changes in users' behavior, especially how conflict resolution strategies changed among Palantír users.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928359","Software engineering;computer-supported collaborative work;programmer workbench;configuration management","Monitoring;Measurement;Instant messaging;Computer architecture;Databases;Context;Laboratories","configuration management;groupware;parallel processing;software management","Palantír;development conflict early detection;parallel code changes;workspace awareness;dependency violations;laboratory experiments;conflict resolution strategies;software configuration management system;computer-supported collaborative work;software engineering","","25","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Selection of Optimal Model Transformation Chains via Shortest-Path Algorithms","F. Basciani; M. D&#x0027;Emidio; D. Di Ruscio; D. Frigioni; L. Iovino; A. Pierantonio","Computer Science, Universita degli Studi dell&#x0027;Aquila Dipartimento di Ingegneria Industriale e dell&#x0027;Informazione e di Economia, 90345 L&#x0027;Aquila, Abruzzo Italy (e-mail: francesco.basciani@graduate.univaq.it); Computer Science, Gran Sasso Science Institute, 476635 L&#x0027;aquila, L&#x0027;Aquila Italy (e-mail: mattia.demidio@gssi.it); Department of Information Engineering Computer Science and Mathematics, University of L&#x0027;Aquila, L&#x0027;Aquila, AQ Italy 67100 (e-mail: davide.diruscio@univaq.it); Dept. of Electrical and Information Engineering, University of L&#x0027;Aquila, L&#x0027;Aquila, aq Italy (e-mail: daniele.frigioni@univaq.it); Computer Science, Gran Sasso Science Institute, l&#x0027;aquila, aq Italy 67100 (e-mail: ludovico.iovino@gssi.it); Department of Information Engineering Computer Science and Mathematics, University of L&#x0027;Aquila, L&#x0027;Aquila, Abruzzo Italy (e-mail: alfonso.pierantonio@univaq.it)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Conventional wisdom on model transformations in Model-Driven Engineering (MDE) suggests that they are crucial components in modeling environments to achieve superior automation, whether it be refactoring, simulation, or code generation. While their relevance is well-accepted, model transformations are challenging to design, implement, and verify because of the inherent complexity that they must encode. Thus, defining transformations by chaining existing ones is key to success for enhancing their reusability. This paper proposes an approach, based on well-established algorithms, to support modellers when multiple transformation chains are available to bridge a source metamodel with a target one. The all-important goal of selecting the optimal chain has been based on the quality criteria of coverage and information loss. The feasibility of the approach has been demonstrated by means of experiments operated on chains obtained from transformations borrowed from a publicly available repository.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2846223","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8383962","Model-driven engineering;Model Transformation Composition;Graph Algorithms;Shortest Paths","Unified modeling language;Adaptation models;Bridges;Analytical models;Model driven engineering;Ecosystems","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"An Evaluation of Two New Inference Control Methods","Y. H. Chin; Weng-Ling Peng","Institute of Computer and Decision Science, National Tsing Hua University; NA","IEEE Transactions on Software Engineering","","1987","SE-13","12","1329","1339","An evaluation method is developed to measure the cost/ effectiveness of two new inference control methods. The factors of the evaluation function consist of: 1) preparation cost for the control method, 2) query complexity, and 3) security level under various attacks. Each control method combines the merit of some popular concepts; the first method is based on restriction, and the second on perturbation. Simulation results indicate that both methods have higher preparation cost, better security, and faster response time than Cox's method and Beck's method. Finally these two new methods are compared to each other.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233143","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702182","Cell suppression;cost factors;data perturbation;evaluation methods;inference control;partitioning;query-set-size control;random-sample-query control;security;statistical database","Control systems;Data security;Cost function;Delay;Information security;Protection;Hospitals;Relational databases","","Cell suppression;cost factors;data perturbation;evaluation methods;inference control;partitioning;query-set-size control;random-sample-query control;security;statistical database","","","","35","","","","","","IEEE","IEEE Journals & Magazines"
"A new approach to the modeling of recovery block structures","G. Pucci","Comput. Lab., Newcastle-upon-Tyne Univ., UK","IEEE Transactions on Software Engineering","","1992","18","2","159","167","A reliability model is proposed for recovery block structures based on error events which can be observed and distinguished during testing. Strategies are then described for the collection of failure histories needed to estimate the model parameters and obtain dependability predictions. Given that the software goes through different testing stages, the model can be employed at different points of the development cycle to assess or forecast the quality of project choices and the resulting product.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.121757","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=121757","","Redundancy;Fault tolerance;Predictive models;Software testing;Software reliability;History;Software quality;Humans;Organizing;Hardware","program testing;reliability theory;software reliability;system recovery","reliability model;recovery block structures;error events;failure histories;model parameters;dependability predictions;testing stages;development cycle;project choices","","11","","26","","","","","","IEEE","IEEE Journals & Magazines"
"The automatic generation of load test suites and the assessment of the resulting software","A. Avritzer; E. R. Weyuker","AT&T Bell Labs., Red Hill, NJ, USA; NA","IEEE Transactions on Software Engineering","","1995","21","9","705","716","Three automatic test case generation algorithms intended to test the resource allocation mechanisms of telecommunications software systems are introduced. Although these techniques were specifically designed for testing telecommunications software, they can be used to generate test cases for any software system that is modelable by a Markov chain provided operational profile data can either be collected or estimated. These algorithms have been used successfully to perform load testing for several real industrial software systems. Experience generating test suites for five such systems is presented. Early experience with the algorithms indicate that they are highly effective at detecting subtle faults that would have been likely to be missed if load testing had been done in the more traditional way, using hand-crafted test cases. A domain-based reliability measure is applied to systems after the load testing algorithms have been used to generate test data. Data are presented for the same five industrial telecommunications systems in order to track the reliability as a function of the degree of system degradation experienced.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.464549","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=464549","","Automatic testing;System testing;Software testing;Software systems;Software algorithms;Resource management;Performance evaluation;Computer industry;Fault detection;Communication industry","program testing;automatic test software;software reliability;Markov processes;resource allocation;telecommunication computing","automatic test case generation algorithms;load test suites;resource allocation mechanisms;telecommunications software;software testing;Markov chain;load testing;industrial software systems;fault detection;domain-based reliability measure;reliability;system degradation","","84","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Requirements validation through viewpoint resolution","J. C. S. P. Leite; P. A. Freeman","Dept. de Inf., Pontificia Univ. Catolica do Rio de Janeiro, Brazil; NA","IEEE Transactions on Software Engineering","","1991","17","12","1253","1269","A specific technique-viewpoint resolution-is proposed as a means of providing early validation of the requirements for a complex system, and some initial empirical evidence of the effectiveness of a semi-automated implementation of the technique is provided. The technique is based on the fact that software requirements can and should be elicited from different viewpoints, and that examination of the differences resulting from them can be used as a way of assisting in the early validation of requirements. A language for expressing views from different viewpoints and a set of analogy heuristics for performing a syntactically oriented analysis of views are proposed. This analysis of views is capable of differentiating between missing information and conflicting information, thus providing support for viewpoint resolution.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.106986","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=106986","","Biomedical engineering;Systems engineering and theory;Performance analysis;Information analysis;Knowledge engineering;Packaging;Software systems;Vacuum systems;Patient monitoring;Airplanes","software engineering;systems analysis","viewpoint resolution;software requirements;analogy heuristics;syntactically oriented analysis of views","","101","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Internet Locus: Extending transparency to an Internet environment","A. B. Sheltzer; G. J. Popek","Department of Computer Science, University of California, Los Angeles, CA 90024; Department of Computer Science, University of California, Los Angeles, CA 90024; Locus Computing Corporation, Santa Monica, CA","IEEE Transactions on Software Engineering","","1986","SE-12","11","1067","1075","Network transparency refers to the ability of a distributed system to hide machine boundaries from people and application programs; i.e. all resources are accessed in the same manner, independent of their locations. It is demonstrated that transparency across a long-haul network is both highly desirable and technically feasible. A case study of the transparent, distributed operating system Locus, extended to operate transparently across an internet system that includes long-haul links, is discussed at length. New protocols, distributed cache management, and process execution site selection are all used to achieve the results reported.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312996","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312996","Internet Locus;long haul network;network transparency","Internet;Delay;Operating systems;Local area networks;Protocols;Logic gates;Bandwidth","computer networks;distributed processing;file organisation;operating systems (computers);protocols","network transparency;distributed system;machine boundaries;application programs;distributed operating system;Locus;internet system;protocols;distributed cache management","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Discovering Documentation for Java Container Classes","J. Henkel; C. Reichenbach; A. Diwan","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","8","526","543","Modern programs make extensive use of reusable software libraries. For example, we found that 17 percent to 30 percent of the classes in a number of large Java applications use the container classes from the java.util package. Given this extensive code reuse in Java programs, it is important for the reusable interfaces to have clear and unambiguous documentation. Unfortunately, most documentation is expressed in English and, therefore, does not always satisfy these requirements. Worse yet, there is no way of checking that the documentation is consistent with the associated code. Formal specifications present an alternative that does not suffer from these problems; however, formal specifications are notoriously hard to write. To alleviate this difficulty, we have implemented a tool that automatically derives documentation in the form of formal specifications. Our tool probes Java classes by invoking them on dynamically generated tests and captures the information observed during their execution as algebraic axioms. Although the tool is not complete or correct from a formal perspective, we demonstrate that it discovers many useful axioms when applied to container classes. These axioms then form an initial formal documentation of the class they describe.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70705","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267024","","Documentation;Java;Containers;Formal specifications;Libraries;Packaging;Probes;Testing;Data structures;Natural languages","algebraic specification;Java;software libraries;system documentation","Java container class;software library;formal specification;documentation discovery;algebraic axiom","","19","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Database Integrity Block Construct: Concepts and Design Issues","L. Lilien; B. Bhargava","Department of Electrical Engineering and Computer Science, University of Illinois; NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","865","885","When a crash occurs in a transaction processing system, the database can enter an unacceptable state. To continue the processing, the recovery system has three tasks: 1) verification of the database state for acceptability, 2) restoration of an acceptable database state, and 3) restoration of an acceptable history of transaction processing. Unfortunately these tasks are not trivial and the computational complexity of the algorithms for most of them is either NP-complete or NP-hard. In this paper we discuss the concepts and design issues of a construct called database integrity block (DIB). The implementation of this construct allows for efficient verification of the database state by employing a set of integrity assertions and restoration of transaction history by utilizing any database restoration technique such as audit trail or differential file. This paper presents approximation algorithms for minimizing the costs of evaluation of integrity assertions by modeling the problem as the directed traveling salesman problem, and presents a methodology to compare the costs of audit trail and differential file techniques for database restoration. The applicability of integrity verification research to the problem of multiple-query optimization is also included.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232546","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702106","Approximation algorithms;audit trail;database crash and recovery;database system;differential file;directed traveling salesman problem;NP-completeness;query optimization;semantic database integrity;software fault-tolerance","Transaction databases;History;Computer crashes;Error correction;Costs;Traveling salesman problems;Database systems;Fault tolerant systems;Computational complexity;Approximation algorithms","","Approximation algorithms;audit trail;database crash and recovery;database system;differential file;directed traveling salesman problem;NP-completeness;query optimization;semantic database integrity;software fault-tolerance","","","","54","","","","","","IEEE","IEEE Journals & Magazines"
"An economic model to estimate software rewriting and replacement times","Taizan Chan; Siu Leung Chung; Teck Hua Ho","Nat. Univ. of Singapore, Singapore; NA; NA","IEEE Transactions on Software Engineering","","1996","22","8","580","598","The effort required to service maintenance requests on a software system increases as the software system ages and deteriorates. Thus, it may be economical to replace an aged software system with a freshly written one to contain the escalating cost of maintenance. We develop a normative model of software maintenance and replacement effort that enables us to study the optimal policies for software replacement. Based on both analytical and simulation solutions, we determine the timings of software rewriting and replacement, and hence the schedule of rewriting, as well as the size of the rewriting team as functions of the: user environment, effectiveness of rewriting, technology platform, development quality, software familiarity, and maintenance quality of the existing and the new software systems. Among other things, we show that a volatile user environment often leads to a delayed rewriting and an early replacement (i.e., a compressed development schedule). On the other hand, a greater familiarity with either the existing or the new software system allows for a less-compressed development schedule. In addition, we also show that potential savings from rewriting will be higher if the new software system is developed with a superior technology platform, if programmers' familiarity with the new software system is greater, and if the software system is rewritten with a higher initial quality.","0098-5589;1939-3520;2326-3881","","10.1109/32.536958","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=536958","","Software systems;Software maintenance;Environmental economics;Software quality;Aging;Costs;Analytical models;Timing;Delay;Programming profession","software cost estimation;economics;software maintenance;software development management;software quality;human resource management","economic model;software rewriting time estimation;software replacement time estimation;normative model;software maintenance;optimal policies;simulation;rewriting schedule;user environment;technology platform;development quality;software familiarity;software quality;project management","","23","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing the Refactorability of Software Clones","N. Tsantalis; D. Mazinanian; G. P. Krishnan","Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada","IEEE Transactions on Software Engineering","","2015","41","11","1055","1090","The presence of duplicated code in software systems is significant and several studies have shown that clones can be potentially harmful with respect to the maintainability and evolution of the source code. Despite the significance of the problem, there is still limited support for eliminating software clones through refactoring, because the unification and merging of duplicated code is a very challenging problem, especially when software clones have gone through several modifications after their initial introduction. In this work, we propose an approach for automatically assessing whether a pair of clones can be safely refactored without changing the behavior of the program. In particular, our approach examines if the differences present between the clones can be safely parameterized without causing any side-effects. The evaluation results have shown that the clones assessed as refactorable by our approach can be indeed refactored without causing any compile errors or test failures. Additionally, the computational cost of the proposed approach is negligible (less than a second) in the vast majority of the examined cases. Finally, we perform a large-scale empirical study on over a million clone pairs detected by four different clone detection tools in nine open-source projects to investigate how refactorability is affected by different clone properties and tool configuration options. Among the highlights of our conclusions, we found that (a) clones in production code tend to be more refactorable than clones in test code, (b) clones with a close relative location (i.e., same method, type, or file) tend to be more refactorable than clones in distant locations (i.e., same hierarchy, or unrelated types), (c) Type-1 clones tend to be more refactorable than the other clone types, and (d) clones with a small size tend to be more refactorable than clones with a larger size.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2448531","European Union (European Social Fund—ESF); Greek national funds through the Operational Program; National Strategic Reference Framework (NSRF); Thalis—Athens University of Economics; Business—Software Engineering Research Platform; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7130676","Code duplication;Software clone management;Clone refactoring;Refactorability assessment;Empirical study;Code duplication;software clone management;clone refactoring;refactorability assessment;empirical study","Cloning;Arrays;Java;Software systems;Production;Space exploration","software maintenance;software management;source code (software)","software clone refactorability assessment;duplicated code;software systems;source code evolution;source code maintainability;duplicated code merging;duplicated code unification;compile errors;test failures;computational cost;large-scale empirical;clone pairs;clone detection tools;open-source projects;clone properties;tool configuration;test code;relative location;distant locations;type-1 clones","","17","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Formal derivation of rule-based programs","G. -. Roma; R. F. Gamble; W. E. Ball","Dept. of Comput. Sci., Washington Univ., St. Louis, MO, USA; NA; NA","IEEE Transactions on Software Engineering","","1993","19","3","277","296","It is shown that a combination of specification and program refinement may be applied to deriving efficient concurrent rule-based programs. Specification refinement is used to generate an initial rule-based program that is refined into a program which is highly concurrent and efficient. This program derivation strategy is divided into two major tasks. The first task relies on specification refinement. Techniques similar to those employed in the derivation of UNITY programs are used to produce a correct rule-based program having a static knowledge base. The second task involves program refinement and is specific to the development of concurrent rule-based programs. It relies heavily on the availability of a computational model, such as Swarm, that has the ability to dynamically restructure the knowledge base. The ways in which a Swarm program can be translated to OPS5 specifically, given some restrictions, while maintaining the correctness criteria are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.221138","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=221138","","Parallel processing;Concurrent computing;Formal specifications;Logic programming;Expert systems;Refining;Formal verification;Parallel programming;Hardware;Parallel algorithms","formal specification;knowledge based systems;logic programming;parallel programming","program refinement;efficient concurrent rule-based programs;initial rule-based program;program derivation strategy;specification refinement;UNITY programs;correct rule-based program;static knowledge base;computational model;Swarm;OPS5;correctness criteria","","8","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Stochastic Modeling of Branch-and-Bound Algorithms with Best-First Search","B. W. Wah; Chee Fen Yu","Department of Electrical and Computer Engineering and the Coordinated Science Laboratory, University of Illinois; NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","922","934","Branch-and-bound algorithms are organized and intelligently structured searches of solutions in a combinatorially large problem space. In this paper, we propose an approximate stochastic model of branch-and-bound algorithms with a best-first search. We have estimated the average memory space required and have predicted the average number of subproblems expanded before the process terminates. Both measures are exponentials of sublinear exponent. In addition, we have also compared the number of subproblems expanded in a best-first search to that expanded in a depth-first search. Depth-first search has been found to have computational complexity comparable to best-first search when the lower-bound function is very accurate or very inaccurate; otherwise, best-fit search is usually better. The results obtained are useful in studying the efficient evaluation of branch-and-bound algorithms in a virtual memory environment. They also confirm that approximations are very effective in reducing the total number of iterations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232550","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702110","Approximations;best-first search;branch-and-bound algorithms;depth-first search;iterations;memory space;subproblem","Stochastic processes;Partitioning algorithms;Constraint optimization;Intelligent structures;Computational complexity;Artificial intelligence;Operations research;Search problems;Expert systems","","Approximations;best-first search;branch-and-bound algorithms;depth-first search;iterations;memory space;subproblem","","12","","35","","","","","","IEEE","IEEE Journals & Magazines"
"An examination of fault exposure ratio","Y. K. Malaiya; A. von Mayrhauser; P. K. Srimani","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA","IEEE Transactions on Software Engineering","","1993","19","11","1087","1094","The fault exposure ratio, K, is an important factor that controls the per-fault hazard rate, and hence, the effectiveness of the testing of software. The authors examine the variations of K with fault density, which declines with testing time. Because faults become harder to find, K should decline if testing is strictly random. However, it is shown that at lower fault densities K tends to increase. This is explained using the hypothesis that real testing is more efficient than strictly random testing especially at the end of the test phase. Data sets from several different projects (in USA and Japan) are analyzed. When the two factors, e.g., shift in the detectability profile and the nonrandomness of testing, are combined the analysis leads to the logarithmic model that is known to have superior predictive capability.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.256855","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=256855","","Hazards;Software reliability;Fault detection;Density measurement;Software testing;USA Councils;Predictive models;Neural networks;Debugging","program testing;software reliability","fault exposure ratio;per-fault hazard rate;software testing;detectability profile;logarithmic model;predictive capability;software reliability;fault density","","31","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Exploiting Dynamic Information in IDEs Improves Speed and Correctness of Software Maintenance Tasks","D. Rothlisberger; M. Harry; W. Binder; P. Moret; D. Ansaloni; A. Villazon; O. Nierstrasz","Universit&#x0E1;t Bern, Bern; Universit&#x0E1;t Bern, Bern; University of Lugano (USI), Lugano; University of Lugano (USI), Lugano; University of Lugano (USI), Lugano; Universidad Privada Boliviana (UPB), Cochabamba; Universit&#x0E1;t Bern, Bern","IEEE Transactions on Software Engineering","","2012","38","3","579","591","Modern IDEs such as Eclipse offer static views of the source code, but such views ignore information about the runtime behavior of software systems. Since typical object-oriented systems make heavy use of polymorphism and dynamic binding, static views will miss key information about the runtime architecture. In this paper, we present an approach to gather and integrate dynamic information in the Eclipse IDE with the goal of better supporting typical software maintenance activities. By means of a controlled experiment with 30 professional developers, we show that for typical software maintenance tasks, integrating dynamic information into the Eclipse IDE yields a significant 17.5 percent decrease of time spent while significantly increasing the correctness of the solutions by 33.5 percent. We also provide a comprehensive performance evaluation of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6178187","Object-oriented programming;integrated environments;restructuring;reverse engineering;reengineering;complexity measures;performance measures.","Runtime;Measurement;Java;Context;Software maintenance;Concrete;Weaving","dynamic programming;object-oriented programming;program compilers;software maintenance","exploiting dynamic information;IDE;software maintenance tasks;Eclipse;source code;runtime behavior;software systems;object-oriented systems;dynamic binding;runtime architecture;dynamic information","","13","","35","","","","","","IEEE","IEEE Journals & Magazines"
"An experimental evaluation of the assumption of independence in multiversion programming","J. C. Knight; N. G. Leveson","Department of Computer Science, University of Virginia, Charlottesville, VA 22903; Department of Computer Science, University of California, Irvine, CA 92717","IEEE Transactions on Software Engineering","","1986","SE-12","1","96","109","<i>N</i>-version programming has been proposed as a method of incorporating fault tolerance into software. Multiple versions of a program (i.e. `<i>N</i>') are prepared and executed in parallel. Their outputs are collected and examined by a voter, and, if they are not identical, it is assumed that the majority is correct. This method depends for its reliability improvement on the assumption that programs that have been developed independently will fail independently. An experiment is described in which the fundamental axiom is tested. In all, 27 versions of a program were prepared independently from the same specification at two universities and then subjected to one million tests. The results of the tests revealed that the programs were individually extremely reliable but that the number of tests in which more than one program failed was substantially more than expected. The results of these tests are presented along with an analysis of some of the faults that were found in the programs. Background information on the programmers used is also summarized.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312924","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312924","Design diversity;fault-tolerant software;multiversion programming;N-version programming;software reliability","Programming;Software;Educational institutions;Software reliability;NASA","fault tolerant computing;programming;software reliability","independence;multiversion programming;N-version programming;fault tolerance;voter;reliability improvement","","205","","","","","","","","IEEE","IEEE Journals & Magazines"
"A Survey of Recent Trends in Testing Concurrent Software Systems","F. A. Bianchi; A. Margara; M. Pezzè","Faculty of Informatics, Univeristà della Svizzera italiana, USI Lugano, Lugano, Switzerland; Faculty of Informatics, Univeristà della Svizzera italiana, USI Lugano, Lugano, Switzerland; Faculty of Informatics, Univeristà della Svizzera italiana, USI Lugano, Lugano, Switzerland","IEEE Transactions on Software Engineering","","2018","44","8","747","783","Many modern software systems are composed of multiple execution flows that run simultaneously, spanning from applications designed to exploit the power of modern multi-core architectures to distributed systems consisting of multiple components deployed on different physical nodes. We collectively refer to such systems as concurrent systems. Concurrent systems are difficult to test, since the faults that derive from their concurrent nature depend on the interleavings of the actions performed by the individual execution flows. Testing techniques that target these faults must take into account the concurrency aspects of the systems. The increasingly rapid spread of parallel and distributed architectures led to a deluge of concurrent software systems, and the explosion of testing techniques for such systems in the last decade. The current lack of a comprehensive classification, analysis and comparison of the many testing techniques for concurrent systems limits the understanding of the strengths and weaknesses of each approach and hampers the future advancements in the field. This survey provides a framework to capture the key features of the available techniques to test concurrent software systems, identifies a set of classification criteria to review and compare the available techniques, and discusses in details their strengths and weaknesses, leading to a thorough assessment of the field and paving the road for future progresses.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2707089","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7932530","Survey;classification;testing;concurrent systems;parallel systems;distributed systems","Testing;Software systems;Message passing;History;Concurrent computing;Computer architecture;Synchronization","formal specification;multiprocessing systems;object-oriented programming;parallel architectures;program testing","distributed systems;testing techniques;concurrency aspects;modern software systems;modern multicore architectures;concurrent software system testing;multiple execution flows;parallel architecture;distributed architecture;classification criteria","","","","216","","","","","","IEEE","IEEE Journals & Magazines"
"A testing framework for mobile computing software","I. Satoh","Nat. Inst. of Informatics, Tokyo, Japan","IEEE Transactions on Software Engineering","","2003","29","12","1112","1121","We present a framework for testing applications for mobile computing devices. When a device is moved into and attached to a new network, the proper functioning of applications running on the device often depends on the resources and services provided locally in the current network. This framework provides an application-level emulator for mobile computing devices to solve this problem. Since the emulator is constructed as a mobile agent, it can carry applications across networks on behalf of its target device and allow the applications to connect to local servers in its current network in the same way as if they had been moved with and executed on the device itself. This paper also demonstrates the utility of this framework by describing the development of typical network-dependent applications in mobile and ubiquitous computing settings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265525","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265525","","Software testing;Mobile computing;Computer networks;Application software;Mobile agents;Computer applications;Network servers;Pervasive computing;Floors;Wireless networks","mobile computing;mobile agents;program testing;wireless LAN;cellular radio","testing framework;mobile computing software;mobile computing devices;application-level emulator;mobile agent;network-dependent applications;ubiquitous computing","","52","","21","","","","","","IEEE","IEEE Journals & Magazines"
"An automated verification method for distributed systems software based on model extraction","G. J. Holzmann; M. H. Smith","Lucent Technol. Bell Labs., Murray Hill, NJ, USA; NA","IEEE Transactions on Software Engineering","","2002","28","4","364","377","Software verification methods are used only sparingly in industrial software development today. The most successful methods are based on the use of model checking. There are, however, many hurdles to overcome before the use of model checking tools can truly become mainstream. To use a model checker, the user must first define a formal model of the application, and to do so requires specialized knowledge of both the application and of model checking techniques. For larger applications, the effort to manually construct a formal model can take a considerable investment of time and expertise, which can rarely be afforded. Worse, it is hard to secure that a manually constructed model can keep pace with the typical software application, as it evolves from the concept stage to the product stage. We describe a verification method that requires far less specialized knowledge in model construction. It allows us to extract models mechanically from source code. The model construction process now becomes easily repeatable, as the application itself continues to evolve. Once the model is constructed, existing model checking techniques allow us to perform all checks in a mechanical fashion, achieving nearly complete automation. The level of thoroughness that can be achieved with this new type of software testing is significantly greater than for conventional techniques. We report on the application of this method in the verification of the call processing software for a new telephone switch that was developed at Lucent Technologies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995426","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995426","","System software","program verification;distributed processing;program testing;telecommunication computing","automated verification method;distributed systems software;model extraction;software verification methods;industrial software development;formal model;source code;software testing;formal methods;call processing software;telephone switch;Lucent Technologies;reactive systems;case studies","","39","","29","","","","","","IEEE","IEEE Journals & Magazines"
"A decompositional approach to the design of parallel programs","Ying Liu; A. K. Singh; R. L. Bagrodia","Dept. of Comput. Sci., California Univ., Santa Barbara, CA, USA; Dept. of Comput. Sci., California Univ., Santa Barbara, CA, USA; NA","IEEE Transactions on Software Engineering","","1994","20","12","914","932","A methodology for the derivation of parallel implementations from program specifications is developed. The goal of the methodology is to decompose a program specification into a collection of module specifications via property refinement, such that each module may be implemented independently by a subprogram. The correctness of the implementation is then deduced from the correctness of the property refinement procedure and the correctness of the individual subprograms. The refinement strategy is based on identifying frequently occurring control structures such as sequential composition and iteration. The methodology is developed in the context of the UNITY logic and the UC programming language, and illustrated through the solution of diffusion aggregation in fluid flow simulations.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.368135","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=368135","","Computer languages;Computer science;Fluid flow control;Logic programming;Fluid flow;Context modeling;Algorithm design and analysis","complete computer programs;program control structures;parallel programming;formal specification;program verification;flow simulation;digital simulation;diffusion;physics computing","decompositional approach;parallel program design;program specifications;parallel implementation correctness;module specifications;frequently occurring control structures;subprograms;property refinement procedure;sequential composition;iteration;UNITY logic;UC programming language;diffusion aggregation;fluid flow simulations","","2","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Research on Knowledge-Based Software Environments at Kestrel Institute","D. R. Smith; G. B. Kotik; S. J. Westfold","Kestrel Institute; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","11","1278","1295","We present a summary of the CHI project conducted at Kestrel Institute through mid-1984. The objective of this project was to perform research on knowledge-based software environments. Toward this end, key portions of a prototype environment, called CHI, were built that established the feasibility of this approach. One result of this research was the development of a wide-spectrum language that could be used to express all stages of the program development process in the system. Another result was that the prototype compiler was used to synthesize itself from very-high-level description of itself. In this way the system was bootstrapped. We describe the overall nature of the work done on this project, give highlights of implemented prototypes, and describe the implications that this work suggests for the future of software engineering. In addition to this historical perspective, current research projects at Kestrel Institute as well as commercial applications of the technology at Reasoning Systems are briefly surveyed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231879","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701947","Automatic programming;knowledge-based systems;program synthesis;programming environments;software environments very-high-level languages;wide-spectrum languages","Software prototyping;Prototypes;Programming profession;Software performance;Application software;Logic programming;Software engineering;Knowledge based systems;Programming environments;Hardware","","Automatic programming;knowledge-based systems;program synthesis;programming environments;software environments very-high-level languages;wide-spectrum languages","","75","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Loop monotonic statements","M. Spezialetti; R. Gupta","Packard Lab., Lehigh Univ., Bethlehem, PA, USA; NA","IEEE Transactions on Software Engineering","","1995","21","6","497","505","A statement is considered to be monotonic with respect to a loop if its execution, during the successive iterations of a given execution of the loop, assigns a monotonically increasing or decreasing sequence of values to a variable. We present static analysis techniques to identify loop monotonic statements. The knowledge of loop monotonicity characteristics of statements which compute array subscript expressions is of significant value in a number of applications. We illustrate the use of this information in improving the efficiency of run-time array bound checking, run-time dependence testing, and on-the-fly detection of access anomalies. Given that a significant percentage of subscript expressions are monotonic, substantial savings can be expected by using these techniques.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.391376","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=391376","","Runtime;Testing;Computer Society;Parallel processing;Program processors;Computer science;Performance evaluation","program diagnostics;parallel programming;parallelising compilers;program control structures","loop monotonic statements;successive iterations;static analysis techniques;loop monotonicity characteristics;array subscript expressions;run-time array bound checking;run-time dependence testing;on-the-fly detection;access anomalies;array bound checking;run-time dependence checking;static analysis;induction variables;data races","","5","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Design of reliable software in distributed systems using the conversation scheme","A. M. Tyrrell; D. J. Holding","Department of Electrical, Electronic, and Systems Engineering, Coventry (Lanchester) Polytechnic, Coventry CV1 5FB, England; Department of Electrical and Electronic Engineering and Applied Physics, Aston University, Birmingham B4 7ET, England","IEEE Transactions on Software Engineering","","1986","SE-12","9","921","928","The problems of error detection and recovery are examined in a number of concurrent processes expressed as a set of communicating sequential processes (CSP). A method is proposed which uses a Petri net model to formally identify both the state and the state reachability tree of a distributed system. These are used to define systematically the boundaries of a conversation, including the recovery and test lines which are essential parts of the fault-tolerant mechanism. The techniques are implemented using the OCCAM programming language, which is derived from CSP. The application of this method is shown by a control example.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313047","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313047","Communicating sequential processes;concurrent processes;conversation;distributed systems;fault-tolerant software;occam;Petri nets;recovery block","Process control;Robot kinematics;Petri nets;Software;Computer languages;Synchronous motors","distributed processing;error detection","error recovery;reliable software;distributed systems;conversation scheme;error detection;concurrent processes;communicating sequential processes;CSP;Petri net model;state reachability tree;distributed system;fault-tolerant mechanism;OCCAM programming language;CSP;control example","","9","","","","","","","","IEEE","IEEE Journals & Magazines"
"A Dynamic Slicing Technique for UML Architectural Models","J. T. Lallchandani; R. Mall","Indian Institute of Technology Kharagpur, WB INDIA; Indian Institute of Technology Kharagpur, WB INDIA","IEEE Transactions on Software Engineering","","2011","37","6","737","771","This paper proposes a technique for dynamic slicing of UML architectural models. The presence of related information in diverse model parts (or fragments) makes dynamic slicing of Unified Modeling Language (UML) models a complex problem. We first extract all relevant information from a UML model specifying a software architecture into an intermediate representation, which we call a Model Dependency Graph (MDG). For a given slicing criterion, our slicing algorithm traverses the constructed MDG to identify the relevant model parts that are directly or indirectly affected during the execution of a specified scenario. One novelty of our approach is computation of dynamic slice based on the structural and behavioral (interactions only) UML models as against independently processing separate UML models, and determining the implicit interdependencies among different model elements distributed across model views. We also briefly discuss a prototype tool named Archlice, which we have developed to implement our algorithm.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.112","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680909","Software architecture;UML;architectural metamodel;dynamic slicing;impact analysis.","Unified modeling language;Computational modeling;Heuristic algorithms;Computer architecture;Analytical models;Software architecture;Software algorithms","program slicing;software architecture;software prototyping;Unified Modeling Language","dynamic slicing technique;UML architectural models;unified modeling language models;model dependency graph;software architecture;prototype tool;Archlice","","15","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Automatically Categorizing Software Technologies","M. Nassif; C. Treude; M. Robillard","School of Computer Science, McGill University School of Computer Science, 348406 Montreal, Quebec Canada H3A 0E9 (e-mail: mnassif@cs.mcgill.ca); School of Computer Science, McGill University, Montreal, Quebec Canada (e-mail: ctreude@cs.mcgill.ca); School of Computer Science, McGill University, Montreal, Quebec Canada H3A 2A7 (e-mail: martin@cs.mcgill.ca)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Informal language and the absence of a standard taxonomy for software technologies make it difficult to reliably analyze technology trends on discussion forums and other on-line venues. We propose an automated approach called Witt for the categorization of software technology (an expanded version of the hypernym discovery problem). Witt takes as input a phrase describing a software technology or concept and returns a general category that describes it (e.g., integrated development environment), along with attributes that further qualify it (commercial, php, etc.). By extension, the approach enables the dynamic creation of lists of all technologies of a given type (e.g., web application frameworks). Our approach relies on Stack Overflow and Wikipedia, and involves numerous original domain adaptations and a new solution to the problem of normalizing automatically-detected hypernyms. We compared Witt with six independent taxonomy tools and found that, when applied to software terms, Witt demonstrated better coverage than all evaluated alternate solutions, without a corresponding degradation in false positive rate.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2836450","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8359344","","","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt","E. d. S. Maldonado; E. Shihab; N. Tsantalis","Department of Computer Science and Software Engineering, Data-Driven Analysis of Software (DAS) Lab, Concordia University, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Data-Driven Analysis of Software (DAS) Lab, Concordia University, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada","IEEE Transactions on Software Engineering","","2017","43","11","1044","1062","The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-admitted technical debt), and that the most common types of self-admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-admitted technical debt, significantly outperforming the current state-of-the-art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2654244","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7820211","Technical debt;source code comments;natural language processing;empirical study","Software;Natural language processing;Manuals;Entropy;Unified modeling language;Java;Structured Query Language","computer crime;Java;natural language processing;project management;public domain software;software maintenance;software management;software quality;SQL","Natural Language Processing;requirement debt;self-admitted technical debt detection;open source projects;source code quality;source code comment classification;NLP;design debt","","7","","58","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Communication and organization: an empirical study of discussion in inspection meetings","C. B. Seaman; V. R. Basili","Inst. for Adv. Comput. Studies, Maryland Univ., College Park, MD, USA; NA","IEEE Transactions on Software Engineering","","1998","24","7","559","572","This paper describes an empirical study that addresses the issue of communication among members of a software development organization. In particular, data was collected concerning code inspections in one software development project. The question of interest is whether or not organizational structure (the network of relationships between developers) has an effect on the amount of effort expended on communication between developers. The independent variables in this study are various attributes of the organizational structure in which the inspection participants work. The dependent variables are measures of the communication effort expended in various parts of the code inspection process, focusing on the inspection meeting. Both quantitative and qualitative methods were used, including participant observation, structured interviews, generation of hypotheses from field notes, statistical tests of relationships, and interpretation of results with qualitative anecdotes. The study results show that past and present working relationships between inspection participants affect the amount of meeting time spent in different types of discussion, thus affecting the overall inspection meeting length. Reporting relationships and physical proximity also have an effect. The contribution of the study is a set of well-supported hypotheses for further investigation.","0098-5589;1939-3520;2326-3881","","10.1109/32.708569","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=708569","","Inspection;Programming;Productivity;Testing;Information processing;Context;Professional communication;Software engineering","software development management","inspection meetings;software development organization;software development project;organizational structure;inspection participants;communication effort;code inspection process","","34","","21","","","","","","IEEE","IEEE Journals & Magazines"
"The Role of Domain Expenence in Software Design","B. Adelson; E. Soloway","Department of Computer Science, Yale University, New Haven, CT 06520, and the Division of Information Science and Technology, National Science Foundation; NA","IEEE Transactions on Software Engineering","","1985","SE-11","11","1351","1360","A designer's expertise rests on the knowledge and skills which develop with experience in a domain. As a result, when a designer is designing an object in an unfamiliar domain he will not have the same knowledge and skills available to him as when he is designing an object in a familiar domain. In this paper we look at the software designer's underlying constellation of knowledge and skills, and at the way in which this constellation is dependent upon experience in a domain. What skills drop out, what skills, or interactions of skills come forward as experience with the domain changes? To answer the above question, we studied expert designers in experimentally created design contexts with which they were differentially familiar. In this paper we describe the knowledge and skills we found were central to each of the above contexts and discuss the functional utility of each. In addition to discussing the knowledge and skills we observed in expert designers, we will also compare novice and expert behavior.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231883","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701951","Artificial intelligence;cognitive models;cognitive science;software design","Software design;Protocols;Computer science;Cognitive science;Information science;Problem-solving","","Artificial intelligence;cognitive models;cognitive science;software design","","45","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing detection methods for software requirements inspections: a replicated experiment","A. A. Porter; L. G. Votta; V. R. Basili","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","1995","21","6","563","575","Software requirements specifications (SRS) are often validated manually. One such process is inspection, in which several reviewers independently analyze all or part of the specification and search for faults. These faults are then collected at a meeting of the reviewers and author(s). Usually, reviewers use Ad Hoc or Checklist methods to uncover faults. These methods force all reviewers to rely on nonsystematic techniques to search for a wide variety of faults. We hypothesize that a Scenario-based method, in which each reviewer uses different, systematic techniques to search for different, specific classes of faults, will have a significantly higher success rate. We evaluated this hypothesis using a 3/spl times/2/sup 4/ partial factorial, randomized experimental design. Forty eight graduate students in computer science participated in the experiment. They were assembled into sixteen, three-person teams. Each team inspected two SRS using some combination of Ad Hoc, Checklist or Scenario methods. For each inspection we performed four measurements: (1) individual fault detection rate, (2) team fault detection rate, (3) percentage of faults first identified at the collection meeting (meeting gain rate), and (4) percentage of faults first identified by an individual, but never reported at the collection meeting (meeting loss rate). The experimental results are that (1) the Scenario method had a higher fault detection rate than either Ad Hoc or Checklist methods, (2) Scenario reviewers were more effective at detecting the faults their scenarios are designed to uncover, and were no less effective at detecting other faults than both Ad Hoc or Checklist reviewers, (3) Checklist reviewers were no more effective than Ad Hoc reviewers, and (4) Collection meetings produced no net improvement in the fault detection rate-meeting gains were offset by meeting losses.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.391380","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=391380","","Inspection;Fault detection;Fault diagnosis;Design for experiments;Computer science;Assembly;Performance evaluation;Gain measurement;Loss measurement;Performance gain","formal specification;formal verification;software development management","detection methods;software requirements inspections;replicated experiment;software requirements specifications;nonsystematic techniques;scenario-based method;individual fault detection rate;team fault detection rate;fault detection rate","","178","","20","","","","","","IEEE","IEEE Journals & Magazines"
"A statistical methodology for the study of the software failure process and its application to the ARGOS center","R. Troy; Y. Romain","Verilog S.A., 3, Chemin du Pigeonnier de la C&#x00E9;pi&#x00E8;re, 31081 Toulouse Cedex, France; Verilog S.A., 3, Chemin du Pigeonnier de la C&#x00E9;pi&#x00E8;re, 31081 Toulouse Cedex, France","IEEE Transactions on Software Engineering","","1986","SE-12","9","968","978","The authors propose a stepwise statistical methodology for the study of operating system reliability and associated tools. An example of the application of this method for the ARGOS data processing center of France's CNES is presented. It is shown that each evaluation of software reliability is considered as a special case. Two major consequences are that the reliability models need improvement and that every evaluation must be supported by a statistical analysis of the product and process characteristics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313051","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313051","Software failure;software reliability;software reliability models","Software;Software reliability;Reliability theory;Hardware;Iris;Environmental factors","operating systems (computers);software reliability;statistical analysis","software failure process;ARGOS center;stepwise statistical methodology;operating system reliability;ARGOS data processing center;CNES;software reliability;reliability models;statistical analysis","","4","","","","","","","","IEEE","IEEE Journals & Magazines"
"TUNEX: a knowledge-based system for performance tuning of the UNIX operating system","B. Samadi","AT&T Bell Labs., Holmdel, NJ, USA","IEEE Transactions on Software Engineering","","1989","15","7","861","874","TUNEX, an expert system developed for performance tuning of the UNIX operating system, is described. TUNEX was developed on UNIX system V. It uses the properties, commands and utilities of this version. The tuning activities it is concerned with include: (1) adjusting operating system tunable parameters, such as number of disk buffers; (2) running maintenance routines, i.e. reorganizing file systems; (3) developing operation rules, such as off-peak hour runs of backups; and (4) modifying hardware, buying an additional disk drive. The structure of TUNEX is presented and performance analysis modules which provide quantitative information to this tool are briefly described. The overhead in the resource usage introduced by the performance monitoring and tuning tool itself is discussed; the author points to the areas in which additional resources are required by TUNEX.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.29486","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=29486","","Knowledge based systems;Operating systems;Expert systems;Performance analysis;Encoding;Computerized monitoring;Prototypes;File systems;Hardware;Disk drives","expert systems;Unix","TUNEX;knowledge-based system;performance tuning;UNIX operating system;expert system;UNIX system V;commands;utilities;disk buffers;maintenance routines;reorganizing file systems;operation rules;performance monitoring","","7","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Putting Preemptive Time Petri Nets to Work in a V-Model SW Life Cycle","L. Carnevali; L. Ridi; E. Vicario","Universit&#x0E0; di Firenze, Firenze; Universit&#x0E0; di Firenze, Firenze; Universit&#x0E0; di Firenze, Firenze","IEEE Transactions on Software Engineering","","2011","37","6","826","844","Preemptive Time Petri Nets (pTPNs) support modeling and analysis of concurrent timed SW components running under fixed priority preemptive scheduling. The model is supported by a well-established theory based on symbolic state space analysis through Difference Bounds Matrix (DBM) zones, with specific contributions on compositional modularization, trace analysis, and efficient overapproximation and cleanup in the management of suspension deriving from preemptive behavior. In this paper, we devise and implement a framework that brings the theory to application. To this end, we cast the theory into an organic tailoring of design, coding, and testing activities within a V-Model SW life cycle in respect of the principles of regulatory standards applied to the construction of safety-critical SW components. To implement the toolchain subtended by the overall approach into a Model Driven Development (MDD) framework, we complement the theory of state space analysis with methods and techniques supporting semiformal specification and automated compilation into pTPN models and real-time code, measurement-based Execution Time estimation, test case selection and execution, coverage evaluation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680913","Real-time systems;safety-critical SW components;SW life cycle;V-Model;preemptive Time Petri Nets;symbolic state space analysis;model driven development;automated model transformation;automated code generation;Execution Time estimation;real-time testing;test case selection and execution;coverage analysis.","Real time systems;Analytical models;Unified modeling language;Petri nets;Mathematical model;Computer architecture","formal specification;Petri nets;program diagnostics;program testing;safety-critical software;scheduling","preemptive time Petri nets;V-model SW life cycle;concurrent timed SW components;fixed priority preemptive scheduling;symbolic state space analysis;difference bounds matrix zones;compositional modularization;trace analysis;overapproximation;safety-critical SW components;model driven development framework;semiformal specification;automated compilation;pTPN models;real-time code;measurement-based execution time estimation;test case selection;test case execution;coverage evaluation","","8","","71","","","","","","IEEE","IEEE Journals & Magazines"
"A General Testability Theory: Classes, Properties, Complexity, and Testing Reductions","I. Rodríguez; L. Llana; P. Rabanal","Department of Sistemas Informáticos y Computación, Universidad Complutense de Madrid, Madrid, Spain; Department of Sistemas Informáticos y Computación, Universidad Complutense de Madrid, Madrid, Spain; Department of Sistemas Informáticos y Computación, Universidad Complutense de Madrid, Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","9","862","894","In this paper we develop a general framework to reason about testing. The difficulty of testing is assessed in terms of the amount of tests that must be applied to determine whether the system is correct or not. Based on this criterion, five testability classes are presented and related. We also explore conditions that enable and disable finite testability, and their relation to testing hypotheses is studied. We measure how far incomplete test suites are from being complete, which allows us to compare and select better incomplete test suites. The complexity of finding that measure, as well as the complexity of finding minimum complete test suites, is identified. Furthermore, we address the reduction of testing problems to each other, that is, we study how the problem of finding test suites to test systems of some kind can be reduced to the problem of finding test suites for another kind of systems. This enables to export testing methods. In order to illustrate how general notions are applied to specific cases, many typical examples from the formal testing techniques domain are presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2331690","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6839051","Formal testing techniques;general testing frameworks","Testing;Complexity theory;Proposals;Computational modeling;Probabilistic logic;Abstracts;Computer languages","formal languages;formal verification","general testability theory;complexity;testing reductions;testability classes;finite testability;incomplete test suites;formal testing techniques domain","","14","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Implementation of an FP-Shell","Y. H. Kamath; M. M. Matthews","AT&T Bell Laboratories; NA","IEEE Transactions on Software Engineering","","1987","SE-13","5","532","539","One of the best features of the UNIX™ Shell is that it provides a framework which can be used to build complex programs by interconnecting existing simple programs. However, it is limited to linear combinations of programs, and building of more complex programs must be accomplished by executing sequences of commands. This paper introduces Backus' FP (Functional Programming) as an alternative command language for UNIX. In FP, programs are true functions and another distinctive feature of FP languages is that they contain functional forms, which are constructs for combining programs to build new programs. Also, the functional style of programming provides a natural way of exploiting parallel machine architecture.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233198","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702253","Command language;functional language;functional programming environment;shells;UNIX","Functional programming;Central Processing Unit;LAN interconnection;Buildings;Command languages;Parallel programming;Parallel machines;Computer architecture;Concurrent computing;Computer science","","Command language;functional language;functional programming environment;shells;UNIX","","","","22","","","","","","IEEE","IEEE Journals & Magazines"
"The external Heapsort","L. M. Wegner; J. I. Teuhola","Univ.-GH-Kassel, West Germany; NA","IEEE Transactions on Software Engineering","","1989","15","7","917","925","Heapsort is an internal sorting method which sorts an array of n records in place in O(n log n) time. Heapsort is generally considered unsuitable for external random-access sorting. By replacing key comparisons with merge operations on pages, it is shown how to obtain an in-place external sort which requires O(m log m) page references, where m is the number of pages which the file occupies. The new sort method (called Hillsort) has several useful properties for advanced database management systems. Not only does Hillsort operate in place, i.e., no additional external storage space is required assuming that the page table can be kept in core memory, but accesses to adjacent pages in the heap require one seek only if the pages are physically contiguous. The authors define the Hillsort model of computation for external random-access sorting, develop the complete algorithm and then prove it correct. The model is next refined and a buffer management concept is introduced so as to reduce the number of merge operations and page references, and make the method competitive to a basic balanced two-way external merge. Performance characteristics are noted such as the worst-case upper bound, which can be carried over from Heapsort, and the average-case behavior, deduced from experimental findings. It is shown that the refined version of the algorithm which is on a par with the external merge sort.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.29490","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=29490","","Sorting;Database systems;Turning;Merging;Prototypes;Computer science;Resumes;Computational modeling;Upper bound","sorting;storage management","performance characteristics;external Heapsort;sorting method;random-access sorting;Hillsort;database management systems;page table;buffer management concept;merge operations;page references;worst-case upper bound","","12","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Deep Semantic Feature Learning for Software Defect Prediction","S. Wang; T. Liu; J. Nam; L. Tan","Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Ontario Canada N2L 3G1 (e-mail: song.wang@uwaterloo.ca); Electrical and Computer Engineering, University of Waterloo, Waterloo, Ontario Canada (e-mail: t67liu@uwaterloo.ca); Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, - Hong Kong - (e-mail: jcnam@postech.edu); Electrical and Computer Engineering, University of Waterloo, Waterloo, Ontario Canada N2L 3G1 (e-mail: lintan@uwaterloo.ca)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Software defect prediction, which predicts defective code regions, can assist developers in finding bugs and prioritizing their testing efforts. Traditional defect prediction features often fail to capture the semantic differences between different programs. This degrades the performance of the prediction models built on these traditional features. Thus, the capability to capture the semantics in programs is required to build accurate prediction models. To bridge the gap between semantics and defect prediction features, we propose leveraging a powerful representation-learning algorithm, deep learning, to learn the semantic representations of programs automatically from source code files and code changes. Specifically, we leverage a deep belief network (DBN) to automatically learn semantic features using token vectors extracted from the programs' abstract syntax trees (AST) (for file-level defect prediction models) and source code changes (for change-level defect prediction models). We examine the effectiveness of our approach on two file-level defect prediction tasks (i.e., file-level within-project defect prediction and file-level cross-project defect prediction) and two change-level defect prediction tasks (i.e., change-level within-project defect prediction and change-level cross-project defect prediction). Our experimental results indicate that the DBN-based semantic features can significantly improve the examined defect prediction tasks. Specifically, the improvements of semantic features against existing traditional features (in F1) range from 2.1 to 41.9 percentage points for file-level within-project defect prediction, from 1.5 to 13.4 percentage points for file-level cross-project defect prediction, from 1.0 to 8.6 percentage points for change-level within-project defect prediction, and from 0.6 to 9.9 percentage points for change-level cross-project defect prediction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2877612","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8502853","Defect prediction;quality assurance;deep learning;semantic features","Semantics;Predictive models;Feature extraction;Task analysis;Computer bugs;Data models","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A framework for specification-based testing","P. Stocks; D. Carrington","Dept. of Comput. Sci., Rutgers Univ., Piscataway, NJ, USA; NA","IEEE Transactions on Software Engineering","","1996","22","11","777","793","Test templates and a test template framework are introduced as useful concepts in specification-based testing. The framework can be defined using any model-based specification notation and used to derive tests from model-based specifications-in this paper, it is demonstrated using the Z notation. The framework formally defines test data sets and their relation to the operations in a specification and to other test data sets, providing structure to the testing process. Flexibility is preserved, so that many testing strategies can be used. Important application areas of the framework are discussed, including refinement of test data, regression testing, and test oracles.","0098-5589;1939-3520;2326-3881","","10.1109/32.553698","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=553698","","Software testing;Application software;Formal specifications;Computer science;Life testing;Object oriented modeling;Computer Society;Programming;Performance evaluation;Software design","formal specification;program testing;specification languages;statistical analysis","specification-based testing;test templates;model-based specification notation;test oracles;Z notation;test data sets;test data refinement;regression testing","","127","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating the effect of a delegated versus centralized control style on the maintainability of object-oriented software","E. Arisholm; D. I. K. Sjoberg","Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway","IEEE Transactions on Software Engineering","","2004","30","8","521","534","A fundamental question in object-oriented design is how to design maintainable software. According to expert opinion, a delegated control style, typically a result of responsibility-driven design, represents object-oriented design at its best, whereas a centralized control style is reminiscent of a procedural solution, or a ""bad"" object-oriented design. We present a controlled experiment that investigates these claims empirically. A total of 99 junior, intermediate, and senior professional consultants from several international consultancy companies were hired for one day to participate in the experiment. To compare differences between (categories of) professionals and students, 59 students also participated. The subjects used professional Java tools to perform several change tasks on two alternative Java designs that had a centralized and delegated control style, respectively. The results show that the most skilled developers, in particular, the senior consultants, require less time to maintain software with a delegated control style than with a centralized control style. However, more novice developers, in particular, the undergraduate students and junior consultants, have serious problems understanding a delegated control style, and perform far better with a centralized control style. Thus, the maintainability of object-oriented software depends, to a large extent, on the skill of the developers who are going to maintain it. These results may have serious implications for object-oriented development in an industrial context: having senior consultants design object-oriented systems may eventually pose difficulties unless they make an effort to keep the designs simple, as the cognitive complexity of ""expert"" designs might be unmanageable for less skilled maintainers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316869","Index Terms- Design principles;responsibility delegation;control styles;object-oriented design;object-oriented programming;software maintainability;controlled experiment.","Software maintenance;Centralized control;Software design;Java;Design methodology;Unified modeling language;Business communication;Logic;Electrical equipment industry;Object oriented programming","software maintenance;object-oriented programming;Java;software development management","object-oriented software maintainability;object-oriented design;professional Java tool;Java design;centralized control style;delegated control style;object-oriented development;object-oriented programming","","75","","33","","","","","","IEEE","IEEE Journals & Magazines"
"The delay due to dynamic two-phase locking","C. S. Hartzman","Dept. of Math., Stat. & Comput. Sci., Dalhousie Univ., Halifax, NS, Canada","IEEE Transactions on Software Engineering","","1989","15","1","72","82","An analytic formula for the delay due to two-phase locking is developed in terms of mean values for the input parameters using an open queuing network model in equilibrium. The results of simulations, using various realistic probability distributions governing the number of locks that transactions request, are presented to validate the formula. Reasonably good accuracy is achieved for gamma distributions over a wide range of parameter settings. The simulations also provided evidence that the rate of deadlock, often disregarded in the literature, can be high in certain heavily utilized databases.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21728","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21728","","Concurrency control;Transaction databases;Delay effects;Analytical models;Queueing analysis;Probability distribution;Mathematics;Statistical distributions;System recovery;Costs","database theory;distributed databases;queueing theory","dynamic two-phase locking;open queuing network model;probability distributions;gamma distributions;parameter settings;heavily utilized databases","","7","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying extensions required by RUP (rational unified process) to comply with CMM (capability maturity model) levels 2 and 3","L. V. Manzoni; R. T. Price","Univ. Fed. do Rio Grande do Sul, Porto Alegre, Brazil; Univ. Fed. do Rio Grande do Sul, Porto Alegre, Brazil","IEEE Transactions on Software Engineering","","2003","29","2","181","192","This paper describes an assessment of the rational unified process (RUP) based on the capability maturity model (CMM). For each key practice (KP) identified in each key process area (KPA) of CMM levels 2 and 3, the Rational Unified Process was assessed to determine whether it satisfied the KP or not. For each KPA, the percentage of the key practices supported was calculated, and the results were tabulated. The report includes considerations about the coverage of each key process area, describing the highlights of the RUP regarding its support for CMM levels 2 and 3, and suggests where an organization using it will need to complement it to conform to CMM. The assessment resulted in the elaboration of proposals to enhance the RUP in order to satisfy the key process areas of CMM. Some of these are briefly described in this article.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178058","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178058","","Coordinate measuring machines;Capability maturity model;Software engineering;Software quality;Unified modeling language;Software tools;Proposals;Software standards;Systems engineering and theory;Production","software process improvement","rational unified process;capability maturity model;RUP;CMM;key practice;key process area;KP;KPA","","19","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Methodbook: Recommending Move Method Refactorings via Relational Topic Models","G. Bavota; R. Oliveto; M. Gethers; D. Poshyvanyk; A. De Lucia","University of Sannio, Benevento, Italy; University of Molise, Pesche (IS), Italy; Information Systems Department , University of Maryland, Baltimore County, 1000 Hilltop Circle, Baltimore; College of William and Mary, McGlothlin-Street Hall 006, Williamsburg; University of Salerno, Fisciano (SA), Italy","IEEE Transactions on Software Engineering","","2014","40","7","671","694","During software maintenance and evolution the internal structure of the software system undergoes continuous changes. These modifications drift the source code away from its original design, thus deteriorating its quality, including cohesion and coupling of classes. Several refactoring methods have been proposed to overcome this problem. In this paper we propose a novel technique to identify Move Method refactoring opportunities and remove the Feature Envy bad smell from source code. Our approach, coined as Methodbook, is based on relational topic models (RTM), a probabilistic technique for representing and modeling topics, documents (in our case methods) and known relationships among these. Methodbook uses RTM to analyze both structural and textual information gleaned from software to better support move method refactoring. We evaluated Methodbook in two case studies. The first study has been executed on six software systems to analyze if the move method operations suggested by Methodbook help to improve the design quality of the systems as captured by quality metrics. The second study has been conducted with eighty developers that evaluated the refactoring recommendations produced by Methodbook. The achieved results indicate that Methodbook provides accurate and meaningful recommendations for move method refactoring operations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6684534","Refactoring;relational topic models;empirical studies","Software systems;Couplings;Measurement;Object oriented modeling;Educational institutions;Electronic mail","software maintenance;software metrics;source code (software)","Methodbook;recommending move method refactorings;relational topic model;software maintenance;source code;modifications drift;quality metrics;software development","","53","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Integrated concurrency-coherency controls for multisystem data sharing","D. M. Dias; B. R. Iyer; J. T. Robinson; P. S. Yu","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","1989","15","4","437","448","The authors propose an integrated control mechanism and analyze the performance gain due to its use. An extension to the data sharing system structure is examined in which a shared intermediate memory is used for buffering and for early commit processing. Read-write-synchronization and write-serialization problems arise. The authors show how the integrated concurrency protocol can be used to overcome both problems. A queueing model is used to quantify the performance improvement. Although using intermediate memory as a buffering device produces a moderate performance benefit, the analysis shows that more substantial gains can be realized when this technique is combined with the use of an integrated concurrency-coherency control protocol.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.16604","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=16604","","Control systems;Access protocols;Concurrent computing;Centralized control;Performance analysis;Concurrency control;Costs;Computer buffers;Performance gain;Microprocessors","buffer storage;concurrency control;distributed databases;performance evaluation;protocols;queueing theory","read-write synchronisation;performance analysis;multisystem data sharing;data sharing system structure;shared intermediate memory;buffering;early commit processing;write-serialization;integrated concurrency protocol;queueing model;integrated concurrency-coherency control protocol","","27","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Use of common time base for checkpointing and rollback recovery in a distributed system","P. Ramanathan; K. G. Shin","Dept. of Electr. & Comput. Eng., Wisconsin Univ., Madison, WI, USA; NA","IEEE Transactions on Software Engineering","","1993","19","6","571","583","An approach to checkpointing and rollback recovery in a distributed computing system using a common time base is proposed. A common time base is established in the system using a hardware clock synchronization algorithm. This common time base is coupled with the idea of pseudo-recovery points to develop a checkpointing algorithm that has the following advantages: reduced wait for commitment for establishing recovery lines, fewer messages to be exchanged, and less memory requirement. These advantages are assessed quantitatively by developing a probabilistic model.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232022","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232022","","Checkpointing;Testing;Real time systems;Clocks;Synchronization;Distributed computing;Hardware;Fault tolerant systems;Resumes;NASA","distributed processing;fault tolerant computing;system recovery","message exchange;distributed system;checkpointing;rollback recovery;common time base;hardware clock synchronization algorithm;pseudo-recovery points;recovery lines;memory requirement;probabilistic model","","19","","16","","","","","","IEEE","IEEE Journals & Magazines"
"The Modular Structure of Complex Systems","D. L. Parnas; P. C. Clements; D. M. Weiss","University of Victoria, Victoria, B.C., Canada, and the Computer Science and Systems Branch, U.S. Naval Research Laboratory; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","3","259","266","This paper discusses the organization of software that is inherently complex because of very many arbitrary details that must be precisely right for the software to be correct. We show how the software design technique known as information hiding, or abstraction, can be supplemented by a hierarchically structured document, which we call a module guide. The guide is intended to allow both designers and maintainers to identify easily the parts of the software that they must understand, without reading irrelevant details about other parts of the software. The paper includes an extract from a software module guide to illustrate our proposals.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232209","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702002","Abstract interfaces;information hiding;modular structure of software;software engineering","Software engineering;Software maintenance;Laboratories;Project management;Software design;Data mining;Proposals;Computer industry;Documentation;Application software","","Abstract interfaces;information hiding;modular structure of software;software engineering","","139","","10","","","","","","IEEE","IEEE Journals & Magazines"
"A visual user interface for map information retrieval based on semantic significance","M. Tanaka; T. Ichikawa","Dept. of Inf. Syst., Hiroshima Univ., Japan; Dept. of Inf. Syst., Hiroshima Univ., Japan","IEEE Transactions on Software Engineering","","1988","14","5","666","670","User-interface facilities of a map information system HI-MAP that provide visual feedback to the user are presented. The facilities include semantic panning and zooming, overlaying of thematic maps, etc., and are available through an interactive menu system. HI-MAP retrieves map elements in a specified region on the basis of their relevance and their categorical classification. It has a data structure that includes logical and physical hierarchies for the management of semantic relationships and graphic map elements. The software for implementing these facilities is well modularized, and a variety of interfacing modes can be realized by simple communication between modules. The system contributes toward a reduction of the difficulties in obtaining what is really required from databases.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6144","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6144","","User interfaces;Information retrieval;Image databases;Visual databases;Information systems;Database languages;Feedback;Management information systems;Data structures;Graphics","cartography;database management systems;information retrieval;query languages;user interfaces","visual user interface;map information retrieval;semantic significance;HI-MAP;semantic panning;zooming;overlaying;thematic maps;interactive menu system;data structure;semantic relationships;graphic map elements;databases","","19","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Atomicity Analysis of Service Composition across Organizations","C. Ye; S. C. Cheung; W. K. Chan; C. Xu","The Hong Kong University of Science and Technology, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong; City University of Hong Kong, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong","IEEE Transactions on Software Engineering","","2009","35","1","2","28","Atomicity is a highly desirable property for achieving application consistency in service compositions. To achieve atomicity, a service composition should satisfy the atomicity sphere, a structural criterion for the backend processes of involved services. Existing analysis techniques for atomicity sphere generally assume complete knowledge of all involved backend processes. Such an assumption is invalid when some service providers do not release all details of their backend processes to service consumers outside the organizations. To address this problem, we propose a process algebraic framework to publish atomicity-equivalent public views from the backend processes. These public views extract relevant task properties and reveal only partial process details that service providers need to expose. Our framework enables the analysis of atomicity sphere for service compositions using these public views instead of their backend processes. This allows service consumers to choose suitable services such that their composition satisfies the atomicity sphere without disclosing the details of their backend processes. Based on the theoretical result, we present algorithms to construct atomicity-equivalent public views and to analyze the atomicity sphere for a service composition. Two case studies from supply chain and insurance domains are given to evaluate our proposal and demonstrate the applicability of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4641941","Analysis;Specification;Software and System Safety;System integration and implementation;Formal methods;Model checking;Validation;Analysis;Specification;Software and System Safety;System integration and implementation;Formal methods;Model checking;Validation","Web services;Privacy;System recovery;Supply chains;Protection;Application software;Algorithm design and analysis;Insurance;Algebra;Internet","process algebra;Web services","atomicity sphere analysis;Web service composition;backend process;process algebraic framework;atomicity-equivalent public view;supply chain;insurance","","19","","76","","","","","","IEEE","IEEE Journals & Magazines"
"Introducing Objectcharts or how to use Statecharts in object-oriented design","D. Coleman; F. Hayes; S. Bear","Hewlett-Packard Lab., Bristol, UK; Hewlett-Packard Lab., Bristol, UK; Hewlett-Packard Lab., Bristol, UK","IEEE Transactions on Software Engineering","","1992","18","1","8","18","A notation called Objectcharts for specifying object classes is introduced. An Objectchart diagram is an extended form of a Statechart, which characterizes the behavior of a class as a state machine. The Objectchart transitions correspond to the state-changing methods that the class provides and those that it requires of other classes. Object attributes and observer methods annotate Objectchart states. Firing and postconditions are used to specify the effect of transitions on class attributes. The Objectchart notions is described through the development of an alarm clock application. How Objectcharts can be used to find subtyping inheritance relationships between classes and a systematic approach for evolving Objectchart specifications are shown.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.120312","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=120312","","Design methodology;Object oriented modeling;Software design;Clocks;Information analysis;Cyclic redundancy check;Laboratories;Object oriented programming;Protocols;Encapsulation","data structures;diagrams;formal specification;object-oriented programming","object classes;Statechart;state machine;Objectchart transitions;state-changing methods;observer methods;Objectchart states;postconditions;Objectchart notions;alarm clock application;subtyping inheritance relationships;Objectchart specifications","","85","","16","","","","","","IEEE","IEEE Journals & Magazines"
"A formal specification framework for object-oriented distributed systems","D. Buchs; N. Guelfi","Software Eng. Lab., Swiss Federal Inst. of Technol., Lausanne, Switzerland; NA","IEEE Transactions on Software Engineering","","2000","26","7","635","652","In this paper, we present the Concurrent Object-Oriented Petri Nets (CO-OPN/2) formalism devised to support the specification of large distributed systems. Our approach is based on two underlying formalisms: order-sorted algebra and algebraic Petri nets. With respect to the lack of structuring capabilities of Petri nets, CO-OPN/2 has adopted the object-oriented paradigm. In this hybrid approach (model- and property-oriented), classes of objects are described by means of algebraic Petri nets, while data structures are expressed by order-sorted algebraic specifications. An original feature is the sophisticated synchronization mechanism. This mechanism allows to involve many partners in a synchronization and to describe the synchronization policy. A typical example of distributed systems, namely the Transit Node, is used throughout this paper to introduce our formalism and the concrete specification language associated with it. By successive refinements of the components of the example, we present, informally, most of the notions of CO-OPN/2. We also give some insights about the coordination layer, Context and Objects Interface Language (COIL), which is built on top of CO-OPN/2. This coordination layer is used for the description of the concrete distributed architecture of the system. Together, CO-OPN/2 and COIL provide a complete formal framework for the specification of distributed systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.859532","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859532","","Formal specifications;Petri nets;Concrete;Data structures;Concurrent computing;Distributed processing;Communication systems;Algebra;Object oriented modeling;Specification languages","algebraic specification;Petri nets;object-oriented methods","formal specification;object-oriented distributed systems;Concurrent Object-Oriented Petri Nets;Petri nets;order-sorted algebra;data structures;classes of objects;distributed systems","","21","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Quantifying the Effect of Code Smells on Maintenance Effort","D. I. K. Sjøberg; A. Yamashita; B. C. D. Anda; A. Mockus; T. Dybå","University of Oslo, Oslo; University of Oslo, Oslo; University of Oslo, Oslo; Avaya Labs Research, Basking Ridge; University of Oslo, Oslo and SINTEF, Trondheim","IEEE Transactions on Software Engineering","","2013","39","8","1144","1156","Context: Code smells are assumed to indicate bad design that leads to less maintainable code. However, this assumption has not been investigated in controlled studies with professional software developers. Aim: This paper investigates the relationship between code smells and maintenance effort. Method: Six developers were hired to perform three maintenance tasks each on four functionally equivalent Java systems originally implemented by different companies. Each developer spent three to four weeks. In total, they modified 298 Java files in the four systems. An Eclipse IDE plug-in measured the exact amount of time a developer spent maintaining each file. Regression analysis was used to explain the effort using file properties, including the number of smells. Result: None of the 12 investigated smells was significantly associated with increased effort after we adjusted for file size and the number of changes; Refused Bequest was significantly associated with decreased effort. File size and the number of changes explained almost all of the modeled variation in effort. Conclusion: The effects of the 12 smells on maintenance effort were limited. To reduce maintenance effort, a focus on reducing code size and the work practices that limit the number of changes may be more beneficial than refactoring code smells.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6392174","Maintainability;object-oriented design;product metrics;code churn","Maintenance engineering;Java;Software;Surgery;Time measurement;Context;Electronic mail","Java;regression analysis;software maintenance","code smell effect quantification;maintenance effort;maintainable code;maintenance tasks;Java systems;Java files;Eclipse IDE plug-in;regression analysis;file properties;refused bequest;file size;code size reduction;code smell refactoring","","99","","46","","","","","","IEEE","IEEE Journals & Magazines"
"An authentication logic with formal semantics supporting synchronization, revocation, and recency","S. G. Stubblebine; R. N. Wright","Stubblebine Consulting, Madison, NJ, USA; NA","IEEE Transactions on Software Engineering","","2002","28","3","256","285","Distributed systems inherently involve dynamic changes to the value of security-relevant attributes such as the goodness of encryption keys, trustworthiness of participants, and synchronization between principals. Since concurrent knowledge is usually infeasible or impractical, it is often necessary for the participants of distributed protocols to determine and act on beliefs that may not be supported by the current state of the system. Policies for determining beliefs in such situations can range from extremely conservative, such as only believing statements if they are very recent, to extremely optimistic, such as believing all statements that are not yet known to be revoked. Such security policies often are heavily dependent on timing of received messages and on synchronization between principals. We present a logic for analyzing cryptographic protocols that has the capability to specify time and synchronization details. This capability considerably advances the scope of known techniques both for expressing practical authentication policies of protocol participants as constraints and for reasoning about protocol goals subject to these constraints.","0098-5589;1939-3520;2326-3881","","10.1109/32.991320","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=991320","","Authentication;Logic","formal logic;synchronisation;distributed processing;protocols;cryptography;message authentication","formal methods;authentication logic;security policies;secure authentication;revocation;temporal reasoning;clock synchronization;distributed systems security;cryptographic protocols;public key infrastructures;private key infrastructures","","10","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Algorithms for multidimensional partitioning of static files","D. Rotem; A. Segev","California Univ., Berkeley, CA, USA; California Univ., Berkeley, CA, USA","IEEE Transactions on Software Engineering","","1988","14","11","1700","1710","The problem of multidimensional file partitioning (MDFP) arises in large databases that are subject to frequent range queries on one or more attributes. In an MDFP scheme, the search attribute space is partitioned into cells, which are mapped to physical disk locations. This mapping preserves the order of the search attribute values so that range queries can be answered most efficiently, while maintaining good performance for other types of queries. Recently, MDFP schemes have been suggested to include both dynamic and static file organizations. Optimal and heuristic MDFP algorithms are developed for the static case. The results of extensive computational experiments show that the proposed heuristics perform better than known static ones. It is also shown that incorporating a static algorithm into a dynamic MDFP such as a grid file at conversion and/or periodical reorganization points significantly improves the resulting storage utilization of the data file and decreases the size of the directory file.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9056","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9056","","Partitioning algorithms;Multidimensional systems;Databases;Heuristic algorithms;Frequency;Size measurement;Time measurement;Size control","database management systems;database theory;file organisation","database theory;multidimensional partitioning;static files;multidimensional file partitioning;search attribute space;physical disk locations;range queries;file organizations;static algorithm;storage utilization","","1","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Global analysis and transformations in preprocessed languages","D. Spinellis","Dept. of Manage. Sci. & Technol., Athens Univ. of Econ. & Bus., Greece","IEEE Transactions on Software Engineering","","2003","29","11","1019","1030","Tool support for refactoring code written in mainstream languages such as C and C++ is currently lacking due to the complexity introduced by the mandatory preprocessing phase that forms part of the C/C++ compilation cycle. The definition and use of macros complicates the notions of scope and of identifier boundaries. The concept of token equivalence classes can be used to bridge the gap between the language proper semantic analysis and the non-preprocessed source code. The CScout toolchest uses the developed theory to analyze large interdependent program families. A Web-based interactive front end allows the precise realization of rename and remove refactorings on the original C source code. In addition, CScout can convert programs into a portable obfuscated format or store a complete and accurate representation of the code and its identifiers in a relational database.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245303","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245303","","Humans;Bridges;Relational databases;Reverse engineering;Design methodology;Performance analysis;Tagging;Programming profession;Encapsulation;Automation","equivalence classes;software engineering;reverse engineering;C++ language;relational databases","global analysis;preprocessed languages;tool support;refactoring code;macros;token equivalence classes;semantic analysis;nonpreprocessed source code;CScout toolchest;Web-based interactive front end;relational database;preprocessor;reverse engineering","","21","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Comprehending object and process models: an empirical study","R. Agarwal; P. De; A. P. Sinha","Dept. of Decision & Inf. Technol., Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","1999","25","4","541","556","We report the results of an empirical study comparing user comprehension of object oriented (OO) and process oriented (PO) models. The fundamental difference is that while OO models tend to focus on structure, PO models tend to emphasize behaviour or processes. Proponents of the OO modeling approach argue that it lends itself naturally to the way humans think. However, evidence from research in cognitive psychology and human factors suggests that human problem solving is innately procedural. Given these conflicting viewpoints, we investigate empirically if OO models are in fact easier to understand than PO models. But, as suggested by the theory of cognitive fit, model comprehension may be influenced by task-specific characteristics. We therefore compare OO and PO models based on whether the comprehension activity involves: 1) only structural aspects, 2) only behavioral aspects, or 3) a combination of structural and behavioral aspects. We measure comprehension through subjects' responses to questions designed along these three dimensions. Results show that for most of the simple questions, no significant difference was observed insofar as model comprehension is concerned. For most of the complex questions, however, the PO model was found to be easier to understand than the OO model. In addition to describing the process and the outcomes of the experiments, we present the experimental method employed as a viable approach for conducting research into various phenomena related to the efficacy of alternative systems analysis and design methods. We also identify areas where future research is necessary, along with a recommendation of appropriate research methods for empirical examination.","0098-5589;1939-3520;2326-3881","","10.1109/32.799953","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=799953","","Object oriented modeling;Human factors;Psychology;Problem-solving;System analysis and design;Software testing;System testing;Software systems;Information analysis;Robustness","reverse engineering;object-oriented methods;human factors;cognitive systems;user interfaces;psychology","OO models;empirical study;user comprehension;object oriented models;process oriented models;PO models;OO modeling approach;cognitive psychology;human factors;human problem solving;cognitive fit;model comprehension;task-specific characteristics;comprehension activity;structural aspects;behavioral aspects;experimental method;alternative systems analysis;design methods;empirical examination","","47","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Too Many User-Reviews, What Should App Developers Look at First?","E. Noei; F. Zhang; Y. Zou","Electrical and Computer Engineering, Queen's University, Kingston, Ontario Canada K7L 3N6 (e-mail: e.noei@queensu.ca); School of Computing, Queen's University, Kingston, Ontario Canada K7L 2N8 (e-mail: feng@cs.queensu.ca); Electrical and Computer Enginereing, Queen's University, Kingston, Ontario Canada K7L 3N6 (e-mail: ying.zou@queensu.ca)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Due to the rapid growth in the number of mobile applications (apps) in the past few years, succeeding in mobile app markets has become ruthless. Online app markets, such as Google Play Store, let users rate apps on a five-star scale and leave feedback. Given the importance of high star-ratings to the success of an app, it is crucial to help developers find the key topics of user-reviews that are significantly related to star-ratings of a given category. Having considered the key topics of user-reviews, app developers can narrow down their effort to the user-reviews that matter to be addressed for receiving higher star-ratings. We study 4,193,549 user-reviews of 623 Android apps that were collected from Google Play Store in ten different categories. The results show that few key topics commonly exist across categories, and each category has a specific set of key topics. We also evaluated the identified key topics with respect to the changes that are made to each version of the apps for 19 months. We observed, for 77% of the apps, considering the key topics in the next versions shares a significant relationship with increases in star-ratings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2893171","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8613795","Mobile application;Empirical study;Software release;User-review","Google;Measurement;Crawlers;Natural language processing;Computer bugs;Tools;Filtering","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"QDA-a method for systematic informal program analysis","W. E. Howden; B. Wieand","Dept. of Comput. Sci. & Eng., California Univ., San Diego, La Jolla, CA, USA; NA","IEEE Transactions on Software Engineering","","1994","20","6","445","462","Formal verification of program properties may be infeasible or impractical, and informal analysis may be sufficient. Informal analysis involves the informal acceptance, by inspection, of the validity of program properties or steps in an analysis. Informal analysis may also involve abstraction. Abstraction can be used to eliminate details and concentrate on more general properties. Abstraction will result in informal analysis if it includes the use of undefined properties. A systematic, informal method for analysis called QDA (Quick Defect Analysis) is described. QDA is a comments analysis process based on facts and hypotheses. Facts are used to create an abstract program model, and hypotheses are selected, nonobvious program properties which are identified as needing verification. Hypotheses are proved from the facts that define an abstraction. QDA is hypothesis-driven in the sense that only those parts of an abstraction that are needed to prove hypotheses are created. The QDA approach was applied to a previously well tested operational flight program (OFP). The QDA method and the results of the OFP experiment are presented. The problems of incomplete or unsound informal analysis are analyzed, the relationship of QDA to other analysis methods is discussed, and suggested improvements to the QDA method are described.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.295893","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=295893","","Inspection;Aerospace electronics;Application software;Testing;Programming profession;Maintenance;Formal verification;Costs;Formal specifications;Computer science","program verification;programming theory;formal specification;program debugging;program diagnostics","QDA;systematic informal program analysis;formal verification;program properties;program validity;Quick Defect Analysis;comments analysis;abstract program model;program verification;hypothesis-driven method;operational flight program;specification;code reading","","8","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling software measurement data","B. A. Kitchenham; R. T. Hughes; S. G. Linkman","Dept. of Comput. Sci., Keele Univ., UK; NA; NA","IEEE Transactions on Software Engineering","","2001","27","9","788","804","This paper proposes a method for specifying models of software data sets in order to capture the definitions and relationships among software measures. We believe a method of defining software data sets is necessary to ensure that software data are trustworthy. Software companies introducing a measurement program need to establish procedures to collect and store trustworthy measurement data. Without appropriate definitions it is difficult to ensure data values are repeatable and comparable. Software metrics researchers need to maintain collections of software data sets. Such collections allow researchers to assess the generality of software engineering phenomena. Without appropriate safeguards, it is difficult to ensure that data from different sources are analyzed correctly. These issues imply the need for a standard method of specifying software data sets so they are fully documented and can be exchanged with confidence. We suggest our method of defining data sets can be used as such a standard. We present our proposed method in terms of a conceptual entity-relationship data model that allows complex software data sets to be modeled and their data values stored. The standard can, therefore, contribute both to the definition of a company measurement program and to the exchange of data sets among researchers.","0098-5589;1939-3520;2326-3881","","10.1109/32.950316","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=950316","","Software measurement;Memory;Particle measurements;Data analysis;Software metrics;Software maintenance;Computer Society;Software engineering;Software standards;Data models","software metrics;software engineering;software development management","software measurement data;software data sets;definitions;relationships;software measures;software companies;measurement program;trustworthy measurement data;software metrics;software engineering phenomena;generality;conceptual entity-relationship data model;data values;company measurement program","","64","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Distributed shared abstractions (DSA) on multiprocessors","C. Clemencon; B. Mukherjee; K. Schwan","Integrated Syst. Eng. AG, Zurich, Switzerland; NA; NA","IEEE Transactions on Software Engineering","","1996","22","2","132","152","Any parallel program has abstractions that are shared by the program's multiple processes. Such shared abstractions can considerably affect the performance of parallel programs, on both distributed and shared memory multiprocessors. As a result, their implementation must be efficient, and such efficiency should be achieved without unduly compromising program portability and maintainability. The primary contribution of the DSA library is its representation of shared abstractions as objects that may be internally distributed across different nodes of a parallel machine. Such distributed shared abstractions (DSA) are encapsulated so that their implementations are easily changed while maintaining program portability across parallel architectures. The principal results presented are: a demonstration that the fragmentation of object state across different nodes of a multiprocessor machine can significantly improve program performance; and that such object fragmentation can be achieved without compromising portability by changing object interfaces. These results are demonstrated using implementations of the DSA library on several medium scale multiprocessors, including the BBN Butterfly, Kendall Square Research, and SGI shared memory multiprocessors. The DSA library's evaluation uses synthetic workloads and a parallel implementation of a branch and bound algorithm for solving the traveling salesperson problem (TSP).","0098-5589;1939-3520;2326-3881","","10.1109/32.485223","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=485223","","Libraries;Parallel machines;Topology;Electronic mail;Data structures;Parallel architectures;Computer networks;Concurrent computing;Distributed computing;Workstations","parallel programming;parallel machines;parallel architectures;data structures;software portability;software maintenance;distributed memory systems","distributed shared abstractions;multiprocessors;parallel program;shared memory multiprocessors;parallel architectures;maintainability;DSA library;internally distributed;parallel machine;program portability;object state;multiprocessor machine;object interfaces;medium scale multiprocessors;BBN Butterfly;Kendall Square Research;SGI shared memory multiprocessors;synthetic workloads;parallel implementation;branch and bound algorithm;traveling salesperson problem","","7","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Spatial complexity metrics: an investigation of utility","N. E. Gold; A. M. Mohan; P. J. Layzell","Dept. of Comput. Sci., King's Coll., London, UK; NA; NA","IEEE Transactions on Software Engineering","","2005","31","3","203","212","Software comprehension is one of the largest costs in the software lifecycle. In an attempt to control the cost of comprehension, various complexity metrics have been proposed to characterize the difficulty of understanding a program and, thus, allow accurate estimation of the cost of a change. Such metrics are not always evaluated. This paper evaluates a group of metrics recently proposed to assess the ""spatial complexity"" of a program (spatial complexity is informally defined as the distance a maintainer must move within source code to build a mental model of that code). The evaluation takes the form of a large-scale empirical study of evolving source code drawn from a commercial organization. The results of this investigation show that most of the spatial complexity metrics evaluated offer no substantially better information about program complexity than the number of lines of code. However, one metric shows more promise and is thus deemed to be a candidate for further use and investigation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423992","Index Terms- Maintenance measurement;complexity measures;maintainability;software psychology.","Costs;Software maintenance;Cognitive science;Large-scale systems;Software measurement;Psychology;Gold;Computer Society;Monitoring;Preventive maintenance","software maintenance;software metrics;software cost estimation","software comprehension;software lifecycle;spatial complexity metrics;program complexity;software maintenance;software psychology","","8","","36","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study of the Complex Relationships between Requirements Engineering Processes and Other Processes that Lead to Payoffs in Productivity, Quality, and Risk Management","D. Damian; J. Chisan","NA; NA","IEEE Transactions on Software Engineering","","2006","32","7","433","453","Requirements engineering is an important component of effective software engineering, yet more research is needed to demonstrate the benefits to development organizations. While the existing literature suggests that effective requirements engineering can lead to improved productivity, quality, and risk management, there is little evidence to support this. We present empirical evidence showing how requirements engineering practice relates to these claims. This evidence was collected over the course of a 30-month case study of a large software development project undergoing requirements process improvement. Our findings add to the scarce evidence on RE payoffs and, more importantly, represent an in-depth explanation of the role of requirements engineering processes in contributing to these benefits. In particular, the results of our case study show that an effective requirements process at the beginning of the project had positive outcomes throughout the project lifecycle, improving the efficacy of other project processes, ultimately leading to improvements in project negotiation, project planning, and managing feature creep, testing, defects, rework, and product quality. Finally, we consider the role collaboration had in producing the effects we observed and the implications of this work to both research and practice","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.61","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677531","Requirements engineering;process improvement;process interactions;empirical investigation.","Productivity;Risk management;Software engineering;Programming;Process planning;Quality management;Project management;Creep;Life testing;Collaborative work","formal specification;project management;risk management;software development management;software process improvement;software quality","requirements engineering;software engineering;risk management;software development project;software process improvement;project lifecycle;software quality","","85","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Timing analysis of Ada tasking programs","J. C. Corbett","Dept. of Inf. & Comput. Sci., Hawaii Univ., Honolulu, HI, USA","IEEE Transactions on Software Engineering","","1996","22","7","461","483","Concurrent real-time software is increasingly used in safety-critical embedded systems. Assuring the quality of such software requires the rigor of formal methods. In order to analyze a program formally, we must first construct a mathematical model of its behavior. In this paper, we consider the problem of constructing such models for concurrent real-time software. In particular, we provide a method for building mathematical models of real-time Ada tasking programs that are accurate enough to verify interesting timing properties, and yet abstract enough to yield a tractable analysis on nontrivial programs. Our approach differs from schedulability analysis in that we do not assume that the software has a highly restricted structure (e.g. a set of periodic tasks). Also, unlike most abstract models of real-time systems, we account for essential properties of real implementations, such as resource constraints and run-time overhead.","0098-5589;1939-3520;2326-3881","","10.1109/32.538604","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=538604","","Timing;Real time systems;Mathematical model;Embedded software;Software safety;Embedded system;Software quality;Buildings;Periodic structures;Runtime","Ada;real-time systems;timing;program verification;program diagnostics;multiprogramming;safety-critical software;software quality","timing analysis;schedulability analysis;concurrent real-time software;safety-critical embedded systems;software quality assurance;formal program analysis;mathematical program behaviour model;real-time Ada tasking programs;software structure;resource constraints;run-time overhead;program verification;hybrid systems","","35","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Automated test case generation for programs specified by relational algebra queries","W. T. Tsai; D. Volovik; T. F. Keefe","Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA; Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA; Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA","IEEE Transactions on Software Engineering","","1990","16","3","316","324","Black-box software testing requires test cases to be generated from specifications alone. However, it is impossible to automate the process completely for arbitrary specifications. Specifications are thus restricted to being written entirely in terms of relational algebra expressions. An automated test case generation method is developed for such specifications.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.48939","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=48939","","Automatic testing;Computer aided software engineering;Algebra;Software testing;Programming;Process design;Information retrieval;Computer science;System testing;Automatic generation control","automatic programming;formal specification;program testing;relational databases","black-box software testing;relational algebra queries;arbitrary specifications;relational algebra expressions;automated test case generation method","","36","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Mixed programming metaphors in a shared dataspace model of concurrency","G. -. Roman; H. C. Cunningham","Dept. of Comput. Sci., Washington Univ., Saint Louis, MO, USA; NA","IEEE Transactions on Software Engineering","","1990","16","12","1361","1373","A simple language called Swarm is used as a vehicle for the investigation of the shared dataspace approach to concurrent computation. An important feature of Swarm is its ability to bring a variety of programming paradigms under a single, unified model. In a series of related examples Swarm's capacity to express shared-variable, message-passing, and rule-based computations; to specify synchronous and asynchronous processing modes; and to accommodate highly dynamic program and data structure is explored. Several illustrations make use of a programming construct unique to Swarm, the synchrony relation and explain how this feature can be used to construct dynamically structured, partially synchronous computations. An overview of the Swarm programming notation, an examination of Swarm programming strategies via a series of related example programs, and a discussion of the distinctive features of the shared dataspace model are given. A formal operational model for Swarm is presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.62445","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=62445","","Concurrent computing;Data structures;Computer science;Message passing;Vehicle dynamics;Dynamic programming;Technology management;Parallel processing;Protocols;Information science","data structures;parallel languages;parallel programming","mixed programming metaphors;concurrency;simple language;Swarm;shared dataspace approach;concurrent computation;programming paradigms;unified model;shared-variable;message-passing;rule-based computations;asynchronous processing modes;highly dynamic program;data structure;programming construct;synchrony relation;partially synchronous computations;programming notation;programming strategies;example programs;shared dataspace model;formal operational model","","29","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Fundamentals of deductive program synthesis","Z. Manna; R. Waldinger","Dept. of Comput. Sci., Stanford Univ., Palo Alto, CA, USA; Dept. of Comput. Sci., Stanford Univ., Palo Alto, CA, USA","IEEE Transactions on Software Engineering","","1992","18","8","674","704","An informal tutorial for program synthesis is presented, with an emphasis on deductive methods. According to this approach, to construct a program meeting a given specification, the authors prove the existence of an object meeting the specified conditions. The proof is restricted to be sufficiently constructive, in the sense that, in establishing the existence of the desired output, the proof is forced to indicate a computational method for finding it. That method becomes the basis for a program that can be extracted from the proof. The exposition is based on the deductive-tableau system, a theorem-proving framework particularly suitable for program synthesis. The system includes a nonclausal resolution rule, facilities for reasoning about equality, and a well-founded induction rule.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.153379","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=153379","","Computer science;Military computing;Computer languages;Contracts;Logic functions;Algebra;Artificial intelligence;Programming;Sorting","artificial intelligence;formal specification;inference mechanisms;program testing;theorem proving","deductive program synthesis;specification;proof;deductive-tableau system;theorem-proving framework;nonclausal resolution rule;reasoning;induction rule","","54","","47","","","","","","IEEE","IEEE Journals & Magazines"
"The formal specification of a small bookshop information system","D. Gray","Dept. of Comput. Sci., Queen's Univ., Belfast, UK","IEEE Transactions on Software Engineering","","1988","14","2","263","272","A specification, and its development, for a small bookshop information system are discussed. the specification is presented using mathematics and the scheme calculus of C. Morgan and B. Sufrin (see ibid., vol.SE-10, no.2, p.128-142, 1984). An insight is given into how the specification was developed and why a formal specification is appropriate.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4644","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4644","","Formal specifications;Information systems;Calculus;Mathematics;Set theory;Books;Computer science;Computational geometry;Terminology","programming theory;software engineering","set theory;formal specification;small bookshop information system;scheme calculus","","1","","10","","","","","","IEEE","IEEE Journals & Magazines"
"An Infornation-Theoretic Analysis of Relational Databases—Part I: Data Dependencies and Information Metric","T. T. Lee","Bell Communications Research","IEEE Transactions on Software Engineering","","1987","SE-13","10","1049","1061","Database design is based on the concept of data dependency, which is the interrelationship between data contained in various sets of attributes. In particular, functional, multivalued and acyclic join, dependencies play an essential role in the design of database schemas. The basic definition of an information metric and how this notion can be used in relational database are discussed in this paper. We use Shannon entropy as an information metric to quantify the information associated with a set of attributes. Thus, we prove that data dependencies can be formulated in terms of entropies. These formulas make the numerical computation and testing of data dependencies feasible. Among the different types of data dependencies, the acyclic join dependency is most important to the design of a relational database schema. The acyclic join dependency, with multivalued dependency as a special case, impose a constraint on the information-preserving decomposition of a relation. It is interesting that this constraint on a relation is similar to Gibbs' condition for separating physical systems in statistical mechanics. They both assert that entropy is preserved during the decomposition process. That is, the entropies of the corresponding set of attributes must satisfy the inclusion–exclusion identity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232847","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702145","Acyclic join dependency;entropy;functional dependency;information-preserving decomposition;multivalued dependency","Data analysis;Information analysis;Relational databases;Entropy;Data models;Testing;Set theory;Sufficient conditions;Next generation networking","","Acyclic join dependency;entropy;functional dependency;information-preserving decomposition;multivalued dependency","","9","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Toward an architectural knowledge base for wireless service engineering","E. Niemela; J. Kalaoja; P. Lago","VTT Tech. Res. Centre of Finland, Oulu, Finland; VTT Tech. Res. Centre of Finland, Oulu, Finland; NA","IEEE Transactions on Software Engineering","","2005","31","5","361","379","Wireless services are software-based services that exploit distribution infrastructure embedded in our everyday life as various communication and computing technologies. Service architecture defines concepts and principles to develop and maintain services to obtain the quality issues with minimum cost and faster time-to-market. In order to boost the development of wireless services, more effective means of using existing architectural know-how and artifacts are required. Our contribution is the architectural knowledge base that introduces three cornerstones: the service taxonomy, reference service architecture, and basic services that alt together provide an efficient means of creating added value with wireless services. The service taxonomy assists in identifying the required functional and quality properties of services and the constraints of the underlying technology platforms. The reference architecture realizes the required properties, based on a selected set of architectural styles and patterns, and provides a skeleton upon which a new end-user service can be developed faster and more easily by using partially ready-made solutions, and furthermore, to keep the architectural knowledge base evolving at the same time. The architectural knowledge base has been validated in several research projects with industrial companies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1438373","Index Terms- Service architecture;reference architecture;quality attribute;service category;wireless service.","Knowledge engineering;Application software;Computer architecture;Middleware;Open source software;Pervasive computing;Maintenance engineering;Time to market;Taxonomy;Web and internet services","software architecture;knowledge based systems;software quality;mobile computing;time to market;middleware","architectural knowledge base;wireless service engineering;software-based service;quality issue;time-to-market;service taxonomy;reference service architecture;quality attribute;service category","","15","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Performance Analysis of Disk Modulo Allocation Method for Cartesian Product Files","Y. Y. Sung","School of Electrical Engineering and Computer Science, University of Oklahoma","IEEE Transactions on Software Engineering","","1987","SE-13","9","1018","1026","Cartesian product files have been shown to exhibit attractive properties for partial match queries. The Disk Modulo (DM) allocation method is shown to have good performance on the distribution of Cartesian product files into an m-disk system. However, there was no explicit expression made before to represent the DM method's response time to a given partial match query. In this paper, based upon discrete Fourier transform, we derive one formula for such a computation. After obtaining this representation, the performance characteristics of the DM method can now be given an analytic interpretation. Some theoretical results are derived from this formula. We also use our formula to analyze the performance of several popular Disk Modulo algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233524","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702324","Cartesian product files;discrete Fourier transform;disk modulo methods;partial match queries;performance","Performance analysis;Delta modulation;Delay;Discrete Fourier transforms;Algorithm design and analysis;File systems;Information retrieval;Concurrent computing;Computer science","","Cartesian product files;discrete Fourier transform;disk modulo methods;partial match queries;performance","","3","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Design synthesis from interaction and state-based specifications","J. Sun; J. S. Dong","Dept. of Comput. Sci., Nat. Univ. of Singapore, Singapore; Dept. of Comput. Sci., Nat. Univ. of Singapore, Singapore","IEEE Transactions on Software Engineering","","2006","32","6","349","364","Interaction-based and state-based modeling are two complementary approaches of behavior modeling. The former focuses on global interactions between system components. The latter concentrates on the internal states of individual components. Both approaches have been proven useful in practice. One challenging and important research objective is to combine the modeling power of both effectively and then use the combination as the basis for automatic design synthesis. We present a combination of interaction-based and state-based modeling, namely, live sequence charts and Z, for system specification. We then propose a way of generating distributed design from the combinations. Our approach handles systems with intensive interactive behaviors as well as complex state structures","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650212","Z language;live sequence charts;specification;synthesis.","Power system modeling;Automata;Sun;Distributed power generation;System testing;Large-scale systems;Data structures;State-space methods;Explosions;Open systems","charts;formal specification;object-oriented programming;specification languages","automatic design synthesis;interaction-based modeling;state-based modeling;behavior modeling;live sequence charts;system specification;Z language","","32","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Scheduling processes with release times, deadlines, precedence and exclusion relations","J. Xu; D. L. Parnas","Dept. of Comput. Sci., York Univ., North York, Ont., Canada; NA","IEEE Transactions on Software Engineering","","1990","16","3","360","369","An algorithm that finds an optimal schedule on a single processor for a given set of processes is presented. Each process starts executing after its release time and completes its computation before its deadline and a given set of precedence relations and exclusion relations defined on ordered pairs of process segments are satisfied. This algorithm can be applied to the important and previously unsolved problem of automated pre-run-time scheduling of processes with arbitrary precedence and exclusion in hard-real-time systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.48943","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=48943","","Optimal scheduling;Scheduling algorithm;Processor scheduling;Real time systems;Algorithm design and analysis;Councils;Timing;Runtime","optimisation;scheduling;search problems","optimal schedule;single processor;release time;precedence relations;exclusion relations;ordered pairs;process segments;automated pre-run-time scheduling;arbitrary precedence;hard-real-time systems","","192","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental Maintenance of Software Artifacts","S. P. Reiss","NA","IEEE Transactions on Software Engineering","","2006","32","9","682","697","Software is multidimensional, but the tools that support it are not. This lack of tool support causes the software artifacts representing different dimensions to evolve independently and to become inconsistent over time. In order to properly support the evolution of software, one must ensure that the different dimensions evolve concurrently. We have built a software development tool, CLIME that uses constraints implemented as database queries to ensure just this. Our approach makes the tool responsible for detecting inconsistencies between software design, specifications, documentation, source code, test cases, and other artifacts without requiring any of these to be a primary representation. The tool works incrementally as the software evolves, without imposing a particular methodology or process. It includes a front end that lets the user explore and fix current inconsistencies. This paper describes the basis for CLIME, the techniques underlying the tool, the interface provided to the programmer, the incremental maintenance of constraints between these artifacts, and our experiences","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707667","Software maintenance;evolution;programming tools.","Software maintenance;Documentation;Programming profession;Software systems;System testing;Multidimensional systems;Software tools;Software testing;Databases;Software design","software maintenance;software tools","incremental software maintenance;software artifact;software evolution;software development tool;database query","","19","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Privilege transfer and revocation in a port-based system","K. Ramamritham; D. Stemple; D. A. Briggs; S. Vinter","Department of Computer and Information Science, University of Massachusetts, Amherst, MA 01003; Department of Computer and Information Science, University of Massachusetts, Amherst, MA 01003; Department of Mathematics and Computer Science, University of Southern Maine, Portland, ME 04104; BBN Laboratories, Cambridge, MA 02238","IEEE Transactions on Software Engineering","","1986","SE-12","5","635","648","Gutenberg is a port-based operating system being designed to study protection issues in distributed systems. All shared resources are viewed as protected objects and hence can be assessed only via specific operations defined on them. Processes communicate and access objects through the use of ports. Each port is associated with an abstract data type operation and can be created by a process only if the process has the capability to execute the operation on the type. Thus, a port represents the privilege of the port's client process to request a service. Capabilities to create ports for requesting operations are contained in a capability directory, which is navigated by processes to gain these capabilities. Privilege transfer is a means of providing servers access to the resources they need to perform their services. In Gutenberg, privilege transfer is accomplished by allowing access to subdirectories of the capability directory and by passing capabilities, including port access capabilities, to processes via ports. It should be possible to revoke transferred privileges when breaches of trust are detected or suspected, when a period of time has passed beyond which the distributor of a privilege does not want the privilege shared, or when an error has been detected.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312959","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312959","Interprocess communication;operating systems;privilege transfer;protection;revocation","Kernel;Servers;Bibliographies;Transient analysis;Abstracts","distributed processing;operating systems (computers);security of data","port-based system;Gutenberg;operating system;protection issues;distributed systems;abstract data type operation;capability directory;privilege transfer;port access","","2","","","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic scheduling of hard real-time tasks and real-time threads","K. Schwan; H. Zhou","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Transactions on Software Engineering","","1992","18","8","736","748","The authors investigate the dynamic scheduling of tasks with well-defined timing constraints. They present a dynamic uniprocessor scheduling algorithm with an O(n log n) worst-case complexity. The preemptive scheduling performed by the algorithm is shown to be of higher efficiency than that of other known algorithms. Furthermore, tasks may be related by precedence constraints, and they may have arbitrary deadlines and start times (which need not equal their arrival times). An experimental evaluation of the algorithm compares its average case behavior to the worst case. An analytic model used for explanation of the experimental results is validated with actual system measurements. The dynamic scheduling algorithm is the basis of a real-time multiprocessor operating system kernel developed in conjunction with this research. Specifically, this algorithm is used at the lowest, threads-based layer of the kernel whenever threads are created.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.153383","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=153383","","Dynamic scheduling;Real time systems;Scheduling algorithm;Timing;Operating systems;Performance analysis;Kernel;Algorithm design and analysis;Switches","computational complexity;network operating systems;real-time systems;scheduling","hard real-time tasks;real-time threads;dynamic scheduling;timing constraints;worst-case complexity;preemptive scheduling;precedence constraints;real-time multiprocessor operating system kernel","","55","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Lightweight Assessment of Test-Case Effectiveness using Source-Code-Quality Indicators","G. Grano; F. Palomba; H. C. Gall","Department of Informatics, University of Zurich, Zurich, Zurich Switzerland (e-mail: grano@ifi.uzh.ch); Department of Informatics, University of Zurich, Zurich, Zurich Switzerland 8050 (e-mail: palomba@ifi.uzh.ch); Department of Informatics, University of Zurich, Zurich, Zurich Switzerland 8050 (e-mail: gall@ifi.uzh.ch)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Test cases are crucial to help developers preventing the introduction of software faults. Unfortunately, not all the tests are properly designed or can effectively capture faults in production code. Some measures have been defined to assess test-case effectiveness: the most relevant one is the mutation score, which highlights the quality of a test by generating the so-called mutants, ie variations of the production code that make it faulty and that the test is supposed to identify. However, previous studies revealed that mutation analysis is extremely costly and hard to use in practice. The approaches proposed by researchers so far have not been able to provide practical gains in terms of mutation testing efficiency. This leaves the problem of efficiently assessing test-case effectiveness as still open. In this paper, we investigate a novel, orthogonal, and lightweight methodology to assess test-case effectiveness: in particular, we study the feasibility to exploit production and test-code-quality indicators to estimate the mutation score of a test case. We firstly select a set of 67 factors and study their relation with test-case effectiveness. Then, we devise a mutation score estimation model exploiting such factors and investigate its performance as well as its most relevant features. The key results of the study reveal that our estimation model only based on static features has 86% of both F-Measure and AUC-ROC. This means that we can estimate the test-case effectiveness, using source-code-quality indicators, with high accuracy and without executing the tests. As a consequence, we can provide a practical approach that is beyond the typical limitations of current mutation testing techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2903057","Schweizerischer Nationalfonds zur Frderung der Wissenschaftlichen Forschung; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8658120","Automated Software Testing;Mutation Testing;Software Quality","Testing;Production;Estimation;Measurement;Predictive models;Machine learning;Computational modeling","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Tracking Load-Time Configuration Options","M. Lillack; C. Kästner; E. Bodden","University of Leipzig, Leipzig, Germany; Carnegie Mellon University, Pittsburgh, PA; Paderborn University & Fraunhofer IEM, Paderborn, Germany","IEEE Transactions on Software Engineering","","2018","44","12","1269","1291","Many software systems are highly configurable, despite the fact that configuration options and their interactions make those systems significantly harder to understand and maintain. In this work, we consider load-time configuration options, such as parameters from the command-line or from configuration files. They are particularly hard to reason about: tracking configuration options from the point at which they are loaded to the point at which they influence control-flow decisions is tedious and error-prone, if done manually. We design and implement Lotrack, an extended static taint analysis to track configuration options automatically. Lotrack derives a configuration map that explains for each code fragment under which configurations it may be executed. An evaluation on Android apps and Java applications from different domains shows that Lotrack yields high accuracy with reasonable performance. We use Lotrack to empirically characterize how much of the implementation of Android apps depends on the platform's configuration options or interactions of these options.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2756048","German Federal Ministry of Education and Research; US National Science Foundation; Science of Security Lablet; AFRL and DARPA; German Research Foundation (DFG); Heinz Nixdorf Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8049300","Variability mining;configuration options;static analysis","Androids;Humanoid robots;Java;Static analysis;Bluetooth;Data mining","Java;mobile computing","configuration map;Lotrack;tracking load-time configuration options;configuration files;tracking configuration options","","1","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Rapid application of lightweight formal methods for consistency analyses","M. S. Feather","Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA","IEEE Transactions on Software Engineering","","1998","24","11","949","959","Lightweight formal methods promise to yield modest analysis results in an extremely rapid manner. To fulfil this promise, they must be able to work with existing information sources, be able to analyze for manifestly desirable properties, be highly automated (especially if dealing with voluminous amounts of information), and be readily customizable and flexible in the face of emerging needs and understanding. Two pilot studies investigate the feasibility of lightweight formal methods that employ a database as the underlying reasoning engine to perform the analyses. The first study concerns aspects of software module interfaces, while the second concerns test logs' adherence to required and expected conditions.","0098-5589;1939-3520;2326-3881","","10.1109/32.730544","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=730544","","Information analysis;Performance analysis;Engines;Software testing;Application software;Failure analysis;Transaction databases;Feathers;System testing;Data analysis","formal verification;inference mechanisms;subroutines;database management systems","lightweight formal methods;consistency analyses;information sources;manifestly desirable properties;highly automated methods;customizable methods;flexible methods;emerging needs;reasoning engine;software module interfaces;required conditions;expected conditions;consistency checking;interface checking;test-log checking;database-based analysis;NASA","","21","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Stochastic automata network of modeling parallel systems","B. Plateau; K. Atif","IMAG-Groupe Calcul Parallele, Grenoble, France; IMAG-Groupe Calcul Parallele, Grenoble, France","IEEE Transactions on Software Engineering","","1991","17","10","1093","1108","A methodology for modeling a system composed of parallel activities with synchronization points is proposed. Specifically, an approach based on a modular state-transition representation of a parallel system called the stochastic automata network (SAN) is developed. The state-space explosion is handled by a decomposition technique. The dynamic behavior of the algorithm is analyzed under Markovian assumptions. The transition matrix of the chain is automatically derived using tensor algebra operators, under a format which involves a very limited storage cost.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.99196","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=99196","","Stochastic systems;Automata;Storage area networks;Explosions;Algorithm design and analysis;Matrix decomposition;Tensile stress;Algebra;Storage automation;Costs","parallel algorithms;parallel architectures;performance evaluation;stochastic automata","parallel activities;synchronization points;modular state-transition representation;parallel system;stochastic automata network;SAN;state-space explosion;decomposition technique;Markovian assumptions;transition matrix;tensor algebra operators;storage cost","","125","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing and improving state-based class testing: a series of experiments","L. C. Briand; M. Di Penta; Y. Labiche","Dept. of syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2004","30","11","770","783","This work describes an empirical investigation of the cost effectiveness of well-known state-based testing techniques for classes or clusters of classes that exhibit a state-dependent behavior. This is practically relevant as many object-oriented methodologies recommend modeling such components with statecharts which can then be used as a basis for testing. Our results, based on a series of three experiments, show that in most cases state-based techniques are not likely to be sufficient by themselves to catch most of the faults present in the code. Though useful, they need to be complemented with black-box, functional testing. We focus here on a particular technique, Category Partition, as this is the most commonly used and referenced black-box, functional testing technique. Two different oracle strategies have been applied for checking the success of test cases. One is a very precise oracle checking the concrete state of objects whereas the other one is based on the notion of state invariant (abstract states). Results show that there is a significant difference between them, both in terms of fault detection and cost. This is therefore an important choice to make that should be driven by the characteristics of the component to be tested, such as its criticality, complexity, and test budget.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359770","Index Terms- State-based testing;testing experimentation;UML statecharts;category partition.","Object oriented modeling;System testing;Costs;Fault detection;Unified modeling language;Computer Society;Concrete;Phase detection;Guidelines;Performance evaluation","Unified Modeling Language;object-oriented methods;formal verification;program debugging;charts;program testing","state-based class testing;state-dependent behavior;object-oriented methodology;functional testing;Category Partition;fault detection;UML statechart","","67","","44","","","","","","IEEE","IEEE Journals & Magazines"
"The SEXTANT Software Exploration Tool","T. Schafer; M. Eichberg; M. Haupt; M. Mezini","Software Technology Group, Darmstadt University of Technology, Hochschultsr. 10, 64289 Darmstadt, Germany; Software Technology Group, Darmstadt University of Technology, Hochschultsr. 10, 64289 Darmstadt, Germany; Software Technology Group, Darmstadt University of Technology, Hochschultsr. 10, 64289 Darmstadt, Germany; Software Technology Group, Darmstadt University of Technology, Hochschultsr. 10, 64289 Darmstadt, Germany","IEEE Transactions on Software Engineering","","2006","32","9","753","768","In this paper, we discuss a set of functional requirements for software exploration tools and provide initial evidence that various combinations of these features are needed to effectively assist developers in understanding software. We observe that current tools for software exploration only partly support these features. This has motivated the development of SEXTANT, a software exploration tool tightly integrated into the Eclipse IDE that has been developed to fill this gap. By means of case studies, we demonstrate how the requirements fulfilled by SEXTANT are conducive to an understanding needed to perform a maintenance task","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.94","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707671","Software exploration;program comprehension;reverse engineering;software maintenance;software visualization.","Software tools;Visualization;Navigation;Software maintenance;Cognition;Computer architecture;Reverse engineering","formal specification;program visualisation;programming environments;reverse engineering;software maintenance","functional requirement;SEXTANT software exploration tool;software understanding;Eclipse IDE;software maintenance;program comprehension;reverse engineering;software visualization","","9","","60","","","","","","IEEE","IEEE Journals & Magazines"
"FlowTalk: Language Support for Long-Latency Operations in Embedded Devices","A. Bergel; W. Harrison; V. Cahill; S. Clarke","University of Chile, Santiago; Software Structure Group; Lero and Trinity College Dublin, Ireland; Lero and Trinity College Dublin, Ireland","IEEE Transactions on Software Engineering","","2011","37","4","526","543","Wireless sensor networks necessitate a programming model different from those used to develop desktop applications. Typically, resources in terms of power and memory are constrained. C is the most common programming language used to develop applications on very small embedded sensor devices. We claim that C does not provide efficient mechanisms to address the implicit asynchronous nature of sensor sampling. C applications for these devices suffer from a disruption in their control flow. In this paper, we present FlowTalk, a new object-oriented programming language aimed at making software development for wireless embedded sensor devices easier. FlowTalk is an object-oriented programming language in which dynamicity (e.g., object creation) has been traded for a reduction in memory consumption. The event model that traditionally comes from using sensors is adapted in FlowTalk with controlled disruption, a light-weight continuation mechanism. The essence of our model is to turn asynchronous long-latency operations into synchronous and blocking method calls. FlowTalk is built for TinyOS and can be used to develop applications that can fit in 4 KB of memory for a large number of wireless sensor devices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5492692","Embedded systems;object-based programming.","Sampling methods;Object oriented modeling;Wireless sensor networks;Application software;Computer languages;Object oriented programming;Java;Biosensors;Automotive engineering;Embedded software","C language;embedded systems;intelligent sensors;object-oriented languages;object-oriented programming;software engineering;wireless sensor networks","FlowTalk;language support;wireless sensor networks;programming language;C language;embedded sensor devices;sensor sampling;object-oriented programming language;memory consumption;light-weight continuation mechanism;asynchronous long-latency operations;TinyOS;memory size 4 KByte","","","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Defining and validating measures for object-based high-level design","L. C. Briand; S. Morasca; V. R. Basili","Fraunhofer-Inst. for Exp. Software Eng., Kaiserslautern, Germany; NA; NA","IEEE Transactions on Software Engineering","","1999","25","5","722","743","The availability of significant measures in the early phases of the software development life-cycle allows for better management of the later phases, and more effective quality assessment when quality can be more easily affected by preventive or corrective actions. We introduce and compare various high-level design measures for object-based software systems. The measures are derived based on an experimental goal, identifying fault-prone software parts, and several experimental hypotheses arising from the development of Ada systems for Flight Dynamics Software at the NASA Goddard Space Flight Center (NASA/GSFC). Specifically, we define a set of measures for cohesion and coupling, which satisfy a previously published set of mathematical properties that are necessary for any such measures to be valid. We then investigate the measures' relationship to fault-proneness on three large scale projects, to provide empirical support for their practical significance and usefulness.","0098-5589;1939-3520;2326-3881","","10.1109/32.815329","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=815329","","Software measurement;NASA;Phase measurement;Programming;Quality management;Software development management;Quality assessment;Software systems;Fault diagnosis;Large-scale systems","object-oriented programming;object-oriented languages;Ada;software quality;software fault tolerance;aerospace computing","object-based high-level design;software development life-cycle;software quality assessment;high-level design;fault-prone software;Ada systems;Flight Dynamics Software;NASA Goddard Space Flight Center","","90","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Creating Rich and Representative Personas by Discovering Affordances","M. Mesgari; C. Okoli; A. Ortiz de Guinea","Management and Entrepreneurship, Elon University, Elon, North Carolina United States (e-mail: mmesgari@elon.edu); SKEMA Business School, Universit&#x00E9; C&#x00F4;te d&#x0027;Azur - SKEMA, Paris, Paris France (e-mail: Chitu.Okoli@skema.edu); Strategy and Information Systems, Universidad de Deusto, 16392 Bilbao, Pais Vasco Spain (e-mail: ana.ortiz-de-guinea@hec.ca)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","During the last decade, information system designers have used the persona technique to put user needs and preferences at the center of all development decisions. Persona development teams draw on qualitative data, quantitative data or a combination of both to develop personas that are representative of the target users. Despite the benefits of both approaches, qualitative methods are limited by the cognitive capabilities of the experts, whereas quantitative methods lack contextual richness. To gain the advantages of both approaches, this article suggests a mixed qualitative-quantitative approach to create user personas based on the patterns of the affordances they actualize rather than merely the actions they take. It enriches personas by referring to the purposes fulfilled through affordance actualizations, and it grounds personas in readily available objective log data. This study illustrates the practical value of the proposed methodology by empirically creating personas based on real user data. Furthermore, it demonstrates its value by having practitioners compare the suggested method to that of qualitative-only and quantitative-only methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2826537","Concordia University; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8337801","Personas;affordances;mixed qualitative and quantitative methods;user modeling;interview;card sorting;cluster analysis;systems design and implementation;design and evaluation of IT infrastructure;questionnaire surveys","Software;Maintenance engineering;Task analysis;Aging;Interviews;Human computer interaction","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Automated Synthesis of Mediators to Support Component Interoperability","A. Bennaceur; V. Issarny","Department of Computing, The Open University, United Kingdom; MiMove team, Inria, France","IEEE Transactions on Software Engineering","","2015","41","3","221","240","Interoperability is a major concern for the software engineering field, given the increasing need to compose components dynamically and seamlessly. This dynamic composition is often hampered by differences in the interfaces and behaviours of independently-developed components. To address these differences without changing the components, mediators that systematically enforce interoperability between functionally-compatible components by mapping their interfaces and coordinating their behaviours are required. Existing approaches to mediator synthesis assume that an interface mapping is provided which specifies the correspondence between the operations and data of the components at hand. In this paper, we present an approach based on ontology reasoning and constraint programming in order to infer mappings between components' interfaces automatically. These mappings guarantee semantic compatibility between the operations and data of the interfaces. Then, we analyse the behaviours of components in order to synthesise, if possible, a mediator that coordinates the computed mappings so as to make the components interact properly. Our approach is formally-grounded to ensure the correctness of the synthesised mediator. We demonstrate the validity of our approach by implementing the MICS (Mediator synthesis to Connect Components) tool and experimenting it with various real-world case studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2364844","ERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6936339","Interoperability;Constraint Programming;Automated Synthesis;Mediators;Protocols;Interoperability;constraint programming;automated synthesis;mediators;protocols","Ontologies;Semantics;Google;Interoperability;Cognition;Programming;Protocols","object-oriented programming;ontologies (artificial intelligence);open systems","automated mediator synthesis;component interoperability;dynamic composition;functionally compatible component;interface mapping;ontology reasoning;constraint programming;component interface;semantic compatibility;MICS;mediator synthesis to connect components","","14","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Some Empirical Observations on Program Behavior with Applications to Program Restructuring","J. B. Peachey; R. B. Bunt; C. J. Colbourn","Department of Agricultural Economics, University of Saskatchewan; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","2","188","193","The dynamic behavior of executing programs is a significant factor in the performance of virtual memory computer systems. Program restructuring attempts to improve the behavior of programs by reorganizing their object code to account for the characteristics of the virtual memory environment. A significant component of the restructuring process involves a restructuring graph. An analysis of restructuring graphs of typical programs found edge weights to be distributed in a Bradford–Zipf fashion, implying that a large fraction of total edge weight is concentrated in relatively few edges. This empirical observation can be used to improve the clustering phase of program restructuring, by limiting consideration to edges of large weight. We consider the effect of this improved clustering in the restructuring process by examining various means of restructuring some typical programs. In our experiments, 95 percent of the total edge value is typically accounted for by 50–60 percent of the edges. For naive clustering algorithms, clustering time is therefore typically halved; for more sophisticated methods, more substantial savings result. Finally, clustering with 95 percent of total edge value typically results in only a small decay in performance measures such as number of page faults and average working set size.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232193","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701986","Bradford–Zipf distribution;clustering;paging;program behavior;program restructuring","Application software;Clustering algorithms;Size measurement;Guidelines;Programming profession;Agricultural engineering;Councils;Economic forecasting;Environmental economics","","Bradford–Zipf distribution;clustering;paging;program behavior;program restructuring","","3","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Communication metrics for software development","A. H. Dutoit; B. Bruegge","Inst. fur Inf., Tech. Univ. Munchen, Germany; NA","IEEE Transactions on Software Engineering","","1998","24","8","615","628","Presents empirical evidence that metrics on communication artifacts generated by groupware tools can be used to gain significant insight into the development process that produced them. We describe a test-bed for developing and testing communication metrics, a senior-level software engineering project course at Carnegie Mellon University, in which we conducted several studies and experiments from 1991-1996 with more than 400 participants. Such a test-bed is an ideal environment for empirical software engineering, providing sufficient realism while allowing for controlled observation of important project parameters. We describe three proof-of-concept experiments to illustrate the value of communication metrics in software development projects. Finally, we propose a statistical framework based on structural equations for validating these communication metrics.","0098-5589;1939-3520;2326-3881","","10.1109/32.707697","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=707697","","Programming;Software testing;Software engineering;Equations;Collaborative software;Collaborative work;Communication system control;Software tools;Context;Computer Society","software metrics;groupware;statistics;equations;educational courses;computer science education","communication metrics;software development;communication artifacts;groupware tools;senior-level software engineering project course;Carnegie Mellon University;empirical software engineering;project parameter controlled observation;statistical framework;structural equations;validation","","15","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Using a concept lattice of decomposition slices for program understanding and impact analysis","P. Tonella","Centro per la Ricerca Sci. e Tecnologica, Trento, Italy","IEEE Transactions on Software Engineering","","2003","29","6","495","509","The decomposition slice graph and concept lattice are two program representations used to abstract the details of code into a higher-level view of the program. The decomposition slice graph partitions the program into computations performed on different variables and shows the dependence relation between computations, holding when a computation needs another computation as a building block. The concept lattice groups program entities which share common attributes and organizes such groupings into a hierarchy of concepts, which are related through generalizations/specializations. This paper investigates the relationship existing between these two program representations. The main result of this paper is a novel program representation, called concept lattice of decomposition slices, which is shown to be an extension of the decomposition slice graph, and is obtained by means of concept analysis, with additional nodes associated with weak interferences between computations, i.e., shared statements which are not decomposition slices. The concept lattice of decomposition slices can be used to support software maintenance by providing relevant information about the computations performed by a program and the related dependences/interferences, as well as by representing a natural data structure on which to conduct impact analysis. Preliminary results on small to medium size code support the applicability of this method at the intraprocedural level or when investigating the dependences among small groups of procedures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205178","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205178","","Lattices;Interference;Software maintenance;Data structures;Performance analysis;Information analysis;Application software","reverse engineering;program slicing;software maintenance","program representations;program understanding;impact analysis;concept lattice;decomposition slice graph;dependence relation;common attributes;generalizations;specializations;weak interferences;shared statements;software maintenance;data structure;intraprocedural level","","89","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Software process representation and analysis for framework instantiation","T. C. Oliveira; I. M. Filho; C. J. P. de Lucena; P. S. C. Alencar; D. D. Cowan","Dept. de Inf., Pontificia Univ. Catolica do Rio de Janeiro, Brazil; Dept. de Inf., Pontificia Univ. Catolica do Rio de Janeiro, Brazil; Dept. de Inf., Pontificia Univ. Catolica do Rio de Janeiro, Brazil; NA; NA","IEEE Transactions on Software Engineering","","2004","30","3","145","159","Object-oriented frameworks are currently regarded as a promising technology for reusing designs and implementations. However, developers find there is still a steep learning curve when extracting the design rationale and understanding the framework documentation during framework instantiation. Thus, instantiation is a costly process in terms of time, people, and other resources. These problems raise a number of questions including: ""How can we raise the level of abstraction in which the framework instantiation is expressed, reasoned about and implemented?"" ""How can the same high-level design abstractions that were used to develop the framework be used during framework instantiation instead of using source code as is done currently?"" ""How can we define extended design abstractions that can allow framework instantiation to be explicitly represented and validated?"" We present an approach to framework instantiation based on software processes that addresses these issues. Our main goal is to represent the framework design models in an explicit and declarative way, and support changes to this design based on explicit instantiation tasks based on software processes while maintaining system integrity, invariants, and general constraints. In this way, the framework instantiation can be performed in a valid and controlled way.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271169","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271169","","Documentation;Productivity;Object oriented modeling;Software maintenance;Software design;Programming;Design methodology;Natural languages;Unified modeling language","software reusability;software process improvement;formal specification;object-oriented methods","software process representation;software analysis;framework instantiation;object-oriented framework;system integrity;software design;formal specification;lightweight analysis;design analysis","","12","","55","","","","","","IEEE","IEEE Journals & Magazines"
"On the File Design Problem for Partial Match Retrieval","Hung-Chang Du","Department of Computer Science, University of Minnesota","IEEE Transactions on Software Engineering","","1985","SE-11","2","213","222","In the past two decades, the increasing usage of databases and integrated information systems has encouraged the development of file structures suited for partial match retrieval. A partial match query is a query with some number of attributes specified and the rest of them unspecified. One interesting file structure proposed and heavily studied recently is called a multikey hashing scheme, but most of the previous results on designing optimal multikey hashing schemes ignored the record distribution of a file. In this paper we show that the problem of designing an optimal multikey hashing scheme taking into consideration the record distribution is computationally intractable (NP-hard). Therefore, a heuristic approach is necessary. In a multikey hashing scheme, although the directory is space efficient and the search algorithm is fast, due to the insufficient information in the directory some accessed buckets may not contain any record satisfying the given query. Thus, certain retrieval effort is wasted. A new class of file structures which combine a multikey hashing scheme and an indexed descriptor technique is introduced in this paper. By adding some extra information (either record descriptors or bucket descriptors) into the directory of a multikey hashing scheme, either only those buckets which contain at least one record satisfying the given query need to be accessed or the number of accessed buckets which do not contain any record satisfying the query is reduced.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232197","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701990","Database design;file structures;multikey hashing;NP-hard;partial match retrieval","Information retrieval;Database systems;Information systems;Distributed computing;Magnetic devices;Computer science;Terminology","","Database design;file structures;multikey hashing;NP-hard;partial match retrieval","","","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Characterizing Communication Channel Deadlocks in Sequence Diagrams","B. Mitchell","University of Surrey, Guilford","IEEE Transactions on Software Engineering","","2008","34","3","305","320","UML sequence diagrams (SDs) are a mainstay of requirements specifications for communication protocols. Mauw and Reniers' algebraic (MRA) semantics formally specifies a behavior for these SDs that guarantees deadlock-free processes. Practitioners commonly use communication semantics that differ from MRA, which may result in deadlocks, for example, FIFO, token ring, etc. We define a process algebra that is an extension of the MRA semantics for regular SDs. Our algebra can describe several commonly used communication semantics. Regular SDs are constructed from concurrent message flows via iteration, branching, and sequential composition. Their behavior is defined in terms of a set of partial orders on the events in the SD. Such partial orders are known as causal orders. We define partial order theoretic properties of a causal order that are particular kinds of race condition. We prove that any of the common communication semantics that we list either guarantees deadlock-free SDs or can result in a deadlock if and only if a causal order of an SD contains one of these types of race condition. This describes a complete classification of deadlocks as specific types of race condition.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4509440","Requirements Analysis;Formal methods;Distributed programming;Distributed networks;Protocol verification;Requirements Analysis;Formal methods;Distributed programming;Distributed networks;Protocol verification","Communication channels;System recovery;Token networks;Telecommunication standards;Protocols;Algebra;Automotive engineering;Unified modeling language;Manufacturing industries;Vehicle dynamics","formal specification;process algebra;system recovery;Unified Modeling Language","communication channel deadlocks;sequence diagrams;UML;communication protocols;algebraic semantics;deadlock-free processes;iteration;branching;sequential composition","","12","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Measuring Errors in Operational Analysis Assumptions","N. M. Bengtson","Department of Computer Science, North Carolina State University","IEEE Transactions on Software Engineering","","1987","SE-13","7","767","776","Operational analysis, an area of study first defined in the computer science field, has been used in the analysis of systems performance. System performance measures for a specific set of output data are obtained using operational analysis formulas derived from assumptions which are verifiable by the observed data. This paper gives relationships which may be used to quantify the errors in these assumptions. Additionally, basic propositions are given which help in understanding operational analysis assumptions. These propositions are used in developing correction terms which can be used to adjust performance measures so that their values are exact for a set of data no matter how much the assumptions used in deriving the performance measure relations are violated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233488","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702288","Operational analysis;performance measure assumptions;performance measures;system performance","Error analysis;Performance analysis;Equations;System performance;Computer errors;Computer science;Delay;Time measurement;Computational modeling;Computer simulation","","Operational analysis;performance measure assumptions;performance measures;system performance","","2","","6","","","","","","IEEE","IEEE Journals & Magazines"
"Domain-specific languages: from design to implementation application to video device drivers generation","S. A. Thibault; R. Marlet; C. Consel","Rennes I Univ., France; NA; NA","IEEE Transactions on Software Engineering","","1999","25","3","363","377","Domain-specific languages (DSL) have many potential advantages in terms of software engineering, ranging from increased productivity to the application of formal methods. Although they have been used in practice for decades, there has been little study of methodology or implementation tools for the DSL approach. We present our DSL approach and its application to a realistic domain: the generation of video display device drivers. The article focuses on the validation of our proposed framework for domain-specific languages, from design to implementation. The framework leads to a flexible design and structure, and provides automatic generation of efficient implementations of DSL programs. Additionally, we describe an example of a complete DSL for video display adaptors and the benefits of the DSL approach for this application. This demonstrates some of the generally claimed benefits of using DSLs: increased productivity, higher-level abstraction, and easier verification. This DSL has been fully implemented with our approach and is available. Compose project URL: http://www.irisa.fr/compose/gal.","0098-5589;1939-3520;2326-3881","","10.1109/32.798325","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=798325","","Domain specific languages;DSL;Application software;Productivity;Software engineering;Displays;Graphics;Uniform resource locators;Telephony;Switching systems","high level languages;application generators;device drivers;video equipment","domain-specific languages;implementation application;video device driver generation;software engineering;formal methods;DSL approach;video display device drivers;automatic program generation;video display adaptors;higher-level abstraction","","39","","","","","","","","IEEE","IEEE Journals & Magazines"
"The cache assignment problem and its application to database buffer management","H. Levy; T. G. Messinger; R. J. T. Morris","Dept. of Comput. Sci., Tel Aviv Univ., Israel; NA; NA","IEEE Transactions on Software Engineering","","1996","22","11","827","838","Given N request streams and L/spl les/N LRU caches, the cache assignment problem asks to which cache each stream should be assigned in order to minimize the overall miss rate. An efficient solution to this problem is provided, based on characterizing each stream using the stack reference model and characterizing the interaction of the streams using a bursty stream model. It is shown that for Bernoulli (purely random) mixing of streams, the optimal cache assignment is to have one cache per stream. In practice streams are mixed in a way that is much ""burstier"" than can be represented by the Bernoulli model. Therefore a method is presented for superposition of bursty streams. The performance of the methods developed for bursty stream superposition and cache assignment are tested using trace data obtained from the database system DB2. The resulting cache assignment recommendations are then applied to the DB2 system, and considerable performance improvement is found to result.","0098-5589;1939-3520;2326-3881","","10.1109/32.553701","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=553701","","Database systems;Memory management;Indexes;Transaction databases;System testing;Pervasive computing;Aging;Cache memory;Computer science;Engineering management","storage allocation;cache storage;relational databases;software performance evaluation;storage management","cache assignment problem;database buffer management;request streams;miss rate;stack reference model;bursty stream model;Bernoulli model;random mixing;bursty stream superposition;performance;trace data;DB2","","1","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Heterogeneous Defect Prediction","J. Nam; W. Fu; S. Kim; T. Menzies; L. Tan","Handong Global University, Pohang, Korea; Department of Computer Science, North Carolina State University, Raleigh, NC; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science, North Carolina State University, Raleigh, NC; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Software Engineering","","2018","44","9","874","896","Many recent studies have documented the success of cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. However, most studies share the same limitations: it requires homogeneous data; i.e., different projects must describe themselves using the same metrics. This paper presents methods for heterogeneous defect prediction (HDP) that matches up different metrics in different projects. Metric matching for HDP requires a “large enough” sample of distributions in the source and target projects-which raises the question on how large is “large enough” for effective heterogeneous defect prediction. This paper shows that empirically and theoretically, “large enough” may be very small indeed. For example, using a mathematical model of defect prediction, we identify categories of data sets were as few as 50 instances are enough to build a defect prediction model. Our conclusion for this work is that, even when projects use different metric sets, it is possible to quickly transfer lessons learned about defect prediction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2720603","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7959597","Defect prediction;quality assurance;heterogeneous metrics;transfer learning","Predictive models;Software metrics;Quality assurance;Training","data handling;pattern matching","metric matching;cross-project defect prediction;defect data;homogeneous data;heterogeneous defect prediction;metric sets","","5","","103","","","","","","IEEE","IEEE Journals & Magazines"
"Software Science Applied to APL","A. H. Konstam; D. E. Wood","Department of Computing and Information Sciences, Trinity University; NA","IEEE Transactions on Software Engineering","","1985","SE-11","10","994","1000","Previous attempts to apply Halstead's software metrics to APL have led to inconsistent and counter-intuitive results. This work is a further investigation into the application of software metrics to APL to try to resolve some of the inconsistency. The effect of variations in the counting rules on values calculated for the software metrics was studied. These rules were used to analyze a set of programs from a previous study. In addition, a large number of APL programs from a university environment were analyzed. Evidence is presented that verifies that APL has a higher language level than any other common programming language previously studied. Counting monadic and dyadic uses of the same APL symbol as an instance of a different operator was found to have a significant effect on the language level calculated for APL. However, decomposing derived APL functions into separate operators did not seem to have a significant effect on language level.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231546","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701914","APL;language level;software metrics;software science","Software metrics;Application software;Computer languages;Software measurement;Assembly;Analysis of variance;Costs;Road transportation;Impurities;Vocabulary","","APL;language level;software metrics;software science","","3","","8","","","","","","IEEE","IEEE Journals & Magazines"
"The performance of flow graph locking","M. H. Eich; S. M. Garard","Dept. of Comput. Sci. & Eng., Southern Methodist Univ., Dallas, TX, USA; Dept. of Comput. Sci. & Eng., Southern Methodist Univ., Dallas, TX, USA","IEEE Transactions on Software Engineering","","1990","16","4","477","483","The performance of flow graph locking (FGL) is compared with that of two-phase locking (2PL). As the data sharing level increases, FGL has a better response time than 2PL. Regardless of the data sharing or multiprogramming levels, FGL usually facilitates a better throughput rate than 2PL.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.54301","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=54301","","Flow graphs;Concurrency control;Transaction databases;Protocols;Authorization;System recovery;Concurrent computing;Delay;Throughput;Computer science","concurrency control;distributed databases","performance;flow graph locking;two-phase locking;response time;data sharing;multiprogramming levels","","8","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Tool support for testing concurrent Java components","B. Long; D. Hoffman; P. Strooper","Sch. of Inf. Technol. & Electr. Eng., Univ. of Queensland, Brisbane, Qld., Australia; NA; NA","IEEE Transactions on Software Engineering","","2003","29","6","555","566","Concurrent programs are hard to test due to the inherent nondeterminism. This paper presents a method and tool support for testing concurrent Java components. Tool support is offered through ConAn (Concurrency Analyser), a tool for generating drivers for unit testing Java classes that are used in a multithreaded context. To obtain adequate controllability over the interactions between Java threads, the generated driver contains threads that are synchronized by a clock. The driver automatically executes the calls in the test sequence in the prescribed order and compares the outputs against the expected outputs specified in the test sequence. The method and tool are illustrated in detail on an asymmetric producer-consumer monitor. Their application to testing over 20 concurrent components, a number of which are sourced from industry and were found to contain faults, is presented and discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205182","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205182","","Java;Monitoring;Automatic testing;Software testing;Concurrent computing;Synchronization;Clocks;System testing;Sequential analysis","Java;object-oriented programming;program testing;software tools;multi-threading;concurrency control","concurrent Java component testing;tool support;concurrent programs;nondeterminism;ConAn tool;Concurrency Analyser;drivers;unit testing;Java classes;multithreaded context;clock;synchronization;test sequence;asymmetric producer-consumer monitor;faults","","29","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Static analysis of XML transformations in Java","C. Kirkegaard; A. Moller; M. I. Schwartzbach","Dept. of Comput. Sci., Aarhus Univ., Denmark; Dept. of Comput. Sci., Aarhus Univ., Denmark; Dept. of Comput. Sci., Aarhus Univ., Denmark","IEEE Transactions on Software Engineering","","2004","30","3","181","192","XML documents generated dynamically by programs are typically represented as text strings or DOM trees. This is a low-level approach for several reasons: 1) traversing and modifying such structures can be tedious and error prone, 2) although schema languages, e.g., DTD, allow classes of XML documents to be defined, there are generally no automatic mechanisms for statically checking that a program transforms from one class to another as intended. We introduce XACT, a high-level approach for Java using XML templates as a first-class data type with operations for manipulating XML values based on XPath. In addition to an efficient runtime representation, the data type permits static type checking using DTD schemas as types. By specifying schemes for the input and output of a program, our analysis algorithm will statically verify that valid input data is always transformed into valid output data and that the operations are used consistently.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271173","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271173","","XML;Java;Web services;Markup languages;Tree data structures;Humans;Computer languages;Runtime;Algorithm design and analysis;Vocabulary","formal specification;formal verification;Java;program diagnostics;XML","static analysis;XML transformation;Java;XML document;static type checking;language constructs;markup language;requirement specification","","25","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Software assurance by bounded exhaustive testing","D. Coppit; Jinlin Yang; S. Khurshid; Wei Le; K. Sullivan","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","4","328","339","Bounded exhaustive testing (BET) is a verification technique in which software is automatically tested for all valid inputs up to specified size bounds. A particularly interesting case of BET arises in the context of systems that take structurally complex inputs. Early research suggests that the BET approach can reveal faults in small systems with inputs of low structural complexity, but its potential utility for larger systems with more complex input structures remains unclear. We set out to test its utility on one such system. We used Alloy and TestEra to generate inputs to test the Galileo dynamic fault tree analysis tool, for which we already had both a formal specification of the input space and a test oracle. An initial attempt to generate inputs using a straightforward translation of our specification to Alloy did not work well. The generator failed to generate inputs to meaningful bounds. We developed an approach in which we factored the specification, used TestEra to generate abstract inputs based on one factor, and passed the results through a postprocessor that reincorporated information from the second factor. Using this technique, we were able to generate test inputs to meaningful bounds, and the inputs revealed nontrivial faults in the Galileo implementation, our specification, and our oracle. Our results suggest that BET, combined with specification abstraction and factoring techniques, could become a valuable addition to our verification toolkit and that further investigation is warranted.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435353","Index Terms- Formal methods;program verification;testing and debugging.","Software testing;Automatic testing;System testing;Fault trees;Formal specifications;Debugging;Data structures;Hardware;Fault tolerant systems;Aircraft","program verification;program testing;fault trees;software tools;formal specification;automatic programming;program debugging;software fault tolerance","software assurance;bounded exhaustive testing;BET;software verification technique;Alloy;TestEra;Galileo dynamic fault tree analysis tool;formal specification;formal methods;program verification;program debugging","","23","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Construction of Universal Instances for Loop-Free Network Databases Using a Join-Like Operation","S. Jajodia; F. N. Springsteel","Computer Science and Systems Branch, Naval Research Laboratory; NA","IEEE Transactions on Software Engineering","","1987","SE-13","7","811","819","In this paper, we give a polynomial-time method to construct effectively the unique universal instance, using as few nulls as possible, from any loop-free network database, via a ""minimal information"" extension of natural join. Our results can be seen as concretely and quickly implementing the universal relation view for databases which are not pairwise consistent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233492","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702292","Bachman diagram;extended join;γ-acyclic relational schema;loop-free network database;natural join;relational database;universal instance;unmarked null value","Relational databases;Polynomials;Computer science;Null value;Laboratories;Terminology;Transaction databases","","Bachman diagram;extended join;γ-acyclic relational schema;loop-free network database;natural join;relational database;universal instance;unmarked null value","","","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Invariants, frames and postconditions: a comparison of the VDM and B notations","J. Bicarregui; B. Ritchie","Dept. of Inf., Rutherford Appleton Lab., Chilton, UK; Dept. of Inf., Rutherford Appleton Lab., Chilton, UK","IEEE Transactions on Software Engineering","","1995","21","2","79","89","VDM and B are two ""model-oriented"" formal methods. Each gives a notation for the specification of systems as state machines in terms of a set of states with operations defined as relations on that set. Each has a notion of refinement of data and operations based on the principles of reduction of nondeterminism and increase in definedness. The paper makes a comparison of the two notations through an example of a communications protocol previously formalized by G. Bruns and S. Anderson (1994). Two abstractions and two reifications of the original specification are given. Particular attention is paid to three areas where the notations differ: the use of postconditions that assume the invariant as opposed to postconditions that enforce it; the explicit ""framing"" of operations as opposed to the ""minimal frame"" approach; and the use of relational postconditions as opposed to generalized substitutions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.345824","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=345824","","Protocols;Data models;Carbon capture and storage;Informatics;Formal specifications;Microprocessors;Safety;Embedded software;Design engineering","Vienna development method;formal specification;specification languages;protocols","VDM;B notations;model-oriented formal methods;state machines;communications protocol;specification;invariant;explicit framing;minimal frame;relational postconditions","","5","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing the Effects of Bugs on Software Interfaces","R. Natella; S. Winter; D. Cotroneo; N. Suri","Dipartimento di Informatica e Sistemistica, Universita degli Studi di Napoli Federico II, Napoli, Napoli Italy 80125 (e-mail: roberto.natella@unina.it); DEEDS, TU Darmstadt, Darmstadt, Hessen Germany 64289 (e-mail: sw@cs.tu-darmstadt.de); Computer and System engineering, University of Naples federico II, Naples, Italy Italy 80125 (e-mail: cotroneo@unina.it); Dept. of Computer Science, TU Darmstadt, Darmstadt, Hessen Germany (e-mail: suri@cs.tu-darmstadt.de)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Critical systems that integrate software components (e.g., from third-parties) need to address the risk of residual software defects in these components. Software fault injection is an experimental solution to gauge such risk. Many error models have been proposed for emulating faulty components, such as by injecting error codes and exceptions, or by corrupting data with bit-flips, boundary values, and random values. Even if these error models have been able to find breaches in fragile systems, it is unclear whether these errors are in fact representative of software faults. To pursue this open question, we propose a methodology to analyze how software faults in<formula><tex>$C/C_{++}$</tex></formula>software components turn into errors at components&#x0027; interfaces (interface error propagation), and present an experimental analysis on what, where, and when to inject interface errors. The results point out that the traditional error models, as used so far, do not accurately emulate software faults, but that richer interface errors need to be injected, by: injecting both fail-stop behaviors and data corruptions; targeting larger amounts of corrupted data structures; emulating silent data corruptions not signaled by the component; combining bit-flips, boundary values, and data perturbations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2850755","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8396273","Dependability;Fault Injection;Software Fault Tolerance;Error Propagation;Software Components;Error Models","Software;Unified modeling language;Computer bugs;Perturbation methods;Testing;Fault tolerance","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A Compiler for an Array and Vector Processing Language","R. H. Perrott; D. Crookes; P. Milligan; W. R. M. Purdy","Department of Computer Science, Queen's University of Belfast; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","5","471","478","A compiler for a Pascal-based language Actus is described. The language is suitable for the expression of the type of parallelism offered by both array and vector processors. The implementation described is for the Cray-1 computer. An objective of the implementation has been to construct an optimizing compiler which can be readily adapted for a range of array and vector processors. As a result the machine-dependent sections of the compiler have been clearly identified.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232486","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702037","Abstract representation;array processors;graph transformations;optimization;vector processors","Parallel processing;Vector processors;Hardware;Optimizing compilers;Computer languages;Programming profession;Computer science;Adaptive arrays;Program processors;Algorithms","","Abstract representation;array processors;graph transformations;optimization;vector processors","","3","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Using the Conceptual Cohesion of Classes for Fault Prediction in Object-Oriented Systems","A. Marcus; D. Poshyvanyk; R. Ferenc","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","287","300","High cohesion is a desirable property of software as it positively impacts understanding, reuse, and maintenance. Currently proposed measures for cohesion in Object-Oriented (OO) software reflect particular interpretations of cohesion and capture different aspects of it. Existing approaches are largely based on using the structural information from the source code, such as attribute references, in methods to measure cohesion. This paper proposes a new measure for the cohesion of classes in OO software systems based on the analysis of the unstructured information embedded in the source code, such as comments and identifiers. The measure, named the Conceptual Cohesion of Classes (C3), is inspired by the mechanisms used to measure textual coherence in cognitive psychology and computational linguistics. This paper presents the principles and the technology that stand behind the C3 measure. A large case study on three open source software systems is presented which compares the new measure with an extensive set of existing metrics and uses them to construct models that predict software faults. The case study shows that the novel measure captures different aspects of class cohesion compared to any of the existing cohesion measures. In addition, combining C3 with existing structural cohesion metrics proves to be a better predictor of faulty classes when compared to different combinations of structural cohesion metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70768","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4384505","Maintainability;Metrics/Measurement;Quality analysis and evaluation;Restructuring;reverse engineering;and reengineering;Code documentation;Document analysis;Document indexing;Maintainability;Metrics/Measurement;Quality analysis and evaluation;Restructuring;reverse engineering;and reengineering;Code documentation;Document analysis;Document indexing","Software measurement;Open source software;Software maintenance;Current measurement;Particle measurements;Software systems;Information analysis;Coherence;Psychology;Computational linguistics","object-oriented programming;software fault tolerance;software metrics","conceptual class cohesion;object-oriented software system;open source software system;software metrics;software fault prediction","","132","","78","","","","","","IEEE","IEEE Journals & Magazines"
"A study of the applicability of complexity measures","J. S. Davis; R. J. LeBlanc","Dept. of Manage., Clemson Univ., SC, USA; NA","IEEE Transactions on Software Engineering","","1988","14","9","1366","1372","A study of the predictive value of a variety of syntax-based problem complexity measures is reported. Experimentation with variants of chunk-oriented measures showed that one should judiciously select measurable software attributes as proper indicators of what one wishes to predict, rather than hoping for a single, all-purpose complexity measure. The authors have shown that it is possible for particular complexity measures or other factors to serve as good predictors of some properties of program but not for others. For example, a good predictor of construction time will not necessarily correlate well with the number of error occurrences. M.H. Halstead's (1977) efforts measure (E) was found to be a better predictor that the two nonchunk measures evaluated, namely, T.J. McCabe's (1976) V(G) and lines of code, but at least one chunk measure predicted better than E in every case.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6179","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6179","","Software measurement;Psychology;Computer science;Terminology;Computer errors;Programming profession","software engineering","software engineering;complexity measures;predictive value;chunk-oriented measures;software attributes;construction time;error occurrences;efforts measure","","49","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Region analysis: a parallel elimination method for data flow analysis","Yong-Fong Lee; B. G. Ryder; M. E. Fiuczynski","Intel Corp., Santa Clara, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","1995","21","11","913","926","Parallel data flow analysis methods offer the promise of calculating detailed semantic information about a program at compile-time more efficiently than sequential techniques. Previous work on parallel elimination methods (Zobel, 1990) has been hampered by the lack of control over interval size; this can prohibit effective parallel execution of these methods. To overcome this problem, we have designed the region analysis method, a new elimination method for data flow analysis. Region analysis emphasizes flow graph partitioning to enable better load balancing in a more effective parallel algorithm. We present the design of region analysis and the empirical results we have obtained that indicate: the prevalence of large intervals in flow graphs derived from real programs; and the performance improvement of region analysis over parallel Allen-Cocke interval analysis. Our implementation analyzed programs from the Perfect Benchmarks and netlib running on a Sequent Symmetry S81.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.473220","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=473220","","Data analysis;Flow graphs;Algorithm design and analysis;Information analysis;Parallel algorithms;Performance analysis;Size control;Load flow analysis;Software tools;Computer science","data flow analysis;data flow graphs;program compilers;parallel algorithms;resource allocation;parallel programming;software performance evaluation","region analysis;parallel elimination method;parallel data flow analysis methods;semantic information;compile-time;interval size;parallel execution;flow graph partitioning;load balancing;parallel algorithm;performance improvement;interval analysis;Perfect Benchmarks;netlib;Sequent Symmetry S81;program optimization","","7","","33","","","","","","IEEE","IEEE Journals & Magazines"
"The State of Software Maintenance","N. F. Schneidewind","Naval Postgraduate School","IEEE Transactions on Software Engineering","","1987","SE-13","3","303","310","A state of software maintenance survey is presented, indicating the incongruity of the simultaneous existence of importance and neglect in this field. An overview is given of selected developments and activities covering the following topics: • The ""Maintenance Problem."" • Models. • Methods for improving maintenance. • Metrics. • Maintenance information management. • Standards. • Maintenance of existing code. • Surveys.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233161","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702216","Metrics;models;software maintenance","Software maintenance;Data processing;Information management;Code standards;Standards development;History;Computer science;Software performance;Software systems;Error correction","","Metrics;models;software maintenance","","54","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Environment evolution: the Prism model of changes","N. H. Madhavji","Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada","IEEE Transactions on Software Engineering","","1992","18","5","380","392","A software development environment supports a complex network of items of at least the following major types: people, policies, laws, resources, processes and results. Such items may need to be changed on an on-going basis. The authors have designed in the Prism project a model of changes and two supporting change-related environment infrastructures with the following key features: separation of changes to the described items from the changes to the environmental facilities encapsulating these items; a facility, called the dependency structure, for describing various items and their interdependencies, and for identifying the items affected by a given change; a facility, called the change structure for classifying, recording, and analyzing change-related data and for making qualitative judgments of the consequences of a change; identification of the many distinct properties of a change; and a built-in mechanism for providing feedback. The author's approach to the problem of change and its rationale is described.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.135771","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=135771","","Programming;Environmental management;Complex networks;Data analysis;Mechanical factors;Feedback;Project management;Engineering management;Councils;Computer science","data structures;programming environments;software tools","Prism model;software development environment;complex network;people;policies;laws;resources;processes;Prism project;change-related environment infrastructures;environmental facilities;dependency structure;change structure;change-related data;qualitative judgments;built-in mechanism;feedback","","39","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Revisiting Java Bytecode Compression for Embedded and Mobile Computing Environments","D. Saougkos; G. Manis; K. Blekas; A. V. Zarras","Computer Science Department, University of Ioannina, PO Box 1186, GR 45110, Greece; Computer Science Department, University of Ioannina, PO Box 1186, GR 45110, Greece; Computer Science Department, University of Ioannina, PO Box 1186, GR 45110, Greece; Computer Science Department, University of Ioannina, PO Box 1186, GR 45110, Greece","IEEE Transactions on Software Engineering","","2007","33","7","478","495","Pattern-based Java bytecode compression techniques rely on the identification of identical instruction sequences that occur more than once. Each occurrence of such a sequence is substituted by a single instruction. The sequence defines a pattern that is used for extending the standard bytecode instruction set with the instruction that substitutes the pattern occurrences in the original bytecode. Alternatively, the pattern may be stored in a dictionary that serves for the bytecode decompression. In this case, the instruction that substitutes the pattern in the original bytecode serves as an index to the dictionary. In this paper, we investigate a bytecode compression technique that considers a more general case of patterns. Specifically, we employ the use of an advanced pattern discovery technique that allows locating patterns of an arbitrary length, which may contain a variable number of wildcards in place of certain instruction opcodes or operands. We evaluate the benefits and the limitations of this technique in various scenarios that aim at compressing the reference implementation of MIDP, a standard Java environment for the development of applications for mobile devices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1021","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4227829","Java;compression (coding).","Java;Mobile computing;Embedded computing;Dictionaries;Computer aided instruction;Standards development;Page description languages;Code standards;Virtual machining","distributed programming;Java;mobile computing","embedded environments;mobile computing environments;pattern-based Java bytecode compression techniques;standard bytecode instruction;bytecode decompression;pattern discovery technique;mobile devices","","","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Metrics for Measuring the Quality of Modularization of Large-Scale Object-Oriented Software","S. Sarkar; A. C. Kak; G. M. Rama","Infosys Technologies Ltd, Bangalore; Purdue University, West Lafayette; Infosys Technologies Ltd, Bangalore","IEEE Transactions on Software Engineering","","2008","34","5","700","720","The metrics formulated to date for characterizing the modularization quality of object-oriented software have considered module and class to be synonymous concepts. But a typical class in object oriented programming exists at too low a level of granularity in large object-oriented software consisting of millions of lines of code. A typical module (sometimes referred to as a superpackage) in a large object-oriented software system will typically consist of a large number of classes. Even when the access discipline encoded in each class makes for ""clean"" class-level partitioning of the code, the intermodule dependencies created by associational, inheritance-based, and method invocations may still make it difficult to maintain and extend the software. The goal of this paper is to provide a set of metrics that characterize large object-oriented software systems with regard to such dependencies. Our metrics characterize the quality of modularization with respect to the APIs of the modules, on the one hand, and, on the other, with respect to such object-oriented inter-module dependencies as caused by inheritance, associational relationships, state access violations, fragile base-class design, etc. Using a two-pronged approach, we validate the metrics by applying them to popular open-source software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4623684","Metrics/Measurement;Maintainability;Modules and interfaces;Object-oriented programming;Coupling;Maintenance and Enhancement;Large-Scale Software;Metrics/Measurement;Maintainability;Modules and interfaces;Object-oriented programming;Coupling;Maintenance and Enhancement;Large-Scale Software","Software measurement;Large-scale systems;Software quality;Software maintenance;Software systems;Application software;Object oriented programming;Software metrics;Aging;Computer architecture","application program interfaces;inheritance;object-oriented programming;software metrics;software quality","modularization quality;large-scale object-oriented software;synonymous concepts;granularity;API;object-oriented intermodule dependencies;inheritance;open-source software systems","","36","","55","","","","","","IEEE","IEEE Journals & Magazines"
"A case study of software process improvement during development","I. Bhandari; M. Halliday; E. Tarver; D. Brown; J. Chaar; R. Chillarege","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1993","19","12","1157","1170","We present a case study of the use of a software process improvement method which is based on the analysis of defect data. The first step of the method is the classification of software defects using attributes which relate defects to specific process activities. Such classification captures the semantics of the defects in a fashion which is useful for process correction. The second step utilizes a machine-assisted approach to data exploration which allows a project team to discover such knowledge from defect data as is useful for process correction. We show that such analysis of defect data can readily lead a project team to improve their process during development.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.249661","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=249661","","Computer aided software engineering;Data analysis;Software systems;Production systems;Software engineering;Laboratories;Programming;Feedback","data handling;project management;software engineering;software metrics","case study;software process improvement;defect data analysis;software defects;attributes;process activities;semantics;process correction;machine-assisted approach;data exploration;project team;software engineering;defect-based process improvement;in-process metrics;knowledge discovery","","33","","25","","","","","","IEEE","IEEE Journals & Magazines"
"An analysis of some problems in managing virtual memory systems with fast secondary storage devices","S. J. Hartley","Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA","IEEE Transactions on Software Engineering","","1988","14","8","1176","1187","Some of the problems that are expected to be encountered in managing virtual memory systems using the newer-technology secondary storage devices are address. The difficulties that two proposed policies have in choosing the most economical program localities of reference to assign to primary memory are analyzed. K. Koh's (1981) criterion for examining the cyclic locality interval (CLI) hierarchy of a program and choosing the least-cost pathway is examined. Koh's criterion is designed for the case of a CLI containing a single inner CLI. The decision to descend the hierarchy is based on the cycle time of the outer CLI. If the outer CLI has two or more inner CLIs, it is possible for Koh's criterion to indicate that it is more economical to descend to one of the inner CLIs without that actually being the case. Choosing which CLI to descend to requires knowledge of its duration, and this is not generally available to the memory management system. An attempt to use Koh's criterion with the loop structure of a program in order to reduce space-time execution cost was not successful.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.7627","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7627","","Memory management;Costs;Algorithm design and analysis;Computer science;Central Processing Unit;Semiconductor memory;Solid state circuits;Technology management;Space technology","storage allocation;storage management;virtual storage","storage allocation;virtual memory;secondary storage;cyclic locality interval;least-cost pathway;memory management;loop structure;space-time execution cost","","2","","32","","","","","","IEEE","IEEE Journals & Magazines"
"GUI Interaction Testing: Incorporating Event Context","X. Yuan; M. B. Cohen; A. M. Memon","Google Kirkland; University of Nebraska-Lincoln, Lincoln; University of Maryland, College Park","IEEE Transactions on Software Engineering","","2011","37","4","559","574","Graphical user interfaces (GUIs), due to their event-driven nature, present an enormous and potentially unbounded way for users to interact with software. During testing, it is important to “adequately cover” this interaction space. In this paper, we develop a new family of coverage criteria for GUI testing grounded in combinatorial interaction testing. The key motivation of using combinatorial techniques is that they enable us to incorporate “context” into the criteria in terms of event combinations, sequence length, and by including all possible positions for each event. Our new criteria range in both efficiency (measured by the size of the test suite) and effectiveness (the ability of the test suites to detect faults). In a case study on eight applications, we automatically generate test cases and systematically explore the impact of context, as captured by our new criteria. Our study shows that by increasing the event combinations tested and by controlling the relative positions of events defined by the new criteria, we can detect a large number of faults that were undetectable by earlier techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5444885","GUI testing;automated testing;model-based testing;combinatorial interaction testing;GUITAR testing system.","Graphical user interfaces;System testing;Software testing;Automatic testing;Fault detection;Context modeling;Computer science;Software performance;Logic testing;User interfaces","automatic test pattern generation;graphical user interfaces;program testing","GUI interaction testing;graphical user interface;event driven nature;combinatorial interaction testing;automatic test case generation","","72","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Directed Explicit State-Space Search in the Generation of Counterexamples for Stochastic Model Checking","H. Aljazzar; S. Leue","University of Konstanz, Konstanz; University of Konstanz, Konstanz","IEEE Transactions on Software Engineering","","2010","36","1","37","60","Current stochastic model checkers do not make counterexamples for property violations readily available. In this paper, we apply directed explicit state-space search to discrete and continuous-time Markov chains in order to compute counterexamples for the violation of PCTL or CSL properties. Directed explicit state-space search algorithms explore the state space on-the-fly, which makes our method very efficient and highly scalable. They can also be guided using heuristics which usually improve the performance of the method. Counterexamples provided by our method have two important properties. First, they include those traces which contribute the greatest amount of probability to the property violation. Hence, they show the most probable offending execution scenarios of the system. Second, the obtained counterexamples tend to be small. Hence, they can be effectively analyzed by a human user. Both properties make the counterexamples obtained by our method very useful for debugging purposes. We implemented our method based on the stochastic model checker PRISM and applied it to a number of case studies in order to illustrate its applicability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5262946","Directed explicit state-space search;heuristic search;counterexamples;stochastic model checking.","Stochastic processes;Debugging;Probabilistic logic;Safety;Performance analysis;Space exploration;State-space methods;Humans;Shape;Sampling methods","formal verification;Markov processes;program debugging;tree searching","directed explicit state-space search;stochastic model checking;discrete-time Markov chains;continuous-time Markov chains;PCTL properties;CSL properties;heuristic search;debugging","","19","","49","","","","","","IEEE","IEEE Journals & Magazines"
"A Controlled Expeniment on the Impact of Software Structure on Maintainability","H. D. Rombach","Department of Computer Science, University of Maryland","IEEE Transactions on Software Engineering","","1987","SE-13","3","344","354","This paper describes a study on the impact of software structure on maintainability aspects such as comprehensibility, locality, modifiability, and reusability in a distributed system environment. The study was part of a project at the University of Kaiserslautern, West Germany, to design and implement LADY, a LAnguage for Distributed systems. The study addressed the impact of software structure from two perspectives. The language designer's perspective was to evaluate the general impact of the set of structural concepts chosen for LADY on the maintainability of software systems implemented in LADY. The language user's perspective was to derive structural criteria (metrics), measurable from LADY systems, that allow the explanation or prediction of the software maintenance behavior. A controlled maintenance experiment was conducted involving twelve medium-size distributed software systems; six of these systems were implemented in LADY, the other six systems in an extended version of sequential Pascal. The benefits of the structural LADY concepts were judged based on a comparison of the average maintenance behavior of the LADY systems and the Pascal systems; the maintenance metrics were derived by analyzing the interdependence between structure and maintenance behavior of each individual LADY system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233165","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702220","Complexity metrics;comprehensibility;controlled experiments;distributed systems;language comparisons;locality;maintainability lity;modifiability;reusability;software structure","Software maintenance;Software systems;Software reusability;Computer science;Software measurement;Control systems;Space technology;Distributed control;Costs","","Complexity metrics;comprehensibility;controlled experiments;distributed systems;language comparisons;locality;maintainability lity;modifiability;reusability;software structure","","72","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Correctness verification and performance analysis of real-time systems using stochastic preemptive time Petri nets","G. Bucci; L. Sassoli; E. Vicario","Dipt. Sistemi e Informatica, Firenze Univ., Italy; Dipt. Sistemi e Informatica, Firenze Univ., Italy; Dipt. Sistemi e Informatica, Firenze Univ., Italy","IEEE Transactions on Software Engineering","","2005","31","11","913","927","Time Petri nets describe the state of a timed system through a marking and a set of clocks. If clocks take values in a dense domain, state space analysis must rely on equivalence classes. These support verification of logical sequencing and quantitative timing of events, but they are hard to be enriched with a stochastic characterization of nondeterminism necessary for performance and dependability evaluation. Casting clocks into a discrete domain overcomes the limitation, but raises a number of problems deriving from the intertwined effects of concurrency and timing. We present a discrete-time variant of time Petri nets, called stochastic preemptive time Petri nets, which provides a unified solution for the above problems through the adoption of a maximal step semantics in which the logical location evolves through the concurrent firing of transition sets. We propose an analysis technique, which integrates the enumeration of a succession relation among sets of timed states with the calculus of their probability distribution. This enables a joint approach to the evaluation of performance and dependability indexes as well as to the verification of sequencing and timeliness correctness. Expressive and analysis capabilities of the model are demonstrated with reference to a real-time digital control system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.122","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556551","Index Terms- Real-time reactive systems;preemptive scheduling;correctness verification;performance and dependability evaluation;discrete time;maximal step semantics;confusion;well definedness;stochastic preemptive Time Petri nets.","Performance analysis;Real time systems;Stochastic systems;Petri nets;Clocks;Timing;Stochastic processes;State-space methods;Casting;Concurrent computing","software performance evaluation;program verification;real-time systems;stochastic processes;probability;Petri nets;equivalence classes","state space analysis;equivalence classes;logical sequencing verification;quantitative timing;dependability evaluation;discrete-time variant;stochastic preemptive time Petri nets;maximal step semantics;probability distribution;real-time digital control system;correctness verification;performance analysis;real-time system","","24","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Estimation of Defects Based on Defect Decay Model: ED^{3}M","S. W. Haider; J. W. Cangussu; K. M. L. Cooper; R. Dantu; S. Haider","The University of Texas at Dallas, Dallas; The University of Texas at Dallas, Dallas; The University of Texas at Dallas, Dallas; University of North Texas, Denton; The University of Texas at Dallas, Dallas","IEEE Transactions on Software Engineering","","2008","34","3","336","356","An accurate prediction of the number of defects in a software product during system testing contributes not only to the management of the system testing process but also to the estimation of the product's required maintenance. Here, a new approach called ED<sup>3</sup>M is presented that computes an estimate of the total number of defects in an ongoing testing process. ED<sup>3</sup>M is based on estimation theory. Unlike many existing approaches the technique presented here does not depend on historical data from previous projects or any assumptions about the requirements and/or testers' productivity. It is a completely automated approach that relies only on the data collected during an ongoing testing process. This is a key advantage of the ED<sup>3</sup>M approach, as it makes it widely applicable in different testing environments. Here, the ED<sup>3</sup>M approach has been evaluated using five data sets from large industrial projects and two data sets from the literature. In addition, a performance analysis has been conducted using simulated data sets to explore its behavior using different models for the input data. The results are very promising; they indicate the ED<sup>3</sup>M approach provides accurate estimates with as fast or better convergence time in comparison to well-known alternative techniques, while only using defect data as the input.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4492790","Statistical methods;Testing and Debugging;Metrics/Measurement;Defect prediction;system testing;estimation theory;Statistical methods;Testing and Debugging;Metrics/Measurement;Defect prediction;system testing;estimation theory","System testing;Estimation theory;Software testing;Costs;Programming;Phase estimation;Inspection;Software maintenance;Software systems;Productivity","program testing;software metrics","defect decay model;software product;defect estimation;system testing process;estimation theory;software metrics","","6","","49","","","","","","IEEE","IEEE Journals & Magazines"
"A pre-run-time scheduling algorithm for hard real-time systems","T. Shepard; J. A. M. Gagne","Dept. of Electr. & Comput. Eng., R. Mil. Coll. of Canada, Kingston, Ont., Canada; NA","IEEE Transactions on Software Engineering","","1991","17","7","669","677","Process scheduling, an important issue in the design and maintenance of hard real-time systems, is discussed. A pre-run-time scheduling algorithm that addresses the problem of process sequencing is presented. The algorithm is designed for multiprocessor applications with preemptable processes having release times, computation times, deadlines and arbitrary precedence and exclusion constraints. The algorithm uses a branch-and-bound implicit enumeration technique to generate a feasible schedule for each processor. The set of feasible schedules ensures that the timing specifications of the processes are observed and that all the precedence and exclusion constraints between pairs of processes are satisfied. the algorithm was tested using a model derived from the F-18 mission computer operational flight program.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.83903","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=83903","","Scheduling algorithm;Real time systems;Processor scheduling;Timing;Application software;Runtime;Software algorithms;Software maintenance;Senior members;Algorithm design and analysis","aerospace computing;multiprocessing systems;real-time systems;scheduling","hard real-time systems;pre-run-time scheduling algorithm;process sequencing;multiprocessor applications;preemptable processes;release times;computation times;deadlines;arbitrary precedence;exclusion constraints;branch-and-bound implicit enumeration technique;feasible schedule;timing specifications;F-18 mission computer operational flight program","","24","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Conservative Reasoning about the Probability of Failure on Demand of a 1-out-of-2 Software-Based System in Which One Channel Is ""Possibly Perfect""","B. Littlewood; A. Povyakalo","City University London, London; City University London, London","IEEE Transactions on Software Engineering","","2013","39","11","1521","1530","In earlier work, [11] (henceforth LR), an analysis was presented of a 1-out-of-2 software-based system in which one channel was “possibly perfect”. It was shown that, at the aleatory level, the system pfd (probability of failure on demand) could be bounded above by the product of the pfd of channel A and the pnp (probability of nonperfection) of channel B. This result was presented as a way of avoiding the well-known difficulty that for two certainly-fallible channels, failures of the two will be dependent, i.e., the system pfd cannot be expressed simply as a product of the channel pfds. A price paid in this new approach for avoiding the issue of failure dependence is that the result is conservative. Furthermore, a complete analysis requires that account be taken of epistemic uncertainty-here concerning the numeric values of the two parameters pfd<sub>A</sub>and pnp<sub>B</sub>. Unfortunately this introduces a different difficult problem of dependence: estimating the dependence between an assessor's beliefs about the parameters. The work reported here avoids this problem by obtaining results that require only an assessor's marginal beliefs about the individual channels, i.e., they do not require knowledge of the dependence between these beliefs. The price paid is further conservatism in the results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6574864","Software reliability;fault tolerance;software perfection;probability of failure;epistemic uncertainty;software diversity;multiversion software","Phase frequency detector;Uncertainty;Cognition;Software reliability;Software;Safety","belief networks;failure analysis;probability;software reliability;uncertainty handling","conservative reasoning;probability of failure on demand;1-out-of-2 software-based system;PFD;PNP;probability of nonperfection;certainly fallible channel;assessor marginal belief;epistemic uncertainty;software perfection","","4","","15","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study of RefactoringChallenges and Benefits at Microsoft","M. Kim; T. Zimmermann; N. Nagappan","Department of Electrical and Computer Engineering, University of Texas, Austin; Microsoft Research at Redmond; Microsoft Research at Redmond","IEEE Transactions on Software Engineering","","2014","40","7","633","649","It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring benefits or investigate developers' perception towards these benefits. This paper presents a field study of refactoring benefits and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey finds that the refactoring definition in practice is not confined to a rigorous definition of semantics-preserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized effort on refactoring Windows. The quantitative analysis of Windows 7 version history finds the top 5 percent of preferentially refactored modules experience higher reduction in the number of inter-module dependencies and several complexity measures but increase size more than the bottom 95 percent. This indicates that measuring the impact of refactoring requires multi-dimensional assessment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2318734","National Science Foundation; Microsoft SEIF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6802406","Refactoring;empirical study;software evolution;component dependencies;defects;churn","Computer bugs;Software;Interviews;History;Complexity theory;Size measurement;Software metrics","data analysis;software maintenance;software metrics;software quality","refactoring challenges;refactoring benefits;Microsoft;software quality improvement;survey;semi-structured interviews;quantitative Windows 7 version history data analysis;intermodule dependencies;complexity measures;software evolution","","21","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Delta Execution for Efficient State-Space Exploration of Object-Oriented Programs","M. d'Amorim; S. Lauterburg; D. Marinov","University of Illinois at Urbana-Champaign, Urbana; University of Illinois at Urbana-Champaign, Urbana; University of Illinois at Urbana-Champaign, Urbana","IEEE Transactions on Software Engineering","","2008","34","5","597","613","We present Delta execution, a technique that speeds up state-space exploration of object-oriented programs. State-space exploration is the essence of model checking and an increasingly popular approach for automating test generation. A key issue in exploration of object-oriented programs is handling the program state, in particular the heap. We exploit the fact that many execution paths in state-space exploration partially overlap. Delta execution simultaneously operates on several states/heaps and shares the common parts across the executions, separately executing only the ""deltas"" where the executions differ. We implemented Delta execution in two model checkers: JPF, a popular general-purpose model checker for Java programs, and BOX, a specialized model checker that we developed for efficient exploration of sequential Java programs. The results for bounded-exhaustive exploration of ten basic subject programs and one larger case study show that Delta execution reduces exploration time from 1.06x to 126.80x (with median 5.60x) in JPF and from 0.58x to 4.16x (with median 2.23x) in BOX. The results for a non-exhaustive exploration in JPF show that Delta execution reduces exploration time from 0.92x to 6.28x (with median 4.52x).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4528965","Software/Program Verification;Model checking;Testing and Debugging;Software/Program Verification;Model checking;Testing and Debugging","Object oriented modeling;Boolean functions;Sliding mode control;Data structures;Automatic testing;Java;Software testing;Software reliability;Shape;Debugging","object-oriented programming;program debugging;program testing;program verification","Delta execution;state-space exploration;object-oriented programs;model checking;automating test generation;Java programs;sequential Java programs","","5","","51","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Study of Failure Proximity","C. Liu; X. Zhang; J. Han","Microsoft Corporation, Redmond; Purdue University, West Lafayette; University of Illinois at Urbana-Champaign, Urbana","IEEE Transactions on Software Engineering","","2008","34","6","826","843","Software end-users are the best testers, who keep revealing bugs in software that has undergone rigorous in-house testing. In order to leverage their testing efforts, failure reporting components have been widely deployed in released software. Many utilities of the collected failure data depend on an effective failure indexing technique, which, at the optimal case, would index all failures due to the same bug together. Unfortunately, the problem of failure proximity, which underpins the effectiveness of an indexing technique, has not been systematically studied. This article presents the first systematic study of failure proximity. A failure proximity consists of two components: a fingerprinting function that extracts signatures from failures, and a distance function that calculates the likelihood of two failures being due to the same bug. By considering different instantiations of the two functions, we study an array of six failure proximities (two of them are new) in this article. These proximities range from the simplest approach that checks failure points to the most sophisticated approach that utilizes fault localization algorithms to extract failure signatures. Besides presenting technical details of each proximity, we also study the properties of each proximity and tradeoffs between proximities. These altogether deliver a systematic view of failure proximity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4589219","Debugging aids;Dumps;Debugging aids;Dumps","Indexing;Failure analysis;Software maintenance;Software testing;Computer bugs;Software quality;Feedback;Debugging;Chaos;System testing","program debugging;program testing;software fault tolerance;software maintenance","failure proximity;software end-users;in-house testing;failure indexing technique;signature extraction;debugging aids;software maintenance","","16","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Reviewing software diagrams: a cognitive study","B. C. Hungerford; A. R. Hevner; R. W. Collins","Dept. of Manage. Inf. Syst., Wisconsin Univ., Oshkosh, WI, USA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","2","82","96","Reviews and inspections of software artifacts throughout the development life cycle are effective techniques for identifying defects and improving software quality. While review methods for text-based artifacts (e.g., code) are well understood, very little guidance is available for performing reviews of software diagrams, which are rapidly becoming the dominant form of software specification and design. Drawing upon human cognitive theory, we study how 12 experienced software developers perform individual reviews on a software design containing two types of diagrams: entity-relationship diagrams and data flow diagrams. Verbal protocol methods are employed to describe and analyze defect search patterns among the software artifacts, both text and diagrams, within the design. Results indicate that search patterns that rapidly switch between the two design diagrams are the most effective. These findings support the cognitive theory thesis that how an individual processes information impacts processing success. We conclude with specific recommendations for improving the practice of reviewing software diagrams.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265814","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265814","","Software quality;Software performance;Software design;Humans;Programming;Inspection;Protocols;Switches;Guidelines;Unified modeling language","software reviews;software quality;entity-relationship modelling;data flow analysis;specification languages;object-oriented programming","software review;software diagram;software quality;human cognitive theory;entity relationship diagram;data flow diagram;verbal protocol method","","39","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Crosscutting Concerns through Random Walks","C. Zhang; H. Jacobsen","The Hong Kong University of Science and Technology, Hong Kong; University of Toronto, Toronto","IEEE Transactions on Software Engineering","","2012","38","5","1123","1137","Inspired by our past manual aspect mining experiences, this paper describes a probabilistic random walk model to approximate the process of discovering crosscutting concerns (CCs) in the absence of the domain knowledge about the investigated application. The random walks are performed on the concept graphs extracted from the program sources to calculate metrics of “utilization” and “aggregation” for each of the program elements. We rank all the program elements based on these metrics and use a threshold to produce a set of candidates that represent crosscutting concerns. We implemented the algorithm as the Prism CC miner (PCM) and evaluated PCM on Java applications ranging from a small-scale drawing application to a medium-sized middleware application and to a large-scale enterprise application server. Our quantification shows that PCM is able to produce comparable results (95 percent accuracy for the top 125 candidates) with respect to the manual mining effort. PCM is also significantly more effective as compared to the conventional approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.83","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5989837","Aspect mining;mining crosscutting concerns","Phase change materials;Radiation detectors;Data mining;Manuals;Mathematical model;Computational modeling;Algorithm design and analysis","aspect-oriented programming;data mining;graph theory;Java;middleware;probability;small-to-medium enterprises","crosscutting concerns mining;probabilistic random walk model;concept graphs;utilization metric;aggregation metric;program sources;program elements;Prism CC miner;Java applications;small-scale drawing application;medium-sized middleware application;large-scale enterprise application server;aspect mining","","5","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Control and definition modularization: an improved software design technique for organizing programs","S. B. Yadav","Dept. of Inf. Syst. & Quantitative Sci., Texas Tech. Univ., Lubbock, TX, USA","IEEE Transactions on Software Engineering","","1990","16","1","92","99","The author proposes a technique called control and definition modularization (CDM), which derives a systematic program layout from a given structure chart using the concepts of 'control' and 'definition' modules. A control module includes processes for handling a conceptual data object not directly implementable. A definition module defines operations associated with a concrete data object implementable using a primitive or derived data type of a programming language. Grouping the operations available for each concrete data object, and keeping them separated from execution flow, improves programs maintainability. This technique extends the structured design methodology and provides designers with a systematic way of deriving informational strength modules as well as a structured physical layout from the structure chart. A program based on the CDM technique is easier to understand and maintain. This research makes a significant contribution toward bridging the gap between structured design and object-oriented concepts.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44367","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44367","","Software design;Organizing;Design methodology;Software maintenance;Concrete;Programming profession;Control systems;Process control;Computer languages;Costs","software engineering","definition modularization;software design technique;organizing programs;control and definition modularization;systematic program layout;conceptual data object;execution flow;programs maintainability;structured design methodology","","","","22","","","","","","IEEE","IEEE Journals & Magazines"
"A Framework for Programming Robust Context-Aware Applications","D. Kulkarni; A. Tripathi","University of Minnesota, Minneapolis; University of Minnesota, Minneapolis","IEEE Transactions on Software Engineering","","2010","36","2","184","197","In this paper, we present a forward recovery model for programming robust context-aware applications. The mechanisms devised as part of this model fall into two categories: asynchronous event handling and synchronous exception handling. These mechanisms enable designing recovery actions to handle different kinds of failure conditions arising in context-aware applications. These include service discovery failures, service binding failures, exceptions raised by a service, and context invalidations. This model is integrated in the high-level programming framework that we have designed for building context-aware collaborative (CSCW) applications. In this paper, we demonstrate the capabilities of this model for programming various kinds of recovery patterns in context-aware applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5396345","Exception handling;context-aware applications;robustness;fault tolerance;design methodology.","Robustness;Context-aware services;Context modeling;Access control;Application software;Collaboration;Information systems;Buildings;Fault tolerance;Design methodology","exception handling;groupware;object-oriented programming;software fault tolerance;system recovery;ubiquitous computing","robust context aware application programming;forward recovery model;asynchronous event handling;synchronous exception handling;recovery patterns;service discovery failures;service binding failures;context invalidations;high level programming framework;context aware collaborative applications;CSCW","","33","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing Critical Decision-Based Processes","C. Damas; B. Lambeau; A. van Lamsweerde","Department of Computing, Icteam Institute, Université Catholique de Louvain, Louvain-La-Neuve, Belgium; Department of Computing, Icteam Institute, Université Catholique de Louvain, Louvain-La-Neuve, Belgium; Department of Computing, Icteam Institute, Université Catholique de Louvain, Louvain-La-Neuve, Belgium","IEEE Transactions on Software Engineering","","2014","40","4","338","365","Decision-based processes are composed of tasks whose application may depend on explicit decisions relying on the state of the process environment. In specific domains such as healthcare, decision-based processes are often complex and critical in terms of timing and resources. The paper presents a variety of tool-supported techniques for analyzing models of such processes. The analyses allow a variety of errors to be detected early and incrementally on partial models, notably: inadequate decisions resulting from inaccurate or outdated information about the environment state; incomplete decisions; non-deterministic task selections; unreachable tasks along process paths; and violations of non-functional process requirements involving time, resources or costs. The proposed techniques are based on different instantiations of the same generic algorithm that propagates decorations iteratively through the process model. This algorithm in particular allows event-based models to be automatically decorated with state-based invariants. A formal language supporting both event-based and state-based specifications is introduced as a process modeling language to enable such analyses. This language mimics the informal flowcharts commonly used by process stakeholders. It extends High-Level Message Sequence Charts with guards on task-related and environment-related variables. The language provides constructs for specifying task compositions, task refinements, decision trees, multi-agent communication scenarios, and time and resource constraints. The proposed techniques are demonstrated on the incremental building and analysis of a complex model of a real protocol for cancer therapy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2312954","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6776491","Process modeling;process analysis;model verification;decision errors;safety-critical workflows;non-functional requirements;domain-specific languages;formal specification","Analytical models;Unified modeling language;Algorithm design and analysis;Semantics;Blood;Flowcharts;Medical treatment","cancer;decision trees;formal languages;formal specification;multi-agent systems;patient treatment","critical decision-based process analysis;tool-supported techniques;partial models;environment state;incomplete decisions;nondeterministic task selections;unreachable tasks;process paths;nonfunctional process requirement violation;event-based models;state-based invariants;formal language;event-based specification;state-based specification;process modeling language;informal flowcharts;high-level message sequence charts;task-related variables;environment-related variables;task compositions;task refinements;decision trees;multiagent communication scenarios;time constraints;resource constraints;cancer therapy","","2","","80","","","","","","IEEE","IEEE Journals & Magazines"
"The Performance of Alternative Strategies for Dealing with Deadlocks in Database Management Systems","R. Agrawal; M. J. Carey; L. W. Mcvoy","AT&amp;T Bell Laboratories; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","12","1348","1363","There is growing evidence that, for a fairly wide variety of database workloads and system configurations, locking is the concurrency control strategy of choice. With locking, of course, comes the possibility of deadlocks. Although the database literature is full of algorithms for dealing with deadlocks, very little in the way of practical performance information is available to a database system designer faced with the decision of choosing a good deadlock resolution strategy. This paper is an attempt to bridge this gap in our understanding of the behavior and performance of alternative deadlock resolution strategies. We employ a simulation model of a database environment to study the relative performance of several strategies based on deadlock detection, several strategies based on deadlock prevention, and a strategy based on timeouts. We show that the choice of the best deadlock resolution strategy depends upon the level of data contention, the resource utilization levels, and the types of transactions. We provide guidelines for selecting a deadlock resolution strategy for different operating regions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233145","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702184","Concurrency control;database systems;deadlock;modeling and simulation;transaction processing","System recovery;Database systems;Concurrency control;Bridges;Transaction databases;Algorithm design and analysis;Resource management;Guidelines;Control system synthesis;Hardware","","Concurrency control;database systems;deadlock;modeling and simulation;transaction processing","","23","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Cyclomatic complexity density and software maintenance productivity","G. K. Gill; C. F. Kemerer","Sloan Sch. of Manage., MIT, Cambridge, MA, USA; Sloan Sch. of Manage., MIT, Cambridge, MA, USA","IEEE Transactions on Software Engineering","","1991","17","12","1284","1288","A study of the relationship between the cyclomatic complexity metric (T. McCabe, 1976) and software maintenance productivity, given that a metric that measures complexity should prove to be a useful predictor of maintenance costs, is reported. The cyclomatic complexity metric is a measure of the maximum number of linearly independent circuits in a program control graph. The current research validates previously raised concerns about the metric on a new data set. However, a simple transformation of the metric is investigated whereby the cyclomatic complexity is divided by the size of the system in source statements. thereby determining a complexity density ratio. This complexity density ratio is demonstrated to be a useful predictor of software maintenance productivity on a small pilot sample of maintenance projects.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.106988","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=106988","","Software maintenance;Productivity;Circuits;Software measurement;Software performance;Software engineering;Programming;Control systems;Software metrics;Software quality","software maintenance;software metrics","cyclomatic complexity metric;software maintenance productivity;linearly independent circuits;program control graph;data set;complexity density ratio","","66","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Software Quality Assurance","H. M. Sneed; A. Merey","Software Engineering Service; NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","909","916","This paper describes a family of tools which not only supports software development, but also assures the quality of each software product from the requirements definition to the integrated system. It is based upon an explicit definition of the design objectives and includes specification verification, design evaluation, static program analysis, dynamic program analysis, integration test auditing, and configuration management.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232548","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702108","Dynamic analysis;review techniques;software metrics;software quality assurance;static analysis","Software quality;Documentation;Costs;Programming;Testing;Project management;Software tools;Environmental economics;Quality assurance;Software engineering","","Dynamic analysis;review techniques;software metrics;software quality assurance;static analysis","","16","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Test Generation and Test Prioritization for Simulink Models with Dynamic Behavior","R. Matinnejad; S. Nejati; L. Briand; T. Bruckmann","SnT Centre, University of Luxembourg, luxembourg, Luxembourg Luxembourg (e-mail: reza.matinnejad@uni.lu); SnT Centre, University of Luxembourg, luxembourg, Luxembourg Luxembourg (e-mail: shiva.nejati@uni.lu); SnT Centre, University of Luxembourg, Luxembourg, Luxembourg Luxembourg 2721 (e-mail: lionel.briand@uni.lu); Delphi Automotive, Delphi Automotive, Luxembourg, Luxembourg Luxembourg (e-mail: thomas.bruckmann@delphi.com)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","All engineering disciplines are founded and rely on models, although they may differ on purposes and usages of modeling. Among the different disciplines, the engineering of Cyber Physical Systems (CPSs) particularly relies on models with dynamic behaviors (i.e., models that exhibit time-varying changes). The Simulink modeling platform greatly appeals to CPS engineers since it captures dynamic behavior models. It further provides seamless support for two indispensable engineering activities: (1) automated verification of abstract system models via model simulation, and (2) automated generation of system implementation via code generation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2811489","Delphi Automotive; H2020 European Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8305644","Simulink models;search-based software testing;automotive systems;test generation;test prioritization;test oracle;output diversity;signal features;structural coverage","Software packages;Testing;Tools;Computational modeling;Vehicle dynamics;Scalability","","","","2","","","","","","","","IEEE","IEEE Early Access Articles"
"Using test oracles generated from program documentation","D. K. Peters; D. L. Parnas","Dept. of Electr. & Comput. Eng., McMaster Univ., Hamilton, Ont., Canada; NA","IEEE Transactions on Software Engineering","","1998","24","3","161","173","The paper illustrates how software can be described precisely using LD-relations, how these descriptions can be presented in a readable manner using tabular notations, and one way such descriptions can be used to test programs. The authors describe an algorithm that can be used to generate a test oracle from program documentation, and present the results of using a tool based on it to help test part of a commercial network management application. The results demonstrate that these methods can be effective at detecting errors and greatly increase the speed and accuracy of test evaluation when compared with manual evaluation. Such oracles can be used for unit testing, in situ testing, constructing self-checking software, and ensuring consistency between code and documentation.","0098-5589;1939-3520;2326-3881","","10.1109/32.667877","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=667877","","Documentation;Software testing;Automatic testing;System testing;Application software;Formal specifications;Automata;Software engineering;Software maintenance","system documentation;program testing;formal specification;finite state machines","test oracles;program documentation;LD-relations;tabular notations;algorithm;commercial network management application;error detection;test evaluation speed;test evaluation accuracy;in situ testing;unit testing;self-checking software construction;code","","88","","37","","","","","","IEEE","IEEE Journals & Magazines"
"What Makes a Good Bug Report?","T. Zimmermann; R. Premraj; N. Bettenburg; S. Just; A. Schroter; C. Weiss","Microsoft Research, Redmond; Vrije Universiteit Amsterdam, Amsterdam; Queen's University, Kingston; Saarland University, Saarbruecken; University of Victoria, Victoria; University of Zurich, Zürich","IEEE Transactions on Software Engineering","","2010","36","5","618","643","In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.63","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5487527","Testing and debugging;distribution;maintenance;and enhancement;human factors;management;measurement.","Computer bugs;Programming;Prototypes;Software engineering;Information analysis;Software testing;Debugging;Software maintenance;Human factors;Engineering management","program debugging;program testing;software quality","software development;APACHE;ECLIPSE;MOZILLA;CUEZILLA prototype;bug tracking tools","","82","","66","","","","","","IEEE","IEEE Journals & Magazines"
"RUBRIC: A System for Rule-Based Information Retrieval","B. P. Mc Cune; R. M. Tong; J. S. Dean; D. G. Shapiro","Advanced Information &amp; Decision Systems; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","939","945","A research prototype software system for conceptual information retrieval has been developed. The goal of the system, called RUBRIC, is to provide more automated and relevant access to unformatted textual databases. The approach is to use production rules from artificial intelligence to define a hierarchy of retrieval subtopics, with fuzzy context expressions and specific word phrases at the bottom. RUBRIC allows the definition of detailed queries starting at a conceptual level, partial matching of a query and a document, selection of only the highest ranked documents for presentation to the user, and detailed explanation of how and why a particular document was selected. Initial experiments indicate that a RUBRIC rule set better matches human retrieval judgment than a standard Boolean keyword expression, given equal amounts of effort in defining each. The techniques presented may be useful in stand-alone retrieval systems, front-ends to existing information retrieval systems, or real-time document filtering and routing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232827","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702112","Artificial intelligence;evidential reasoning;expert systems;information retrieval","Information retrieval;Software prototyping;Software systems;Databases;Production;Artificial intelligence;Humans;Real time systems;Information filtering;Information filters","","Artificial intelligence;evidential reasoning;expert systems;information retrieval","","10","","6","","","","","","IEEE","IEEE Journals & Magazines"
"FINE: A fault injection and monitoring environment for tracing the UNIX system behavior under faults","W. -. Kao; R. K. Iyer; D. Tang","Center for Reliable & High Performance Comput., Illinois Univ., Urbana, IL, USA; Center for Reliable & High Performance Comput., Illinois Univ., Urbana, IL, USA; Center for Reliable & High Performance Comput., Illinois Univ., Urbana, IL, USA","IEEE Transactions on Software Engineering","","1993","19","11","1105","1118","The authors present a fault injection and monitoring environment (FINE) as a tool to study fault propagation in the UNIX kernel. FINE injects hardware-induced software errors and software faults into the UNIX kernel and traces the execution flow and key variables of the kernel. FINE consists of a fault injector, a software monitor, a workload generator, a controller, and several analysis utilities. Experiments on SunOS 4.1.2 are conducted by applying FINE to investigate fault propagation and to evaluate the impact of various types of faults. Fault propagation models are built for both hardware and software faults. Transient Markov reward analysis is performed to evaluate the loss of performance due to an injected fault. Experimental results show that memory and software faults usually have a very long latency, while bus and CPU faults tend to crash the system immediately. About half of the detected errors are data faults, which are detected when the system is tries to access an unauthorized memory location. Only about 8% of faults propagate to other UNIX subsystems. Markov reward analysis shows that the performance loss incurred by bus faults and CPU faults is much higher than that incurred by software and memory faults. Among software faults, the impact of pointer faults is higher than that of nonpointer faults.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.256857","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=256857","","Monitoring;Kernel;Transient analysis;Performance analysis;Performance loss;Fault detection;Hardware;Performance evaluation;Delay;Computer crashes","program testing;software tools;system monitoring;Unix","FINE;fault injection and monitoring environment;UNIX system behavior;hardware-induced software errors;software faults;fault injector;software monitor;workload generator;analysis utilities;SunOS 4.1.2;transient Markov reward analysis;bus faults;CPU faults;pointer faults","","85","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Software Architecture Visualization: An Evaluation Framework and Its Application","K. Gallagher; A. Hatch; M. Munro","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","260","270","In order to characterize and improve software architecture visualization practice, the paper derives and constructs a qualitative framework, with seven key areas and 31 features, for the assessment of software architecture visualization tools. The framework is derived by the application of the Goal Question Metric paradigm to information obtained from a literature survey and addresses a number of stakeholder issues. The evaluation is performed from multiple stakeholder perspectives and in various architectural contexts. Stakeholders can apply the framework to determine if a particular software architecture visualization tool is appropriate to a given task. The framework is applied in the evaluation of a collection of six software architecture visualization tools. The framework may also be used as a design template for a comprehensive software architecture visualization tool.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70757","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4378398","Software Architectures;Visualization techniques and methodologies;Software Architectures;Visualization techniques and methodologies","Software architecture;Visualization;Application software;Computer architecture;Guidelines;Software systems;Computer Society;Performance evaluation;Navigation;Computer science","software architecture","software architecture visualization tools;multiple stakeholder perspectives;goal question metric paradigm application","","10","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Specification directed module testing","I. J. Hayes","Programming Research Group, Oxford University Computing Laboratory, Oxford OX1 3QD, England; Department of Computer Science, University of Queensland, St. Lucia, Queensland 4067, Australia","IEEE Transactions on Software Engineering","","1986","SE-12","1","124","133","If a program is developed from a specification in a mathematically rigorous manner, work done in the development can be utilized in the testing of the program. The better understanding afforded by these methods provides a more thorough check on the correct operation of the program under test. This should lead to earlier detection of faults (making it easier to determine their causes), more useful debugging information, and a greater confidence in the correctness of the final product. Overall, a more systematic approach should expedite the task of the program tester and improve software reliability. The testing techniques described here apply to the testing of abstract data types (modulus, packages). The techniques utilize information generated during refinement of a data type, such as the data type invariant and the relationship between the specification and implementation states; this information is used to specify parts of the code to be written for testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312926","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312926","Abstract data types;data type invariant;modules;module testing;packages;pre- and postconditions;retrieval function;software reliability;specification language-Z","Testing;Abstracts;Computational modeling;Debugging;Software;Software reliability;Vegetation","program debugging;program testing;software reliability","module testing;specification;faults;debugging information;correctness;program tester;software reliability","","19","","","","","","","","IEEE","IEEE Journals & Magazines"
"Architecture-based performance analysis applied to a telecommunication system","D. Petriu; C. Shousha; A. Jalnapurkar","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2000","26","11","1049","1065","Software architecture plays an important role in determining software quality characteristics, such as maintainability, reliability, reusability, and performance. Performance effects of architectural decisions can be evaluated at an early stage by constructing and analyzing quantitative performance models, which capture the interactions between the main components of the system as well as the performance attributes of the components themselves. The paper proposes a systematic approach to building layered queueing network (LQN) performance models from a UML description of the high-level architecture of a system and more exactly from the architectural patterns used for the system. The performance model structure retains a clear relationship with the system architecture, which simplifies the task of converting performance analysis results into conclusions and recommendations related to the software architecture. The proposed approach is applied to a telecommunication product for which an LQN model is built and analyzed. The analysis shows how the performance bottleneck is moving from component to component (hardware or software) under different loads and configurations and exposes some weaknesses in the original software architecture, which prevent the system from using the available processing power at full capacity due to excessive serialization.","0098-5589;1939-3520;2326-3881","","10.1109/32.881717","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=881717","","Performance analysis;Software architecture;Computer architecture;Software quality;Maintenance;Buildings;Unified modeling language;Power system modeling;Hardware;Software performance","software performance evaluation;software architecture;software quality;queueing theory;telecommunication computing;specification languages;object-oriented programming","architecture based performance analysis;telecommunication system;software architecture;software quality characteristics;maintainability;reliability;reusability;performance effects;architectural decisions;quantitative performance models;performance attributes;systematic approach;layered queueing network;LQN performance models;UML description;high-level architecture;architectural patterns;performance model structure;system architecture;performance analysis results;telecommunication product;LQN model;performance bottleneck;processing power;serialization","","34","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic generation of path covers based on the control flow analysis of computer programs","A. Bertolino; M. Marre","Istituto di Elaborazione dell'Inf., CNR, Pisa, Italy; NA","IEEE Transactions on Software Engineering","","1994","20","12","885","899","Branch testing a program involves generating a set of paths that will cover every arc in the program flowgraph, called a path cover, and finding a set of program inputs that will execute every path in the path cover. This paper presents a generalized algorithm that finds a path cover for a given program flowgraph. The analysis is conducted on a reduced flowgraph, called a ddgraph, and uses graph theoretic principles differently than previous approaches. In particular, the relations of dominance and implication which form two trees of the arcs of the ddgraph are exploited. These relations make it possible to identify a subset of ddgraph arcs, called unconstrained arcs, having the property that a set of paths exercising all the unconstrained arcs also cover all the arcs in the ddgraph. In fact, the algorithm has been designed to cover all the unconstrained arcs of a given ddgraph: the paths are derived one at a time, each path covering at least one as yet uncovered unconstrained arc. The greatest merits of the algorithm are its simplicity and its flexibility. It consists in just visiting recursively in combination the dominator and the implied trees, and is flexible in the sense that it can derive a path cover to satisfy different requirements, according to the strategy adopted for the selection of the unconstrained arc to be covered at each recursive iteration. This feature of the algorithm can be employed to address the problem of infeasible paths, by adopting the most suitable selection strategy for the problem at hand. Embedding of the algorithm into a software analysis and testing tool is recommended.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.368137","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=368137","","Automatic generation control;Software testing;Performance analysis;Algorithm design and analysis;Tree graphs;Software algorithms;Embedded software;Software tools;Councils;Costs","program testing;software tools;trees (mathematics);program diagnostics;flow graphs;program control structures","path covers;control flow analysis;program branch testing;program flowgraph;program inputs;ddgraph;graph theoretic principles;dominance;implication;arc trees;unconstrained arcs;dominator tree;implied tree;flexibility;simplicity;recursive iteration;infeasible paths;selection strategy;software analysis tool;software testing tool;automated testing tool","","57","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Domain-Specific Automatic Programming","D. R. Barstow","Schlumberger-Doll Research","IEEE Transactions on Software Engineering","","1985","SE-11","11","1321","1336","Domain knowledge is crucial to an automatic programming system and the interaction between domain knowledge and programming at the current time. The NIX project at Schlumberger-Doll Research has been investigating this issue in the context of two application domains related to oil well logging. Based on these experiments we have developed a framework for domain-specific automatic programming. Within the framework, programming is modeled in terms of two activities, formalization and implementation, each of which transforms descriptions of the program as it proceeds through intermediate states of development. The activities and transformations may be used to characterize the interaction of programming knowledge and domain knowledge in an automatic programming system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231881","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701949","Automatic programming;programming knowledge;program transformations","Automatic programming;Petroleum;Well logging;Runtime","","Automatic programming;programming knowledge;program transformations","","89","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Using automatic process clustering for design recovery and distributed debugging","T. Kunz; J. P. Black","Dept. of Comput. Sci., Waterloo Univ., Ont., Canada; Dept. of Comput. Sci., Waterloo Univ., Ont., Canada","IEEE Transactions on Software Engineering","","1995","21","6","515","527","Distributed applications written in Hermes typically consist of a large number of sequential processes. The use of a hierarchy of process clusters can facilitate the debugging of such applications. Ideally, such a hierarchy should be derived automatically. This paper discusses two approaches to automatic process clustering, one analyzing runtime information with a statistical approach and one utilizing additional semantic information. Tools realizing these approaches were developed and a quantitative measure to evaluate process clusters is proposed. The results obtained under both approaches are compared, and indicate that the additional semantic information improves the cluster hierarchies derived. We demonstrate the value of automatic process clustering with an example. It is shown how appropriate process clusters reduce the complexity of the understanding process, facilitating program maintenance activities such as debugging.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.391378","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=391378","","Process design;Debugging;Programming profession;Application software;Information analysis;Electronic switching systems;Runtime;Reverse engineering;Costs;Visualization","program debugging;software maintenance;reverse engineering;parallel languages;parallel programming;software tools;computer aided software engineering","automatic process clustering;design recovery;distributed debugging;Hermes;sequential processes;runtime information analysis;statistical approach;semantic information;cluster hierarchies;understanding process;program maintenance activities;reverse engineering","","16","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Real-Time Euclid: A language for reliable real-time systems","E. Kligerman; A. D. Stoyenko","Department of Computer Science, University of Toronto, Toronto, Ont. M5S 1A4, Canada; Department of Computer Science, University of Toronto, Toronto, Ont. M5S 1A4, Canada","IEEE Transactions on Software Engineering","","1986","SE-12","9","941","949","Real-Time Euclid, a language designed specifically to address reliability and guaranteed schedulability issues in real-time systems, is introduced. Real-Time Euclid uses exception handlers and import/export lists to provide comprehensive error detection, isolation, and recovery. The philosophy of the language is that every exception detectable by the hardware or the software must have an exception-handler clause associated with it. Moreover, the language definition forces every construct in the language to be time- and space-bounded. Consequently, Real-Time Euclid programs can always be analyzed for guaranteed schedulability of their processes. Thus, it is felt that Real-Time Euclid is well-suited for writing reliable real-time software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313049","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313049","Compiler;exception handling;guaranteed response time;real-time systems;run-time system;schedulability;software reliability;system programming languages","Real-time systems;Reactive power;Arrays;Monitoring;Syntactics;Software reliability","programming languages;real-time systems","Real-Time Euclid;reliable real-time systems;address reliability;schedulability issues;real-time systems;exception handlers;import/export lists;error detection;isolation;recovery;exception-handler clause;real-time software","","86","","","","","","","","IEEE","IEEE Journals & Magazines"
"Generation and consistency checking of design and program structures","Z. L. Lichtman","Department of Computer Research and Development, Armament Development Authority, P.O. Box 2250, Haifa, Israel","IEEE Transactions on Software Engineering","","1986","SE-12","1","172","181","The author describes a mini methodology for generation and representation of design and program structures and for structural consistency checking between two successive designs or between a design and a program. This methodology comprises a tool (Program Design Language), a representation, and consistency criteria. The Program Design Language (PDL) extracts structure information, in a controlled way, from the top level program design, through layers of detailed designs, down to the source code itself. It generates an actual, complete, concise, and easily comparable structure representation. Structural consistency between levels can be checked, both at the development phase and at the operation and maintenance phase, ensuring continued structural consistency between the design(s) and the program.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312930","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312930","Consistency checking;Program Design Language (PDL);program structure;software development;software quality assurance;software tools","Customer relationship management;Computer languages;Aerospace electronics;Nickel;Data mining;Program processors","software reliability;specification languages","software reliability;consistency checking;program structures;structural consistency checking;Program Design Language;structure information;program design;source code;development phase;maintenance phase","","4","","","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Evaluation of the Impact of Object-Oriented Code Refactoring on Quality Attributes: A Systematic Literature Review","J. Al Dallal; A. Abdin","Department of Information Science, Kuwait University, Safat, Kuwait; Department of Information Science, Kuwait University, Safat, Kuwait","IEEE Transactions on Software Engineering","","2018","44","1","44","69","Software refactoring is a maintenance task that addresses code restructuring to improve its quality. Many studies have addressed the impact of different refactoring scenarios on software quality. This study presents a systematic literature review that aggregates, summarizes, and discusses the results of 76 relevant primary studies (PSs) concerning the impact of refactoring on several internal and external quality attributes. The included PSs were selected using inclusion and exclusion criteria applied to relevant articles published before the end of 2015. We analyzed the PSs based on a set of classification criteria, including software quality attributes and measures, refactoring scenarios, evaluation approaches, datasets, and impact results. We followed the vote-counting approach to determine the level of consistency among the PS reported results concerning the relationship between refactoring and software quality. The results indicated that different refactoring scenarios sometimes have opposite impacts on different quality attributes. Therefore, it is false that refactoring always improves all software quality aspects. The vote-counting study provided a clear view of the impacts of some individual refactoring scenarios on some internal quality attributes such as cohesion, coupling, complexity, inheritance, and size, but failed to identify their impacts on external and other internal quality attributes due to insufficient findings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2658573","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7833023","quality attribute;quality measure;refactoring scenario;systematic literature review","Software quality;Systematics;Unified modeling language;Bibliographies;Libraries;Object oriented modeling","software maintenance;software metrics;software quality","76 relevant primary studies;PSs;internal quality;external quality;classification criteria;software quality attributes;evaluation approaches;vote-counting approach;different refactoring scenarios;opposite impacts;different quality attributes;software quality aspects;vote-counting study;individual refactoring scenarios;systematic literature review;software refactoring;maintenance task;code restructuring","","2","","99","","","","","","IEEE","IEEE Journals & Magazines"
"The Roles of Execution and Analysis in Algorthm Design","D. M. Steier; E. Kant","Department of Computer Science, Carnegie-Mellon University; NA","IEEE Transactions on Software Engineering","","1985","SE-11","11","1375","1386","The analysis and execution of partial algorithm descriptions is an important part of the algorithm design process (as is borne out by studying the behavior of human algorithm designers). In this paper, we describe a language for representing partially designed algorithms and a process, developmental evaluation, that can discover useful knowledge to guide design. Using these and other results from our research in artificial intelligence, we are building a system, DESIGNER, that automatically designs algorithms. This paper also compares developmental evaluation to execution and analysis techniques used for testing complete programs and for validation of abstract specifications; concepts similar to those found in developmental evaluation are thus shown to apply to all stages of the software life cycle.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231885","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701953","Algorithm design;automatic programming;developmental evaluation;meta-evaluation;symbolic execution","Algorithm design and analysis;Computer science;Concrete;Testing;Process design;Artificial intelligence;Production;Cognitive science;Humans;Buildings","","Algorithm design;automatic programming;developmental evaluation;meta-evaluation;symbolic execution","","5","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Software Architecture Optimization Methods: A Systematic Literature Review","A. Aleti; B. Buhnova; L. Grunske; A. Koziolek; I. Meedeniya","Monash University, Australia; Masaryk University, Brno; University of Kaiserslautern, Kaiserslautern; University of Zurich, Zurich; Swinburne University of Technology, Hawthorn","IEEE Transactions on Software Engineering","","2013","39","5","658","683","Due to significant industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture design has become an important development activity and the research domain is rapidly evolving. In the last decades, software architecture optimization methods, which aim to automate the search for an optimal architecture design with respect to a (set of) quality attribute(s), have proliferated. However, the reported results are fragmented over different research communities, multiple system domains, and multiple quality attributes. To integrate the existing research results, we have performed a systematic literature review and analyzed the results of 188 research papers from the different research communities. Based on this survey, a taxonomy has been created which is used to classify the existing research. Furthermore, the systematic analysis of the research literature provided in this review aims to help the research community in consolidating the existing research efforts and deriving a research agenda for future developments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311410","Software architecture optimization;systematic literature review;optimization methods;problem overview","Taxonomy;Computer architecture;Software;Software architecture;Systematics;Optimization methods","software architecture;software quality","software architecture optimization method;software system;software architecture design;software quality attribute","","102","","245","","","","","","IEEE","IEEE Journals & Magazines"
"The application of formal methods to the assessment of high integrity software","R. E. Bloomfield; P. K. D. Froome","Department of Scientific Services, Central Electricity Generating Board, Gravesend, Kent DA 12 2RS, England; Department of Scientific Services, Central Electricity Generating Board, Gravesend, Kent DA 12 2RS, England","IEEE Transactions on Software Engineering","","1986","SE-12","9","988","993","A case study is presented in which the Vienna development method (VDM), a formal specification and development methodology, was used during the analysis phase of the assessment of a prototype nuclear reactor protection system. The VDM specification was also translated into the logic language Prolog to animate the specification and to provide a diverse implementation for use in back-to-back testing. It is claimed that this technique provides a visible and effective method of analysis which is superior to the informal alternatives.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313053","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313053","","History;Animation;Software;Formal specifications;Abstracts;Data structures;Prototypes","formal logic;specification languages","high integrity software;Vienna development method;VDM;formal specification;development methodology;prototype nuclear reactor protection system;logic language Prolog;diverse implementation;informal alternatives","","9","","","","","","","","IEEE","IEEE Journals & Magazines"
"Abstraction mechanisms for event control in program debugging","B. Lazzerini; L. Lopriore","Inst. de Elettronica e Telecomunicazioni, Pisa Univ., Italy; NA","IEEE Transactions on Software Engineering","","1989","15","7","890","901","In the event-action model of interactions between the debugging system and the program being debugged, an event will occur on the evaluation of a conditional defined in terms of the program activity if the evaluation yields the value true, and an action is an operation performed by the debugging system on the occurrence of an event. This paper presents a set of mechanisms for expressing conditionals at different levels of abstraction. At the lowest level, the authors have the simple conditionals, which can be expressed in terms of the values of the program entities and of the execution of the program statements. Simple conditionals can be grouped to form higher-level compound conditionals, which can be expressed in terms of the state and flow histories. The paper shows that the proposed abstraction mechanisms are powerful tools for monitoring program activity. They adequately support different debugging techniques, and offer the user a considerable degree of control over the debugging experiment.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.29488","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=29488","","Debugging;History;Programming profession;Performance evaluation;Target tracking","data structures;program debugging","abstraction mechanisms;event control;program debugging;conditionals;program entities;program statements;monitoring","","4","","42","","","","","","IEEE","IEEE Journals & Magazines"
"CHARMY: A Framework for Designing and Verifying Architectural Specifications","P. Pelliccione; P. Inverardi; H. Muccini","Universit&#x0E0; dell' Aquila, L'Aquila; Universit&#x0E0; dell' Aquila, L'Aquila; Universit&#x0E0; dell' Aquila, L'Aquila","IEEE Transactions on Software Engineering","","2009","35","3","325","346","Introduced in the early stages of software development, the Charmy framework assists the software architect in making and evaluating architectural choices. Rarely, the software architecture of a system can be established once and forever. Most likely poorly defined and understood architectural constraints and requirements force the software architect to accept ambiguities and move forward to the construction of a suboptimal software architecture. Charmy aims to provide an easy and practical tool for supporting the iterative modeling and evaluation of software architectures. From an UML-based architectural design, an executable prototype is automatically created. Charmy simulation and model checking features help in understanding the functioning of the system and discovering potential inconsistencies of the design. When a satisfactory and stable software architecture is reached, Java code conforming to structural software architecture constraints is automatically generated through suitable transformations. The overall approach is tool supported.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.104","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4711062","Software architectures;model checking.;Software Architectures;Model checking;Design notations and documentation;State diagrams;Rapid prototyping","Software architecture;Computer architecture;Software prototyping;Software systems;Prototypes;Connectors;Unified modeling language;Programming;Java;Topology","Java;program verification;software architecture;Unified Modeling Language","software development;Charmy framework;UML-based architectural design;model checking;Java code;structural software architecture constraints","","32","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Aspectizing Java Access Control","R. Toledo; A. Nunez; E. Tanter; J. Noye","University of Chile, Santiago; &#x0C9;cole des Mines de Nantes-INRIA, LINA, Nantes; University of Chile, Santiago; &#x0C9;cole des Mines de Nantes-INRIA, LINA, Nantes","IEEE Transactions on Software Engineering","","2012","38","1","101","117","It is inevitable that some concerns crosscut a sizeable application, resulting in code scattering and tangling. This issue is particularly severe for security-related concerns: It is difficult to be confident about the security of an application when the implementation of its security-related concerns is scattered all over the code and tangled with other concerns, making global reasoning about security precarious. In this study, we consider the case of access control in Java, which turns out to be a crosscutting concern with a nonmodular implementation based on runtime stack inspection. We describe the process of modularizing access control in Java by means of Aspect-Oriented Programming (AOP). We first show a solution based on AspectJ, the most popular aspect-oriented extension to Java, that must rely on a separate automata infrastructure. We then put forward a novel solution via dynamic deployment of aspects and scoping strategies. Both solutions, apart from providing a modular specification of access control, make it possible to easily express other useful policies such as the Chinese wall policy. However, relying on expressive scope control results in a compact implementation, which, at the same time, permits the straightforward expression of even more interesting policies. These new modular implementations allowed by AOP alleviate maintenance and evolution issues produced by the crosscutting nature of access control.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680915","Programming languages;security;aspect-oriented programming;access control.","Aspect-oriented programming;Access control;Computer architecture;Java;Programming;Computer security","aspect-oriented programming;authorisation;automata theory;Java","Java access control aspectization;code scattering;code tangling;security-related concerns;runtime stack inspection;aspect-oriented programming;AspectJ;automata infrastructure;aspects strategies;scoping strategies;Chinese wall policy","","5","","45","","","","","","IEEE","IEEE Journals & Magazines"
"A Component Model for Model Transformations","J. S. Cuadrado; E. Guerra; J. de Lara","Department of Computer Science, Universidad Autónoma de Madrid, Spain; Department of Computer Science, Universidad Autónoma de Madrid, Spain; Department of Computer Science, Universidad Autónoma de Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","11","1042","1060","Model-driven engineering promotes an active use of models to conduct the software development process. In this way, models are used to specify, simulate, verify, test and generate code for the final systems. Model transformations are key enablers for this approach, being used to manipulate instance models of a certain modelling language. However, while other development paradigms make available techniques to increase productivity through reutilization, there are few proposals for the reuse of model transformations across different modelling languages. As a result, transformations have to be developed from scratch even if other similar ones exist. In this paper, we propose a technique for the flexible reutilization of model transformations. Our proposal is based on generic programming for the definition and instantiation of transformation templates, and on component-based development for the encapsulation and composition of transformations. We have designed a component model for model transformations, supported by an implementation currently targeting the Atlas Transformation Language (ATL). To evaluate its reusability potential, we report on a generic transformation component to analyse workflow models through their transformation into Petri nets, which we have reused for eight workflow languages, including UML Activity Diagrams, YAWL and two versions of BPMN.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339852","Spanish Ministry of Economy and Competitivity with project Go-Lite; EU commission with project MONDO; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6858077","Model-driven engineering;model transformation;reusability;genericity;component-based development","Unified modeling language;Adaptation models;Petri nets;Analytical models;Logic gates;Software;Proposals","object-oriented programming;Petri nets;software reusability","BPMN;YAWL;UML activity diagrams;workflow languages;Petri nets;workflow models;generic transformation component;ATL;Atlas transformation language;component-based development;generic programming;modelling language instance models;software development process;model-driven engineering;model transformations;component model","","14","","69","","","","","","IEEE","IEEE Journals & Magazines"
"A classification and comparison framework for software architecture description languages","N. Medvidovic; R. N. Taylor","Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","1","70","93","Software architectures shift the focus of developers from lines-of-code to coarser-grained architectural elements and their overall interconnection structure. Architecture description languages (ADLs) have been proposed as modeling notations to support architecture-based development. There is, however, little consensus in the research community on what is an ADL, what aspects of an architecture should be modeled in an ADL, and which of several possible ADLs is best suited for a particular problem. Furthermore, the distinction is rarely made between ADLs on one hand and formal specification, module interconnection, simulation and programming languages on the other. This paper attempts to provide an answer to these questions. It motivates and presents a definition and a classification framework for ADLs. The utility of the definition is demonstrated by using it to differentiate ADLs from other modeling notations. The framework is used to classify and compare several existing ADLs, enabling us, in the process, to identify key properties of ADLs. The comparison highlights areas where existing ADLs provide extensive support and those in which they are deficient, suggesting a research agenda for the future.","0098-5589;1939-3520;2326-3881","","10.1109/32.825767","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=825767","","Software architecture;Computer architecture;LAN interconnection;Architecture description languages;Application software;Formal specifications;Computer languages;Connectors;Computer science;Computer Society","software architecture;formal specification;specification languages","software architecture description languages;interconnection structure;modeling notations;architecture-based development;formal specification;module interconnection;simulation;programming languages;classification framework","","801","","74","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis and Design in MSG.84: Formalizing Functional Specifications","V. Berzins; M. Gray","Department of Computer Science, University of Minnesota; NA","IEEE Transactions on Software Engineering","","1985","SE-11","8","657","670","Model building is identified as the most important part of the analysis and design process for software systems. A set of primitives to support this process is presented, along with a formal language, MSG.84, for recording the results of analysis and design. The semantics of the notation is defined in terms of the actor formalism, which is based on a message passing paradigm. The automatic derivation of a graphical form of the specification for user review is discussed. Potentials for computer-aided design based on MSG.84 are indicated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232516","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702076","Actor formalism;concurrency;data abstraction;diagrams;formal language;functional specification;modeling;modularity;user review","Software systems;Formal languages;Programming;Specification languages;Prototypes;Formal specifications;Power system modeling;Buildings;Software design;Process design","","Actor formalism;concurrency;data abstraction;diagrams;formal language;functional specification;modeling;modularity;user review","","14","","34","","","","","","IEEE","IEEE Journals & Magazines"
"An Eye-Tracking Study of Java Programmers and Application to Source Code Summarization","P. Rodeghero; C. Liu; P. W. McBurney; C. McMillan","Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN","IEEE Transactions on Software Engineering","","2015","41","11","1038","1054","Source Code Summarization is an emerging technology for automatically generating brief descriptions of code. Current summarization techniques work by selecting a subset of the statements and keywords from the code, and then including information from those statements and keywords in the summary. The quality of the summary depends heavily on the process of selecting the subset: a high-quality selection would contain the same statements and keywords that a programmer would choose. Unfortunately, little evidence exists about the statements and keywords that programmers view as important when they summarize source code. In this paper, we present an eye-tracking study of 10 professional Java programmers in which the programmers read Java methods and wrote English summaries of those methods. We apply the findings to build a novel summarization tool. Then, we evaluate this tool. Finally, we further analyze the programmers' method summaries to explore specific keyword usage and provide evidence to support the development of source code summarization systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2442238","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7118751","Source code summaries;program comprehension;Source code summaries;program comprehension","Java;Software;Documentation;Navigation;XML;Software engineering","Java;program compilers;source code (software)","eye-tracking study;Java programmer;source code summarization;code generation","","7","","75","","","","","","IEEE","IEEE Journals & Magazines"
"EARMO: An Energy-Aware Refactoring Approach for Mobile Apps","R. Morales; R. Saborido; F. Khomh; F. Chicano; G. Antoniol","Polytechynique Montéal, Montreal, QC, Canada; Polytechynique Montéal, Montreal, QC, Canada; Polytechynique Montéal, Montreal, QC, Canada; University of Málaga, Málaga, Spain; Polytechynique Montéal, Montreal, QC, Canada","IEEE Transactions on Software Engineering","","2018","44","12","1176","1206","The energy consumption of mobile apps is a trending topic and researchers are actively investigating the role of coding practices on energy consumption. Recent studies suggest that design choices can conflict with energy consumption. Therefore, it is important to take into account energy consumption when evolving the design of a mobile app. In this paper, we analyze the impact of eight type of anti-patterns on a testbed of 20 android apps extracted from F-Droid. We propose EARMO, a novel anti-pattern correction approach that accounts for energy consumption when refactoring mobile anti-patterns. We evaluate EARMO using three multiobjective search-based algorithms. The obtained results show that EARMO can generate refactoring recommendations in less than a minute, and remove a median of 84 percent of anti-patterns. Moreover, EARMO extended the battery life of a mobile phone by up to 29 minutes when running in isolation a refactored multimedia app with default settings (no Wi-Fi, no location services, and minimum screen brightness). Finally, we conducted a qualitative study with developers of our studied apps, to assess the refactoring recommendations made by EARMO. Developers found 68 percent of refactorings suggested by EARMO to be very relevant.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2757486","Natural Sciences and Engineering Research Council of Canada (NSERC); Consejo Nacional de Ciencia y Tecnología, México; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8052533","Software maintenance;refactoring;anti-patterns;mobile apps;energy consumption;search-based software engineering","Mobile communication;Energy consumption;Software;Androids;Humanoid robots;Energy measurement;Software maintenance","Android (operating system);mobile computing;power aware computing;search problems;smart phones;software maintenance","EARMO;energy-aware refactoring approach;refactoring recommendations;mobile phone;energy consumption;mobile apps;Android apps;antipattern correction approach;mobile antipatterns;F-Droid;multiobjective search-based algorithms","","3","","96","","","","","","IEEE","IEEE Journals & Magazines"
"On the Implementation and Use of Ada on Fault-Tolerant Distributed Systems","J. C. Knight; J. I. A. Urquhart","Department of Computer Science, University of Virginia; NA","IEEE Transactions on Software Engineering","","1987","SE-13","5","553","563","In this paper, we discuss the use of Ada® on distributed systems in which failure of processors has to be tolerated. We assume that tasks are the primary object of distribution, and that communication between tasks on separate processors will take place using the facilities of the Ada language. It would be possible to build a separate set of facilities for communication between processors, and to treat the software on each machine as a separate program. This is unnecessary and undesirable. In addition, the Ada language Reference Manual states specifically that a system consisting of communicating processors with private memories is suitable for executing an Ada program.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233200","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702255","Ada;distributed systems;fault tolerance;highly reliable systems;tolerance of processor failure","Fault tolerant systems;Hardware;Application software;Protocols;Embedded software;Aerospace electronics;Costs;Computer displays;Actuators;Microprocessors","","Ada;distributed systems;fault tolerance;highly reliable systems;tolerance of processor failure","","4","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Dependence Guided Symbolic Execution","H. Wang; T. Liu; X. Guan; C. Shen; Q. Zheng; Z. Yang","Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Computer Science, Western Michigan University, Kalamazoo, MI","IEEE Transactions on Software Engineering","","2017","43","3","252","271","Symbolic execution is a powerful technique for systematically exploring the paths of a program and generating the corresponding test inputs. However, its practical usage is often limited by the<italic>path explosion</italic>problem, that is, the number of explored paths usually grows exponentially with the increase of program size. In this paper, we argue that for the purpose of fault detection it is not necessary to systematically explore the paths, and propose a new symbolic execution approach to mitigate the path explosion problem by predicting and eliminating the redundant paths based on symbolic value. Our approach can achieve the equivalent fault detection capability as traditional symbolic execution without exhaustive path exploration. In addition, we develop a practical implementation called Dependence Guided Symbolic Execution (DGSE) to soundly approximate our approach. Through exploiting program dependence, DGSE can predict and eliminate the redundant paths at a reasonable computational cost. Our empirical study shows that the redundant paths are abundant and widespread in a program. Compared with traditional symbolic execution, DGSE only explores 6.96 to 96.57 percent of the paths and achieves a speedup of 1.02<inline-formula><tex-math notation=""LaTeX"">$\times$</tex-math><alternatives><inline-graphic xlink:href=""liu-ieq1-2584063.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>to 49.56<inline-formula><tex-math notation=""LaTeX"">$\times$</tex-math><alternatives><inline-graphic xlink:href=""liu-ieq2-2584063.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>. We have released our tool and the benchmarks used to evaluate DGSE<inline-formula><tex-math notation=""LaTeX"">$^\ast$</tex-math><alternatives><inline-graphic xlink:href=""liu-ieq3-2584063.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2584063","National Natural Science Foundation of China; Fok Ying-Tong Education Foundation; National Research Program of China; Ministry of Education Innovation Research Team; Fundamental Research Funds for the Central Universities; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7497518","Symbolic execution;path coverage;program dependence","Fault detection;Explosions;Benchmark testing;Electronic mail;Computational efficiency;Input variables","","","","9","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Engineering of Framework-Specific Modeling Languages","M. Antkiewicz; K. Czarnecki; M. Stephan","University of Waterloo, Waterloo; University of Waterloo, Waterloo; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2009","35","6","795","824","Framework-specific modeling languages (FSMLs) help developers build applications based on object-oriented frameworks. FSMLs model abstractions and rules of application programming interfaces (APIs) exposed by frameworks and can express models of how applications use APIs. Such models aid developers in understanding, creating, and evolving application code. We present four exemplar FSMLs and a method for engineering new FSMLs. The method was created postmortem by generalizing the experience of building the exemplars and by specializing existing approaches to domain analysis, software development, and quality evaluation of models and languages. The method is driven by the use cases that the FSML under development should support and the evaluation of the constructed FSML is guided by two existing quality frameworks. The method description provides concrete examples for the engineering steps, outcomes, and challenges. It also provides strategies for making engineering decisions. Our work offers a concrete example of software language engineering and its benefits. FSMLs capture existing domain knowledge in language form and support application code understanding through reverse engineering, application code creation through forward engineering, and application code evolution through round-trip engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4907004","Framework-specific modeling language;domain-specific language;object-oriented framework;application programming interface (API);feature model;framework-specific model;forward engineering;reverse engineering;round-trip engineering;evolution;code pattern;mapping.","Object oriented modeling;Application software;Documentation;Writing;Concrete;Reverse engineering;Knowledge engineering;Java;Scattering;Buildings","application program interfaces;object-oriented programming;software engineering","framework-specific modeling languages;object-oriented frameworks;application programming interfaces;software development;software language engineering;reverse engineering;application code creation through forward engineering;application code evolution through round-trip engineering","","24","","102","","","","","","IEEE","IEEE Journals & Magazines"
"CoMoM: Efficient Class-Oriented Evaluation of Multiclass Performance Models","G. Casale","College of William and Mary, Williamsburg","IEEE Transactions on Software Engineering","","2009","35","2","162","177","We introduce the class-oriented method of moments (CoMoM), a new exact algorithm to compute performance indexes in closed multiclass queuing networks. Closed models are important for performance evaluation of multitier applications, but when the number of service classes is large, they become too expensive to solve with exact methods such as mean value analysis (MVA). CoMoM addresses this limitation by a new recursion that scales efficiently with the number of classes. Compared to the MVA algorithm, which recursively computes mean queue lengths, CoMoM also carries on in the recursion information on higher-order moments of queue lengths. We show that this additional information greatly reduces the number of operations needed to solve the model and makes CoMoM the best-available algorithm for networks with several classes. We conclude the paper by generalizing CoMoM to the efficient computation of marginal queue-length probabilities, which finds application in the evaluation of state-dependent attributes such as quality-of-service metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4641939","Performance of Systems;Modeling techniques;Queuing theory;Performance of Systems;Modeling techniques;Queuing theory","Performance analysis;Moment methods;Quality of service;Queueing analysis;Network servers;Computer networks;Capacity planning;Web server;Transaction databases;Algorithm design and analysis","method of moments;probability;quality of service;queueing theory","class-oriented method of moment;closed multiclass queueing network;marginal queue-length probability;performance evaluation;multitier application;mean value analysis;higher-order moment;J2EE application;state-dependent index evaluation;energy consumption;quality-of-service metrics","","4","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting scenario-based requirements engineering","A. G. Sutcliffe; N. A. M. Maiden; S. Minocha; D. Manuel","Centre for HCI Design, City Univ., London, UK; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","12","1072","1088","Scenarios have been advocated as a means of improving requirements engineering yet few methods or tools exist to support scenario based RE. The paper reports a method and software assistant tool for scenario based RE that integrates with use case approaches to object oriented development. The method and operation of the tool are illustrated with a financial system case study. Scenarios are used to represent paths of possible behavior through a use case, and these are investigated to elaborate requirements. The method commences by acquisition and modeling of a use case. The use case is then compared with a library of abstract models that represent different application classes. Each model is associated with a set of generic requirements for its class, hence, by identifying the class(es) to which the use case belongs, generic requirements can be reused. Scenario paths are automatically generated from use cases, then exception types are applied to normal event sequences to suggest possible abnormal events resulting from human error. Generic requirements are also attached to exceptions to suggest possible ways of dealing with human error and other types of system failure. Scenarios are validated by rule based frames which detect problematic event patterns. The tool suggests appropriate generic requirements to deal with the problems encountered. The paper concludes with a review of related work and a discussion of the prospects for scenario based RE methods and tools.","0098-5589;1939-3520;2326-3881","","10.1109/32.738340","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=738340","","Object oriented modeling;Humans;Software tools;Software engineering;Libraries;Event detection;Sociotechnical systems;Concrete;Animation;Inspection","formal specification;systems analysis;software tools;object-oriented programming;financial data processing;software reusability;software performance evaluation;bibliographies","scenario based requirements engineering;scenario based RE;software assistant tool;use case approaches;object oriented development;financial system case study;abstract models;application classes;exception types;generic requirements reuse;scenario paths;normal event sequences;abnormal events;human error;system failure;rule based frames;problematic event patterns","","168","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Equality to Equals and Unequals: A Revisit of the Equivalence and Nonequivalence Criteria in Class-Level Testing of Object-Oriented Software","H. Y. Chen; T. H. Tse","Jinan University, Guangzhou; The University of Hong Kong, Hong Kong","IEEE Transactions on Software Engineering","","2013","39","11","1549","1563","Algebraic specifications have been used in the testing of object-oriented programs and received much attention since the 1990s. It is generally believed that class-level testing based on algebraic specifications involves two independent aspects: the testing of equivalent and nonequivalent ground terms. Researchers have cited intuitive examples to illustrate the philosophy that even if an implementation satisfies all the requirements specified by the equivalence of ground terms, it may still fail to satisfy some of the requirements specified by the nonequivalence of ground terms. Thus, both the testing of equivalent ground terms and the testing of nonequivalent ground terms have been considered as significant and cannot replace each other. In this paper, we present an innovative finding that, given any canonical specification of a class with proper imports, a complete implementation satisfies all the observationally equivalent ground terms if and only if it satisfies all the observationally nonequivalent ground terms. As a result, these two aspects of software testing cover each other and can therefore replace each other. These findings provide a deeper understanding of software testing based on algebraic specifications, rendering the theory more elegant and complete. We also highlight a couple of important practical implications of our theoretical results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6570471","Software testing;equivalence criterion;nonequivalence criterion;algebraic specification;object-oriented software","Software;Software testing;Observers;Context;Semantics;Computer science","algebraic specification;object-oriented programming;program testing","nonequivalence criteria;class-level testing;object-oriented software;algebraic specifications;object-oriented programs;equivalent ground terms;canonical specification;software testing","","5","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Regeneration with virtual copies for distributed computing systems","N. R. Adam; R. Tewari","MS/CIS Dept., Rutgers Univ., Newark, NJ, USA; NA","IEEE Transactions on Software Engineering","","1993","19","6","594","602","The authors consider the consistency control problem for replicated data in a distributed computing system (DCS) and propose a new algorithm to dynamically regenerate copies of data objects in response to node failures and network partitioning in the system. The DCS is assumed to have strict consistency constraints for data object copies. The algorithm combines the advantages of voting-based algorithms and regeneration mechanisms to maintain mutual consistency of replicated data objects in the case of node failures and network partitioning. The algorithm extends the feasibility of regeneration to DCS on wide area networks and is able to satisfy user queries as long as there is one current partition in the system. A stochastic availability analysis of the algorithm shows that it provides improved availability as compared to previously proposed dynamic voting algorithms.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232024","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232024","","Distributed computing;Partitioning algorithms;Distributed control;Heuristic algorithms;Availability;Control systems;Wide area networks;Stochastic processes;Algorithm design and analysis;Voting","distributed databases;query processing","distributed databases;virtual copies;consistency control problem;replicated data;distributed computing system;node failures;network partitioning;consistency constraints;data object copies;voting-based algorithms;regeneration mechanisms;wide area networks;user queries;stochastic availability analysis;dynamic voting algorithms","","6","","24","","","","","","IEEE","IEEE Journals & Magazines"
"PECAN: Program Development Systems that Support Multiple Views","S. P. Reiss","Department of Computer Science, Brown University","IEEE Transactions on Software Engineering","","1985","SE-11","3","276","285","This paper describes the PECAN family of program development systems. PECAN supports multiple views of the user's program. The views can be representations of the program or of the corresponding semantics. The primary program view is a syntax-directed editor. The current semantic views include expression trees, data type diagrams, flow graphs, and the symbol table. PECAN is designed to make effective use of powerful personal machines with high-resolution graphics displays and is currently implemented on APOLLO workstations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232211","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702004","Incremental compilation;multiple views;program development systems;programming environments;syntax-directed editors","Tree graphs;Data structures;Computer displays;Workstations;Programming environments;Programming profession;Feedback;Flowcharts;Tree data structures;Flow graphs","","Incremental compilation;multiple views;program development systems;programming environments;syntax-directed editors","","89","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Testing from Partial Finite State Machines without Harmonised Traces","R. M. Hierons","Department of Computer Science, Brunel University London, Uxbridge, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","11","1033","1043","This paper concerns the problem of testing from a partial, possibly non-deterministic, finite state machine (FSM) S. Two notions of correctness (quasi-reduction and quasi-equivalence) have previously been defined for partial FSMs but these, and the corresponding test generation techniques, only apply to FSMs that have harmonised traces. We show how quasi-reduction and quasi-equivalence can be generalised to all partial FSMs. We also consider the problem of generating an m-complete test suite from a partial FSM S: a test suite that is guaranteed to determine correctness as long as the system under test has no more than m states. We prove that we can complete S to form a completely-specified non-deterministic FSM S' such that any m-complete test suite generated from S' can be converted into an m-complete test suite for S. We also show that there is a correspondence between test suites that are reduced for S and S' and also that are minimal for S and S'.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2652457","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7815407","Software engineering/software/program verification;software engineering/testing and debugging;systems and software;checking experiment;partial finite state machine","Testing;Fault detection;Redundancy;Automata;Indexes;Software;Debugging","finite state machines;formal specification;program testing","finite state machine;test generation techniques;nondeterministic FSM;partial finite state machines;m-complete test suite;harmonised traces;partial FSMs","","","","30","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"An object-oriented knowledge representation for spatial information","L. Mohan; R. L. Kashyap","Sch. of Electr. Eng., Purdue Univ., West Lafayette, IN, USA; Sch. of Electr. Eng., Purdue Univ., West Lafayette, IN, USA","IEEE Transactions on Software Engineering","","1988","14","5","675","681","An abstract formalism for the representation of spatial knowledge is suggested. The focus is on the development of a comprehensive representation scheme for pictorial information in which the knowledge model of the given world has a high degree of perceptual similarity to a typical user's view of the same world. The model that has been developed uses the object-oriented method of knowledge representation. The intention is that with this model any user of the system will be equipped to depict pictorial information easily and will be able to portray spatial as well as conceptual abstractions, generalizations, and rules at various levels.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6146","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6146","","Knowledge representation;Object oriented modeling;Spatial resolution;Relational databases;Data models;Automatic logic units;Engines;Pattern matching;Object oriented databases;Spatial databases","data structures;knowledge engineering","object-oriented knowledge representation;spatial information;abstract formalism;pictorial information;knowledge model;object-oriented method;conceptual abstractions","","36","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Extracting Development Tasks to Navigate Software Documentation","C. Treude; M. P. Robillard; B. Dagenais","Departamento de Informática e Matemática Aplicada, Universidade Federal do Rio Grande do Norte, Natal, RN, Brazil; School of Computer Science, McGill University, Montréal, QC, Canada; Resulto, Montréal, QC, Canada","IEEE Transactions on Software Engineering","","2015","41","6","565","581","Knowledge management plays a central role in many software development organizations. While much of the important technical knowledge can be captured in documentation, there often exists a gap between the information needs of software developers and the documentation structure. To help developers navigate documentation, we developed a technique for automatically extracting tasks from software documentation by conceptualizing tasks as specific programming actions that have been described in the documentation. More than 70 percent of the tasks we extracted from the documentation of two projects were judged meaningful by at least one of two developers. We present TaskNavigator, a user interface for search queries that suggests tasks extracted with our technique in an auto-complete list along with concepts, code elements, and section headers. We conducted a field study in which six professional developers used TaskNavigator for two weeks as part of their ongoing work. We found search results identified through extracted tasks to be more helpful to developers than those found through concepts, code elements, and section headers. The results indicate that task descriptions can be effectively extracted from software documentation, and that they help bridge the gap between documentation structure and the information needs of software developers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2387172","NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7000568","Software Documentation;Development Tasks;Navigation;Auto-Complete;Natural Language Processing;Software documentation;development tasks;navigation;auto-complete;natural language processing","Documentation;Software;Navigation;Data mining;Programming;Natural language processing;Subscriptions","knowledge management;software engineering;user interfaces","software documentation navigation;knowledge management;software developers;information needs;programming action;TaskNavigator user interface;documentation structure","","14","","59","","","","","","IEEE","IEEE Journals & Magazines"
"X-FEDERATE: a policy engineering framework for federated access management","R. Bhatti; E. Bertino; A. Ghafoor","Dept. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA; Dept. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA; Dept. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","IEEE Transactions on Software Engineering","","2006","32","5","330","346","Policy-based management (PBM) has been considered as a promising approach for design and enforcement of access management policies for distributed systems. The increasing shift toward federated information sharing in the organizational landscape, however, calls for revisiting current PBM approaches to satisfy the unique security requirements of the federated paradigm. This presents a twofold challenge for the design of a PBM approach, where, on the one hand, the policy must incorporate the access management needs of the individual systems, while, on the other hand, the policies across multiple systems must be designed in such a manner that they can be uniformly developed, deployed, and integrated within the federated system. In this paper, we analyze the impact of security management challenges on policy design and formulate a policy engineering methodology based on principles of software engineering to develop a PBM solution for federated systems. We present X-FEDERATE, a policy engineering framework for federated access management using an extension of the well-known role-based access control (RBAC) model. Our framework consists of an XML-based policy specification language, its UML-based meta-model, and an enforcement architecture. We provide a comparison of our framework with related approaches and highlight its significance for federated access management. The paper also presents a federation protocol and discusses a prototype of our framework that implements the protocol in a federated digital library environment","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642680","Federated systems;software engineering;security management;role-based access control.","Engineering management;Access protocols;Information security;Software development management;Design engineering;Software engineering;Access control;Specification languages;Computer architecture;Prototypes","authorisation;data models;digital libraries;software architecture;Unified Modeling Language;XML","X-FEDERATE;policy engineering framework;federated access management;policy-based management;distributed systems;federated information sharing;security management;role-based access control;XML-based policy specification language;UML-based meta-model;enforcement architecture;federated digital library environment","","24","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A Survey on Metamorphic Testing","S. Segura; G. Fraser; A. B. Sanchez; A. Ruiz-Cortés","Department of Computer Languages and Systems, Universidad de Sevilla, Spain; Department of Computer Science, University of Sheffield, Sheffield, United Kingdom; Department of Computer Languages and Systems, Universidad de Sevilla, Spain; Department of Computer Languages and Systems, Universidad de Sevilla, Spain","IEEE Transactions on Software Engineering","","2016","42","9","805","824","A test oracle determines whether a test execution reveals a fault, often by comparing the observed program output to the expected output. This is not always practical, for example when a program's input-output relation is complex and difficult to capture formally. Metamorphic testing provides an alternative, where correctness is not determined by checking an individual concrete output, but by applying a transformation to a test input and observing how the program output “morphs” into a different one as a result. Since the introduction of such metamorphic relations in 1998, many contributions on metamorphic testing have been made, and the technique has seen successful applications in a variety of domains, ranging from web services to computer graphics. This article provides a comprehensive survey on metamorphic testing: It summarises the research results and application areas, and analyses common practice in empirical studies of metamorphic testing as well as the main open challenges.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2532875","European Commission (FEDER); Spanish Government; CICYT projects TAPAS; BELI; Andalusian Government projects THEOS; COPAS; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7422146","Metamorphic testing;oracle problem;survey","Testing;Search engines;Google;Libraries;Concrete;Distance measurement;Web services","program testing","metamorphic testing;test oracle;test execution;metamorphic relations","","45","","148","","","","","","IEEE","IEEE Journals & Magazines"
"Estimating the probability of failure when testing reveals no failures","K. W. Miller; L. J. Morell; R. E. Noonan; S. K. Park; D. M. Nicol; B. W. Murrill; M. Voas","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1992","18","1","33","43","Formulas for estimating the probability of failure when testing reveals no errors are introduced. These formulas incorporate random testing results, information about the input distribution; and prior assumptions about the probability of failure of the software. The formulas are not restricted to equally likely input distributions, and the probability of failure estimate can be adjusted when assumptions about the input distribution change. The formulas are based on a discrete sample space statistical model of software and include Bayesian prior assumptions. Reusable software and software in life-critical applications are particularly appropriate candidates for this type of analysis.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.120314","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=120314","","Software testing;Bayesian methods;Software reusability;Software reliability;Computer errors;Application software;Probability density function;System testing;NASA;Computer science","Bayes methods;probability;program testing","failure probability estimation;formulas;random testing results;input distribution;prior assumptions;failure estimate;discrete sample space statistical model;Bayesian prior assumptions;life-critical applications","","153","","25","","","","","","IEEE","IEEE Journals & Magazines"
"An improved algorithm based on subset closures for synthesizing a relational database scheme","C. -. Yang; G. Li; P. A. -. Ng","North Texas State Univ., Denton, TX, USA; North Texas State Univ., Denton, TX, USA; North Texas State Univ., Denton, TX, USA","IEEE Transactions on Software Engineering","","1988","14","11","1731","1738","An algorithm for synthesizing a better relational database scheme in elementary key normal form (EKNF) is developed. This algorithm eliminates not only extraneous attributes and other redundancies, but also superfluities from a given set of functional dependences (FDs), based primarily on subset closures, Hamiltonian cycles of FDs, and equivalent subsets of attributes. Following this algorithm, a better LR-minimum FD covering is obtained. A more practical and efficient method for designing a relational database scheme in EKNF is then provided. The time complexity of the algorithm is polynomial.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9058","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9058","","Relational databases;Inference algorithms;Art;Design methodology;Software algorithms;Chaos;Polynomials;Algorithm design and analysis;Information science","computational complexity;database theory;relational databases;set theory","subset closures;relational database scheme;elementary key normal form;functional dependences;subset closures;Hamiltonian cycles;time complexity","","3","","10","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical validation of object-oriented metrics in two different iterative software processes","M. Alshayeb; Wei Li","Dept. of Comput. Sci., Alabama Univ., Huntsville, AL, USA; Dept. of Comput. Sci., Alabama Univ., Huntsville, AL, USA","IEEE Transactions on Software Engineering","","2003","29","11","1043","1049","Object-oriented (OO) metrics are used mainly to predict software engineering activities/efforts such as maintenance effort, error proneness, and error rate. There have been discussions about the effectiveness of metrics in different contexts. In this paper, we present an empirical study of OO metrics in two iterative processes: the short-cycled agile process and the long-cycled framework evolution process. We find that OO metrics are effective in predicting design efforts and source lines of code added, changed, and deleted in the short-cycled agile process and ineffective in predicting the same aspects in the long-cycled framework process. This leads us to believe that OO metrics' predictive capability is limited to the design and implementation changes during the development iterations, not the long-term evolution of an established system in different releases.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245305","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245305","","Predictive models;Size measurement;Software maintenance;Object oriented modeling;Software measurement;Software systems;Testing;Software engineering;Error analysis;Lead","object-oriented programming;software metrics;software maintenance","object-oriented metrics;iterative software processes;software maintenance;short-cycled agile process;long-cycled framework evolution process","","65","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Qualitative methods in empirical studies of software engineering","C. B. Seaman","Dept. of Inf. Syst., Maryland Univ., Baltimore, MD, USA","IEEE Transactions on Software Engineering","","1999","25","4","557","572","While empirical studies in software engineering are beginning to gain recognition in the research community, this subarea is also entering a new level of maturity by beginning to address the human aspects of software development. This added focus has added a new layer of complexity to an already challenging area of research. Along with new research questions, new research methods are needed to study nontechnical aspects of software engineering. In many other disciplines, qualitative research methods have been developed and are commonly used to handle the complexity of issues involving human behaviour. The paper presents several qualitative methods for data collection and analysis and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combined with quantitative methods. To illustrate this use of qualitative methods, examples from real software engineering studies are used throughout.","0098-5589;1939-3520;2326-3881","","10.1109/32.799955","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=799955","","Software engineering;Humans;Programming;Data analysis;Design for experiments;Laboratories;Computer industry;Software development management;Design methodology","user interfaces;human factors;software development management;project management","qualitative methods;empirical studies;software engineering;research community;human aspects;software development;research questions;research methods;nontechnical aspects;qualitative research methods;human behaviour;data collection;quantitative methods;real software engineering studies","","422","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Test Case Prioritization Using Lexicographical Ordering","S. Eghbali; L. Tahvildari","Department of Electrical and Computer Engineering, 200 University Ave West, University of Waterloo, Waterloo, Ontario; Department of Electrical and Computer Engineering, 200 University Ave West, University of Waterloo, Waterloo, Ontario","IEEE Transactions on Software Engineering","","2016","42","12","1178","1195","Test case prioritization aims at ordering test cases to increase the rate of fault detection, which quantifies how fast faults are detected during the testing phase. A common approach for test case prioritization is to use the information of previously executed test cases, such as coverage information, resulting in an iterative (greedy) prioritization algorithm. Current research in this area validates the fact that using coverage information can improve the rate of fault detection in prioritization algorithms. The performance of such iterative prioritization schemes degrade as the number of ties encountered in prioritization steps increases. In this paper, using the notion of lexicographical ordering, we propose a new heuristic for breaking ties in coverage based techniques. Performance of the proposed technique in terms of the rate of fault detection is empirically evaluated using a wide range of programs. Results indicate that the proposed technique can resolve ties and in turn noticeably increases the rate of fault detection.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2550441","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7456343","Regression testing;test case prioritization;lexicographical ordering","Software testing;Fault detection;Feature extraction;Regression analysis;Fault diagnosis","fault diagnosis;greedy algorithms;iterative methods;program testing;regression analysis","regression testing;coverage information;iterative greedy prioritization algorithm;fault detection;lexicographical ordering;test case prioritization","","8","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Abstracting runtime heaps for program understanding","M. Marron; C. Sanchez; Z. Su; M. Fahndrich","Imdea Software Institute, Boadilla del Monte; Imdea Software Institute, Boadilla del Monte; University of California, Davis, Davis; Microsoft Research","IEEE Transactions on Software Engineering","","2013","39","6","774","786","Modern programming environments provide extensive support for inspecting, analyzing, and testing programs based on the algorithmic structure of a program. Unfortunately, support for inspecting and understanding runtime data structures during execution is typically much more limited. This paper provides a general purpose technique for abstracting and summarizing entire runtime heaps. We describe the abstract heap model and the associated algorithms for transforming a concrete heap dump into the corresponding abstract model as well as algorithms for merging, comparing, and computing changes between abstract models. The abstract model is designed to emphasize high-level concepts about heap-based data structures, such as shape and size, as well as relationships between heap structures, such as sharing and connectivity. We demonstrate the utility and computational tractability of the abstract heap model by building a memory profiler. We use this tool to identify, pinpoint, and correct sources of memory bloat for programs from DaCapo.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6331492","Heap structure;runtime analysis;memory profiling;program understanding","Abstracts;Concrete;Shape;Runtime;Arrays;Computational modeling","data structures;merging;program diagnostics;program testing","program testing;program analysis;program inspection;program algorithmic structure;runtime data structure relationships;runtime heap abstracting;runtime heap summarization;concrete heap dump;program merging;program comparison;program computing;high-level concepts;heap structure sharing;heap structure connectivity;abstract heap model utility;abstract heap model computational tractability;memory profiler;memory bloat;DaCapo","","7","","37","","","","","","IEEE","IEEE Journals & Magazines"
"A metrics suite for object oriented design","S. R. Chidamber; C. F. Kemerer","MIT, Cambridge, MA, USA; MIT, Cambridge, MA, USA","IEEE Transactions on Software Engineering","","1994","20","6","476","493","Given the central role that software development plays in the delivery and application of information technology, managers are increasingly focusing on process improvement in the software development area. This demand has spurred the provision of a number of new and/or improved approaches to software development, with perhaps the most prominent being object-orientation (OO). In addition, the focus on process improvement has increased the demand for software measures, or metrics with which to manage the process. The need for such metrics is particularly acute when an organization is adopting a new technology for which established practices have yet to be developed. This research addresses these needs through the development and implementation of a new suite of metrics for OO design. Metrics developed in previous research, while contributing to the field's understanding of software development processes, have generally been subject to serious criticisms, including the lack of a theoretical base. Following Wand and Weber (1989), the theoretical base chosen for the metrics was the ontology of Bunge (1977). Six design metrics are developed, and then analytically evaluated against Weyuker's (1988) proposed set of measurement principles. An automated data collection tool was then developed and implemented to collect an empirical sample of these metrics at two field sites in order to demonstrate their feasibility and suggest ways in which managers may use these metrics for process improvement.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.295895","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=295895","","Programming;Information management;Software measurement;Engineering management;Application software;Information technology;Software development management;Technology management;Ontologies;Software engineering","object-oriented programming;object-oriented methods;software metrics","object oriented design;metrics suite;software development;process improvement;software measures;organization;automated data collection tool;measurement principles;object oriented programming","","2155","","50","","","","","","IEEE","IEEE Journals & Magazines"
"ABE: an environment for engineering intelligent systems","L. D. Erman; J. S. Lark; F. Hayes-Roth","Teknowledge Inc., Palo Alto, CA, USA; Teknowledge Inc., Palo Alto, CA, USA; Teknowledge Inc., Palo Alto, CA, USA","IEEE Transactions on Software Engineering","","1988","14","12","1758","1770","The ABE multilevel architecture for developing intelligent systems addresses the key problems of intelligent systems engineering: large-scale applications and the reuse and integration of software components. ABE defines a virtual machine for module-oriented programming and a cooperative operating system that provides access to the capabilities of that virtual machine. On top of the virtual machine, ABE provides a number of system design and development frameworks, which embody such programming metaphors as control flow, blackboards, and dataflow. These frameworks support the construction of capabilities, including knowledge processing tools, which span a range from primitive modules to skeletal systems. Finally, applications can be built on skeletal systems. In addition, ABE supports the importation of existing software, including both conventional and knowledge processing tools.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9062","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9062","","Systems engineering and theory;Intelligent systems;Machine intelligence;Virtual machining;Application software;Computer architecture;Large scale integration;Operating systems;Control systems;Modular construction","knowledge engineering;operating systems (computers);programming environments;software reusability;software tools;virtual machines","software reusability;programming environments;knowledge engineering;ABE;intelligent systems;multilevel architecture;large-scale applications;virtual machine;module-oriented programming;cooperative operating system;control flow;blackboards;dataflow;knowledge processing tools;skeletal systems","","18","","24","","","","","","IEEE","IEEE Journals & Magazines"
"The JEDI event-based infrastructure and its application to the development of the OPSS WFMS","G. Cugola; E. Di Nitto; A. Fuggetta","Dept. of Electron. & Inf., Politecnico di Milano, Italy; NA; NA","IEEE Transactions on Software Engineering","","2001","27","9","827","850","The development of complex distributed systems demands the creation of suitable architectural styles (or paradigms) and related runtime infrastructures. An emerging style that is receiving increasing attention is based on the notion of event. In an event-based architecture, distributed software components interact by generating and consuming events. An event is the occurrence of some state change in a component of a software system, made visible to the external world. The occurrence of an event in a component is asynchronously notified to any other component that has declared some interest in it. This paradigm (usually called ""publish/subscribe"", from the names of the two basic operations that regulate the communication) holds the promise of supporting a flexible and effective interaction among highly reconfigurable, distributed software components. In the past two years, we have developed an object-oriented infrastructure called JEDI (Java event-based distributed infrastructure). JEDI supports the development and operation of event-based systems and has been used to implement a significant example of distributed system, namely, the OPSS workflow management system (WFMS). The paper illustrates the main features of JEDI and how we have used them to implement OPSS. Moreover, the paper provides an initial evaluation of our experiences in using the event-based architectural style and a classification of some of the event-based infrastructures presented in the literature.","0098-5589;1939-3520;2326-3881","","10.1109/32.950318","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=950318","","Middleware;Runtime;Computer architecture;Software systems;Java;Workflow management software;Software architecture;Broadcasting;Computer networks;Distributed computing","reviews;software architecture;workflow management software;distributed object management;Java","JEDI event-based infrastructure;OPSS WFMS;complex distributed systems;architectural styles;runtime infrastructures;event-based architecture;distributed software components;publish/subscribe;object-oriented infrastructure;Java event-based distributed infrastructure;workflow management system;software architectures;business processes;middleware","","263","","61","","","","","","IEEE","IEEE Journals & Magazines"
"On the effectiveness of the test-first approach to programming","H. Erdogmus; M. Morisio; M. Torchiano","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2005","31","3","226","237","Test-driven development (TDD) is based on formalizing a piece of functionality as a test, implementing the functionality such that the test passes, and iterating the process. This paper describes a controlled experiment for evaluating an important aspect of TDD: in TDD, programmers write functional tests before the corresponding implementation code. The experiment was conducted with undergraduate students. While the experiment group applied a test-first strategy, the control group applied a more conventional development technique, writing tests after the implementation. Both groups followed an incremental process, adding new features one at a time and regression testing them. We found that test-first students on average wrote more tests and, in turn, students who wrote more tests tended to be more productive. We also observed that the minimum quality increased linearly with the number of programmer tests, independent of the development strategy employed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423994","Index Terms- General programming techniques;coding tools and techniques;testing and debugging;testing strategies;productivity;Software Quality/SQA;software engineering process;programming paradigms.","Programming profession;Productivity;Quality assurance;Computer Society;Software testing;Feedback;Functional programming;Writing;Debugging;Software quality","program testing;program debugging;software quality","test-driven development;general programming technique;program testing;program debugging;software quality;software engineering process;formal verification","","116","","25","","","","","","IEEE","IEEE Journals & Magazines"
"InterPlay: Horizontal Scale-Up and Transition to Design in Scenario-Based Programming","D. Barak; D. Harel; R. Marelly","Weizmann Institute of Science, Rehovot, Israel; Weizmann Institute of Science, Rehovot, Israel; Weizmann Institute of Science, Rehovot, Israel","IEEE Transactions on Software Engineering","","2006","32","7","467","485","We describe InterPlay, a simulation engine coordinator that supports cooperation and interaction of multiple simulation and execution tools, thus helping to scale up the design and development cycle of reactive systems. InterPlay involves a number of related ideas. In the first, we concentrate on the interobject design approach involving live sequence charts (LSCs) and its support tool, the play-engine, enabling multiple play-engines to run in cooperation. This makes possible the distributed design of large-scale systems by different teams, as well as the refinement of parts of a system using different play-engines. The second idea concerns combining the interobject approach with the more conventional intraobject approach, involving, for example, statecharts and Rhapsody. InterPlay makes it possible to run the play-engine in cooperation with Rhapsody, and is very useful when some system objects have clear and distinct internal behavior, or in an iterative development process where the design is implementation-oriented and the ultimate goal is to end up with an intraobject implementation. Finally, we have expanded the play-engine's ability to delegate some of the system's functionality to complex GUIs. This enables beneficial interaction with ""smart"" GUIs that have built-in behavior of their own, and which are more naturally implemented in code","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677533","Modeling methodologies;scenario-based programming;InterPlay;play-engine;LSCs;intraobject;interobject;transition to design.","Engines;Operating systems;Large-scale systems;Process design;Design methodology;Graphical user interfaces;Distributed computing;Joining processes;Java;Voice mail","digital simulation;graphical user interfaces;object-oriented programming;software tools","simulation engine coordinator;reactive system;interobject design approach;live sequence chart;distributed design;large-scale system;iterative development process;intraobject design approach;GUI;graphical user interface;scenario-based programming;InterPlay;play-engine","","6","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Using partial-order methods in the formal validation of industrial concurrent programs","P. Godefroid; D. Peled; M. Staskauskas","Bell Labs., Lucent Technol. Inc., Naperville, IL, USA; NA; NA","IEEE Transactions on Software Engineering","","1996","22","7","496","507","Formal validation is a powerful technique for automatically checking that a collection of communicating processes is free from concurrency-related errors. Although validation tools invariably find subtle errors that were missed during thorough simulation and testing, the brute-force search they perform can result in excessive memory usage and extremely long running times. Recently, a number of researchers have been investigating techniques known as partial-order methods that can significantly reduce the computational resources needed for formal validation by avoiding redundant exploration of execution scenarios. This paper investigates the behavior of partial-order methods in an industrial setting. We describe the design of a partial-order algorithm or a formal validation tool that has been used on several projects that are developing software for the Lucent Technologies 5ESS/sup (R/) telephone switching system. We demonstrate the effectiveness of the algorithm by presenting the results of experiments with actual industrial examples drawn from a variety of 5ESS application domains.","0098-5589;1939-3520;2326-3881","","10.1109/32.538606","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=538606","","Error correction;Testing;Application software;Electronic switching systems;Communication industry;Computational modeling;Performance evaluation;Algorithm design and analysis;Software algorithms;Software tools","multiprocessing programs;program verification;reachability analysis;parallel programming;electronic switching systems;telecommunication computing;software tools","partial-order methods;formal validation tool;industrial concurrent programs;automatic error checking;communicating processes;concurrency-related errors;computational resources;redundant exploration;execution scenarios;Lucent Technologies 5ESS telephone switching system;automatic verification;reachability analysis","","15","","19","","","","","","IEEE","IEEE Journals & Magazines"
"How Programmers Debug, Revisited: An Information Foraging Theory Perspective","J. Lawrance; C. Bogart; M. Burnett; R. Bellamy; K. Rector; S. D. Fleming","Wentworth Institute of Technology, Boston; Oregon State University, Corvallis; Oregon State University, Corvallis; IBM TJ Watson Research Center, Hawthorne; Oregon State University, Corvallis; Oregon State University, Corvallis","IEEE Transactions on Software Engineering","","2013","39","2","197","215","Many theories of human debugging rely on complex mental constructs that offer little practical advice to builders of software engineering tools. Although hypotheses are important in debugging, a theory of navigation adds more practical value to our understanding of how programmers debug. Therefore, in this paper, we reconsider how people go about debugging in large collections of source code using a modern programming environment. We present an information foraging theory of debugging that treats programmer navigation during debugging as being analogous to a predator following scent to find prey in the wild. The theory proposes that constructs of scent and topology provide enough information to describe and predict programmer navigation during debugging, without reference to mental states such as hypotheses. We investigate the scope of our theory through an empirical study of 10 professional programmers debugging a real-world open source program. We found that the programmers' verbalizations far more often concerned scent-following than hypotheses. To evaluate the predictiveness of our theory, we created an executable model that predicted programmer navigation behavior more accurately than comparable models that did not consider information scent. Finally, we discuss the implications of our results for enhancing software engineering tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.111","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5674060","Information foraging theory;debugging;software maintenance;programmer navigation;information scent;empirical software engineering","Debugging;Navigation;Topology;Programming environments;Predictive models;Approximation methods","cognition;program debugging;public domain software;software maintenance;topology","information foraging theory;human debugging theories;complex mental constructs;navigation theory;programming environment;topology constructs;information scent constructs;open source code program debugging;programmer verbalizations;programmer navigation behavior prediction;software engineering tool enhancement","","38","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Lower bound on the number of processors and time for scheduling precedence graphs with communication costs","M. A. Al-Mouhamed","Dept. of Comput. Eng., King Fahd Univ. of Pet. & Miner., Dharhran, Saudi Arabia","IEEE Transactions on Software Engineering","","1990","16","12","1390","1401","A lower bound on the number of processors and finish time for the problem of scheduling precedence graphs with communication costs is presented. The notion of the earliest starting time of a task is formulated for the context of lower bounds. A lower bound on the completion time is proposed. A task delay which does not increase the earliest completion time of a schedule is defined. Each task can then be scheduled within a time interval without affecting the lower bound performance on the finish time. This leads to definition of a new lower bound on the number of processors required to process the task graph. A derivation of the minimum time increase over the earliest completion time is also proposed for the case of a smaller number of processors. A lower bound on the minimum number of interprocessor communication links required to achieve optimum performance is proposed. Evaluation had been carried out by using a set of 360 small graphs. The bound on the finish time deviates at most by 5% from the optimum solution in 96% of the cases and performs well with respect to the minimum number of processors and communication links.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.62447","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=62447","","Processor scheduling;Optimal scheduling;Scheduling algorithm;Costs;Approximation algorithms;Context;Delay effects;Parallel processing;Operating systems","graph theory;scheduling","scheduling;precedence graphs;communication costs;earliest starting time;lower bounds;completion time;task delay;time interval;finish time;task graph;minimum time increase;interprocessor communication links;optimum performance;small graphs","","68","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Towards a General Concurrency Control Algorithm for Database Systems","A. A. Farrag; M. T. Ozsu","Department of Mathematics and Computing Science, Dalhousie University; NA","IEEE Transactions on Software Engineering","","1987","SE-13","10","1073","1079","The concurrency control problem in database systems has been examined by many people and several concurrency control algorithms have been proposed. The most popular algorithms are two-phase locking and timestamp ordering. This paper shows that two-phase locking and timestamp ordering are special cases of a more general concurrency control algorithm. This general algorithm is described in detail and is proven to work correctly. We show that two-phase locking and timestamp ordering represent the two end points of a series of concurrency control algorithms. Each of them is a special case of the general algorithm proposed in this paper. Moreover, each of these special cases can be selected in advance, and can even be changed dynamically during execution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232849","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702147","Concurrency control;database;deadlock;strictness level;timestamp ordering;two-phase locking","Concurrency control;Database systems;Transaction databases;Processor scheduling;System recovery;Scholarships;Councils;Mathematics;Concurrent computing;Writing","","Concurrency control;database;deadlock;strictness level;timestamp ordering;two-phase locking","","2","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Scenario-based assessment of nonfunctional requirements","A. Gregoriades; A. Sutcliffe","Surrey Defence Technol. Centre, Surrey Univ., Guildford, UK; NA","IEEE Transactions on Software Engineering","","2005","31","5","392","409","This paper describes a method and a tool for validating nonfunctional requirements in complex socio-technical systems. The system requirements analyzer (SRA) tool validates system reliability and operational performance requirements using scenario-based testing. Scenarios are transformed into sequences of task steps and the reliability of human agents performing tasks with computerized technology is assessed using Bayesian belief network (BN) models. The tool tests system performance within an envelope of environmental variations and reports the number of tests that pass a benchmark threshold. The tool diagnoses problematic areas in scenarios representing pathways through system models, assists in the identification of their causes, and supports comparison of alternative requirements specifications and system designs. It is suitable for testing socio-technical systems where operational scenarios are sequential and deterministic, in domains where designs are incrementally modified so set up costs of the BNs can be defrayed over multiple tests.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1438375","Index Terms- Nonfunctional requirements validation;scenario-based testing;Bayesian belief networks;systems engineering.","System testing;Sociotechnical systems;Sequential analysis;Performance analysis;Computer network reliability;Humans;Computer networks;Bayesian methods;System performance;Benchmark testing","formal specification;formal verification;belief networks;systems analysis;software agents;program testing;software reliability","nonfunctional requirement;complex socio-technical system;system requirements analyzer tool validation;system reliability;operational performance requirement;scenario-based testing;human agent;computerized technology;Bayesian belief network model;system performance;benchmark threshold;tool diagnosis;system requirement specification;system design","","25","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Very High Level Concurrent Programming","Y. Shi; N. Prywes; B. Szymanski; A. Pnueli","Department of Computer and Information Science, Temple University; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","9","1038","1046","Concurrent systems are typically large and complex, requiring long, development time and much labor. They are, therefore, prime candidates for simplification and automation of the design and programming process. Their major application areas include real time systems, operating systems and cooperative computation. New applications are emerging with the trends towards wide usage of personal computers connected in a network and towards use of parallel processing in supercomputer architectures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233791","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702326","Automatic program generation;concurrent programming;nonprocedural languages;specification languages","Application software;Design automation;Process design;Automatic programming;Real time systems;Operating systems;Microcomputers;Parallel processing;Supercomputers;Computer architecture","","Automatic program generation;concurrent programming;nonprocedural languages;specification languages","","2","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Reachability testing of concurrent programs","Y. Lei; R. H. Carver","Dept. of Comput. Sci., Texas Univ., Arlington, TX, USA; NA","IEEE Transactions on Software Engineering","","2006","32","6","382","403","One approach to testing concurrent programs, called reachability testing, generates synchronization sequences automatically and on-the-fly, without constructing any static models. In this paper, we present a general execution model for concurrent programs that allows reachability testing to be applied to several commonly used synchronization constructs. We also present a new method for performing reachability testing. This new method guarantees that every partially ordered synchronization sequence will be exercised exactly once without having to save any sequences that have already been exercised. We describe a prototype reachability testing tool called RichTest and report some empirical results, including a comparison between RichTest and a partial order reduction-based tool called VeriSoft. RichTest performed significantly better for the programs in our study","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650214","Software testing;reachability testing;concurrent programming.","Automatic testing;Interleaved codes;Performance evaluation;Prototypes;Multithreading;Computational efficiency;Web server;Java;Libraries","concurrency control;multi-threading;program testing;reachability analysis;synchronisation","reachability testing;concurrent programs;static model;synchronization sequence;RichTest tool;VeriSoft partial order reduction-based tool","","51","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Evolutionary Optimization of Software Quality Modeling with Multiple Repositories","Y. Liu; T. M. Khoshgoftaar; N. Seliya","Georgia College &amp; State University, Milledgeville, GA; Florida Atlantic University, Boca Raton, FL; University of Michigan-Dearborn, Dearborn, MI","IEEE Transactions on Software Engineering","","2010","36","6","852","864","A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5467094","Genetic programming;optimization;software quality;defects;machine learning;software measurement.","Software quality;Software measurement;Predictive models;Software metrics;Electronic mail;Genetic programming;Robustness;Costs;Machine learning;Software engineering","genetic algorithms;software management;software metrics;software quality","evolutionary optimization;software quality modeling;multiple software project repository;genetic programming;baseline classifier;validation classifier;validation-and-voting classifier;robust software quality model;software metrics;machine learner;software data set;search-based software quality model;software measurement data repository","","51","","48","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Study on Explicit-State Non-Zenoness Checking for Timed Automata","T. Wang; J. Sun; X. Wang; Y. Liu; Y. Si; J. S. Dong; X. Yang; X. Li","College of Computer Science, Zhejiang University, P.R., China; ISTD, Singapore University of Technology and Design, Singapore; College of Computer Science, Zhejiang University, P.R., China; School of Computer Engineering, Nanyang Technological University, Singapore; College of Computer Science, Zhejiang University, P.R., China; School of Computing, National University of Singapore, Singapore; College of Computer Science, Zhejiang University, P.R., China; School of Computer Science and Technology, Tianjin University, P.R., China","IEEE Transactions on Software Engineering","","2015","41","1","3","18","Zeno runs, where infinitely many actions occur within finite time, may arise in Timed Automata models. Zeno runs are not feasible in reality and must be pruned during system verification. Thus it is necessary to check whether a run is Zeno or not so as to avoid presenting Zeno runs as counterexamples during model checking. Existing approaches on non-Zenoness checking include either introducing an additional clock in the Timed Automata models or additional accepting states in the zone graphs. In addition, there are approaches proposed for alternative timed modeling languages, which could be generalized to Timed Automata. In this work, we investigate the problem of non-Zenoness checking in the context of model checking LTL properties, not only evaluating and comparing existing approaches but also proposing a new method. To have a systematic evaluation, we develop a software toolkit to support multiple non-Zenoness checking algorithms. The experimental results show the effectiveness of our newly proposed algorithm, and demonstrate the strengths and weaknesses of different approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2359893","National Natural Science Foundation Program; National Key Technology Support Program; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6908008","Timed automata;non-Zenoness;model checking;verification tool","Automata;Clocks;Safety;Educational institutions;Systematics;Cost accounting;Model checking","automata theory;formal verification;graph theory;real-time systems","explicit-state nonzenoness checking;timed automata models;zeno runs;system verification;clocks;zone graphs;timed modeling languages;model checking LTL properties;systematic evaluation;software toolkit","","2","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying Failure Causes in Java Programs: An Application of Change Impact Analysis","Xiaoxia Ren; O. C. Chesley; B. G. Ryder","IEEE Computer Society; NA; NA","IEEE Transactions on Software Engineering","","2006","32","9","718","732","During program maintenance, a programmer may make changes that enhance program functionality or fix bugs in code. Then, the programmer usually will run unit/regression tests to prevent invalidation of previously tested functionality. If a test fails unexpectedly, the programmer needs to explore the edit to find the failure-inducing changes for that test. Crisp uses results from Chianti, a tool that performs semantic change impact analysis, to allow the programmer to examine those parts of the edit that affect the failing test. Crisp then builds a compilable intermediate version of the program by adding a programmer-selected partial edit to the original code, augmenting the selection as necessary to ensure compilation. The programmer can reexecute the test on the intermediate version in order to locate the exact reasons for the failure by concentrating on the specific changes that were applied. In nine initial case studies on pairs of versions from two real Java programs, Daikon and Eclipse jdt compiler, we were able to use Crisp to identify the failure-inducing changes for all but 1 of 68 failing tests. On average, 33 changes were found to affect each failing test (of the 67), but only 1-4 of these changes were found to be actually failure-inducing","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707669","Fault localization;semantic change impact analysis;edit change dependence;regression testing;intermediate versions of programs.","Java;Failure analysis;Testing;Programming profession;Application software;Performance evaluation;Performance analysis;Prototypes;Computer Society;Computer bugs","Java;program compilers;program debugging;program diagnostics;program testing;software maintenance","program maintenance;bug fixing;semantic change impact analysis;compilable intermediate program version;Java program;program compiler;failure-inducing change;fault localization;edit change dependence;unit testing;regression testing","","32","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A Comparison of Program Comprehension Strategies by Blind and Sighted Programmers","A. Armaly; P. Rodeghero; C. McMillan","Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN","IEEE Transactions on Software Engineering","","2018","44","8","712","724","Programmers who are blind use a screen reader to speak source code one word at a time, as though the code were text. This process of reading is in stark contrast to sighted programmers, who skim source code rapidly with their eyes. At present, it is not known whether the difference in these processes has effects on the program comprehension gained from reading code. These effects are important because they could reduce both the usefulness of accessibility tools and the generalizability of software engineering studies to persons with low vision. In this paper, we present an empirical study comparing the program comprehension of blind and sighted programmers. We found that both blind and sighted programmers prioritize reading method signatures over other areas of code. Both groups obtained an equal and high degree of comprehension, despite the different reading processes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2729548","National Science Foundation Graduate Research Fellowship Program; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7987041","Program comprehension;accessibility technology;blindness","Tools;Software;Blindness;Navigation;Programming profession;Software engineering","programming environments;public domain software;software prototyping;source code (software)","source code;sighted programmers;program comprehension strategies;reading processes;stark;program comprehension;reading method signature","","1","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing the Cost Effectiveness of Fault Prediction in Acceptance Testing","A. Monden; T. Hayashi; S. Shinoda; K. Shirai; J. Yoshida; M. Barker; K. Matsumoto","Nara Institute of Science and Technology, Ikoma; NTT West Corporation, Osaka; NTT West Corporation, Osaka; NTT West Corporation, Osaka; NTT West Corporation, Osaka; Nara Institute of Science and Technology, Ikoma; Nara Institute of Science and Technology, Ikoma","IEEE Transactions on Software Engineering","","2013","39","10","1345","1357","Until now, various techniques for predicting fault-prone modules have been proposed and evaluated in terms of their prediction performance; however, their actual contribution to business objectives such as quality improvement and cost reduction has rarely been assessed. This paper proposes using a simulation model of software testing to assess the cost effectiveness of test effort allocation strategies based on fault prediction results. The simulation model estimates the number of discoverable faults with respect to the given test resources, the resource allocation strategy, a set of modules to be tested, and the fault prediction results. In a case study applying fault prediction of a small system to acceptance testing in the telecommunication industry, results from our simulation model showed that the best strategy was to let the test effort be proportional to ""the number of expected faults in a module × log(module size)."" By using this strategy with our best fault prediction model, the test effort could be reduced by 25 percent while still detecting as many faults as were normally discovered in testing, although the company required about 6 percent of the test effort for metrics collection, data cleansing, and modeling. The simulation results also indicate that the lower bound of acceptable prediction accuracy is around 0.78 in terms of an effort-aware measure, Norm(P<sub>opt</sub>). The results indicate that reduction of the test effort can be achieved by fault prediction only if the appropriate test strategy is employed with high enough fault prediction accuracy. Based on these preliminary results, we expect further research to assess their general validity with larger systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6497441","Complexity measures;fault prediction;quality assurance;resource allocation;simulation","Testing;Predictive models;Measurement;Software;Resource management;Companies;Accuracy","program testing;resource allocation;software cost estimation;software fault tolerance;software metrics","cost effectiveness assessment;fault prediction;acceptance testing;quality improvement;cost reduction;software testing;test effort allocation strategies;fault discovery;resource allocation strategy;test resources;telecommunication industry;metrics collection;data cleansing;data modeling;effort-aware measure","","17","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Automated protocol implementation with RTAG","D. P. Anderson","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA","IEEE Transactions on Software Engineering","","1988","14","3","291","300","The RTAG (real-time asynchronous grammars) programming language is discussed. The language is based on an attribute grammar notation for specifying protocols. Its main design goals are: (1) to support concise and easily understood expression of complex real-world protocols; and (2) to serve as the basis of a portable software system for automated protocol implementation. The algorithms used in generating implementations from given specifications are sketched, and a Unix-based automated implementation system for RTAG is described.<<ETX>></ETX>","0098-5589;1939-3520;2326-3881","","10.1109/32.4650","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4650","","Protocols;Operating systems;Software systems;Network interfaces;Hardware;Software engineering;Debugging;Communication standards;Software standards","grammars;protocols","automated protocol implementation;RTAG;real-time asynchronous grammars;programming language;attribute grammar notation;portable software system;Unix-based automated implementation","","16","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Analysis of Discrete Communication Behavior","K. Rea; R. De B. Johnston","Bell Northern Research, 3 Place du Commerce; NA","IEEE Transactions on Software Engineering","","1987","SE-13","10","1115","1126","An objective methodology for the specification and analysis of communicating processes is presented. It is based on an algebraic theory that is a formalization of a particular state machine model. The approach recognizes the fact that the complexity of system interactions is such that computer aid is not only appropriate but necessary for any practical design methodology.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232853","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702151","Algebraic models;analysis;communicating processes;communication protocols;concurrent process specification;distributed systems;verification","Computer languages;Formal specifications;Design methodology;Calculus;Carbon capture and storage;Algebra;Application software;Protocols;Distributed computing;Local area networks","","Algebraic models;analysis;communicating processes;communication protocols;concurrent process specification;distributed systems;verification","","3","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Tolerating deviations in process support systems via flexible enactment of process models","G. Cugola","Dipt. di Elettronica e Inf., Politecnico di Milano, Italy","IEEE Transactions on Software Engineering","","1998","24","11","982","1001","Process support systems (PSSs) support business organizations in modeling, improving and automating their business processes. Thanks to their ability in enacting process models, they can be used to guide people in performing their daily work and to automate the repetitive tasks that do not require human intervention. Given these potential benefits, it is surprising to observe that PSSs are not widely adopted. This is especially true in case of highly flexible and human-intensive processes, such as design processes in general and software processes in particular. This fact can be explained by observing that currently available PSSs do not fulfil some crucial needs of modern business organizations. One of their major drawbacks is that they do not offer adequate mechanisms to cope with unforeseen situations. They are good at supporting business processes if all proceeds as expected, but if an unexpected situation is met, which would require one to deviate from the process model, they often become more an obstacle than a help. This paper deals with the problem of managing unforeseen situations that require deviations from the process model during enactment in the context of the PROSYT (PROcess Support sYstem capable of Tolerating deviations) PSS. During process model enactment, PROSYT is capable of tolerating deviations from the process model by supporting users even when unexpected situations arise. Furthermore, it supports users in reconciling the process model with the process actually followed, if necessary.","0098-5589;1939-3520;2326-3881","","10.1109/32.730546","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=730546","","Organizational aspects;Humans;Workflow management software;Software engineering;Costs;Process design;Context modeling;Standardization;Page description languages","project support environments;workflow management software;computer aided software engineering;business data processing","deviation tolerance;process support systems;flexible process model enactment;business organizations;business processes;repetitive tasks;human-intensive processes;design processes;software processes;unforeseen situations;PROSYT;user support;unexpected situations;workflow management system;process-centered software engineering environment;inconsistencies;event-based components integration","","63","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting Consistency-Maintenance Requirement of Code Clonesat Copy-and-Paste Time","X. Wang; Y. Dang; L. Zhang; D. Zhang; E. Lan; H. Mei","Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, and with the Department of Computer Science, University of Texas, San Antonio; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Corporation, One Microsoft Way, Redmond, WA; Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, and with the Department of Computer Science, University of Texas, San Antonio","IEEE Transactions on Software Engineering","","2014","40","8","773","794","Code clones have always been a double edged sword in software development. On one hand, it is a very convenient way to reuse existing code, and to save coding effort. On the other hand, since developers may need to ensure consistency among cloned code segments, code clones can lead to extra maintenance effort and even bugs. Recently studies on the evolution of code clones show that only some of the code clones experience consistent changes during their evolution history. Therefore, if we can accurately predict whether a code clone will experience consistent changes, we will be able to provide useful recommendations to developers onleveraging the convenience of some code cloning operations, while avoiding other code cloning operations to reduce future consistency maintenance effort. In this paper, we define a code cloning operation as consistency-maintenance-required if its generated code clones experience consistent changes in the software evolution history, and we propose a novel approach that automatically predicts whether a code cloning operation requires consistency maintenance at the time point of performing copy-and-paste operations. Our insight is that whether a code cloning operation requires consistency maintenance may relate to the characteristics of the code to be cloned and the characteristics of its context. Based on a number of attributes extracted from the cloned code and the context of the code cloning operation, we use Bayesian Networks, a machine-learning technique, to predict whether an intended code cloning operation requires consistency maintenance. We evaluated our approach on four subjects-two large-scale Microsoft software projects, and two popular open-source software projects-under two usage scenarios: 1) recommend developers to perform only the cloning operations predicted to be very likely to be consistency-maintenance-free, and 2) recommend developers to perform all cloning operations unless they are predicted very likely to be consistency-maintenance-required. In the first scenario, our approach is able to recommend developers to perform more than 50 percent cloning operations with a precision of at least 94 percent in the four subjects. In the second scenario, our approach is able to avoid 37 to 72 percent consistency-maintenance-required code clones by warning developers on only 13 to 40 percent code clones, in the four subjects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2323972","National 863 Program; National 973 Program; Science Fund for Creative Research Groups; Natural Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6815760","Code cloning;consistency maintenance;programming aid","Cloning;Software;Maintenance engineering;Bayes methods;History;Training;Educational institutions","belief networks;learning (artificial intelligence);public domain software;software maintenance","consistency-maintenance requirement;code clones;copy-and-paste time;software development;maintenance effort;code cloning operations;consistency maintenance effort;Bayesian networks;machine-learning technique;Microsoft software projects;open-source software projects","","7","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic derivation of formal software specifications from informal descriptions","K. Miriyala; M. T. Harandi","Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA","IEEE Transactions on Software Engineering","","1991","17","10","1126","1142","SPECIFIER, an interactive system which derives formal specifications of data types and programs from their informal descriptions, is described. The process of deriving formal specifications is viewed as a problem-solving process. The system uses common problem-solving techniques such as schemas, analogy, and difference-based reasoning to derive formal specifications. If an informal description is a commonly occurring operation for which the system has a schema, then the formal specification is derived by instantiating the schema. If there is a no such schema, SPECIFIER tries to find a previously solved problem which is analogous to the current problem. If the problem found is directly analogous to the current problem, it applies an analogy mapping to obtain a formal specification. On the other hand, if the analogy found is only approximate, it solves the directly analogous part of the problem by analogy and performs difference-based reasoning using the remaining (unmatched) parts to transform the formal specification obtained by analogy to a formal specification for the entire original problem.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.99198","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=99198","","Formal specifications;Problem-solving;Computer science;Feathers;Interactive systems;Programming profession;Contracts;Software measurement;Writing;NASA","automatic programming;data structures;formal specification;software tools","formal software specifications;interactive system;data types;problem-solving process;common problem-solving techniques;schemas;analogy;difference-based reasoning;informal description;SPECIFIER;previously solved problem;analogy mapping","","39","","37","","","","","","IEEE","IEEE Journals & Magazines"
"A cognitive-based mechanism for constructing software inspection teams","J. Miller; Zhichao Yin","Dept. of Electr. & Comput. Eng., Alberta Univ., Edmonton, Alta., Canada; Dept. of Electr. & Comput. Eng., Alberta Univ., Edmonton, Alta., Canada","IEEE Transactions on Software Engineering","","2004","30","11","811","825","Software inspection is well-known as an effective means of defect detection. Nevertheless, recent research has suggested that the technique requires further development to optimize the inspection process. As the process is inherently group-based, one approach to improving performance is to attempt to minimize the commonality within the process and the group. This work proposes an approach to add diversity into the process by using a cognitively-based team selection mechanism. The paper argues that a team with diverse information processing strategies, as defined by the selection mechanism, maximize the number of different defects discovered.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359772","Index Terms- Planning for SQA and V&amp;V;code inspections and walkthroughs;programming teams;software psychology.","Inspection;Costs;NIST;Information processing;Psychology;Computer bugs;Economic indicators;Investments;Testing;Personnel","program testing;program verification;cognition","cognitive-based mechanism;software inspection team;defect detection;code inspection;code walkthrough;programming team;software psychology","","23","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Magiclock: Scalable Detection of Potential Deadlocks in Large-Scale Multithreaded Programs","Y. Cai; W. K. Chan","Department of Computer Science, City University of Hong Kong, Tat Chee Avenue; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue","IEEE Transactions on Software Engineering","","2014","40","3","266","281","We present Magiclock, a novel potential deadlock detection technique by analyzing execution traces (containing no deadlock occurrence) of large-scale multithreaded programs. Magiclock iteratively eliminates removable lock dependencies before potential deadlock localization. It divides lock dependencies into thread specific partitions, consolidates equivalent lock dependencies, and searches over the set of lock dependency chains without the need to examine any duplicated permutations of the same lock dependency chains. We validate Magiclock through a suite of real-world, large-scale multithreaded programs. The experimental results show that Magiclock is significantly more scalable and efficient than existing dynamic detectors in analyzing and detecting potential deadlocks in execution traces of large-scale multithreaded programs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2301725","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6718069","Deadlock detection;multithreaded programs;concurrency;lock order graph;scalability","System recovery;Message systems;Classification algorithms;Instruction sets;Image edge detection;Monitoring;Multicore processing","concurrency control;multi-threading;operating systems (computers);system recovery","Magiclock;potential deadlocks scalable detection;large-scale multithreaded programs;potential deadlock localization;lock order graph;scalability","","19","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Considerations on the insularity of performance evaluation","D. Ferrari","Computer Science Division, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA 94720","IEEE Transactions on Software Engineering","","1986","SE-12","6","678","683","It is argued that systems performance evaluation, in the first 20 years of its existence, has developed in substantial isolation from such disciplines as computer architecture, system organization, operating systems, and software engineering. The possible causes for this phenomenon, which seems to be unique in the history of engineering, are explored. Its positive and negative effects on computer science and technology, as well as on performance evaluation itself, are discussed. The drawbacks of isolated development outweigh its advantages. Thus, instructional and research initiatives to foster the rapid integration of the performance evaluation viewpoint into the mainstream of computer science and engineering are proposed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312965","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312965","Computer engineering;computer science;evaluation techniques;measurement of system performance;modeling of system performance;performance evaluation;system performance","Computers;Performance evaluation;Computational modeling;Computer science;Communities;System performance;Analytical models","computer science;performance evaluation","performance evaluation;systems performance evaluation;computer architecture;system organization;operating systems;software engineering;computer science;technology;isolated development;research initiatives","","7","","","","","","","","IEEE","IEEE Journals & Magazines"
"Polymetric views - a lightweight visual approach to reverse engineering","M. Lanza; S. Ducasse","Inst. fur Informatik & angewandte Math., Bern Univ., Switzerland; Inst. fur Informatik & angewandte Math., Bern Univ., Switzerland","IEEE Transactions on Software Engineering","","2003","29","9","782","795","Reverse engineering software systems has become a major concern in software industry because of their sheer size and complexity. This problem needs to be tackled since the systems in question are of considerable worth to their owners and maintainers. In this article, we present the concept of a polymetric view, a lightweight software visualization technique enriched with software metrics information. Polymetric views help to understand the structure and detect problems of a software system in the initial phases of a reverse engineering process. We discuss the benefits and limits of several predefined polymetric views we have implemented in our tool CodeCrawler. Moreover, based on clusters of different polymetric views, we have developed a methodology which supports and guides a software engineer in the first phases of a reverse engineering of a large software system. We have refined this methodology by repeatedly applying it on industrial systems and illustrate it by applying a selection of polymetric views to a case study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232284","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232284","","Reverse engineering;Software systems;Visualization;Software metrics;Systems engineering and theory;Investments;Costs;Humans;Maintenance engineering;Computer industry","reverse engineering;object-oriented programming;software metrics;program visualisation;bibliographies","reverse engineering;software industry;polymetric view;lightweight software visualization technique;software metrics information;CodeCrawler;software engineer;large software system;object-oriented programming;software visualization","","164","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting Project Velocity in XP Using a Learning Dynamic Bayesian Network Model","P. Hearty; N. Fenton; D. Marquez; M. Neil","Queen Mary University of London-Computer, London; Queen Mary University of London-Computer, London; Queen Mary University of London-Computer, London; Queen Mary University of London-Computer, London","IEEE Transactions on Software Engineering","","2009","35","1","124","137","Bayesian networks, which can combine sparse data, prior assumptions and expert judgment into a single causal model, have already been used to build software effort prediction models. We present such a model of an extreme programming environment and show how it can learn from project data in order to make quantitative effort predictions and risk assessments without requiring any additional metrics collection program. The model's predictions are validated against a real world industrial project, with which they are in good agreement.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4624275","extreme programming;Bayesian networks;causal models;risk assessment;extreme programming;Bayesian networks;causal models;risk assessment","Bayesian methods;Predictive models;Project management;Risk management;Programming environments;Testing;Large-scale systems;Size measurement;Calendars;Uncertainty","belief networks;project management;risk management;software metrics","project velocity;XP;extreme programming;learning dynamic Bayesian network model;software effort prediction models;quantitative effort predictions;risk assessments;metrics collection program;software development","","25","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluation of the File Redundancy in Distributed Database Systems","S. Muro; T. Ibaraki; H. Miyajima; T. Hasegawa","Department of Applied Mathematics and Physics, Faculty of Engineering, Kyoto University; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","2","199","205","This paper treats the file redundancy issue in distributed database systems, asking what is the optimal number of file copies, given the ratio r of the frequency of update requests to the frequency of all file access requests (i.e., queries and updates). Formulations of this type of problem, including optimal file allocation, have been attempted by a number of authors, and some algorithms have been proposed. Although such algorithms can be used to solve particular problems, it seems difficult to draw general conclusions applicable to a wide variety of practical distributed database systems. To probe into this hard to formulate but interesting problem, our paper constructs simplified network models of distributed database systems, and computes the optimal number of file copies, as well as their locations, to minimize the communication cost. For several network types, we plot the optimal number of file copies as a function of the ratio r.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232195","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701988","Communication cost;distributed database systems;file allocation problem;file redundancy;simple plant location problem","Database systems;Costs;Frequency;Mathematics;System performance;Probes;Physics education;Automation;Laboratories;Computer networks","","Communication cost;distributed database systems;file allocation problem;file redundancy;simple plant location problem","","5","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Pointcut Rejuvenation: Recovering Pointcut Expressions in Evolving Aspect-Oriented Software","R. Khatchadourian; P. Greenwood; A. Rashid; G. Xu","Ohio State University, Columbus; Lancaster University, Lancaster; Lancaster University, Lancaster; Ohio State University, Columbus","IEEE Transactions on Software Engineering","","2012","38","3","642","657","Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base code can lead to join points incorrectly falling in or out of the scope of pointcuts. In this paper, we present an automated approach that limits fragility problems by providing mechanical assistance in pointcut maintenance. The approach is based on harnessing arbitrarily deep structural commonalities between program elements corresponding to join points selected by a pointcut. The extracted patterns are then applied to later versions to offer suggestions of new join points that may require inclusion. To illustrate that the motivation behind our proposal is well founded, we first empirically establish that join points captured by a single pointcut typically portray a significant amount of unique structural commonality by analyzing patterns extracted from 23 AspectJ programs. Then, we demonstrate the usefulness of our technique by rejuvenating pointcuts in multiple versions of three of these programs. The results show that our parameterized heuristic algorithm was able to accurately and automatically infer the majority of new join points in subsequent software versions that were not captured by the original pointcuts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5710951","Software development environments;software maintenance;software tools.","Software;Fuels;Software engineering;Observers;Robustness;Programming;Proposals","aspect-oriented programming","pointcut rejuvenation;pointcut expression recovery;aspect-oriented software;pointcut fragility;aspect-oriented programming;pointcut maintenance;deep structural commonalities harnessing;program elements;join points;pattern analysis;AspectJ programs;parameterized heuristic algorithm","","2","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Enhancing the Description-to-Behavior Fidelity in Android Apps with Privacy Policy","L. Yu; X. Luo; C. Qian; S. Wang; H. K. N. Leung","Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong","IEEE Transactions on Software Engineering","","2018","44","9","834","854","Since more than 96 percent of mobile malware targets the Android platform, various techniques based on static code analysis or dynamic behavior analysis have been proposed to detect malicious apps. As malware is becoming more complicated and stealthy, recent research proposed a promising detection approach that looks for the inconsistency between an app's permissions and its description. In this paper, we first revisit this approach and reveal that using description and permission will lead to many false positives because descriptions often fail to declare all sensitive operations. Then, we propose exploiting an app's privacy policy and its bytecode to enhance the malware detection based on description and permissions. It is non-trivial to automatically analyze privacy policy and perform the cross-verification among these four kinds of software artifacts including, privacy policy, bytecode, description, and permissions. To address these challenging issues, we first propose a novel data flow model for analyzing privacy policy, and then develop a new system, named TAPVerifier, for carrying out investigation of individual software artifacts and conducting the cross-verification. The experimental results show that TAPVerifier can analyze privacy policy with a high accuracy and recall rate. More importantly, integrating privacy policy and bytecode level information can remove up to 59.4 percent false alerts of the state-of-the-art systems, such as AutoCog, CHABADA, etc.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2730198","Hong Kong GRF; Shenzhen City Science and Technology R&D Fund; Hong Kong RGC Project; HKPolyU Research; National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7987793","Mobile applications;privacy policy","Privacy;Data privacy;Semantics;Malware;Permission;Google;Androids","Android (operating system);data privacy;invasive software;mobile computing;program diagnostics","privacy policy;permission;description-to-behavior fidelity;Android apps;static code analysis;dynamic behavior analysis;malicious apps;promising detection approach;malware detection;mobile malware;TAPVerifier","","4","","91","","","","","","IEEE","IEEE Journals & Magazines"
"Probabilistic Model Checking of Regenerative Concurrent Systems","M. Paolieri; A. Horváth; E. Vicario","Department of Information Engineering, Università di Firenze, Firenze, Italy; Department of Computer Science, Università di Torino, Torino, Italy; Department of Information Engineering, Università di Firenze, Firenze, Italy","IEEE Transactions on Software Engineering","","2016","42","2","153","169","We consider the problem of verifying quantitative reachability properties in stochastic models of concurrent activities with generally distributed durations. Models are specified as stochastic time Petri nets and checked against Boolean combinations of interval until operators imposing bounds on the probability that the marking process will satisfy a goal condition at some time in the interval [α, β] after an execution that never violates a safety property. The proposed solution is based on the analysis of regeneration points in model executions: a regeneration is encountered after a discrete event if the future evolution depends only on the current marking and not on its previous history, thus satisfying the Markov property. We analyze systems in which multiple generally distributed timers can be started or stopped independently, but regeneration points are always encountered with probability 1 after a bounded number of discrete events. Leveraging the properties of regeneration points in probability spaces of execution paths, we show that the problem can be reduced to a set of Volterra integral equations, and we provide algorithms to compute their parameters through the enumeration of finite sequences of stochastic state classes encoding the joint probability density function (PDF) of generally distributed timers after each discrete event. The computation of symbolic PDFs is limited to discrete events before the first regeneration, and the repetitive structure of the stochastic process is exploited also before the lower bound α, providing crucial benefits for large time bounds. A case study is presented through the probabilistic formulation of Fischer's mutual exclusion protocol, a well-known real-time verification benchmark.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2468717","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7202875","Probabilistic Model Checking;Reachability;Stochastic Petri Net;Markov Regenerative Process;Markov Renewal Theory;Probabilistic model checking;reachability;stochastic Petri net;Markov regenerative process;Markov renewal theory","Probabilistic logic;Probability density function;Markov processes;Computational modeling;Numerical models;Petri nets","concurrency control;Markov processes;Petri nets;program verification;Volterra equations","probabilistic model checking;regenerative concurrent systems;quantitative reachability properties;stochastic models;concurrent activities;stochastic time Petri nets;Boolean combinations;regeneration point analysis;model executions;discrete event;Markov property;distributed timers;Volterra integral equations;stochastic state classes;joint probability density function;symbolic PDFs;stochastic process;Fischer's mutual exclusion protocol;real-time verification benchmark","","12","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Interface compilation: steps toward compiling program interfaces as languages","D. R. Engler","Stanford Univ., CA, USA","IEEE Transactions on Software Engineering","","1999","25","3","387","400","Interfaces-the collection of procedures and data structures that define a library, a subsystem, a module-are syntactically poor programming languages. They have state (defined both by the interface's data structures and internally), operations on this state (defined by the interface's procedures), and semantics associated with these operations. Given a way to incorporate interface semantics into compilation, interfaces can be compiled in the same manner as traditional languages such as ANSI C or FORTRAN. The article makes two contributions. First, it proposes and explores the metaphor of interface compilation, and provides the beginnings of a programming methodology for exploiting it. Second, it presents MAGIK, a system built to support interface compilation. Using MAGIK, software developers can build optimizers and checkers for their interface languages, and have these extensions incorporated into compilation, with a corresponding gain in efficiency and safety. This organization contrasts with traditional compilation, which relegates programmers to the role of passive consumers, rather than active exploiters of a compiler's transformational abilities.","0098-5589;1939-3520;2326-3881","","10.1109/32.798327","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=798327","","Programming profession;Optimizing compilers;Program processors;Data structures;Libraries;Computer languages;Software safety;File systems;High level languages;Control systems","program compilers;application program interfaces;data structures;programming language semantics","interface compilation;program interface compilation;data structures;programming languages;programming methodology;interface semantics;MAGIK;software developers;language optimizers;transformational abilities","","7","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing, Comparing, and Combining State Machine-Based Testing and Structural Testing: A Series of Experiments","S. Mouchawrab; L. C. Briand; Y. Labiche; M. Di Penta","Carleton University, Ottawa, Canada; Simula Research Laboratory, Lysaker, Norway; Carleton University, Ottawa, Canada; University of Sannio, Benevento, Italy","IEEE Transactions on Software Engineering","","2011","37","2","161","187","A large number of research works have addressed the importance of models in software engineering. However, the adoption of model-based techniques in software organizations is limited since these models are perceived to be expensive and not necessarily cost-effective. Focusing on model-based testing, this paper reports on a series of controlled experiments. It investigates the impact of state machine testing on fault detection in class clusters and its cost when compared with structural testing. Based on previous work showing this is a good compromise in terms of cost and effectiveness, this paper focuses on a specific state-based technique: the round-trip paths coverage criterion. Round-trip paths testing is compared to structural testing, and it is investigated whether they are complementary. Results show that even when a state machine models the behavior of the cluster under test as accurately as possible, no significant difference between the fault detection effectiveness of the two test strategies is observed, while the two test strategies are significantly more effective when combined by augmenting state machine testing with structural testing. A qualitative analysis also investigates the reasons why test techniques do not detect certain faults and how the cost of state machine testing can be brought down.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5416729","State-based software testing;structural testing;controlled experiments;state machines.","Object oriented modeling;Costs;Fault detection;Unified modeling language;System testing;Software testing;Automatic testing;Software engineering;Software design;Logic testing","fault tolerant computing;finite state machines;program testing;software engineering","state machine based testing;software engineering;model based techniques;software organizations;fault detection;round trip paths testing;structural testing","","33","","69","","","","","","IEEE","IEEE Journals & Magazines"
"An Investigation into the Functional Form of the Size-Defect Relationship for Software Modules","A. G. Koru; D. Zhang; K. El Emam; H. Liu","University of Maryland Baltimore County, Baltimore; University of Maryland Baltimore County, Baltimore; University of Ottawa, Ottawa; Georgetown University, Washington","IEEE Transactions on Software Engineering","","2009","35","2","293","304","The importance of the relationship between size and defect proneness of software modules is well recognized. Understanding the nature of that relationship can facilitate various development decisions related to prioritization of quality assurance activities. Overall, the previous research only drew a general conclusion that there was a monotonically increasing relationship between module size and defect proneness. In this study, we analyzed class-level size and defect data in order to increase our understanding of this crucial relationship. In order to obtain validated and more generalizable results, we studied four large-scale object-oriented products, Mozilla, Cn3d, JBoss, and Eclipse. Our results consistently revealed a significant effect of size on defect proneness; however, contrary to common intuition, the size-defect relationship took a logarithmic form, indicating that smaller classes were proportionally more problematic than larger classes. Therefore, practitioners should consider giving higher priority to smaller modules when planning focused quality assurance activities with limited resources. For example, in Mozilla and Eclipse, an inspection strategy investing 80% of available resources on 100-LOC classes and the rest on 1,000-LOC classes would be more than twice as cost effective as the opposite strategy. These results should be immediately useful to guide focused quality assurance activities in large-scale software projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4693715","Software science;Product metrics;Planning for SQA and Measurement applied to SQA and Software Quality/SQA;Software Engineering;Software/Software Engin;Open-source software;Software science;Product metrics;Planning for SQA and Measurement applied to SQA and Software Quality/SQA;Software Engineering;Software/Software Engin;Open-source software","Open source software;Software quality;Size measurement;Inspection;Object oriented modeling;Predictive models;Quality assurance;Large-scale systems;Software measurement;Density measurement","object-oriented methods;project management;software management;software metrics;software quality","software modules;size-defect relationship;quality assurance activities;object-oriented products;Mozilla;Cn3d;JBoss;Eclipse;software projects;product metrics","","65","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Timed Wp-method: testing real-time systems","A. En-Nouaary; R. Dssouli; F. Khendek","Dept. of Electr. & Comput. Eng., Concordia Univ., Montreal, Que., Canada; Dept. of Electr. & Comput. Eng., Concordia Univ., Montreal, Que., Canada; Dept. of Electr. & Comput. Eng., Concordia Univ., Montreal, Que., Canada","IEEE Transactions on Software Engineering","","2002","28","11","1023","1038","Real-time systems interact with their environment using time constrained input/output signals. Examples of real-time systems include patient monitoring systems, air traffic control systems, and telecommunication systems. For such systems, a functional misbehavior or a deviation from the specified time constraints may have catastrophic consequences. Therefore, ensuring the correctness of real-time systems becomes necessary. Two different techniques are usually used to cope with the correctness of a software system prior to its deployment, namely, verification and testing. In this paper, we address the issue of testing real-time software systems specified as a timed input output automaton (TIOA). TIOA is a variant of timed automaton. We introduce the syntax and semantics of TIOA. We present the potential faults that can be encountered in a timed system implementation. We study these different faults based on TIOA model and look at their effects on the execution of the system using the region graph. We present a method for generating timed test cases. This method is based on a state characterization technique and consists of the following three steps: First, we sample the region graph using a suitable granularity, in order to construct a subautomaton easily testable, called grid automaton. Then, we transform the grid automaton into a nondeterministic timed finite state machine (NTFSM). Finally, we adapt the generalized Wp-method to generate timed test cases from NTFSM. We assess the fault coverage of our test cases generation method and prove its ability to detect all the possible faults. Throughout the paper, we use examples to illustrate the various concepts and techniques used in our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049402","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049402","","System testing;Real time systems;Automata;Time factors;Software systems;Software testing;Automatic testing;Patient monitoring;Air traffic control;Fault detection","program testing;real-time systems;formal specification;automata theory","real-time systems testing;constrained input/output signals;correctness;software system;timed input output automaton;semantics;syntax;region graph;state characterization technique;granularity;subautomaton;grid automaton;nondeterministic timed finite state machine;generalized Wp-method;timed Wp-method;fault detection","","73","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Interfacing UNIX to Data Communications Networks","F. Panzieri; B. Randell","Suma-Sistemi vomo Macchina; NA","IEEE Transactions on Software Engineering","","1985","SE-11","10","1016","1032","We propose an interface for use from within UNIX1 user programs for communicating over multiple and varied local and wide area networks. This interface aids the design of a distributed application program by hiding the actual communications protocols used over each network, and providing instead simple primitives for sending and receiving (possibly large) datagrams, using a simple standardized network addressing scheme based on a &lt;host number, port number&gt; pair.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231548","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701916","Datagram;network protocols;networks;Newcastle Connection;UNIX","Data communication;Protocols;Network interfaces;Wide area networks;Application software;Computer architecture;Sockets;Intelligent networks;LAN interconnection;Packet switching","","Datagram;network protocols;networks;Newcastle Connection;UNIX","","1","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Experience with Charlotte: simplicity and function in a distributed operating system","R. A. Finkel; M. L. Scott; Y. Artsy; H. -. Chang","Dept. of Comput. Sci., Kentucky Univ., Lexington, KY, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","6","676","685","A retrospective view is presented of the Charlotte distributed operating system, a testbed for developing techniques and tools to solve computation-intensive problems with large-grain parallelism. The final version of Charlotte runs on the Crystal multicomputer, a collection of VAX-11/750 computers connected by a local area network. The kernel/process interface is unique in its support for symmetric, bidirectional communication paths (called links), and synchronous nonblocking communications. Several lessons were learned in implementing Charlotte. Links have proven to be a useful abstraction, but the primitives do not seem to be at quite the right level of abstraction. The implementation uses finite-state machines and a multitask kernel, both of which work well. It also maintains absolute distributed information which is more expensive that using hints. The development of high-level tools, particularly the Lynx distributed programming language, has simplified the use of kernal primitives and helps to manage concurrency at the process level.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24721","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24721","","Concurrent computing;Kernel;Operating systems;System testing;Distributed computing;Parallel processing;Computer networks;Local area networks;Bidirectional control;Computer languages","computer communications software;local area networks;multiprocessing programs;operating systems (computers);software packages","Charlotte distributed operating system;computation-intensive problems;large-grain parallelism;Crystal multicomputer;VAX-11/750 computers;local area network;kernel/process interface;bidirectional communication paths;synchronous nonblocking communications;abstraction;finite-state machines;multitask kernel;absolute distributed information;high-level tools;Lynx distributed programming language;kernal primitives;concurrency","","10","","45","","","","","","IEEE","IEEE Journals & Magazines"
"Response to ""Comments on 'Formal methods application: an empirical tale of software development""'","A. E. K. Sobel; M. R. Clarkson","NA; NA","IEEE Transactions on Software Engineering","","2003","29","6","572","575","We respond to criticism by D. Berry and W. Tichy of our paper that appeared in the March 2002 issue of IEEE Transactions on Software Engineering. Many of the supposed faults they identify in our experiment are a result of a misunderstanding on their part, while others are inherent aspects of an educational experiment. We present counterarguments that explain why our experiment is valid.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205184","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205184","","Application software;Problem-solving;Education;Fault diagnosis;Programming profession;Laboratories;Design for experiments;Software engineering;Software testing","formal verification;formal specification","formal methods application;software development","","12","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Bidirectional Symbolic Analysis for Effective Branch Testing","M. Baluda; G. Denaro; M. Pezzè","Secure Software Engineering Group, Fraunhofer SIT, Darmstadt, Germany; Department of Informatics, Systems and Communication, Università di Milano-Bicocca, Milano, Italy; Faculty of Informatics, Università della Svizzera italiana, Lugano, Switzerland","IEEE Transactions on Software Engineering","","2016","42","5","403","426","Structural coverage metrics, and in particular branch coverage, are popular approaches to measure the thoroughness of test suites. Unfortunately, the presence of elements that are not executable in the program under test and the difficulty of generating test cases for rare conditions impact on the effectiveness of the coverage obtained with current approaches. In this paper, we propose a new approach that combines symbolic execution and symbolic reachability analysis to improve the effectiveness of branch testing. Our approach embraces the ideal definition of branch coverage as the percentage of executable branches traversed with the test suite, and proposes a new bidirectional symbolic analysis for both testing rare execution conditions and eliminating infeasible branches from the set of test objectives. The approach is centered on a model of the analyzed execution space. The model identifies the frontier between symbolic execution and symbolic reachability analysis, to guide the alternation and the progress of bidirectional analysis towards the coverage targets. The experimental results presented in the paper indicate that the proposed approach can both find test inputs that exercise rare execution conditions that are not identified with state-of-the-art approaches and eliminate many infeasible branches from the coverage measurement. It can thus produce a modified branch coverage metric that indicates the amount of feasible branches covered during testing, and helps team leaders and developers in estimating the amount of not-yet-covered feasible branches. The approach proposed in this paper suffers less than the other approaches from particular cases that may trap the analysis in unbounded loops.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2490067","SNF; AVATAR; Italian PRIN; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7296670",";Structural testing;branch coverage;program analysis;symbolic execution;symbolic reachability analysis","Analytical models;Testing;Reachability analysis;Valves;Computational modeling;Measurement;Concrete","program diagnostics;program testing","bidirectional symbolic analysis;branch testing;structural coverage metrics;branch coverage;test suite thoroughness measure;test case generation;symbolic execution;symbolic reachability analysis;test objectives;coverage measurement","","5","","137","","","","","","IEEE","IEEE Journals & Magazines"
"A*: a language for implementing language processors","D. A. Ladd; J. C. Ramming","AT&T Bell Labs., Naperville, IL, USA; AT&T Bell Labs., Naperville, IL, USA","IEEE Transactions on Software Engineering","","1995","21","11","894","901","A* is an experimental language designed to facilitate the creation of language-processing tools. It is analogous either to an interpreted yacc with Awk as its statement language, or to a version of Awk which processes programs rather than records. A* offers two principal advantages over the combination of lex, yacc, and C: a high-level interpreted base language and built-in parse tree construction. A* programmers are thus able to accomplish many useful tasks with little code. This paper describes the motivation for A*, its design, and its evolution. Experience with A* is described, and then the paper concludes with an analysis of that experience.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.473218","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=473218","","Programming profession;Humans;Computer languages;Computer science;Costs;Application software;Software engineering","software tools;program processors;high level languages;program compilers;programming","A*;experimental language;language processors;language-processing tools;interpreted yacc;Awk;statement language;lex;yacc;C;high-level interpreted base language;parse tree construction","","20","","13","","","","","","IEEE","IEEE Journals & Magazines"
"A functional approach to program testing and analysis","W. E. Howden","Department of Electrical Engineering and Computer Science, University of California, San Diego, CA 92093","IEEE Transactions on Software Engineering","","1986","SE-12","10","997","1005","An integrated approach to testing is described which includes both static and dynamic analysis methods and which is based on theoretical results that prove both its effectiveness and efficiency. Programs are viewed as consisting of collections of functions that are joined together using elementary functional forms or complex functional structures. Functional testing is identified as the input-output analysis of functional forms. Classes of faults are defined for these forms, and results are presented which prove the fault-revealing effectiveness of well defined sets of tests. Functional analysis is identified as the analysis of the sequences of operators, functions, and data type transformations which occur in functional structures. Theoretical results are presented which prove that it is only necessary to look at interfaces between pairs of operators and data type transformations in order to detect the presence of operator or data type sequencing errors. The results depend on the definition of normal forms for operator and data type sequencing diagrams.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313016","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313016","Analysis;dynamic analysis;functions;input-output;interfaces;operators;sequence analysis;static analysis;testing;theory;validation","Testing;Measurement;Functional analysis;Data structures;Availability;Fault detection;Systematics","program testing","program analysis;static analysis;functional testing;functional approach;program testing;dynamic analysis;elementary functional forms;complex functional structures;input-output analysis","","20","","","","","","","","IEEE","IEEE Journals & Magazines"
"Generation of interactive parsers with error handling","E. Steegmans; J. Lewi; I. van Horebeek","Dept. of Comput. Sci., Katholieke Univ. Leuven, Heverlee, Belgium; Dept. of Comput. Sci., Katholieke Univ. Leuven, Heverlee, Belgium; Dept. of Comput. Sci., Katholieke Univ. Leuven, Heverlee, Belgium","IEEE Transactions on Software Engineering","","1992","18","5","357","367","The generation scheme discussed, produces interactive transducers in the form of Ada programs, with an underlying parser that is of type ELL(1). The emphasis is on error recovery in interactive parsers. A generation scheme is proposed containing powerful error-recovery generation capabilities. The interaction between syntactic and semantic error recovery is also discussed. The generation scheme has been implemented as part of the MIRA transducer writing system. With MIRA, a number of industrial case studies have been worked out, from which considerable feedback has been obtained to test and improve the adopted error-recovery strategy. One of the case studies worked out with MIRA consists of the design and implementation of an interactive software package called ABACUS. A subset called MINI-ABACUS is used as an illustration of the error-recovery principles discussed throughout this work.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.135769","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=135769","","Transducers;Power generation;Computer errors;Writing;Error correction;Software engineering;Batch production systems;Feedback;Testing;Software packages","Ada;grammars;interactive systems;program compilers;system recovery","ELL 1;interactive parsers;interactive transducers;Ada programs;underlying parser;generation scheme;error-recovery generation capabilities;semantic error recovery;MIRA transducer writing system;industrial case studies;adopted error-recovery strategy;interactive software package;MINI-ABACUS;error-recovery principles","","1","","30","","","","","","IEEE","IEEE Journals & Magazines"
"A Conceptual Analysis of the Draco Approach to Constructing Software Systems","P. Freeman","Department of Information and Computer Science, University of California, Irvine","IEEE Transactions on Software Engineering","","1987","SE-13","7","830","844","This paper analyzes the concepts of software construction embodied in the Draco approach. The analysis relates specific aspects of Draco to particular software engineering (SE) principles and suggests future research needed to extend the approach. The purpose of this analysis is to help researchers understand Draco better and thus to enhance future research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233494","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702294","Domain-based development;reusability;SE principles;transformational development","Software systems;Software engineering;Costs;Computer science;Software reusability;Software prototyping;Prototypes;Programming;Information analysis;Software testing","","Domain-based development;reusability;SE principles;transformational development","","16","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Specifying a safety-critical control system in Z","J. Jacky","Dept. of Radiat. Oncology, Washington Univ., Seattle, WA, USA","IEEE Transactions on Software Engineering","","1995","21","2","99","106","The paper presents a formal specification in the Z notation for a safety-critical control system. It describes a particular medical device but is quite generic and should be widely applicable. The specification emphasizes safety interlocking and other discontinuous features that are not considered in classical control theory. A method for calculating interlock conditions for particular operations from system safety assertions is proposed; it is similar to ordinary Z precondition calculation, but usually results in stronger preconditions. The specification is presented as a partially complete framework that can be edited and filled in with the specific features of a particular control system. Our system is large but the specification is concise. It is built up from components, subsystems, conditions and modes that are developed separately, but also accounts for behaviors that emerge at the system level. The specification illustrates several useful idioms of the Z notation, and demonstrates that an object-oriented specification style can be expressed in ordinary Z.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.345826","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=345826","","Control systems;Medical control systems;Formal specifications;Safety;Cyclotrons;Medical treatment;Control theory;Process control;Application software;Programming","safety-critical software;computerised control;biomedical equipment;safety;formal specification;specification languages","safety-critical control system;Z notation;formal specification;medical device;safety interlocking;interlock conditions;system safety assertions;Z precondition calculation;partially complete framework;object-oriented specification style","","17","","17","","","","","","IEEE","IEEE Journals & Magazines"
"How Software Developers Use Tagging to Support Reminding and Refinding","M. Storey; J. Ryall; J. Singer; D. Myers; L. Cheng; M. Muller","University of Victoria, Victoria; University of Victoria, Victoria; National Research Council Canada, Ottawa; University of Victoria, Victoria; IBM Research, Cambridge; IBM Research, Cambridge","IEEE Transactions on Software Engineering","","2009","35","4","470","483","Developers frequently add annotations to source code to help them remember pertinent information and mark locations of interest for future investigation. Finding and refinding these notes is a form of navigation that is integral to software maintenance. Although there is some tool support in modern development environments for authoring and navigating these comments, we have observed that these annotations often fail to remind and are sometimes difficult to find by the programmer. To address these shortcomings, we have designed a new approach for software navigation called tags for software engineering activities (TagSEA). TagSEA combines the notion of waypointing (a mechanism for marking locations in spatial navigation) with social tagging to support programmers in defining semantically rich annotations to source code comments. The tool provides support for creating, editing, navigating, and managing these annotations. We present the results from two empirical studies, where we observed and then analyzed how professional programmers used source code annotations to support their development activities over 24 months. Our findings indicate that the addition of semantic information to annotations can improve their value. We also provide suggestions on how annotation tools in general may be improved.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4782972","Annotations;software navigation;software tagging;tags;software development tools.","Tagging;Navigation;Programming profession;Software maintenance;Software tools;Software engineering;Filters;Software development management;Maintenance engineering;Computer science","authoring systems;software maintenance","source code annotations;software maintenance;modern development environments;software navigation;software engineering activities;waypointing;social tagging;software development tools","","24","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A Two-Person Inspection Method to Improve Prog ramming Productivity","D. B. Bisant; J. R. Lyle","Department of Natural Sciences, George Washington University, Washhgton, DC 20057.; NA","IEEE Transactions on Software Engineering","","1989","15","10","1294","1304","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1989.559782","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=559782","","Inspection;Productivity;Programming profession;Software quality;Logic testing;Error correction codes;Analysis of variance;Application software;Computer industry;Costs","","Fatal detection;inspections;programmer productivity;watkthrough","","64","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Software process model evolution in the SPADE environment","S. C. Bandinelli; A. Fuggetta; C. Ghezzi","Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy; Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy; Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy","IEEE Transactions on Software Engineering","","1993","19","12","1128","1144","Software processes are long-lived entities. Careful design and thorough validation of software process models are necessary to ensure the quality of the process. They do not prevent, however, process models from undergoing change. Change requests may occur in the context of reuse, i.e. statically, in order to support software process model customization. They can also occur dynamically, while software process models are being executed, in order to support timely reaction as data are gathered from the field during process enactment. We discuss the mechanisms a process language should possess in order to support changes. We illustrate the solution adopted in the context of the SPADE environment and discuss how the proposed mechanisms can be used to model different policies for changing a software process model.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.249659","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=249659","","Object oriented modeling;Humans;Context modeling;Software engineering;Automation;Programming;Computer architecture;Software quality;Vents;Object oriented databases","formal languages;Petri nets;software engineering","software process model evolution;SPADE environment;long-lived entities;change requests;reuse;software process model customization;timely reaction;process enactment;process language;SLANG;high-level Petri nets","","130","","34","","","","","","IEEE","IEEE Journals & Magazines"
"An Availability Model for Distributed Transaction Systems","G. Martella; B. Pernici; F. A. Schreiber","Dipartimento di Elettronica, Politecnico di Milano; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","5","483","491","A method is proposed for quantitatively evaluating the availability of a distributed transaction system (DTS). The DTS dynamics can be modeled as a Markov process. The problem of formulating the set of linear homogeneous equations is considered, obtaining the related coefficient matrix, that is, the transition rate matrices of the DTS elements. Such operations can be performed according to the rules of Kronecker algebra. The transition rate matrices are used to calculate the probabilities of the different possible states of the DTS. The availability with respect to a transaction T is computed through its representation by means of a structure graph and a structure vector related to the probabilistic state of the DTS element relevant to the transaction T itself.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232488","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702039","Availability;distributed databases;Markov models;performability;reliability","Testing;Yield estimation;Software reliability;Computer bugs;Availability;Error analysis;Decision theory;Computer errors;Software measurement;Computer performance","","Availability;distributed databases;Markov models;performability;reliability","","4","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Data Compression in Scientific and Statistical Databases","M. A. Bassiouni","Department of Computer Science, University of Central Florida","IEEE Transactions on Software Engineering","","1985","SE-11","10","1047","1058","Scientific and statistical database systems heavily depend on data compression techniques to make possible the management and storage of their large databases. The efficiency of data compression methods has a signficant impact on the overall performance of these systems. The purpose of this paper is to show the importance of data compression to scientific/statistical databases, to discuss the pros and cons of data compression, and to survey data compression techniques relevant to scientific/statistical databases. The emphasis is on the basic idea, motivation, and tradeoffs of each approach. Both software and hardware methods are covered. The paper is concluded by a discussion of several points of research that seem worthy of further investigation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231852","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701920","Coding techniques;data compression;data storage;data transmission;scientific/statistical databases","Data compression;Database systems;Economic forecasting;Memory;Data communication;Computer science;Transaction databases;Costs;Information retrieval;Demography","","Coding techniques;data compression;data storage;data transmission;scientific/statistical databases","","52","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Maintaining Configurations of Evolving Software Systems","K. Narayanaswamy; W. Scacchi","the USC/Information Sciences Institute; NA","IEEE Transactions on Software Engineering","","1987","SE-13","3","324","334","Software configuration management ( SCM) is an emerging discipline. An important aspect of realizing SCM is the task of maintaining the configurations of evolving software systems. In this paper, we provide an approach to resolving some of the conceptual and technical problems in maintaining configurations of evolving software systems. The approach provides a formal basis for existing notions of system architecture. The formal properties of this view of configurations provide the underpinnings for a rigorous notion of system integrity, and mechanisms to control the evolution of configurations. This approach is embodied in a language, NuMIL, to describe software system configurations, and a prototype environment to maintain software system configurations. We believe that the approach and the prototype environment offer a firm base to maintain software system configurations and, therefore, to implement SCM.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233163","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702218","Configuration;module and subsystem families;module and subsystem interfaces;software configuration maintenance system;upward compatibility","Software systems;Software maintenance;Software prototyping;Control systems;Computer science;Computer architecture;Mechanical factors;Large-scale systems;Application software;Information systems","","Configuration;module and subsystem families;module and subsystem interfaces;software configuration maintenance system;upward compatibility","","18","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Controllability of computer performance tradeoffs obtained using controlled-share queue schedulers","C. M. Woodside","Department of Systems and Computer Engineering, Carleton University, Ottawa, Ont. K1S 5B6, Canada","IEEE Transactions on Software Engineering","","1986","SE-12","10","1041","1048","Adjustable feedback schedulers control the relative performance obtained from a computer system by different classes of users. This work examines the control of relative throughputs (or related measures) by feedback of departure counts. Counts from key resources in the system are used to dynamically adjust the priorities of the different classes at certain queues in an attempt to achieve preset values of the ratios of the class throughputs. An iterative mean-value algorithm is given which approximates the throughputs actually achieved by this scheduler within a few percent of error. Limits of controllability are observed beyond which the achieved throughput ratios cannot follow the preset values. A characterization is given for this `controllable performance set'. It is also shown how, using this set, the scheduler can be tuned to have approximately optimal set points for the ratios.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6313020","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6313020","Adjustable priorities;computer scheduling;control of queue networks;dynamic properties;fair sharing;queue networks","Throughput;Time factors;Controllability;Approximation algorithms;Indexes;Approximation methods;Computers","controllability;performance evaluation;queueing theory","controllability;adjustable feedback schedulers;computer performance tradeoffs;controlled-share queue schedulers;key resources;iterative mean-value algorithm","","5","","","","","","","","IEEE","IEEE Journals & Magazines"
"Requirements Elicitation and Specification Using the Agent Paradigm: The Case Study of an Aircraft Turnaround Simulator","T. Miller; B. Lu; L. Sterling; G. Beydoun; K. Taveter","Department of Computing and Information Systems, The University of Melbourne, Melbourne, Victoria, Autralia; Department of Computing and Information Systems, The University of Melbourne, Melbourne, Victoria, Autralia; Faculty of ICT, Swinburne University of Technology, Melbourne, Victoria, Autralia; Faculty of Informatics, University of Wollongong, Wollongong, NSW 2522, Australia; Institute of Informatics, Tallinn University of Technology, Tallinn, EU, Estonia","IEEE Transactions on Software Engineering","","2014","40","10","1007","1024","In this paper, we describe research results arising from a technology transfer exercise on agent-oriented requirements engineering with an industry partner. We introduce two improvements to the state-of-the-art in agent-oriented requirements engineering, designed to mitigate two problems experienced by ourselves and our industry partner: (1) the lack of systematic methods for agent-oriented requirements elicitation and modelling; and (2) the lack of prescribed deliverables in agent-oriented requirements engineering. We discuss the application of our new approach to an aircraft turnaround simulator built in conjunction with our industry partner, and show how agent-oriented models can be derived and used to construct a complete requirements package. We evaluate this by having three independent people design and implement prototypes of the aircraft turnaround simulator, and comparing the three prototypes. Our evaluation indicates that our approach is effective at delivering correct, complete, and consistent requirements that satisfy the stakeholders, and can be used in a repeatable manner to produce designs and implementations. We discuss lessons learnt from applying this approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339827","Australian Research Council Linkage; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6860260","Agent-oriented software engineering;agent-oriented modelling;technology transfer","Object oriented modeling;Aircraft;Atmospheric modeling;Software;Industries;Analytical models;Educational institutions","aerospace computing;aircraft;formal specification;multi-agent systems;object-oriented programming;software agents","requirements elicitation;requirements specification;agent paradigm;aircraft turnaround simulator;agent oriented requirements engineering;industry partner;systematic methods;agent-oriented requirements elicitation;agent-oriented requirement modelling;independent people design","","15","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis of a virtual memory model for maintaining database views","K. C. Kinsley; C. E. Hughes","Datawise Inc., Orlando, FL, USA; NA","IEEE Transactions on Software Engineering","","1992","18","5","402","409","An analytical model is given for predicting the performance of a new support strategy for database views. This strategy, called the virtual method, is compared with traditional methods for supporting views. The analytical model's predictions of improved performance by the virtual method are then validated by comparing these results with those achieved in an experimental implementation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.135773","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=135773","","Analytical models;Performance analysis;Relational databases;Queueing analysis;Computer science;Predictive models;Costs;NASA;Space technology;Memory management","database management systems;database theory;virtual storage","virtual memory model;analytical model;support strategy;database views;virtual method;traditional methods;experimental implementation","","","","16","","","","","","IEEE","IEEE Journals & Magazines"
"A model for software product quality","R. G. Dromey","Software Quality Inst., Griffith Univ., Brisbane, Qld., Australia","IEEE Transactions on Software Engineering","","1995","21","2","146","162","A model for software product quality is defined, it has been formulated by associating a set of quality-carrying properties with each of the structural forms that are used to define the statements and statement components of a programming language. These quality-carrying properties are in turn linked to the high-level quality attributes of the International Standard for Software Product Evaluation ISO-9126. The model supports building quality into software, definition of language-specific coding standards, systematically classifying quality defects, and the development of automated code auditors for detecting defects in software.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.345830","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=345830","","Software quality;Software standards;Software maintenance;Buildings;Refining;Computer languages;Code standards;Standards development;Australia;Knowledge management","software quality;software standards;ISO standards;program debugging;software tools","software product quality;quality-carrying properties;structural forms;programming language;high-level quality attributes;International Standard for Software Product Evaluation;ISO-9126;language-specific coding standards;quality defects;automated code auditors;software defect detection","","164","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Program translation via abstraction and reimplementation","R. C. Waters","Artificial Intelligence Lab., MIT, Cambridge, MA, USA","IEEE Transactions on Software Engineering","","1988","14","8","1207","1228","An abstraction-and-reimplementation paradigm is presented in which the source program is first analyzed in order to obtain a programming-language-independent abstract understanding of the computation performed by the program as a whole. The program is then reimplemented in the target language based on this understanding. The key to this approach is the abstract understanding obtained. It allows the translator to benefit from an appreciation of the global features of the source program without being distracted by what are considered irrelevant details. Knowledge-based translation via abstraction and reimplementation is described as one of the goals of the Programmer's Apprentice project. A translator which translates Cobol programs into Hibol (a very-high-level business data processing language) has been constructed. A computer which generates extremely efficient PDP-11 object code for Pascal programs has been designed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.7629","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7629","","Program processors;Artificial intelligence;Performance analysis;Data processing;Prototypes;Writing","COBOL;data structures;expert systems;Pascal;program compilers;program interpreters","compilers;knowledge based system;data structures;abstract understanding;reimplementation;Programmer's Apprentice;translator;Cobol;Hibol;business data processing language;PDP-11 object code;Pascal programs","","56","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Applying synthesis principles to create responsive software systems","C. U. Smith","L&S Comput. Technol. Inc., Austin, TX, USA","IEEE Transactions on Software Engineering","","1988","14","10","1394","1408","The general principles for formulating software requirements and designs that meet response-time goals are reviewed. The principles are related to the system performance parameters that they improve, and thus their application may not be obvious to those whose speciality is system architecture and design. The author addresses the designer's perspective and illustrates how these principles apply to typical design problems. The examples illustrate requirements and design of: communication, user interfaces, information storage, retrieval and update, information hiding, and data availability. Strategies for effective use of the principles are described.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6185","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6185","","Software systems;Delay;Software performance;Software design;Design engineering;Information retrieval;Maintenance engineering;Application software;User interfaces;Systems engineering and theory","systems analysis;user interfaces","systems analysis;synthesis principles;responsive software systems;software requirements;system performance parameters;system architecture;user interfaces;information storage","","6","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Completeness and consistency in hierarchical state-based requirements","M. P. E. Heimdahl; N. G. Leveson","Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA; NA","IEEE Transactions on Software Engineering","","1996","22","6","363","377","This paper describes methods for automatically analyzing formal, state-based requirements specifications for some aspects of completeness and consistency. The approach uses a low-level functional formalism, simplifying the analysis process. State-space explosion problems are eliminated by applying the analysis at a high level of abstraction; i.e., instead of generating a reachability graph for analysis, the analysis is performed directly on the model. The method scales up to large systems by decomposing the specification into smaller, analyzable parts and then using functional composition rules to ensure that verified properties hold for the entire specification. The analysis algorithms and tools have been validated on TCAS II, a complex, airborne, collision-avoidance system required on all commercial aircraft with more than 30 passengers that fly in U.S. Airspace.","0098-5589;1939-3520;2326-3881","","10.1109/32.508311","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=508311","","Performance analysis;Error correction;Robustness;Aircraft;Software safety;Computer science;Explosions;Algorithm design and analysis;Software systems;Timing","formal specification;reachability analysis;program diagnostics","hierarchical state-based requirements;formal state-based requirements specifications;completeness;consistency;low-level functional formalism;state-space explosion problems;abstraction;reachability graph;TCAS II;complex airborne collision-avoidance system;static analysis;reactive systems;state-based requirements;formal semantics;formal methods","","131","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Evidence-based guidelines for assessment of software development cost uncertainty","M. Jorgensen","Simula Res. Lab., Lysaker, Norway","IEEE Transactions on Software Engineering","","2005","31","11","942","954","Several studies suggest that uncertainty assessments of software development costs are strongly biased toward overconfidence, i.e., that software cost estimates typically are believed to be more accurate than they really are. This overconfidence may lead to poor project planning. As a means of improving cost uncertainty assessments, we provide evidence-based guidelines for how to assess software development cost uncertainty, based on results from relevant empirical studies. The general guidelines provided are: 1) Do not rely solely on unaided, intuition-based uncertainty assessment processes, 2) do not replace expert judgment with formal uncertainty assessment models, 3) apply structured and explicit judgment-based processes, 4) apply strategies based on an outside view of the project, 5) combine uncertainty assessments from different sources through group work, not through mechanical combination, 6) use motivational mechanisms with care and only if greater effort is likely to lead to improved assessments, and 7) frame the assessment problem to fit the structure of the relevant uncertainty information and the assessment process. These guidelines are preliminary and should be updated in response to new evidence.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.128","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556553","Index Terms- Cost estimation;management;software psychology;uncertainty of software development cost.","Guidelines;Programming;Uncertainty;Cost function;Humans;Software development management;Psychology;Process planning;Estimation error;Terminology","software cost estimation;formal specification;project management","evidence-based guideline;software development cost uncertainty;software psychology;software cost estimation;project planning;intuition-based uncertainty assessment;formal uncertainty assessment model;mechanical combination","","29","","75","","","","","","IEEE","IEEE Journals & Magazines"
"An experimental evaluation of software redundancy as a strategy for improving reliability","D. E. Eckhardt; A. K. Caglayan; J. C. Knight; L. D. Lee; D. F. McAllister; M. A. Vouk; J. P. J. Kelly","NASA Langley Res. Center, Hampton, VA, USA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1991","17","7","692","702","The strategy of using multiple versions of independently developed software as a means to tolerate residual software design faults is discussed. The effectiveness of multiversion software is studied by comparing estimates of the failure probabilities of these systems with the failure probabilities of single versions. The estimates are obtained under a model of dependent failures and compared with estimates obtained when failures are assumed to be independent. The experimental results are based on 20 versions of an aerospace application developed and independently validated by 60 programmers from 4 universities. Descriptions of the application and development process are given, together with an analysis of the 20 versions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.83905","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=83905","","Redundancy;Hardware;Application software;Educational institutions;Software reliability;Software measurement;Fault tolerance;Reliability engineering;Design engineering;Software design","fault tolerant computing;program testing;redundancy;software reliability","experimental evaluation;software redundancy;multiple versions;independently developed software;residual software design faults;multiversion software;failure probabilities;dependent failures;experimental results;aerospace application;programmers;development process","","64","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Proactive Self-Adaptation for Improving the Reliability of Mission-Critical, Embedded, and Mobile Software","D. Cooray; E. Kouroshfar; S. Malek; R. Roshandel","VeriSign Inc., Reston; George Mason University, Fairfax; George Mason University, Fairfax; Seattle University, Seattle","IEEE Transactions on Software Engineering","","2013","39","12","1714","1735","Embedded and mobile software systems are marked with a high degree of unpredictability and dynamism in the execution context. At the same time, such systems are often mission-critical, meaning that they need to satisfy strict reliability requirements. Most current software reliability analysis approaches are not suitable for these types of software systems, as they do not take the changes in the execution context of the system into account. We propose an approach geared to such systems which continuously furnishes refined reliability predictions at runtime by incorporating various sources of information, including the execution context of the system. The reliability predictions are leveraged to proactively place the software in the (near-)optimal configuration with respect to changing conditions. Our approach considers two representative architectural reconfiguration decisions that impact the system's reliability: reallocation of components to processes and changing the number of component replicas. We have realized the approach as part of a framework intended for mission-critical settings, called REsilient SItuated SofTware system (RESIST), and evaluated it using a mobile emergency response system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6574866","Context awareness;software architecture;self-adaptive systems;reliability;mobility","Mobile communication;Software reliability;Context awareness;Reliability engineering;Software  architecture;Computer architecture","embedded systems;mobile computing;software architecture;software reliability","proactive self-adaptation;mission-critical software;embedded software;mobile software;unpredictability degree;dynamism degree;execution context;reliability requirements;software reliability analysis approach;architectural reconfiguration decisions;component reallocation;component replicas;RESIST approach;resilient situated software system;mobile emergency response system","","7","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Incentivizing Deep Fixes in Software Economies","M. Rao; D. F. Bacon; D. Parkes; M. Seltzer","John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts United States 02138 (e-mail: malvikar@gmail.com); Software, Google Inc New York, 470984 New York, New York United States (e-mail: dfb@google.com); John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts United States (e-mail: parkes@eecs.harvard.edu); John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts United States (e-mail: margo@eecs.harvard.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","An important question in a software economy is how to incentivize deep rather than shallow fixes. A deep fix corrects the root cause of a bug instead of suppressing the symptoms. This paper initiates the study of the problem of incentive design for open workflows in fixing code. We model the dynamics of the software ecosystem and introduce subsumption mechanisms. These mechanisms only make use of externally observable information in determining payments and promote competition between workers. We use a mean field equilibrium methodology to evaluate the performance of these mechanisms, demonstrating in simulation that subsumption mechanisms perform robustly across various environment configurations and satisfy important criteria for market design.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2842188","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8384304","market design;mean field equilibrium;software engineering;payment mechanisms","Computer bugs;Task analysis;Ecosystems;Open source software;Testing;Software engineering","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Measuring design-level cohesion","J. M. Bieman; Byung-Kyoo Kang","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; NA","IEEE Transactions on Software Engineering","","1998","24","2","111","124","Cohesion was first introduced as a software attribute that, when measured, could be used to predict properties of implementations that would be created from a given design. Unfortunately, cohesion, as originally defined, could not be objectively assessed, while more recently developed objective cohesion measures depend on code-level information. We show that association-based and slice-based approaches can be used to measure cohesion using only design-level information. An analytical and empirical analysis shows that the design-level measures correspond closely with code-level cohesion measures. They can be used as predictors of or surrogates for the code-level measures. The design-level cohesion measures are formally defined, have been implemented, and can support software design, maintenance and restructuring.","0098-5589;1939-3520;2326-3881","","10.1109/32.666825","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=666825","","Software measurement;Software maintenance;Software design;Sliding mode control;Visualization;Debugging;Guidelines;Software quality;Packaging;Information analysis","software metrics;software maintenance;software reusability;systems re-engineering","design-level cohesion measurement;software attribute;implementation properties prediction;objective assessment;code-level information;association-based approaches;slice-based approaches;design-level measures;code-level measures;software design;software maintenance;software restructuring;software reengineering;software visualization;software reuse;software metrics","","41","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing real-time equational rule-based systems","Y. -. Lee; A. M. K. Cheng","Dept. of Comput. Sci., Houston Univ., TX, USA; Dept. of Comput. Sci., Houston Univ., TX, USA","IEEE Transactions on Software Engineering","","2004","30","2","112","125","Analyzing and reducing the execution-time upper bound of real-time rule-based expert systems is a very important task because of the stringent timing constraints imposed on this class of systems. We present a new runtime optimization to reduce the execution-time upper bound of real-time rule-based expert systems. In order to determine rules to be evaluated at runtime, a predicate dependency list, which consists of a predicate, its active rule set and corresponding inactive rule set, is created for each predicate in a real-time rule-based program. Based on the predicate dependency list and the current value of each variable, the new runtime optimization dynamically selects rules to be evaluated at runtime. For the timing analysis of the proposed algorithm, we introduce a predicate-based rule dependency graph, a predicate-based enable-rule graph, and their construction algorithm. We also discuss the bounded time of the equational logic rule-based program using the predicate-based rule dependency graph as well as the predicate-based enable-rule graph. The implementation and performance evaluation of the proposed algorithm using both synthetic and practical real-time rule-base programs are also presented. The performance evaluation shows that the runtime optimizer reduces the number of rule evaluations and predicate evaluations as well as the response time upper bound significantly, and the new algorithm yields better execution-time upper bound compared to other optimization methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265816","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265816","","Real time systems;Equations;Knowledge based systems;Runtime;Upper bound;Expert systems;Timing;Optimization methods;Algorithm design and analysis;Logic","knowledge based systems;logic programming;real-time systems;specification languages;optimisation","real-time rule based expert system;equational logic rule-based program;runtime optimization;predicate based rule dependency graph;EQL language","","7","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Smart Bound Selection for the Verification of UML/OCL Class Diagrams","R. Clariso; C. A. Gonzalez; J. Cabot","IT, Multimedia and Telecommunication Department, Universitat Oberta de Catalunya, 16766 Barcelona, Barcelona Spain 08018 (e-mail: rclariso@uoc.edu); SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg, Luxembourg Luxembourg (e-mail: caglezp@gmail.com); IN3-UOC, Institucio Catalana de Recerca i Estudis Avancats, 117370 Barcelona, Catalunya Spain (e-mail: jordi.cabot@icrea.cat)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Correctness of UML class diagrams annotated with OCL constraints can be checked using bounded verification techniques, e.g., SAT or constraint programming (CP) solvers. Bounded verification detects faults efficiently but, on the other hand, the absence of faults does not guarantee a correct behavior outside the bounded domain. Hence, choosing suitable bounds is a non-trivial process as there is a trade-off between the verification time (faster for smaller domains) and the confidence in the result (better for larger domains). Unfortunately, bounded verification tools provide little support in the bound selection process. In this paper, we present a technique that can be used to (i) automatically infer verification bounds whenever possible, (ii) tighten a set of bounds proposed by the user and (iii) guide the user in the bound selection process. This approach may increase the usability of UML/OCL bounded verification tools and improve the efficiency of the verification process.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2777830","Internet Interdisciplinary Institute IN3-UOC; Spanish Ministry of Economy and Competitivity; H2020 ECSEL Joint Undertaking; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8119996","Formal Verification;UML;Class Diagram;OCL;Constraint Propagation;SAT","Unified modeling language;Tools;Sociology;Statistics;Analytical models;Computational modeling;Software","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Prediction of software reliability using connectionist models","N. Karunanithi; D. Whitley; Y. K. Malaiya","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA","IEEE Transactions on Software Engineering","","1992","18","7","563","574","The usefulness of connectionist models for software reliability growth prediction is illustrated. The applicability of the connectionist approach is explored using various network models, training regimes, and data representation methods. An empirical comparison is made between this approach and five well-known software reliability growth models using actual data sets from several different software projects. The results presented suggest that connectionist models may adapt well across different data sets and exhibit a better predictive accuracy. The analysis shows that the connectionist approach is capable of developing models of varying complexity.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.148475","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=148475","","Software reliability;Predictive models;Parametric statistics;Educational institutions;Artificial neural networks;Senior members;Analytical models;Testing;Accuracy;Application software","neural nets;software reliability","software reliability;connectionist models;network models;training regimes;data representation methods;complexity","","122","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Commitment-based software development","W. Mark; S. Tyler; J. McGuire; J. Schlossberg","Lockheed Palo Alto Res. Labs., CA, USA; Lockheed Palo Alto Res. Labs., CA, USA; Lockheed Palo Alto Res. Labs., CA, USA; Lockheed Palo Alto Res. Labs., CA, USA","IEEE Transactions on Software Engineering","","1992","18","10","870","885","During the development of a system, software modules can be viewed in terms of their commitments: the constraints imposed by their own structure and behavior, and by their relationships with other modules (in terms of resource consumption, data requirements. etc.). The Comet system uses explicit representation and reasoning with commitments to aid the software design and development process-in particular, to lead software developers to make decisions that result in reuse. Developers can examine the commitments that must be met in order to include an existing module, and can explore how commitments change when modules are modified. Comet has been applied to the domain of sensor-based tracker software.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.163604","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=163604","","Programming;Software design;Software systems;Feedback;Design automation;Control systems;Ontologies;Couplings","artificial intelligence;knowledge representation;software engineering;software reusability","software development;software modules;constraints;resource consumption;data requirements;Comet system;explicit representation;reasoning;commitments;reuse;sensor-based tracker software","","12","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A model for software development effort and cost estimation","K. Pillai; V. S. Sukumaran Nair","Dept. of Comput. Sci. & Eng., Southern Methodist Univ., Dallas, TX, USA; NA","IEEE Transactions on Software Engineering","","1997","23","8","485","497","Several algorithmic models have been proposed to estimate software costs and other management parameters. Early prediction of completion time is absolutely essential for proper advance planning and aversion of the possible ruin of a project. L.H. Putnam's (1978) SLIM (Software LIfecycle Management) model offers a fairly reliable method that is used extensively to predict project completion times and manpower requirements as the project evolves. However, the nature of the Norden/Rayleigh curve used by Putnam renders it unreliable during the initial phases of the project, especially in projects involving a fast manpower buildup, as is the case with most software projects. In this paper, we propose the use of a model that improves early prediction considerably over the Putnam model. An analytic proof of the model's improved performance is also demonstrated on simulated data.","0098-5589;1939-3520;2326-3881","","10.1109/32.624305","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=624305","","Programming;Mathematical model;Predictive models;Cost function;Software algorithms;Hardware;Performance analysis;Analytical models;Art;Data analysis","software development management;software cost estimation;project management","software development effort;software cost estimation;algorithmic models;project completion time prediction;advance planning;SLIM model;software lifecycle management model;manpower requirements;Norden/Rayleigh curve;initial project phases;fast manpower buildup;model performance;software development time;Gamma model","","83","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Design of flexible static program analyzers with PQL","S. Jarzabek","Dept. of Inf. Syst. & Comput. Sci., Nat. Univ. of Singapore, Singapore","IEEE Transactions on Software Engineering","","1998","24","3","197","215","Static program analyzers (SPA) are interactive tools that enhance program understanding during maintenance by answering queries about programs. Depending on the maintenance task in hand, SPAs must process different source programs and answer different types of program queries. Flexibility is, therefore, a desirable property of SPAs. The author describes a program query language, called PQL, that facilitates the design of flexible SPAs. PQL is a conceptual level, source language-independent notation to specify program queries and program views. In PQL, one can query global program design as well as search for detail code patterns. PQL queries are answered automatically by a query evaluation mechanism built into an SPA. Program design models and POL form the core of an SPA conceptual model. He based the SPA's architecture on this conceptual model. By separating the conceptual model from the implementation decisions, one can design SPAs that are customizable to the needs of the maintenance project at hand. Depending on criteria such as efficiency of query evaluation or simplicity of the SPA design, one can implement the same functional specifications of an SPA on a variety of program representations to meet the required criteria. Apart from its role in the design of SPAs, the conceptual model also allows one to rigorously study SPA functionality in the context of the underlying maintenance process and programmer behavior models, in isolation from tool implementation details.","0098-5589;1939-3520;2326-3881","","10.1109/32.667879","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=667879","","Programming profession;Database languages;Query processing;Context modeling;User interfaces;Computer Society;Reverse engineering;Software maintenance;Cost function;Information systems","system monitoring;reverse engineering;software maintenance;query languages;query processing;software tools;relational databases","flexible static program analyzer design;PQL;interactive tools;program understanding;maintenance;query answering;source programs;program query language;conceptual level source language-independent notation;program views;query global program design;code pattern searching;query evaluation mechanism;maintenance project;functional specifications;programmer behavior models","","12","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Impact of Introducing Domain-Specific Modelling in Software Maintenance: An Industrial Case Study","N. Mellegård; A. Ferwerda; K. Lind; R. Heldal; M. R. V. Chaudron","Electromobility Group at the Research Institute Viktoria Swedish ICT, Gothenburg, Sweden; Centric, Gouda, The Netherlands; Electromobility Group at the Research Institute Viktoria Swedish ICT, Gothenburg, Sweden; Software Engineering Division at the joint Department of Computer Science and Engineering, Chalmers and Gothenborg University, Gothenburg, Sweden; Software Engineering Division at the joint Department of Computer Science and Engineering, Chalmers and Gothenborg University, Gothenburg, Sweden","IEEE Transactions on Software Engineering","","2016","42","3","245","260","Domain-specific modelling (DSM) is a modern software development technology that aims at enhancing productivity. One of the claimed advantages of DSM is increased maintainability of software. However, current empirical evidence supporting this claim is lacking. In this paper, we contribute evidence from a case study conducted at a software development company. We study how the introduction of DSM affected the maintenance of a legacy system. We collected data about the maintenance phase of a system that was initially developed using manual programming, but which was gradually replaced by DSM development. We performed statistical analyses of the relation between the use of DSM and the time needed to resolve defects, the defect density, and the phase in which defects were detected. The results show that after introducing DSM the defect density is lower, that defects are found earlier, but resolving defects takes longer. Other observed benefits are that the number of developers and the number of person-hours needed for maintaining the system decreased, and the portability to new platforms increased. Our findings are useful for organizations that consider introducing DSM and would like to know which benefits can be realized in software maintenance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2479221","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7270333","Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity;Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity","DSL;Maintenance engineering;Unified modeling language;Business;Software maintenance;Productivity","software maintenance;statistical analysis","domain-specific modelling;software maintenance;DSM;software development technology;software maintainability;software development company;legacy system;manual programming;statistical analysis","","8","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Time and Cost Evaluation Schemes of Multiple Copies of Data in Distributed Database Systems","M. Yoshida; K. Mizumachi; A. Wakino; I. Oyake; Y. Matsushita","OKI Electric Industry Company, Limited; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","954","959","In comparison to centralized database systems, distributed database systems have certain advantages depending on the manner in which data are redundantly distributed. These advantages are improvement in response time, better data availability, reduction in transmission cost, etc.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232829","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702114","Application environments;concurrency control;consistency;data allocation;distributed database;simulation model;transaction","Costs;Database systems;Delay;Availability;Concurrency control;Distributed databases;System recovery;Communication system control;Transaction databases;Control systems","","Application environments;concurrency control;consistency;data allocation;distributed database;simulation model;transaction","","3","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting Domain Analysis through Mining and Recommending Features from Online Product Listings","N. Hariri; C. Castro-Herrera; M. Mirakhorli; J. Cleland-Huang; B. Mobasher","DePaul University, Chicago; Google Inc.; DePaul University, Chicago; DePaul University, Chicago; DePaul University, Chicago","IEEE Transactions on Software Engineering","","2013","39","12","1736","1752","Domain analysis is a labor-intensive task in which related software systems are analyzed to discover their common and variable parts. Many software projects include extensive domain analysis activities, intended to jumpstart the requirements process through identifying potential features. In this paper, we present a recommender system that is designed to reduce the human effort of performing domain analysis. Our approach relies on data mining techniques to discover common features across products as well as relationships among those features. We use a novel incremental diffusive algorithm to extract features from online product descriptions, and then employ association rule mining and the (k)-nearest neighbor machine learning method to make feature recommendations during the domain analysis process. Our feature mining and feature recommendation algorithms are quantitatively evaluated and the results are presented. Also, the performance of the recommender system is illustrated and evaluated within the context of a case study for an enterprise-level collaborative software suite. The results clearly highlight the benefits and limitations of our approach, as well as the necessary preconditions for its success.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6582404","Domain analysis;recommender systems;clustering;association rule mining;k-nearest neighbor","Feature extraction;Recommender systems;Clustering algorithms;Domain analysis;Data mining;Algorithm design and analysis;Electronic mail;Nearest neighbor search;Clustering","data mining;groupware;Internet;learning (artificial intelligence);pattern classification;recommender systems;software engineering","enterprise-level collaborative software suite;feature recommendation algorithms;feature mining;k-nearest neighbor machine learning method;association rule mining;online product descriptions;feature extraction;incremental diffusive algorithm;data mining techniques;recommender system;domain analysis activity;software projects;software systems;labor-intensive task;online product listings","","36","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Asynchronous parallel simulation of parallel programs","S. Prakash; E. Deelman; R. Bagrodia","TIBCO Software Inc., Palo Alto, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","5","385","400","Parallel simulation of parallel programs for large datasets has been shown to offer significant reduction in the execution time of many discrete event models. The paper describes the design and implementation of MPI-SIM, a library for the execution driven parallel simulation of task and data parallel programs. MPI-SIM can be used to predict the performance of existing programs written using MPI for message passing, or written in UC, a data parallel language, compiled to use message passing. The simulation models can be executed sequentially or in parallel. Parallel execution of the models are synchronized using a set of asynchronous conservative protocols. The paper demonstrates how protocol performance is improved by the use of application-level, runtime analysis. The analysis targets the communication patterns of the application. We show the application-level analysis for message passing and data parallel languages. We present the validation and performance results for the simulator for a set of applications that include the NAS Parallel Benchmark suite. The application-level optimization described in the paper yielded significant performance improvements in the simulation of parallel programs, and in some cases completely eliminated the synchronizations in the parallel execution of the simulation model.","0098-5589;1939-3520;2326-3881","","10.1109/32.846297","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846297","","Discrete event simulation;Protocols;Frequency synchronization;Parallel languages;Runtime;Hardware;Program processors;Computational modeling;Libraries;Performance analysis","parallel programming;parallel languages;virtual machines;message passing;application program interfaces;synchronisation;discrete event simulation","asynchronous parallel simulation;parallel programs;large datasets;execution time;discrete event models;MPI-SIM;execution driven parallel simulation;data parallel programs;MPI;message passing;UC;data parallel language;simulation models;parallel execution;asynchronous conservative protocols;protocol performance;application-level runtime analysis;communication patterns;NAS Parallel Benchmark suite;application-level optimization;simulation model","","25","","30","","","","","","IEEE","IEEE Journals & Magazines"
"The Role of Ethnographic Studies in Empirical Software Engineering","H. Sharp; Y. Dittrich; C. R. B. de Souza","Open University, Walton Hall, Milton Keynes, UK; Software and Systems Section, IT University of Copenhagen, Rued Langgaards Vej 7, Copenhagen S, Denmark; Vale Institute of Technology and the Federal University of Pará, Tv. Boaventura da Silva, 955, Belém, PA, Brazil","IEEE Transactions on Software Engineering","","2016","42","8","786","804","Ethnography is a qualitative research method used to study people and cultures. It is largely adopted in disciplines outside software engineering, including different areas of computer science. Ethnography can provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice, i.e., it can help to uncover not only what practitioners do, but also why they do it. Despite its potential, ethnography has not been widely adopted by empirical software engineering researchers, and receives little attention in the related literature. The main goal of this paper is to explain how empirical software engineering researchers would benefit from adopting ethnography. This is achieved by explicating four roles that ethnography can play in furthering the goals of empirical software engineering: to strengthen investigations into the social and human aspects of software engineering; to inform the design of software engineering tools; to improve method and process development; and to inform research programmes. This article introduces ethnography, explains its origin, context, strengths and weaknesses, and presents a set of dimensions that position ethnography as a useful and usable approach to empirical software engineering research. Throughout the paper, relevant examples of ethnographic studies of software practice are used to illustrate the points being made.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2519887","CNPq; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7387744","Design tools and techniques;human factors in software design;software engineering process;computer-supported collaborative work","Software engineering;Software;Context;Sociology;Electronic mail;Computer science;Guidelines","cultural aspects;human factors;software process improvement","sociotechnological realities;software development practice;ethnography;empirical software engineering;human aspects;social aspects;software process development;software engineering tools","","11","","135","","","","","","IEEE","IEEE Journals & Magazines"
"Building reliable interactive information systems","A. I. Wasserman; P. A. Pircher; D. T. Shewmake","Section of Medical Information Science, University of California, San Francisco, San Francisco, CA 94143; Section of Medical Information Science, University of California, San Francisco, San Francisco, CA 94143; Section of Medical Information Science, University of California, San Francisco, San Francisco, CA 94143","IEEE Transactions on Software Engineering","","1986","SE-12","1","147","156","User software engineering (USE) is a methodology, with supporting tools, for the specification, design, and implementation of interactive information systems. With the USE approach, the user interface is formally specified with augmented state transition diagrams, and the operations may be formally specified with preconditions and postconditions. The USE state transition diagrams may be directly executed with the application development tool RAPID/USE. RAPID/USE and its associated tool RAPSUM create and analyze logging information that is useful for system testing, and for evaluation and modification of the user interface. The authors briefly describe the USE transition diagrams and the formal specification approach, and show how these tools and techniques aid in the creation of reliable interactive information systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312928","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312928","Interactive information systems;RAPID/USE;software development methodology;software reliability;transition diagrams;User Software Engineering","Libraries;Information systems;User interfaces;Testing;Databases;Software reliability","software engineering;software reliability;software tools;specification languages;user interfaces","user software engineering;reliable interactive information systems;software engineering;specification;design;implementation;interactive information systems;USE;user interface;state transition diagrams;application development tool;RAPID/USE;RAPSUM;logging information;system testing","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Simulation and comparison of Albrecht's function point and DeMarco's function bang metrics in a CASE environment","R. Rask; P. Laamanen; K. Lyyttinen","Dept. of Comput. Sci., Joensuu Univ., Finland; Dept. of Comput. Sci., Joensuu Univ., Finland; NA","IEEE Transactions on Software Engineering","","1993","19","7","661","671","Software size estimates provide a basis for software cost estimation during software development. Hence, it is important to measure the system size reliably as early as possible. Two of the best known specification level metrics, Albrecht's function points (A.J. Albrecht, 1979) and DeMarco's function bang metrics (T. DeMarco, 1982) are compared by a simulation study in which automatically generated randomized dataflow diagrams (DFDs) were used as a statistical sample to automatically count function points and function bang in a built CASE environment. These value counts were correlated statistically using correlation coefficients and regression analysis. The simulation study permits sufficient variation in the base material to cover most types of system specifications. Moreover, it allows sufficient sampling sizes to make statistical analysis of data. The obtained results show that in certain cases there is a relatively good statistical correlation between these metrics.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.238567","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=238567","","Computer aided software engineering;Cost function;Computer science;Programming;Size measurement;Regression analysis;Sampling methods;Statistical analysis;Software metrics;Software measurement","software cost estimation;software metrics;software tools","software size estimates;software cost estimation;software development;specification level metrics;function points;function bang metrics;simulation study;automatically generated randomized dataflow diagrams;DFDs;statistical sample;built CASE environment;system specifications;statistical correlation","","13","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Complexity of points-to analysis of Java in the presence of exceptions","R. Chatterjee; B. G. Ryder; W. A. Landi","Oracle Corp., Nashua, NH, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","6","481","512","At each program point, points-to analysis for statically typed object oriented programming languages (e.g., Java, C++) determines those objects to which a reference may refer (or a pointer may point) during execution. Points-to analysis is necessary for any semantics based software tools for object oriented systems. Our new complexity results for points-to analysis distinguish the difficulty of intraprocedural and interprocedural points-to analyses for languages with combinations of single-level types (i.e., types with data members only of primitive type), exceptions with or without subtyping, and dynamic dispatch. Our results include: 1) the first polynomial-time algorithm for points-to analysis in the presence of exceptions that handles a robust subset of Java without threads and can be applied to C++; 2) proof that the above algorithm is safe, in general, and provably precise on programs with single-level types and exceptions without subtyping, but not dynamic dispatch, thus, this case is in P; 3) proof that an interprocedural points-to analysis problem with single-level types and exceptions with subtyping, but without dynamic dispatch, is PSPACE-hard, while the intraprocedural problem is PSPACE-complete. Other complexity characterizations of points-to analysis in programs without exceptions are presented, including an algorithm with worst-case bound of O(n/sup 5/), which improves over the O(n/sup 7/) worst-case bound achievable from previous approaches of T. Reps et al. (1995) and W.A. Landi and B.G. Ryder (1991).","0098-5589;1939-3520;2326-3881","","10.1109/32.926173","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=926173","","Java;Algorithm design and analysis;Information analysis;Object oriented programming;Runtime;Software tools;Polynomials;Robustness;Object oriented modeling","Java;C++ language;program diagnostics;programming language semantics;exception handling;type theory;computational complexity","points-to analysis complexity;Java;exceptions;program point;statically typed object oriented programming languages;semantics based software tools;object oriented systems;complexity results;single-level types;data members;dynamic dispatch;polynomial-time algorithm;C++;interprocedural points-to analysis problem;PSPACE-hard;intraprocedural problem;PSPACE-complete;complexity characterizations;worst-case bound","","11","","76","","","","","","IEEE","IEEE Journals & Magazines"
"Synthesizing Modal Transition Systems from Triggered Scenarios","G. E. Sibay; V. Braberman; S. Uchitel; J. Kramer","Imperial College London, London; University of Buenos Aires, Buenos Aires; Imperial College London, London and University of Buenos Aires, Buenos Aires; Imperial College, London","IEEE Transactions on Software Engineering","","2013","39","7","975","1001","Synthesis of operational behavior models from scenario-based specifications has been extensively studied. The focus has been mainly on either existential or universal interpretations. One noteworthy exception is Live Sequence Charts (LSCs), which provides expressive constructs for conditional universal scenarios and some limited support for nonconditional existential scenarios. In this paper, we propose a scenario-based language that supports both existential and universal interpretations for conditional scenarios. Existing model synthesis techniques use traditional two-valued behavior models, such as Labeled Transition Systems. These are not sufficiently expressive to accommodate specification languages with both existential and universal scenarios. We therefore shift the target of synthesis to Modal Transition Systems (MTS), an extension of labeled Transition Systems that can distinguish between required, unknown, and proscribed behavior to capture the semantics of existential and universal scenarios. Modal Transition Systems support elaboration of behavior models through refinement, which complements an incremental elicitation process suitable for specifying behavior with scenario-based notations. The synthesis algorithm that we define constructs a Modal Transition System that uses refinement to characterize all the Labeled Transition Systems models that satisfy a mixed, conditional existential and universal scenario-based specification. We show how this combination of scenario language, synthesis, and Modal Transition Systems supports behavior model elaboration.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.62","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311408","Scenarios;MTS;synthesis;partial behavior models","Semantics;Analytical models;Online banking;Merging;Unified modeling language;Indexes;Cognition","formal verification;reasoning about programs","modal transition systems;triggered scenarios;operational behavior models;scenario-based specifications;live sequence charts;LSC;conditional universal scenarios;nonconditional existential scenarios;scenario-based language;model synthesis techniques;two-valued behavior models;specification languages;MTS;incremental elicitation process;scenario-based notations;conditional existential scenario-based specification;universal scenario-based specification;labeled transition system models;behavior model elaboration","","7","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Layout appropriateness: a metric for evaluating user interface widget layout","A. Sears","Dept. of Computer Sci., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1993","19","7","707","719","Numerous methods for evaluating user interfaces have been investigated to develop a metric that incorporates simple task descriptions which can assist designers in organizing their user interface. The metric, Layout Appropriateness (LA), requires a description of the sequences of actions users perform and how frequently each sequence is used. This task description can either be from observations of an existing system or from a simplified task analysis. The appropriateness of a given layout is computed by weighting the cost of each sequence of actions by how frequently the sequence is performed, which emphasizes frequent methods of accomplishing tasks while incorporating less frequent methods in the design. In addition to providing a comparison of proposed or existing layouts, an LA-optimal layout can be presented to the designer. The designer can compare the LA-optimal and existing layouts or start with the LA-optimal layout and modify it to take additional factors into consideration.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.238571","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=238571","","User interfaces;Performance analysis;Design engineering;Computer science;Performance evaluation;Organizing;Design methodology;NASA;Cost function;Data mining","human factors;performance evaluation;software metrics;user interfaces","layout appropriateness;user interface widget layout;simple task descriptions;Layout Appropriateness;simplified task analysis;weighting;LA-optimal layout","","37","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Finding Atomicity-Violation Bugs through Unserializable Interleaving Testing","S. Lu; S. Park; Y. Zhou","University of Wisconsin-Madison, Madison; University of California, San Diego, La Jolla; University of California, San Diego, La Jolla","IEEE Transactions on Software Engineering","","2012","38","4","844","860","Multicore hardware is making concurrent programs pervasive. Unfortunately, concurrent programs are prone to bugs. Among different types of concurrency bugs, atomicity violations are common and important. How to test the interleaving space and expose atomicity-violation bugs is an open problem. This paper makes three contributions. First, it designs and evaluates a hierarchy of four interleaving coverage criteria using 105 real-world concurrency bugs. This study finds a coverage criterion (Unserializable Interleaving Coverage) that balances the complexity and the capability of exposing atomicity-violation bugs well. Second, it studies stress testing to understand why this common practice cannot effectively expose atomicity-violation bugs from the perspective of unserializable interleaving coverage. Third, it designs CTrigger following the unserializable interleaving coverage criterion. CTrigger uses trace analysis to identify feasible unserializable interleavings, and then exercises low-probability interleavings to expose atomicity-violation bugs. We evaluate CTrigger with real-world atomicity-violation bugs from seven applications. CTrigger efficiently exposes these bugs within 1-235 seconds, two to four orders of magnitude faster than stress testing. Without CTrigger, some of these bugs do not manifest even after seven days of stress testing. Furthermore, once a bug is exposed, CTrigger can reliably reproduce it, usually within 5 seconds, for diagnosis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5740930","Testing and debugging;debugging aids;diagnostics;testing strategies;test coverage of code;concurrent programming;bug characteristics","Computer bugs;Testing;Instruction sets;Concurrent computing;Stress;Complexity theory;Synchronization","formal verification;multiprocessing systems;probability;program debugging;ubiquitous computing","finding atomicity violation bugs;unserializable interleaving testing;multicore hardware;pervasive concurrent programs;concurrency bugs;unserializable interleaving coverage;trace analysis;low probability interleavings","","12","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Requirements Engineering for Safety-Critical Systems: An Interview Study with Industry Practitioners","L. E. G. Martins; T. Gorschek","Department of Science and Technology, Federal University of Sao Paulo, Sao Jose dos Campos, Sao Paulo Brazil (e-mail: martinsleg@hotmail.com); Department of Systems and Software Engineering, Blekinge Institute of Technology, Karlskrona, Blekinge Sweden SE-371 79 (e-mail: tony.gorschek@bth.se)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","We have conducted in-depth interviews with experienced practitioners in the Safety-Critical Systems (SCS) domain in order to investigate several aspects related to requirements specification and safety analysis for SCS. We interviewed 19 practitioners from eleven SCS companies in different domains with the intention of verifying which approaches they use day-today, and what their perceptions are in relation to the approaches used to elicit, analyze, specify and validate safety requirements. The aim of this study is to obtain an in-depth understanding of how requirements engineering is carried out in companies that develop SCS.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2854716","CNPq Conselho Nacional de Desenvolvimento Cientifico e Tecnologico; The Knowledge Foundation in Sweden; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8409284","Requirements;Specification;Software and System Safety;Requirements Engineering;Safety Critical Systems;Software Engineering;SCS","Safety;Companies;Requirements engineering;Software;Certification;Interviews;Unified modeling language","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"The ""Software Engineering"" of Expert Systems: Is Prolog Appropriate?","P. A. Subrahmanyam","AT&amp;T Bell Laboratories","IEEE Transactions on Software Engineering","","1985","SE-11","11","1391","1400","This paper is a preliminary assessment of the viability of Prolog as a basis for the design of expert systems, where the major competition is assumed to be from Lisp and Lisp-based systems. We critically examine the basic features of Prolog from various perspectives to see to what extent they support (or hinder) expert system development. Our conclusion is that while Prolog has significant assets along several dimensions, Prolog as it exists today needs to be modified and appropriately enhanced to make it competitive to extant Lisp-based systems; we suggest the nature of some of these modifications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231887","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701955","Expert systems;Lisp;logic programming;programming environments;Prolog","Software engineering;Expert systems;Logic programming;Programming environments;Prototypes;Subspace constraints;Computational modeling;Hardware;Costs;Sections","","Expert systems;Lisp;logic programming;programming environments;Prolog","","19","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Developing a Single Model and Test Prioritization Strategies for Event-Driven Software","R. C. Bryce; S. Sampath; A. M. Memon","Utah State University, Logan; University of Maryland, Baltimore; University of Maryland, College Park","IEEE Transactions on Software Engineering","","2011","37","1","48","64","Event-Driven Software (EDS) can change state based on incoming events; common examples are GUI and Web applications. These EDSs pose a challenge to testing because there are a large number of possible event sequences that users can invoke through a user interface. While valuable contributions have been made for testing these two subclasses of EDS, such efforts have been disjoint. This work provides the first single model that is generic enough to study GUI and Web applications together. In this paper, we use the model to define generic prioritization criteria that are applicable to both GUI and Web applications. Our ultimate goal is to evolve the model and use it to develop a unified theory of how all EDS should be tested. An empirical study reveals that the GUI and Web-based applications, when recast using the new model, show similar behavior. For example, a criterion that gives priority to all pairs of event interactions did well for GUI and Web applications; another criterion that gives priority to the smallest number of parameter value settings did poorly for both. These results reinforce our belief that these two subclasses of applications should be modeled and studied together.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401169","Combinatorial interaction testing;covering arrays;event-driven software (EDS);t-way interaction coverage;test suite prioritization;user-session testing;Web application testing;GUI testing.","Software testing;Graphical user interfaces;Application software;Computer science;User interfaces;Protocols;Embedded software;Information systems;Educational institutions;Abstracts","graphical user interfaces;Internet;program testing;service-oriented architecture","event-driven software;test prioritization strategy;EDS;GUI testing;Web application testing;graphical user interface","","63","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Forecasting Risk Impact on ERP Maintenance with Augmented Fuzzy Cognitive Maps","J. L. Salmeron; C. Lopez","University Pablo de Olavide, Seville; Pablo de Olavide University, Seville","IEEE Transactions on Software Engineering","","2012","38","2","439","452","Worldwide, firms have made great efforts to implement Enterprise Resource Planning (ERP) systems. Despite these efforts, ERP adoption success is not guaranteed. Successful adoption of an ERP system also depends on proper system maintenance. For this reason, companies should follow a maintenance strategy that drives the ERP system toward success. However, in general, ERP maintenance managers do not know what conditions they should target to successfully maintain their ERP systems. Furthermore, numerous risks threaten these projects, but they are normally dealt with intuitively. To date, there has been limited literature published regarding ERP maintenance risks or ERP maintenance success. To address this need, we have built a dynamic simulation tool that allows ERP managers to foresee the impact of risks on maintenance goals. This research would help professionals manage their ERP maintenance projects. Moreover, it covers a significant gap in the literature.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680917","ERP;fuzzy cognitive maps;risk management;simulation;software maintenance.","Decision support systems","cognition;enterprise resource planning;forecasting theory;fuzzy set theory;project management;risk analysis;software maintenance;software management","enterprise resource planning system;ERP adoption success;ERP system maintenance;ERP maintenance risks;dynamic simulation tool;ERP maintenance project management;augmented fuzzy cognitive maps;risk impact forecasting","","37","","111","","","","","","IEEE","IEEE Journals & Magazines"
"An Approach to User Specification of Interactive Display Interfaces","L. J. Bass","Department of Computer Science and Statistics, University of Rhode Island","IEEE Transactions on Software Engineering","","1985","SE-11","8","686","698","Forms have become widely used as a user interface for database systems. By analyzing the components of forms, a unified treatment of aggregation operators, headings, subheadings, and internal logic of a form is possible. In this paper we present a theory and a system based on that theory which allows users to easily specify displays. Displays are composed of components and the user can specify geometry of the components and grouping relationships between the components which allow for the generation of the desired display.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232518","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702078","Display system;form managers;report writer;screen management;user dialog;user interface","Database systems;Logic;User interfaces;Computer displays;Writing;Geometry;Computer peripherals;Data analysis;Computer science;Statistics","","Display system;form managers;report writer;screen management;user dialog;user interface","","6","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Design Rule Spaces: A New Model for Representing and Analyzing Software Architecture","Y. Cai; L. Xiao; R. Kazman; R. Mo; Q. Feng","Computer Science, Drexel University, philadelphia, Pennsylvania United States 19104 (e-mail: yfcai@cs.drexel.edu); School of Systems and Enterprises, Stevens Institute of Technology, 33694 Hoboken, New Jersey United States 07030 (e-mail: lxiao6@stevens.edu); Department of Information Technology Management, University of Hawaii, Honolulu, Hawaii United States 96822 (e-mail: kazman@hawaii.edu); Computer Science, Drexel University, philadelphia, Pennsylvania United States (e-mail: rm859@drexel.edu); Computer Science, Drexel University, philadelphia, Pennsylvania United States (e-mail: qf28@drexel.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","In this paper, we propose an architecture model called Design Rule Space (DRSpace). We model the architecture of a software system as multiple overlapping DRSpaces, reflecting the fact that any complex software system must contain multiple aspects, features, patterns, etc. We show that this model provides new ways to analyze software quality. In particular, we introduce an Architecture Root detection algorithm that captures DRSpaces containing large numbers of a project&#x0027;s bug-prone files, which are called Architecture Roots (ArchRoots). After investigating ArchRoots calculated from 15 open source projects, the following observations become clear: from 35% to 91% of a project&#x0027;s most bug-prone files can be captured by just 5 ArchRoots, meaning that bug-prone files are likely to be architecturally connected. Furthermore, these ArchRoots tend to live in the system for significant periods of time, serving as the major source of bug-proneness and high maintainability costs. Moreover, each ArchRoot reveals multiple architectural flaws that propagate bugs among files and this will incur high maintenance costs over time. The implication of our study is that the quality, in terms of bug-proneness, of a large, complex software project cannot be fundamentally improved without first fixing its architectural flaws.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2797899","Division of Computing and Communication Foundations; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8268667","Software architecture;reverse-engineering;defect prediction;technical debt;code smells;bug localization","Computer bugs;Computer architecture;Software architecture;Production facilities;Analytical models;Software systems","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Transformation and Verification of Office Procedures","Shi-Kuo Chang; Wu-Lung Chan","Information Systems Laboratory, Department of Electrical and Computer Engineering, Illinois Institute of Technology; NA","IEEE Transactions on Software Engineering","","1985","SE-11","8","724","734","An office procedure is a structured set of office activities for accomplishing a specific office task. A unified model, called office procedure model (OPM), is presented to model office procedures. The OPM describes the relationships among messages, databases, alerters, and activities. The OPM can be used to coordinate and integrate the activities of an office procedure. The OPM also allows the specification of office protocols in an office information system. A methodology for the verification of office procedures is presented. With this methodology, potential problems in office procedure specification, such as deadlock, unspecified message reception, etc., can be analyzed effectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232522","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702082","Message exchange theory;office automation;office information system;office procedure model;Petri net;protocol analysis","Office automation;Protocols;Information systems;Databases;Information retrieval;Costs;Management information systems;System recovery;Information analysis;Computer science","","Message exchange theory;office automation;office information system;office procedure model;Petri net;protocol analysis","","3","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Support for managing design-time decisions","A. Egyed; D. S. Wile","Teknowledge Corp., Marina del Rey, CA, USA; Teknowledge Corp., Marina del Rey, CA, USA","IEEE Transactions on Software Engineering","","2006","32","5","299","314","The desirability of maintaining multiple stakeholders' interests during the software design process argues for leaving choices undecided as long as possible. Yet, any form of underspecification, either missing information or undecided choices, must be resolved before automated analysis tools can be used. This paper demonstrates how constraint satisfaction problem solution techniques (CSTs) can be used to automatically reduce the space of choices for ambiguities by incorporating the local effects of constraints, ultimately with more global consequences. As constraints typical of those encountered during the software design process, we use UML consistency and well-formedness rules. It is somewhat surprising that CSTs are suitable for the software modeling domain since the constraints may relate many ambiguities during their evaluation, encountering a well-known problem with CSTs called the k-consistency problem. This paper demonstrates that our CST-based approach is computationally scalable and effective-as evidenced by empirical experiments based on dozens of industrial models","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.48","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642678","UML;design choices;consistency checking;design alternatives;choice elimination.","Unified modeling language;Software design;Programming;Predictive models;Information analysis;Computer industry;Process design;Software algorithms;Application software","constraint handling;constraint theory;decision making;formal specification;software maintenance;software management;Unified Modeling Language","design-time decisions;software design process;automated analysis tools;constraint satisfaction problem solution techniques;UML;software modeling;k-consistency problem","","10","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Performance characterization of quorum-consensus algorithms for replicated data","M. Ahamad; M. H. Ammar","Sch. of Inf. & Comput. Sci., Georgia Inst. of Technol., Atlanta, GA, USA; Sch. of Inf. & Comput. Sci., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Transactions on Software Engineering","","1989","15","4","492","496","The authors develop a model and define performance measures for a replicated data system that makes use of a quorum-consensus algorithm to maintain consistency. They consider two measures: the proportion of successfully completed transactions in systems where a transaction aborts if data is not available, and the mean response time in systems where a transaction waits until data becomes available. Based on the model, the authors show that for some quorum assignment there is an optimal degree of replication beyond which performance degrades. There exist other quorum assignments which have no optimal degree of replication. The authors also derive optimal read and write quorums which maximize the proportion of successful transactions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.16608","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=16608","","Voting;Availability;Delay;Data systems;Time measurement;Degradation;Database systems;Hardware;Application software;Industrial control","concurrency control;distributed databases;performance evaluation;transaction processing","quorum-consensus algorithms;replicated data;performance measures;quorum-consensus algorithm;consistency;successfully completed transactions;transaction waits;quorum assignment","","66","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Accessing files in an Internet: the Jade file system","H. C. Rao; L. L. Peterson","AT&T Bell Lab., Murray Hill, NJ, USA; NA","IEEE Transactions on Software Engineering","","1993","19","6","613","624","The Jade file system, which provides a uniform way to name and access files in an Internet environment, is introduced. Jade is a logical system that integrates a heterogeneous collection of existing file systems in which underlying file systems support different file access protocols. Because of autonomy, Jade is designed under the restriction that the underlying file systems may not be modified. In order to avoid the complexity of maintaining an Internet-wide, global name space, Jade permits each user to define a private name space. Jade's name space supports two features: it allows multiple file systems to be mounted under one directory, and it permits one logical name space to mount other logical name spaces. A prototype of Jade has been implemented to examine and validate its design. The prototype consists of interfaces to the Unix File System, the Sun Network File System, and the File Transfer Protocol. An overview of Jade's design is reported, and the authors' experiences in designing and implementing a large scale file system are reviewed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232026","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232026","","Internet;File systems;Access protocols;Prototypes;Computer networks;Scalability;File servers;Sun;Large-scale systems;Availability","distributed processing;file organisation;information services","Jade file system;Internet;file access protocols;global name space;multiple file systems;logical name space;Unix File System;Sun Network File System;File Transfer Protocol;large scale file system","","16","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Steps to an Advanced Ada<sup>1</sup>Programming Environment","R. N. Taylor; T. A. Standish","Programming Environment Project, Department of Information and Computer Science, University of California; NA","IEEE Transactions on Software Engineering","","1985","SE-11","3","302","310","Conceptual simplicity, tight coupling of tools, and effective support of host-target software development will characterize advanced Ada programming support environments. Several important principles have been demonstrated in the Arcturus system, including template-assisted Ada editing, command completion using Ada as a command language, and combining the advantages of interpretation and compilation. Other principles, relating to analysis, testing, and debugging of concurrent Ada programs, have appeared in other contexts. This paper discusses several of these topics, considers how they can be integrated, and argues for their inclusion in an environment appropriate for software development in the late 1980's.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232213","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702006","Ada;concurrency;debugging;programming environments;static analysis","Programming environments;Debugging;Prototypes;Software maintenance;Testing;Software prototyping;Software tools;Page description languages;Command languages;Concurrent computing","","Ada;concurrency;debugging;programming environments;static analysis","","11","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Performance evaluation of parallel systems by using unbounded generalized stochastic Petri nets","M. Granda; J. M. Drake; J. A. Gregorio","Dept. of Electron., Cantabria Univ., Santander, Spain; Dept. of Electron., Cantabria Univ., Santander, Spain; Dept. of Electron., Cantabria Univ., Santander, Spain","IEEE Transactions on Software Engineering","","1992","18","1","55","71","Methods of calculating efficiently the performance measures of parallel systems by using unbounded generalized stochastic Petri nets are presented. An explosion in the number of states to be analyzed occurs when unbounded places appear in the model. The state space of such nets is infinite, but it is possible to take advantage of the natural symmetries of the system to aggregate the states of the net and construct a finite graph of lumped states which can easily be analyzed. With the methods developed, the unbounded places introduce a complexity similar to that of safe places of the net. These methods can be used to evaluate models of open parallel systems in which unbounded places appear; systems which are k-bounded but are complex and have large values of k can also be evaluated in an appropriate way. From the steady-state solution of the model, it is possible to obtain automatically the performance measures of parallel systems represented by this type of net.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.120316","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=120316","","Stochastic systems;Petri nets;Performance analysis;State-space methods;Steady-state;Parallel processing;Markov processes;Explosions;Aggregates;Time measurement","parallel machines;parallel programming;performance evaluation;Petri nets;stochastic processes","performance measures;parallel systems;unbounded generalized stochastic Petri nets;unbounded places;state space;natural symmetries;finite graph;lumped states;unbounded places;open parallel systems;k-bounded;steady-state solution","","4","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Need for Sleep: the Impact of a Night of Sleep Deprivation on Novice Developers&#x0027; Performance","D. Fucci; G. Scanniello; S. Romano; N. Juristo","HITeC, University of Hamburg, Hamburg, - Germany (e-mail: fucci@informatik.uni-hamburg.de); Dipartimento di Matematica e Informatica, Universit&#x00E0; della Basilicata, Potenza, Italy Italy 85100 (e-mail: gscanniello@unisa.it); DiMIE, Universita degli Studi della Basilicata, 19006 Potenza, Basilicata Italy (e-mail: simone.romano@unibas.it); Facultad de Informatica, Universidad Politecnica de Madrid, Boadilla del Monte, Madrid Spain 28660 (e-mail: natalia@fi.upm.es)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","We present a quasiexperiment to investigate whether, and to what extent, sleep deprivation impacts performance of novice developers using the agile practice of test-first development (TFD). We recruited 45 undergraduates, and asked them to tackle a programming task. Among participants, 23 agreed to stay awake the night before carrying out the task, while 22 slept normally. We analyzed the quality of the implementations delivered by the participants in both groups, their engagement in writing source code and ability to apply TFD. By comparing the two groups of participants, we found that a single night of sleep deprivation leads to a reduction of 50% in the quality of the implementations. There is notable evidence that the developers&#x0027; engagement and their prowess to apply TFD arenegatively impacted. Our results also show that sleep-deprived developers make more fixes to syntactic mistakes in the sourcecode. We conclude that sleep deprivation has possibly disruptive effects on development activities. Results open opportunities for improving developers&#x0027; performance by integrating the study of sleep with other psycho-physiological factors in which the software engineering research community has recently taken an interest in.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2834900","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8357494","sleep deprivation;psycho-physiological factors;test-first development","Sleep;Software;Task analysis;Biomedical monitoring;Software engineering;Programming;Functional magnetic resonance imaging","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"The Cactis project: database support for software environments","S. E. Hudson; R. King","Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA; NA","IEEE Transactions on Software Engineering","","1988","14","6","709","719","The Cactis project is an on-going effort oriented toward extending database support from traditional business-oriented applications to software environments. The main goals of the project are to construct an appropriate model, and develop new techniques to support the unusual data management needs of software environments, including program compilations, software configurations, load modules, project schedules, software versions, nested and long transactions, and program transformations. The ability to manage derived information is common to many of these data needs, and the Cactis database management system has the ability to represent and maintain derived data in a time- and space-efficient fashion. A central contribution of Cactis is its integration of the type constructors of semantic models and the localized behavior capabilities of object-oriented database management systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6152","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6152","","Database systems;Object oriented databases;Application software;Data models;Environmental management;Object oriented modeling;Software development management;Information management;Computer science;Project management","database management systems;program compilers;programming environments","database support;software environments;Cactis;program compilations;software configurations;load modules;project schedules;software versions;program transformations;database management system;semantic models;object-oriented","","28","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal partitioning of random programs across two processors","D. M. Nicol","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA","IEEE Transactions on Software Engineering","","1989","15","2","134","141","B. Indurkhya et al. (1986) concluded that the optimal partitioning of a homogeneous random program over a homogeneous distributed system either assigns all modules to a single processor or distributes the modules as evenly as possible among all processors. Their analysis rests heavily on the approximation that equates the expected maximum of a set of independent random variables with the set's maximum expectation. The author strengthens this result by providing an approximation-free proof of this result for two processors under general conditions on the module execution time distribution. It is found that additional rigor leads to a different characterization of the optimality points. The author also shows that under a rigorous analysis one is led to different conclusions in the general P-processor case than those reached using B. Indurkhya et al.'s approximation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21740","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21740","","Cost function;Error analysis;Random variables;NASA;Parallel processing;Delay;Time measurement;Postal services;Computer science;Statistical analysis","distributed processing;parallel programming;programming theory","random programs;optimal partitioning;homogeneous distributed system;expected maximum;maximum expectation;approximation-free proof;module execution time distribution","","17","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Two-state self-stabilizing algorithms for token rings","M. Flatebo; A. K. Datta","Dept. of Comput. Sci., Nevada Univ., Las Vegas, NV, USA; Dept. of Comput. Sci., Nevada Univ., Las Vegas, NV, USA","IEEE Transactions on Software Engineering","","1994","20","6","500","504","A self-stabilizing system is a network of processors, which, when started from an arbitrary (and possibly illegal) initial state, always returns to a legal state in a finite number of steps. This implies that the system can automatically deal with infrequent errors. One issue in designing self-stabilizing algorithms is the number of states required by each machine. This paper presents mutual exclusion algorithms which will be self-stabilizing while only requiring each machine in the network to have two states. The concept of a randomized central demon is also introduced in this paper. The first algorithm is a starting point where no randomization is needed (the randomized central demon is not necessary). The other two algorithms require randomization. The second algorithm builds on the first algorithm and reduces the number of network connections required. Finally, the number of necessary connections is again reduced yielding the final two-state, probabilistic algorithm for an asynchronous, unidirectional ring of processes.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.295897","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=295897","","Law;Legal factors;Algorithm design and analysis;Distributed algorithms;Propagation delay;Error correction;Automata;Computer science;System recovery","distributed algorithms;token networks;local area networks;probability;fault tolerant computing;reliability","two-state self-stabilizing algorithms;token rings;legal state;illegal state;infrequent errors;mutual exclusion algorithms;randomized central demon;network connections;probabilistic algorithm;asynchronous unidirectional ring;binary state machines;distributed algorithms;distributed system","","19","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting cooperation in the SPADE-1 environment","S. Bandinelli; E. Di Nitto; A. Fuggetta","Eur. Software Inst., Bizkaia, Spain; NA; NA","IEEE Transactions on Software Engineering","","1996","22","12","841","865","Software development is a cooperative activity that relies heavily on the quality and effectiveness of the communication channels established within the development team and with the end-user. Process-centered software engineering environments (PSEEs) support the definition and the execution of various phases of the software process. This is achieved by explicitly defining cooperation procedures, and by supporting synchronization and data sharing among its users. PSEE and CSCW technologies have been developed rather independently from each other, leading to a large amount of research results, tools and environments, and practical experiences. We have reached a stage in technology development where it is necessary to assess and evaluate the effectiveness of the research efforts carried out so far. Moreover, it is important to understand how to integrate and exploit the results of these different efforts. The goal of the paper is to understand which kind of basic functionalities PSEEs can and should offer, and how these environments can be integrated with other tools to effectively support cooperation in software development. In particular, the paper introduces a process model we have built to support a cooperative activity related to anomaly management in an industrial software factory. The core of the paper presents and discusses the experiences and results that we have derived from this modeling activity, and how they related to the general problem of supporting cooperation in software development. The project was carried out using the SPADE (Software Process Analysis, Design and Enactment) PSEE and the ImagineDesk CSCW toolkit.","0098-5589;1939-3520;2326-3881","","10.1109/32.553634","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=553634","","Programming;Computer Society;Software engineering;Collaborative work;Communication channels;Research initiatives;Computer industry;Production facilities;Application software","groupware;computer aided software engineering;project support environments;synchronisation","computer-supported cooperative work;SPADE-1 environment;communication channels;process-centered software engineering environments;software process;cooperation procedures;synchronization;data sharing;technology development;research efforts;basic functionalities;software development;process model;cooperative activity;anomaly management;industrial software factory;ImagineDesk CSCW toolkit","","70","","53","","","","","","IEEE","IEEE Journals & Magazines"
"PARLOG and its applications","K. L. Clark","Dept. of Comput., Imperial Coll., London, UK","IEEE Transactions on Software Engineering","","1988","14","12","1792","1804","The key concepts of the parallel logic programming language PARLOG are introduced by comparing the language with Prolog. Some familiarity with Prolog and with the concepts of logic programming is assumed. Two major application areas of PARLOG, systems programming and object-oriented programming, are illustrated. Other applications are briefly surveyed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9064","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9064","","Logic programming;Testing;Object oriented programming;Parallel programming","high level languages;logic programming;object-oriented programming;parallel programming;PROLOG","PARLOG;parallel logic programming language;PARLOG;Prolog;systems programming;object-oriented programming","","7","","48","","","","","","IEEE","IEEE Journals & Magazines"
"A style-aware architectural middleware for resource-constrained, distributed systems","S. Malek; M. Mikic-Rakic; N. Medvidovic","Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","3","256","272","A recent emergence of small, resource-constrained, and highly mobile computing platforms presents numerous new challenges for software developers. We refer to development in this new setting as programming-in-the-small-and-many (Prism). This paper provides a description and evaluation of Prism-MW, a middleware platform intended to support software architecture-based development in the Prism setting. Prism-MW provides efficient and scalable implementation-level support for the key aspects of Prism application architectures, including their architectural styles. Additionally, Prism-MW is extensible to support different application requirements suitable for the Prism setting. Prism-MW has been applied in a number of applications and used as an educational tool in graduate-level software architecture and embedded systems courses. Recently, Prism-MW has been successfully evaluated by a major industrial organization for use in one of their key distributed embedded systems. Our experience with the middleware indicates that the principles of architecture-based software development can be successfully, and flexibly, applied in the Prism setting.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423996","Index Terms- Software architecture;architectural style;middleware;Prism-MW.","Middleware;Computer architecture;Software architecture;Application software;Software systems;Guidelines;Mobile computing;Embedded system;Software engineering;Connectors","software architecture;middleware","mobile computing;Prism-middleware platform;software architecture;distributed embedded systems;software development;resource constraints","","51","","42","","","","","","IEEE","IEEE Journals & Magazines"
"System Test Planning of Software: An Optimization Approach","K. Chari; A. Hevner","Department of Information Systems and Decision Sciences, University of South Florida, 4202 East Fowler Avenue, CIS 1040, Tampa, FL 33620-7800; Department of Information Systems and Decision Sciences, University of South Florida, 4202 East Fowler Avenue, CIS 1040, Tampa, FL 33620-7800","IEEE Transactions on Software Engineering","","2006","32","7","503","5099","This paper extends an exponential reliability growth model to determine the optimal number of test cases to be executed for various use case scenarios during the system testing of software. An example demonstrates a practical application of the optimization model for system test planning","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677535","Nonlinear programming;reliability;test management.","Software testing;System testing;Software reliability;Software systems;Cost function;Software quality;Application software;Quality control;Software maintenance;Law","optimisation;program testing;software reliability","exponential reliability growth model;test case;use case scenario;system test planning;optimization;nonlinear programming;software testing","","7","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Work Item Tagging: Communicating Concerns in Collaborative Software Development","C. Treude; M. Storey","University of Victoria, Victoria; University of Victoria, Victoria","IEEE Transactions on Software Engineering","","2012","38","1","19","34","In collaborative software development projects, work items are used as a mechanism to coordinate tasks and track shared development work. In this paper, we explore how “tagging,” a lightweight social computing mechanism, is used to communicate matters of concern in the management of development tasks. We present the results from two empirical studies over 36 and 12 months, respectively, on how tagging has been adopted and what role it plays in the development processes of several professional development projects with more than 1,000 developers in total. Our research shows that the tagging mechanism was eagerly adopted by the teams, and that it has become a significant part of many informal processes. Different kinds of tags are used by various stakeholders to categorize and organize work items. The tags are used to support finding of tasks, articulation work, and information exchange. Implicit and explicit mechanisms have evolved to manage the tag vocabulary. Our findings indicate that lightweight informal tool support, prevalent in the social computing domain, may play an important role in improving team-based software development practices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5611552","Tagging;collaboration;software development;task management;articulation work;work items.","Tagging;Programming;Collaboration;Software engineering;Data mining","groupware;information retrieval;software development management","work item tagging;collaborative software development project;social computing mechanism;tag vocabulary;lightweight informal tool support;team-based software development practice","","23","","44","","","","","","IEEE","IEEE Journals & Magazines"
"On the specification and synthesis of communicating processes","M. H. Erdogmus; R. Johnston","INRS-Telecommun., Quebec Univ., Verdun, Que., Canada; INRS-Telecommun., Quebec Univ., Verdun, Que., Canada","IEEE Transactions on Software Engineering","","1990","16","12","1412","1426","An objective methodology for the specification and synthesis of communicating processes is presented. It is demonstrated that algebraic operators can be used to formulate communicating processes in terms of behavioral constraints and that the corresponding state-machine-type process descriptions can be derived automatically or synthesized from these formulations. The behavioral constraints serve as high-level specifications for communicating processes. These constraints indicate the desired behavior of a process, possibly embedded in a system, by defining its range. The proposed approach is shown to be applicable to a common problem which concerns the synthesis of the central module serving a number of clients in a specific distributed system configuration.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.62449","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=62449","","Formal specifications;Protocols;Business;High level languages;Context modeling","automatic programming;formal specification;parallel programming","objective methodology;communicating processes;algebraic operators;behavioral constraints;state-machine-type process descriptions;behavioral constraints;high-level specifications;common problem;central module;specific distributed system configuration","","2","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Orthogonal defect classification-a concept for in-process measurements","R. Chillarege; I. S. Bhandari; J. K. Chaar; M. J. Halliday; D. S. Moebus; B. K. Ray; M. -. Wong","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1992","18","11","943","956","Orthogonal defect classification (ODC), a concept that enables in-process feedback to software developers by extracting signatures on the development process from defects, is described. The ideas are evolved from an earlier finding that demonstrates the use of semantic information from defects to extract cause-effect relationships in the development process. This finding is leveraged to develop a systematic framework for building measurement and analysis methods. The authors define ODC and discuss the necessary and sufficient conditions required to provide feedback to a developer; illustrate the use of the defect type distribution to measure the progress of a product through a process; illustrate the use of the defect trigger distribution to evaluate the effectiveness and eventually the completeness of verification processes such as inspection or testing; provides sample results from pilot projects using ODC; and open the doors to a wide variety of analysis techniques for providing effective and fast feedback based on the concepts of ODC.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.177364","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=177364","","Feedback;Software quality;Software measurement;Area measurement;Data mining;Predictive models;Sufficient conditions;Inspection;Testing;Computer industry","software quality;software reliability","orthogonal defect classification;software development;in-process measurements;feedback;semantic information;cause-effect relationships;measurement and analysis methods;necessary and sufficient conditions;defect trigger distribution;completeness;verification processes;inspection;testing","","358","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A Comprehensive Model for the Design of Distributed Computer Systems","H. K. Jain","School of Business Administration, the University of Wisconsin","IEEE Transactions on Software Engineering","","1987","SE-13","10","1092","1104","The availability of micro-, mini-, and supercomputers has complicated the laws governing the economies of scale in computers. A recent study by Ein-Dor [7] concludes that it is most effective to accomplish any task on the least powerful type of computer capable of performing it. This change in cost/performance, and the promise of increased reliability, modularity, and better response time has resulted in an increased tendency to decentralize and distribute computing power. But some economic factors, such as the communication expenses incurred and increased storage with distributed systems are working against the tendency to decentralize. It is clear that in many instances the optimal solution will be an integration of computers of varying power.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232851","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702149","Distributed data management;distributed system;file allocation;file availability;goal programming;processing cost;software path length","Distributed computing;Power system modeling;Availability;Supercomputers;Economies of scale;Costs;Power system reliability;Delay;Power system economics;Power generation economics","","Distributed data management;distributed system;file allocation;file availability;goal programming;processing cost;software path length","","14","","27","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Classifier Configuration and Classifier Combination on Bug Localization","S. W. Thomas; M. Nagappan; D. Blostein; A. E. Hassan","Queen's University, Kingston; Queen's University, Kingston; Queen's University, Kingston; Queen's University, Kingston","IEEE Transactions on Software Engineering","","2013","39","10","1427","1443","Bug localization is the task of determining which source code entities are relevant to a bug report. Manual bug localization is labor intensive since developers must consider thousands of source code entities. Current research builds bug localization classifiers, based on information retrieval models, to locate entities that are textually similar to the bug report. Current research, however, does not consider the effect of classifier configuration, i.e., all the parameter values that specify the behavior of a classifier. As such, the effect of each parameter or which parameter values lead to the best performance is unknown. In this paper, we empirically investigate the effectiveness of a large space of classifier configurations, 3,172 in total. Further, we introduce a framework for combining the results of multiple classifier configurations since classifier combination has shown promise in other domains. Through a detailed case study on over 8,000 bug reports from three large-scale projects, we make two main contributions. First, we show that the parameters of a classifier have a significant impact on its performance. Second, we show that combining multiple classifiers--whether those classifiers are hand-picked or randomly chosen relative to intelligently defined subspaces of classifiers--improves the performance of even the best individual classifiers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6520844","Software maintenance;bug localization;information retrieval;VSM;LSI;LDA;classifier combination","Large scale integration;Measurement;Vectors;Information retrieval;Matrix decomposition;Indexes;Resource management","information retrieval;pattern classification;program debugging","classifier configuration;classifier combination;source code entity determination;bug report;bug localization classifiers;information retrieval models;parameter value;classifier parameter","","31","","62","","","","","","IEEE","IEEE Journals & Magazines"
"The IC* model of parallel computation and programming environment","E. J. Cameron; D. M. Cohen; B. Gopinath; W. M. Keese; L. Ness; P. Uppaluru; J. R. Vollaro","Bell Commun. Res., Morristown, NJ, USA; Bell Commun. Res., Morristown, NJ, USA; Bell Commun. Res., Morristown, NJ, USA; Bell Commun. Res., Morristown, NJ, USA; Bell Commun. Res., Morristown, NJ, USA; Bell Commun. Res., Morristown, NJ, USA; Bell Commun. Res., Morristown, NJ, USA","IEEE Transactions on Software Engineering","","1988","14","3","317","326","The IC* project is an effort to create an environment for the design, specification, and development of complex systems such as communication protocols, parallel machines, and distributed systems. The basis of the project is the IC* model of parallel computation, in which a system is specified by a set of invariant expressions which describe its behavior in time. The features of this model include temporal and structural constraints, inherent parallelism, explicit modeling of time, nondeterministic evolution, and dynamic activation. The project also includes the construction of a parallel computer specifically designed to support the model of computation. The authors discuss the IC* model and the current user language, and describe the architecture and hardware of the prototype supercomputer built to execute IC* programs.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4652","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4652","","Integrated circuit modeling;Computational modeling;Concurrent computing;Programming environments;Protocols;Parallel machines;Parallel processing;Computer architecture;Hardware;Prototypes","parallel processing;programming environments;protocols","temporal constraints;parallel computation;programming environment;IC* project;design;specification;communication protocols;parallel machines;distributed systems;structural constraints;explicit modeling;nondeterministic evolution;dynamic activation;parallel computer","","17","","24","","","","","","IEEE","IEEE Journals & Magazines"
"On the Reliability of the IBM MVS/XA Operating System","S. Mourad; D. Andrews","Center for Reliable Computing, Computer Systems Laboratory, Departments of Electrical Engineering and Computer Science, Stanford University, Stanford, CA 94305, and Santa Clara University; NA","IEEE Transactions on Software Engineering","","1987","SE-13","10","1135","1139","This paper describes an analysis of system-detected errors on the MVS operating system under the eXtended Architecture (XA) for two IBM 3081 systems. The analysis classifies the errors in categories and examines the effectiveness of the recovery system of the MVS/XA. Comparison of the results for the two IBM 3081's confirm the dependence of the error distribution on the type of system utilization. The results for one of the machines are compared to those obtained for the same system when operating under MVS/SP. The comparison of the MVS/XA to the MVS/SP reveals the following: more addressing errors, a better error recording system and improved reliability and fault tolerance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232855","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702153","Fault tolerance;IBM 3081;large computing systems;MVS/XA;operating system;recovery;reliability;workload","Operating systems;Military computing;Error analysis;Computer errors;Fault tolerant systems;Hardware;Laboratories;Large-scale systems;Fault detection;Frequency","","Fault tolerance;IBM 3081;large computing systems;MVS/XA;operating system;recovery;reliability;workload","","26","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Proving properties of real-time systems through logical specifications and Petri net models","M. Felder; D. Mandrioli; A. Morzenti","Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy; Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy; Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy","IEEE Transactions on Software Engineering","","1994","20","2","127","141","Addresses the problem of formally analyzing the properties of real-time systems. We propose a method based on modeling the system as a timed Petri net and on specifying its properties in TRIO, an extension of temporal logic suitable for dealing explicitly with time and for measuring it. Timed Petri nets are axiomatized in terms of TRIO, so that their properties can be derived as theorems in the same spirit as the classical Hoare method allows one to prove properties of programs coded in a Pascal-like language. The method is also illustrated through an example.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.265634","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=265634","","Real time systems;Petri nets;Logic programming;Formal specifications;Automata;Time measurement;Embedded system;Resource management;Timing;Control systems","real-time systems;temporal logic;Petri nets;formal specification;theorem proving","real-time systems;property proving;logical specifications;TRIO;formal analysis;timed Petri net;temporal logic;axiomatization;Hoare method;embedded systems;first-order logic;formal specification;dual language","","49","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Performance properties of vertically partitioned object-oriented systems","S. P. Hufnagel; J. C. Browne","Dept. of Comput Sci. Eng., Texas Univ., Arlington, TX, USA; NA","IEEE Transactions on Software Engineering","","1989","15","8","935","946","A vertically partitioned structure for the design and implementation of object-oriented systems is proposed, and their performance is demonstrated. It is shown that the application-independent portion of the execution overheads in object-oriented systems can be less than the application-independent overheads in conventionally organized systems built on layered structures. Vertical partitioning implements objects through extended type managers. Two key design concepts result in performance improvement: object semantics can be used in the state management functions of an object type and atomicity is maintained at the type manager boundaries providing efficient recovery points. The performance evaluation is based on a case study of a simple but nontrivial distributed real-time system application.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31351","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31351","","Application software;Costs;Access control;Concurrency control;Error correction;Fault detection;Control systems;Operating systems;Software performance;Software systems","object-oriented programming;performance evaluation;software engineering","object-oriented system design;object-oriented system implementation;vertical partitioning;vertically partitioned structure;object-oriented systems;application-independent overheads;conventionally organized systems;layered structures;extended type managers;design concepts;performance improvement;object semantics;state management functions;object type;atomicity;type manager boundaries;recovery points;performance evaluation;distributed real-time system application","","5","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Operational survivability in gracefully degrading distributed processing systems","E. W. Martin; R. A. De Millo","Boeing Electronics Company, Seattle, WA 98124; Department of Information and Computer Science, Georgia Institute of Technology, Atlanta, GA 30332","IEEE Transactions on Software Engineering","","1986","SE-12","6","693","704","The use of experimental methods and statistical analysis techniques to study factors influencing operational survivability in gracefully degrading systems is investigated. Survivability data are generated using a statistically designed experiment in conjunction with a simulation model of network survivability. Thirty two factors having stable regression coefficients are used to identify ten regression models explaining survivability. Influential factors include the distributed system network, the application system, and the distribution policy. Nine factors are found in all models: the number of nodes in the distribution system, distributed system connectivity, module memory requirements, module-to-module interaction frequency, distribution policy, percent of nodes lost, initial assignment results, available processing capacity at the end of the subcase, and the interaction of all application-related variables. Models that are acceptable from both an estimation and prediction viewpoint are developed. Possible commercial and military applications are suggested.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312967","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312967","Distributed processing system;graceful degradation;operational survivability","Topology;Network topology;Degradation;Memory management;Distributed processing;Analytical models;Computational modeling","distributed processing","operational survivability;distributed processing system degrading;experimental methods;statistical analysis;degrading systems;simulation model;network survivability;stable regression coefficients;distributed system network;application system;distribution policy;distribution system;distributed system connectivity;module memory requirements;module-to-module interaction frequency;distribution policy;initial assignment results","","6","","","","","","","","IEEE","IEEE Journals & Magazines"
"Input Domain Reduction through Irrelevant Variable Removal and Its Effect on Local, Global, and Hybrid Search-Based Structural Test Data Generation","P. McMinn; M. Harman; K. Lakhotia; Y. Hassoun; J. Wegener","University of Sheffield, Sheffield; University College London, London; University College London, London; King's College London, London; Berner & Mattner Systemtechnik GmbH, Berlin","IEEE Transactions on Software Engineering","","2012","38","2","453","477","Search-Based Test Data Generation reformulates testing goals as fitness functions so that test input generation can be automated by some chosen search-based optimization algorithm. The optimization algorithm searches the space of potential inputs, seeking those that are “fit for purpose,” guided by the fitness function. The search space of potential inputs can be very large, even for very small systems under test. Its size is, of course, a key determining factor affecting the performance of any search-based approach. However, despite the large volume of work on Search-Based Software Testing, the literature contains little that concerns the performance impact of search space reduction. This paper proposes a static dependence analysis derived from program slicing that can be used to support search space reduction. The paper presents both a theoretical and empirical analysis of the application of this approach to open source and industrial production code. The results provide evidence to support the claim that input domain reduction has a significant effect on the performance of local, global, and hybrid search, while a purely random search is unaffected.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5710949","Search-based software testing;evolutionary testing;automated test data generation;input domain reduction.","Input variables;Software testing;Optimization;Algorithm design and analysis;Search problems;Software algorithms","automatic test pattern generation;optimisation;program compilers;program slicing;program testing;public domain software;search problems","input domain reduction;irrelevant variable removal;hybrid search-based structural test data generation;fitness functions;test input generation;search-based optimization algorithm;key determining factor;search-based software testing;search space reduction;static dependence analysis;program slicing;open source approach;industrial production code","","26","","52","","","","","","IEEE","IEEE Journals & Magazines"
"A ranking of software engineering measures based on expert opinion","Ming Li; C. S. Smidts","Center for Reliability Eng., Maryland Univ., College Park, MD, USA; Center for Reliability Eng., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","2003","29","9","811","824","This research proposes a framework based on expert opinion elicitation, developed to select the software engineering measures which are the best software reliability indicators. The current research is based on the top 30 measures identified in an earlier study conducted by Lawrence Livermore National Laboratory. A set of ranking criteria and their levels were identified. The score of each measure for each ranking criterion was elicited through expert opinion and then aggregated into a single score using multiattribute utility theory. The basic aggregation scheme selected was a linear additive scheme. A comprehensive sensitivity analysis was carried out. The sensitivity analysis included: variation of the ranking criteria levels, variation of the weights, variation of the aggregation schemes. The top-ranked measures were identified. Use of these measures in each software development phase can lead to a more reliable quantitative prediction of software reliability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232286","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232286","","Software engineering;Software measurement;Software reliability;Analysis of variance;Sensitivity analysis;Programming;Software quality;Density measurement;Current measurement;Laboratories","software reliability;software metrics;sensitivity analysis","expert opinion elicitation;software engineering measures;best software reliability indicators;ranking criteria;multiattribute utility theory;basic aggregation scheme;linear additive scheme;comprehensive sensitivity analysis;software development phase;software reliability","","39","","41","","","","","","IEEE","IEEE Journals & Magazines"
"A Practical Approach to Size Estimation of Embedded Software Components","K. Lind; R. Heldal","Saab Automobile AB, Trollhättan; Chalmers University of Technology, Göteborg","IEEE Transactions on Software Engineering","","2012","38","5","993","1007","To estimate software code size early in the development process is important for developing cost-efficient embedded systems. We have applied the COSMIC Functional Size Measurement (FSM) method for size estimation of embedded software components in the automotive industry. Correlational studies were conducted using data from two automotive companies. The studies show strong correlation between functional size and software code size, which is important for obtaining accurate estimation results. This paper presents the characteristics and results of our work, and aims to provide a practical framework for how to use COSMIC FSM for size estimation purposes. We investigate the results from our earlier correlational studies, and conduct further studies to identify such a framework. Based on these activities, we conclude that a clear purpose of the estimation process, a well-defined domain allowing categorization of software, consistent content and quality of requirements, and historical data from implemented software are key factors for size estimation of embedded software components.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5999672","Real-time and embedded systems;software product metrics;COSMIC FSM;software components","Software;Estimation;Vehicles;Size measurement;Automotive engineering;Industries;Memory management","automotive engineering;object-oriented programming;production engineering computing;software cost estimation;software metrics","software code size estimation;embedded software components;development process;cost-efficient embedded systems;COSMIC functional size measurement method;FSM method;automotive industry;automotive companies;domain allowing categorization;consistent content;requirements quality;historical data;software product metrics","","5","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic distribution of reactive systems for asynchronous networks of processors","P. Caspi; A. Girault; D. Pilaud","Lab. VERIMAG, Gieres, France; NA; NA","IEEE Transactions on Software Engineering","","1999","25","3","416","427","The paper addresses the problem of automatically distributing reactive systems. We first show that the use of synchronous languages allows a natural parallel description of such systems, regardless of any distribution problems. Then, a desired distribution can be easily specified, and achieved with the algorithm presented here. This distribution technique provides distributed programs with the same safety, test, and debug facilities as ordinary sequential programs. Finally, the implementation of such distributed programs only requires a very simple communication protocol (""first in first out"" queues), thereby reducing the need for large distributed real time executives.","0098-5589;1939-3520;2326-3881","","10.1109/32.798329","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=798329","","Parallel processing;Debugging;Interleaved codes;Safety;Sequential analysis;Protocols;Distributed processing;Program processors;Interactive systems;Operating systems","distributed programming;real-time systems;distributed algorithms;program compilers","automatic distribution;reactive systems;asynchronous networks of processors;synchronous languages;natural parallel description;distribution problems;distribution technique;distributed programs;debug facilities;communication protocol;first in first out queues;asynchronous communications","","33","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Aligning Qualitative, Real-Time, and Probabilistic Property Specification Patterns Using a Structured English Grammar","M. Autili; L. Grunske; M. Lumpe; P. Pelliccione; A. Tang","Dipartimento di Ingegneria e Scienze dell’Informazione e Matematica, Università dell’Aquila, Aquila, Italy; Institute of Software Technology, University of Stuttgart, Stuttgart, Germany; Swinburne University of Technology, Hawthorn, Australia; Dipartimento di Ingegneria e Scienze dell’Informazione e Matematica, Università dell’Aquila, Aquila, Italy; Swinburne University of Technology, Hawthorn, Australia","IEEE Transactions on Software Engineering","","2015","41","7","620","638","Formal methods offer an effective means to assert the correctness of software systems through mathematical reasoning. However, the need to formulate system properties in a purely mathematical fashion can create pragmatic barriers to the application of these techniques. For this reason, Dwyer et al. invented property specification patterns which is a system of recurring solutions to deal with the temporal intricacies that would make the construction of reactive systems very hard otherwise. Today, property specification patterns provide general rules that help practitioners to qualify order and occurrence, to quantify time bounds, and to express probabilities of events. Nevertheless, a comprehensive framework combining qualitative, real-time, and probabilistic property specification patterns has remained elusive. The benefits of such a framework are twofold. First, it would remove the distinction between qualitative and quantitative aspects of events; and second, it would provide a structure to systematically discover new property specification patterns. In this paper, we report on such a framework and present a unified catalogue that combines all known plus 40 newly identified or extended patterns. We also offer a natural language front-end to map patterns to a temporal logic of choice. To demonstrate the virtue of this new framework, we applied it to a variety of industrial requirements, and use PSPWizard, a tool specifically developed to work with our unified pattern catalogue, to automatically render concrete instances of property specification patterns to formulae of an underlying temporal logic of choice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2398877","PRESTO; European Commission; DFG; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7029714","Specification Patterns;Real-time Properties;Probabilistic Properties;Specification patterns;real-time properties;probabilistic properties","Probabilistic logic;Real-time systems;Natural languages;Software;Grammar;Electronic mail;Educational institutions","formal specification;natural language processing;probability;temporal logic","qualitative property specification pattern;real-time property specification pattern;probabilistic property specification pattern;structured English grammar;formal methods;software system correctness;mathematical reasoning;temporal intricacies;order qualification;occurrence qualification;time bound quantification;event probability;event qualitative aspect;event quantitative aspect;natural language front-end;pattern mapping;temporal logic;PSPWizard;unified pattern catalogue","","19","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Behavior protocols for software components","F. Plasil; S. Visnovsky","Inst. of Comput. Sci., Acad. of Sci. of the Czech Republic, Prague, Czech Republic; Inst. of Comput. Sci., Acad. of Sci. of the Czech Republic, Prague, Czech Republic","IEEE Transactions on Software Engineering","","2002","28","11","1056","1076","In this paper, we propose a means to enhance an architecture description language with a description of component behavior. A notation used for this purpose should be able to express the ""interplay"" on the component's interfaces and reflect step-by-step refinement of the component's specification during its design. In addition, the notation should be easy to comprehend and allow for formal reasoning about the correctness of the specification refinement and also about the correctness of an implementation in terms of whether it adheres to the specification. Targeting all these requirements together, the paper proposes employing behavior protocols which are based on a notation similar to regular expressions. As proof of the concept, the behavior protocols are used in the SOFA architecture description language at three levels: interface, frame, and architecture. Key achievements of this paper include the definitions of bounded component behavior and protocol conformance relation. Using these concepts, the designer can verify the adherence of a component's implementation to its specification at runtime, while the correctness of refining the specification can be verified at design time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049404","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049404","","Access protocols;Architecture description languages;Runtime;Software reusability;Computer Society;Software architecture;Programming profession;Object oriented programming;Object oriented modeling;Automata","object-oriented programming;formal specification;software architecture;software reusability","software component behavior;component interfaces;formal reasoning;correctness;component specification refinement;behavior protocols;regular expressions;SOFA architecture description language;architecture;frame;bounded component behavior;protocol conformance relation","","156","","33","","","","","","IEEE","IEEE Journals & Magazines"
"On distributing JASMIN's optimistic multiversioning page manager","M. -. Lai; W. K. Wilkinson; V. Lanin","Bell Commun. Res., Morristown, NJ, USA; Bell Commun. Res., Morristown, NJ, USA; Bell Commun. Res., Morristown, NJ, USA","IEEE Transactions on Software Engineering","","1989","15","6","696","704","JASMIN is a functionally distributed database system running on multiple microcomputers that communicate with each other by message passing. The software modules in JASMIN can be cloned and distributed across computer boundaries. One important module is the intelligent store, a page manager that includes transaction-management facilities. It provides an optimistic, multiversioning concurrency control scheme. This scheme allows read-only transactions to run almost without conflict checking; this is important in some real-time database applications like telephone switching and routing services. The initial implementation of the intelligent store deals with centralized database only. Experiences in modifying the JASMIN intelligent store module to handle distributed databases are described. Design principles and system implementation techniques in the following areas are explored; process structure, data structures and synchronization on data structures. The process structure is aimed to provide high throughput and the data structures are designed to facilitate fast response time.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24723","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24723","","Transaction databases;Data structures;Deductive databases;Database systems;Microcomputers;Message passing;Distributed computing;Concurrency control;Application software;Telephony","concurrency control;data structures;distributed databases;microcomputer applications;software packages;transaction processing","optimistic multiversioning page manager;functionally distributed database system;multiple microcomputers;message passing;software modules;page manager;transaction-management facilities;multiversioning concurrency control scheme;read-only transactions;conflict checking;real-time database applications;telephone switching;routing services;centralized database;JASMIN intelligent store module;system implementation techniques;process structure;data structures;synchronization;high throughput;fast response time","","2","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Time-Sensitive Cost Models in the Commercial MIS Environment","D. R. Jeffery","Department of Information Systems, University of New South Wales","IEEE Transactions on Software Engineering","","1987","SE-13","7","852","859","Current time-sensitive cost models suggest a significant impact on project effort if elapsed time compression or expansion is implemented. This paper reports an empirical study into the applicability of these models in the management information systems environment. It is found that elapsed time variation does not consistently affect project effort. This result is analyzed in terms of the theory supporting such a relationship, and an alternate relationship is suggested.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233496","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702296","Cost models;productivity;Putnam model;software project management;transformation of variables","Costs;Management information systems;Project management;Productivity;Predictive models;Time factors;Software engineering;Environmental management;Databases;Business","","Cost models;productivity;Putnam model;software project management;transformation of variables","","18","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Machine learning approaches to estimating software development effort","K. Srinivasan; D. Fisher","Personal Comput. Consultants Inc., Washington, DC, USA; NA","IEEE Transactions on Software Engineering","","1995","21","2","126","137","Accurate estimation of software development effort is critical in software engineering. Underestimates lead to time pressures that may compromise full functional development and thorough testing of software. In contrast, overestimates can result in noncompetitive contract bids and/or over allocation of development resources and personnel. As a result, many models for estimating software development effort have been proposed. This article describes two methods of machine learning, which we use to build estimators of software development effort from historical data. Our experiments indicate that these techniques are competitive with traditional estimators on one dataset, but also illustrate that these methods are sensitive to the data on which they are trained. This cautionary note applies to any model-construction strategy that relies on historical data. All such models for software effort estimation should be evaluated by exploring model sensitivity on a variety of historical data.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.345828","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=345828","","Machine learning;Programming;Contracts;Integrated circuit modeling;Software testing;Personnel;Regression tree analysis;Software development management;Costs;Machine learning algorithms","learning (artificial intelligence);software development management;contracts;human resource management","machine learning;software development effort estimation;software engineering;time pressures;software testing;contract bids;personnel;development resources;historical data;model-construction strategy","","223","","25","","","","","","IEEE","IEEE Journals & Magazines"
"OBEY: Optimal Batched Refactoring Plan Execution for Class Responsibility Redistribution","H. C. Jiau; L. W. Mar; J. C. Chen","National Cheng Kung University, Tainan; National Cheng Kung University, Tainan; National Cheng Kung University, Tainan","IEEE Transactions on Software Engineering","","2013","39","9","1245","1263","The redistribution of class responsibilities is a common reengineering practice in object-oriented (OO) software evolution. During the redistribution, developers frequently construct batched refactoring plans for moving multiple methods and fields among various classes. With an objective of carefully maintaining the cohesion and coupling degree of the class design, executing a batched refactoring plan without introducing any objective-violating side effect into the refactored code is essential. However, using most refactoring engines for batched refactoring plan execution introduces coupling-increasing Middle Man bad smell in the final refactored code and therefore makes the refactoring execution suboptimal in achieving the redistribution objective. This work proposes Obey, a methodology for optimal batched refactoring plan execution. Obey analyzes a batched refactoring plan, identifies Middle Man symptoms that cause suboptimal execution, and renovates the plan for optimal execution. We have conducted an empirical study on three open-source software projects to confirm the effectiveness of Obey in a practical context.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6493333","Reengineering;class responsibility redistribution;batched refactoring execution;change impact analysis;optimization","Couplings;Engines;Software systems;Measurement;Optimization;Context","object-oriented programming;software maintenance","OBEY methodology;optimal batched refactoring plan execution;class responsibility redistribution;software reengineering practice;object-oriented software evolution;OO software evolution;refactoring engine;coupling-increasing middle man bad smell;open-source software project","","3","","74","","","","","","IEEE","IEEE Journals & Magazines"
"Studying Bad Updates of Top Free-to-Download Apps in the Google Play Store","S. Hassan; C. Bezemer; A. E. Hassan","Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen's University School of Computing, 374016 Kingston, Ontario Canada (e-mail: shassan@cs.queensu.ca); School of Computing, Queen's University, Kingston, Ontario Canada (e-mail: bezemer@cs.queensu.ca); School of Computing, Queen's University, Kingston, Ontario Canada K7L 3N6 (e-mail: ahmed@cs.queensu.ca)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Developers always focus on delivering high-quality updates to improve, or maintain the rating of their apps. Prior work has studied user reviews by analyzing all reviews of an app. However, this app-level analysis misses the point that users post reviews to provide their feedback on a certain update. For example, two bad updates of an app with a history of good updates would not be spotted using app-level analysis. In this paper, we examine reviews at the update-level to better understand how users perceive bad updates. We focus our study on the top 250 bad updates (i.e., updates with the highest increase in the percentage of negative reviews relative to the prior updates of the app) from 26,726 updates of 2,526 top free-to-download apps in the Google Play Store. We find that feature removal and UI issues have the highest increase in the percentage of negative reviews. Bad updates with crashes and functional issues are the most likely to be fixed by a later update. However, developers often do not mention these fixes in the release notes. Our work demonstrates the necessity of an update-level analysis of reviews to capture the feelings of an app's user-base about a particular update.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2869395","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8457304","mobile app reviews;Google Play Store;bad updates;Android mobile apps","Google;Computer bugs;Feature extraction;Global Positioning System;User interfaces;History","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Formal verification of algorithms for critical systems","J. M. Rushby; F. von Henke","SRI Int., Menlo Park, CA, USA; NA","IEEE Transactions on Software Engineering","","1993","19","1","13","23","The authors describe their experience with formal, machine-checked verification of algorithms for critical applications, concentrating on a Byzantine fault-tolerant algorithm for synchronizing the clocks in the replicated computers of a digital flight control system. The problems encountered in unsynchronized systems and the necessity, and criticality, of fault-tolerant synchronization are described. An overview of one such algorithm and of the arguments for its correctness are given. A verification of the algorithm performed using the authors' EHDM system for formal specification and verification is described. The errors found in the published analysis of the algorithm and benefits derived from the verification are indicated. Based on their experience, the authors derive some key requirements for a formal specification and verification system adequate to the task of verifying algorithms of the type considered. The conclusions regarding the benefits of formal verification in this domain and the capabilities required of verification systems in order to realize those benefits are summarized.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.210304","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=210304","","Formal verification;Aerospace control;Synchronization;Fault tolerant systems;Clocks;Formal specifications;Control systems;Fault tolerance;Aircraft;Application software","fault tolerant computing;formal specification;formal verification;safety;software reliability;synchronisation","critical systems;machine-checked verification;Byzantine fault-tolerant algorithm;digital flight control system;fault-tolerant synchronization;EHDM system;formal specification","","47","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Central Server Models with Multiple Job Classes, State Dependent Routing, and Rejection Blocking","I. F. Akyildiz; H. Von Brand","School of Information and Computer Science, Georgia Institute of Technology, Atlanta, GA 30332.; NA","IEEE Transactions on Software Engineering","","1989","15","10","1305","1312","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1989.559784","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=559784","","Routing;Network servers;Throughput;Job shop scheduling;Capacity planning;Flexible manufacturing systems;Virtual manufacturing;Computer science","","Blocking;finite station capacities;performance evaluation;performance measures;queueing network models","","7","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis of Restart Mechanisms in Software Systems","A. P. A. van Moorsel; K. Wolter","NA; NA","IEEE Transactions on Software Engineering","","2006","32","8","547","558","Restarts or retries are a common phenomenon in computing systems, for instance, in preventive maintenance, software rejuvenation, or when a failure is suspected. Typically, one sets a time-out to trigger the restart. We analyze and optimize time-out strategies for scenarios in which the expected required remaining time of a task is not always decreasing with the time invested in it. Examples of such tasks include the download of Web pages, randomized algorithms, distributed queries, and jobs subject to network or other failures. Assuming the independence of the completion time of successive tries, we derive computationally attractive expressions for the moments of the completion time, as well as for the probability that a task is able to meet a deadline. These expressions facilitate efficient algorithms to compute optimal restart strategies and are promising candidates for pragmatic online optimization of restart timers","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703386","Restart;software rejuvenation;time-out;fault-tolerant systems;performance and reliability modeling;completion time;adaptive systems;self-management.","Software systems;Internet;Failure analysis;Preventive maintenance;Software maintenance;Web pages;Software performance;Fault tolerant systems;Computer network reliability;Adaptive systems","software fault tolerance;software maintenance;system recovery","software system restart mechanism analysis;software preventive maintenance;software rejuvenation;software failure;software time-out strategy analysis;optimal restart strategy;fault-tolerant system;software performance modeling;software reliability modeling","","33","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Statistical Database Query Languages","G. Ozsoyoglu; Z. M. Ozsoyoglu","Department of Computer Engineering and Science, Case Institute of Technology, Case Western Reserve University; NA","IEEE Transactions on Software Engineering","","1985","SE-11","10","1071","1081","Databases that are mainly used for statistical analysis are called statistical databases (SDB). A statistical database management system (SDBMS) may be defined as a database management system that provides capabilities 1) to model, store, and manipulate data in a manner suitable for the needs of SDB users, and 2) to apply statistical data analysis techniques that range from simple summary statistics to advanced procedures. This paper surveys the existing and proposed SDB data definition and data manipulation (i.e., query) languages.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231854","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701922","Database systems;data definition;data manipulation;query languages;statistical databases","Database languages;Statistical analysis;Database systems;Packaging;Data analysis;Software packages;Application software;Taxonomy;Medical services;Power generation economics","","Database systems;data definition;data manipulation;query languages;statistical databases","","10","","64","","","","","","IEEE","IEEE Journals & Magazines"
"The detection of fault-prone programs","J. C. Munson; T. M. Khoshgoftaar","Div. of Comput. Sci., Univ. of West Florida, Pensacola, FL, USA; NA","IEEE Transactions on Software Engineering","","1992","18","5","423","433","The use of the statistical technique of discriminant analysis as a tool for the detection of fault-prone programs is explored. A principal-components procedure was employed to reduce simple multicollinear complexity metrics to uncorrelated measures on orthogonal complexity domains. These uncorrelated measures were then used to classify programs into alternate groups, depending on the metric values of the program. The criterion variable for group determination was a quality measure of faults or changes made to the programs. The discriminant analysis was conducted on two distinct data sets from large commercial systems. The basic discriminant model was constructed from deliberately biased data to magnify differences in metric values between the discriminant groups. The technique was successful in classifying programs with a relatively low error rate. While the use of linear regression models has produced models of limited value, this procedure shows great promise for use in the detection of program modules with potential for faults.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.135775","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=135775","","Fault detection;Software quality;Software measurement;Computer errors;Predictive models;Software metrics;Time measurement;Computer science;Error analysis;Linear regression","computational complexity;program testing;quality control;software metrics;software reliability","statistical technique;discriminant analysis;fault-prone programs;principal-components procedure;simple multicollinear complexity metrics;uncorrelated measures;orthogonal complexity domains;group determination;quality measure;large commercial systems;deliberately biased data;metric values;relatively low error rate;linear regression models;program modules","","253","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Visualizing Co-Change Information with the Evolution Radar","M. D'Ambros; M. Lanza; M. Lungu","University of Lugano, Lugano; University of Lugano, Lugano; University of Lugano, Lugano","IEEE Transactions on Software Engineering","","2009","35","5","720","735","Software evolution analysis provides a valuable source of information that can be used both to understand a system's design and predict its future development. While for many program comprehension purposes, it is sufficient to model a single version of a system, there are types of information that can only be recovered when the history of a system is taken into account. Logical coupling, the implicit dependency between software artifacts that have been changed together, is an example of such information. Previous research has dealt with low-level couplings between files, leading to an explosion of the data to be analyzed, or has abstracted the logical couplings to the level of modules, leading to a loss of detailed information. In this paper, we present a visualization-based approach that integrates logical coupling information at different levels of abstraction. This facilitates an in-depth analysis of the logical couplings, and at the same time, leads to a characterization of a system's modules in terms of their logical coupling. The presented approach supports the retrospective analysis of a software system and maintenance activities such as restructuring and redocumentation. We illustrate retrospective analysis on two large open-source software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815274","Software evolution;software visualization;logical coupling.","Radar;Information analysis;Software systems;Information resources;System analysis and design;History;Explosions;Data analysis;Data visualization;Software maintenance","data visualisation;software engineering;systems re-engineering","evolution radar;software evolution analysis;software artifacts;visualization;logical coupling information;abstraction;system module;open source software system analysis","","47","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Using Mutation Analysis for Assessing and Comparing Testing Coverage Criteria","J. H. Andrews; L. C. Briand; Y. Labiche; A. S. Namin","Computer Science Department, University of Western Ontario, London, Ontario, Canada; Simula Research Laboratory, Department of Software Engineering, Fornebu, Lysaker, Norway; Software Quality Engineering Laboratory, Carleton University, Ottawa, Ontario, Canada; Computer Science Department, University of Western Ontario, London, Ontario, Canada","IEEE Transactions on Software Engineering","","2006","32","8","608","624","The empirical assessment of test techniques plays an important role in software testing research. One common practice is to seed faults in subject software, either manually or by using a program that generates all possible mutants based on a set of mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults, thus facilitating the statistical analysis of fault detection effectiveness of test suites; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. Focusing on four common control and data flow criteria (block, decision, C-use, and P-use), this paper investigates this important issue based on a middle size industrial program with a comprehensive pool of test cases and known faults. Based on the data available thus far, the results are very consistent across the investigated criteria as they show that the use of mutation operators is yielding trustworthy results: generated mutants can be used to predict the detection effectiveness of real faults. Applying such a mutation analysis, we then investigate the relative cost and effectiveness of the above-mentioned criteria by revisiting fundamental questions regarding the relationships between fault detection, test suite size, and control/data flow coverage. Although such questions have been partially investigated in previous studies, we can use a large number of mutants, which helps decrease the impact of random variation in our analysis and allows us to use a different analysis approach. Our results are then; compared with published studies, plausible reasons for the differences are provided, and the research leads us to suggest a way to tune the mutation analysis process to possible differences in fault detection probabilities in a specific environment","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.83","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703390","Testing and debugging;testing strategies;test coverage of code;experimental design.","Genetic mutations;Fault detection;Software testing;Size control;Failure analysis;Statistical analysis;System testing;Industrial control;Costs;Debugging","data flow analysis;design of experiments;probability;program control structures;program debugging;program testing;software fault tolerance","mutation analysis;software testing coverage criteria;empirical assessment;software fault detection probability;industrial program;program control criteria;program data flow criteria;mutation operators;program debugging;experimental design","","192","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A methodological framework for viewpoint-oriented conceptual modeling","J. Andrade; J. Ares; R. Garcia; J. Pazos; S. Rodriguez; A. Silva","Fac. de Informatica, Univ. da Coruna, Spain; Fac. de Informatica, Univ. da Coruna, Spain; Fac. de Informatica, Univ. da Coruna, Spain; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","5","282","294","To solve any nontrivial problem, it first needs to be conceptualized, taking into account the individual who has the problem. However, a problem is generally associated with more than one individual, as is usually the case in software development. Therefore, this process has to take into account different viewpoints about the problem and any discrepancies that could arise as a result. Traditionally, conceptualization in software engineering has omitted the different viewpoints that the individuals may have of the problem and has inherently enforced consistency in the event of any discrepancies, which are considered as something to be systematically rejected. The paper presents a methodological framework that explicitly drives the conceptualization of different viewpoints and manages the different types of discrepancies that arise between them, which become really important in the process. The definition of this framework is generic, and it is therefore independent of any particular software development paradigm. Its application to software engineering means that viewpoints and their possible discrepancies can be considered in the software process conceptual modeling phase. This application is illustrated by means of what is considered to be a standard problem: the IFIP case.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291832","Conceptual modeling;multiple viewpoint;discrepancies;conflicts;inconsistencies;methodological framework.","Programming;Application software;Physics;Software engineering;Drives;Humans;Diffraction;Optical refraction;Optical reflection","software engineering","nontrivial problem;software development;software engineering;IFIP case;viewpoint-oriented conceptual modeling","","25","","44","","","","","","IEEE","IEEE Journals & Magazines"
"PHILAN: a LAN providing a reliable message service for distributed processing","J. L. W. Kessels","Philips Res. Lab., Eindhoven, Netherlands","IEEE Transactions on Software Engineering","","1988","14","10","1424","1431","A local area network (LAN) design based on a ring topology is presented which can support both packet-switched and circuit-switched traffic. The packet-switching service is reliable in that the LAN controllers deal with all protocol problems, i.e., medium arbitrations as well as flow and error control. The service can meet real-time constraints, since the performance is stable under high load conditions and the arbitration delays are bounded. Moreover, the processing speed of the LAN controller is independent of the transmission speed, and the speed requirements are such that they can be met by a microprocessor (no need for dedicated hardware to process the information on the fly). Before the design of PHILAN is presented, an analysis is given of the protocol problems that have to be dealt with when establishing a reliable packet-switching service on a LAN.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6187","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6187","","Local area networks;Message service;Protocols;Network topology;Circuit topology;Communication system traffic control;Error correction;Delay;Microprocessors;Hardware","local area networks;packet switching;protocols","reliable message service;distributed processing;local area network;LAN;ring topology;packet-switched;circuit-switched;packet-switching;protocol;real-time constraints;processing speed;PHILAN","","","","11","","","","","","IEEE","IEEE Journals & Magazines"
"A component- and message-based architectural style for GUI software","R. N. Taylor; N. Medvidovic; K. M. Anderson; E. J. Whitehead; J. E. Robbins; K. A. Nies; P. Oreizy; D. L. Dubrow","California Univ., Irvine, CA, USA; California Univ., Irvine, CA, USA; California Univ., Irvine, CA, USA; California Univ., Irvine, CA, USA; California Univ., Irvine, CA, USA; California Univ., Irvine, CA, USA; California Univ., Irvine, CA, USA; NA","IEEE Transactions on Software Engineering","","1996","22","6","390","406","While a large fraction of application code is devoted to graphical user interface (GUI) functions, support for reuse in this domain has largely been confined to the creation of GUI toolkits (""widgets""). We present a novel architectural style directed at supporting larger grain reuse and flexible system composition. Moreover, the style supports design of distributed, concurrent applications. Asynchronous notification messages and asynchronous request messages are the sole basis for intercomponent communication. A key aspect of the style is that components are not built with any dependencies on what typically would be considered lower-level components, such as user interface toolkits. Indeed, all components are oblivious to the existence of any components to which notification messages are sent. While our focus has been on applications involving graphical user interfaces, the style has the potential for broader applicability. Several trial applications using the style are described.","0098-5589;1939-3520;2326-3881","","10.1109/32.508313","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=508313","","Graphical user interfaces;User interfaces;Application software;Computer architecture;Runtime;Concurrent computing;Artificial intelligence;Software tools;Graphics;Computer languages","graphical user interfaces;user interface management systems;software reusability;message passing;parallel programming","message-based architectural style;component-based architectural style;graphical user interface;reuse;widgets;flexible system composition;distributed concurrent applications;asynchronous notification messages;asynchronous request messages;intercomponent communication;user interface toolkits;message-based architectures;heterogeneity;concurrency","","188","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Cycle Time Properties Of The FDDI Token Ring Protocol","K. C. Sevick; M. J. Johnson","Computer Systems Research Institute, University of Toronto; NA","IEEE Transactions on Software Engineering","","1987","SE-13","3","376","385","The FDDI Token Ring Protocol controls communication over fiber optic rings with transmission rates in the range of 100 megabits per second. It is intended to give guaranteed response to time-critical messages by using a ""timed token"" protocol, in which non-critical messages may be transmitted only if recent movement of the token among stations has been sufficiently fast relative to a ""target"" token rotation time (TTRT).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233169","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702224","FDDI protocol;performance;token ring","FDDI;Token networks;Protocols;Optical fibers;Data communication;Optical fiber LAN;NASA;Optical control;Communication system control;Communications technology","","FDDI protocol;performance;token ring","","134","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Shallow knowledge as an aid to deep understanding in early phase requirements engineering","P. Sawyer; P. Rayson; K. Cosh","Dept. of Comput., Lancaster Univ., UK; Dept. of Comput., Lancaster Univ., UK; NA","IEEE Transactions on Software Engineering","","2005","31","11","969","981","Requirements engineering's continuing dependence on natural language description has made it the focus of several efforts to apply language engineering techniques. The raw textual material that forms an input to early phase requirements engineering and which informs the subsequent formulation of the requirements is inevitably uncontrolled and this makes its processing very hard. Nevertheless, sufficiently robust techniques do exist that can be used to aid the requirements engineer provided that the scope of what can be achieved is understood. In this paper, we show how combinations of lexical and shallow semantic analysis techniques developed from corpus linguistics can help human analysts acquire the deep understanding needed as the first step towards the synthesis of requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.129","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556555","Index Terms- Specification;elicitation methods;tools;linguistic processing;document analysis.","Knowledge engineering;Humans;Computer Society;Frequency;Natural languages;Information analysis;Speech analysis;Raw materials;Robustness;Text analysis","formal specification;formal verification;computational linguistics","requirements engineering;natural language description;language engineering technique;lexical analysis;shallow semantic analysis;corpus linguistics;specification method;elicitation method;linguistic processing;document analysis","","47","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Does code decay? Assessing the evidence from change management data","S. G. Eick; T. L. Graves; A. F. Karr; J. S. Marron; A. Mockus","Lucent Technol. Bell Labs., Naperville, IL, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","1","1","12","A central feature of the evolution of large software systems is that change-which is necessary to add new functionality, accommodate new hardware, and repair faults-becomes increasingly difficult over time. We approach this phenomenon, which we term code decay, scientifically and statistically. We define code decay and propose a number of measurements (code decay indices) on software and on the organizations that produce it, that serve as symptoms, risk factors, and predictors of decay. Using an unusually rich data set (the fifteen-plus year change history of the millions of lines of software for a telephone switching system), we find mixed, but on the whole persuasive, statistical evidence of code decay, which is corroborated by developers of the code. Suggestive indications that perfective maintenance can retard code decay are also discussed.","0098-5589;1939-3520;2326-3881","","10.1109/32.895984","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=895984","","Software maintenance;Software systems;Hardware;Telephony;History;Statistical analysis;Computer Society;Software measurement;Switching systems;Operating systems","software maintenance;software metrics;management of change;statistical analysis","code decay;change management data;large software systems evolution;software measurements;risk factors;data set;telephone switching system;perfective maintenance;software maintenance;statistical analysis","","254","","25","","","","","","IEEE","IEEE Journals & Magazines"
"An optimistic locking technique for concurrency control in distributed databases","U. Halici; A. Dogac","Middle East Tech. Univ., Ankara, Turkey; Middle East Tech. Univ., Ankara, Turkey","IEEE Transactions on Software Engineering","","1991","17","7","712","724","A method called optimistic method with dummy locks (ODL) is suggested for concurrency control in distributed databases. It is shown that by using long-term dummy locks, the need for the information about the write sets of validated transactions is eliminated and, during the validation test, only the related sites are checked. The transactions to be aborted are immediately recognized before the validation test, reducing the costs of restarts. Usual read and write locks are used as short-term locks during the validation test. The use of short-term locks in the optimistic approach eliminates the need for the system-wide critical section and results in a distributed and parallel validation test. The performance of ODL is compared with strict two-phase locking (2PL) through simulation, and it is found out that for the low conflict cases they perform almost the same, but for the high conflicting cases, ODL performs better than strict 2PL.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.83907","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=83907","","Concurrency control;Distributed databases;System recovery;System testing;Costs;Optimization methods;Transaction databases;Database systems;Certification;Protocols","concurrency control;distributed databases;system recovery;transaction processing","optimistic locking technique;concurrency control;distributed databases;optimistic method;dummy locks;write sets;validated transactions;validation test;short-term locks;ODL;strict two-phase locking;low conflict cases;strict 2PL","","5","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Understanding and controlling software costs","B. W. Boehm; P. N. Papaccio","TRW Inc., Redondo Beach, CA, USA; TRW Inc., Redondo Beach, CA, USA","IEEE Transactions on Software Engineering","","1988","14","10","1462","1477","A discussion is presented of the two primary ways of understanding software costs. The black-box or influence-function approach provides useful experimental and observational insights on the relative software productivity and quality leverage of various management, technical, environmental, and personnel options. The glass-box or cost distribution approach helps identify strategies for integrated software productivity and quality improvement programs using such structures as the value chain and the software productivity opportunity tree. The individual strategies for improving software productivity are identified. Issues related to software costs and controlling them are examined and discussed. It is pointed out that a good framework of techniques exists for controlling software budgets, schedules, and work completed, but that a great deal of further progress is needed to provide an overall set of planning and control techniques covering software product qualities and end-user system objectives.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6191","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6191","","Costs;Software quality;Productivity;Personnel;Programming profession;Control systems;Environmental management;Quality management;Writing;Software engineering","DP management;software engineering","software costs;black-box;influence-function;software productivity;glass-box;cost distribution;integrated software productivity;quality improvement;value chain;software productivity opportunity tree;software budgets;end-user system objectives","","246","","129","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic Testing for Deadlocks via Constraints","Y. Cai; Q. Lu","State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Technology Center of Software Engineering, Institute of Software, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Software Engineering","","2016","42","9","825","842","Existing deadlock detectors are either not scalable or may report false positives when suggesting cycles as potential deadlocks. Additionally, they may not effectively trigger deadlocks and handle false positives. We propose a technique called ConLock<sup>+</sup>, which firstly analyzes each cycle and its corresponding execution to identify a set of scheduling constraints that are necessary conditions to trigger the corresponding deadlock. The ConLock<sup>+</sup>technique then performs a second run to enforce the set of constraints, which will trigger a deadlock if the cycle is a real one. Or if not, ConLock<sup>+</sup>reports a steering failure for that cycle and also identifies other similar cycles which would also produce steering failures. For each confirmed deadlock, ConLock<sup>+</sup>performs a static analysis to identify conflicting memory access that would also contribute to the occurrence of the deadlock. This analysis is helpful to enable developers to understand and fix deadlocks. ConLock<sup>+</sup>has been validated on a suite of real-world programs with 16 real deadlocks. The results show that across all 811 cycles, ConLock<sup>+</sup>confirmed all of the 16 deadlocks with a probability of ≥80 percent. For the remaining cycles, ConLock<sup>+</sup>reported steering failures and also identified that five deadlocks also involved conflicting memory accesses.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2537335","National Basic Research (973) Program of China; National Science Foundation of China (NSFC); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7423814","Deadlock triggering;scheduling;should-happen-before relation;constraint;reliability;verification","System recovery;Instruction sets;Schedules;Testing;Synchronization;Detectors;Probabilistic logic","concurrency control;program diagnostics;program testing;scheduling","static analysis;memory access;real-world programs;steering failure;scheduling constraints;ConLock+;deadlock detectors;dynamic testing","","6","","55","","","","","","IEEE","IEEE Journals & Magazines"
"A causal model for software cost estimating error","A. L. Lederer; J. Prasad","Kentucky Univ., Lexington, KY, USA; NA","IEEE Transactions on Software Engineering","","1998","24","2","137","148","Software cost estimation is an important concern for software managers and other software professionals. The hypothesized model in this research suggests that an organization's use of an estimate influences its estimating practices which influence both the basis of the estimating process and the accuracy of the estimate. The model also suggests that the estimating basis directly influences the accuracy of the estimate. A study of business information systems managers and professionals at 112 different organizations using causal analysis with the Equations Modeling System (EQS) refined the model. The refined model shows that no managerial practice in this study discourages the use of intuition, guessing and personal memory in cost estimating. Although user commitment and accountability appear to foster algorithm-based estimating, such an algorithmic basis does not portend greater accuracy. Only one managerial practice-the use of the estimate in performance evaluations of software managers and professionals-presages greater accuracy. By implication, the research suggests somewhat ironically that the most effective approach to improve estimating accuracy may be to make estimators, developers and managers more accountable for the estimate even though it may be impossible to direct them explicitly on how to produce a more accurate one.","0098-5589;1939-3520;2326-3881","","10.1109/32.666827","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=666827","","Costs;Management information systems;Programming;Software tools;Information management;Differential equations;Software reusability;Information analysis;Memory management;Software performance","software cost estimation;error analysis;software management;management information systems","causal model;software cost estimation error;software managers;accuracy;business information systems managers;causal analysis;Equations Modeling System;refined model;managerial practice;intuition;guessing;personal memory;user commitment;user accountability;algorithm-based estimating;performance evaluation;accountability;software development","","27","","69","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing hard-real-time programs for guaranteed schedulability","A. D. Stoyenko; V. C. Hamacher; R. C. Holt","Dept. of Comput. & Inf. Sci., New Jersey Inst. of Technol., Newark, NJ, USA; NA; NA","IEEE Transactions on Software Engineering","","1991","17","8","737","750","A set of language-independent schedulability analysis techniques is presented. Utilizing knowledge of implementation- and hardware-dependent information in a table-driven fashion, these techniques provide accurate worst-case time bounds and other schedulability information. A prototype schedulability analyzer has been developed to demonstrate the effectiveness of these techniques. The analyzer consists of a partially language-dependent front-end, targeted at real-time Euclid, a real-time language specifically designed with a set of schedulability analysis provisions built-in, and a language-dependent back-end. The analyzer has been used on a number of realistic real-time programs run on a multiple-microprocessor system. Predicted program performance differs only marginally from the actual performance.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.83911","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=83911","","Scheduling;Real time systems;Program processors;Process control;Timing;Chemical processes;Software performance;Programming profession;Performance analysis;Failure analysis","high level languages;program verification;real-time systems;scheduling;systems analysis","language-independent schedulability analysis;hardware-dependent information;table-driven fashion;worst-case time bounds;prototype schedulability analyzer;partially language-dependent front-end;real-time Euclid;real-time language;schedulability analysis provisions;language-dependent back-end;realistic real-time programs;multiple-microprocessor system;program performance","","42","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Model Checking Probabilistic and Stochastic Extensions of the π-Calculus","G. Norman; C. Palamidessi; D. Parker; P. Wu","Oxford University, Oxford; INRIA Saclay and École Polytechnique, Paris; Oxford University, Oxford; University College London, Ipswich","IEEE Transactions on Software Engineering","","2009","35","2","209","223","We present an implementation of model checking for probabilistic and stochastic extensions of the pi-calculus, a process algebra which supports modelling of concurrency and mobility. Formal verification techniques for such extensions have clear applications in several domains, including mobile ad-hoc network protocols, probabilistic security protocols and biological pathways. Despite this, no implementation of automated verification exists. Building upon the pi-calculus model checker MMC, we first show an automated procedure for constructing the underlying semantic model of a probabilistic or stochastic pi-calculus process. This can then be verified using existing probabilistic model checkers such as PRISM. Secondly, we demonstrate how for processes of a specific structure a more efficient, compositional approach is applicable, which uses our extension of MMC on each parallel component of the system and then translates the results into a high-level modular description for the PRISM tool. The feasibility of our techniques is demonstrated through a number of case studies from the pi-calculus literature.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4626962","Model checking;Markov processes;Stochastic processes;Model checking;Markov processes;Stochastic processes","Stochastic processes;Biological system modeling;Protocols;Stochastic systems;Calculus;Algebra;Formal verification;Mobile ad hoc networks;Mobile communication;Communication system security","formal verification;pi calculus;probability;stochastic processes","model checking;probabilisty;stochastic extension;pi-calculus;process algebra;formal verification;mobile ad-hoc network protocol;probabilistic security protocol;biological pathway;semantic model;high-level modular description","","15","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Performance analysis of dynamic locking with the no-waiting policy","I. K. Ryu; A. Thomasian","Digital Equipment Corp., Mountain View, CA, USA; NA","IEEE Transactions on Software Engineering","","1990","16","7","684","698","A transaction processing system with two-phase dynamic locking with the no waiting policy (DLNW) for concurrency control is considered. In this method, transactions making conflicting lock requests are aborted and restarted rather than blocked, thereby eliminating blocking delays (and deadlocks), but making it susceptible to cyclic restarts. Cyclic restarts are dealt with by delaying the restart of a transaction encountering a lock conflict or replacing it with a new transaction. Analytic solution methods for evaluating the performance of the variants of the DLNW method are described. The analytic methods, validated against simulation and shown to be acceptably accurate, are used to study the effect of the following parameters on system performance: transaction size and its distribution, degree of concurrency, the throughput characteristic of the computer system, and the mixture of read-only query and update transactions. A comparison of the DLNW and dynamic locking with waiting (DLW) methods shows that DLW provides higher throughput than DLNW, except when there is no hardware resource contention and conflicted transactions can be replaced by new transactions. The DLNW method outperforms the time-stamp ordering method, as observed from simulation results as well as case by case analyses of possible scenarios.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.56095","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=56095","","Performance analysis;Delay;Analytical models;Throughput;Concurrency control;System recovery;Computational modeling;Computer simulation;System performance;Concurrent computing","concurrency control;performance evaluation;transaction processing","performance analysis;dynamic locking;no-waiting policy;transaction processing system;concurrency control;blocking delays;deadlocks;cyclic restarts;throughput characteristic;read-only query;update transactions;time-stamp ordering method","","14","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Maintenance support for object-oriented programs","N. Wilde; R. Huitt","Dept. of Comput. Sci., University of West Florida; NA","IEEE Transactions on Software Engineering","","1992","18","12","1038","1044","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1992.1263033","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1263033","","Software maintenance;Documentation;Communication system control;Data encapsulation;Costs;Software engineering;Collaborative software;Collaborative work;Councils","","","","104","","32","","","","","","IEEE","IEEE Journals & Magazines"
"A Feature-Based Classification of Model Repair Approaches","N. Macedo; T. Jorge; A. Cunha","High-Assurance Software Laboratory (HASLab)INESC TEC; European Space Agency (ESA), Paris, France; High-Assurance Software Laboratory (HASLab)INESC TEC","IEEE Transactions on Software Engineering","","2017","43","7","615","640","Consistency management, the ability to detect, diagnose and handle inconsistencies, is crucial during the development process in Model-driven Engineering (MDE). As the popularity and application scenarios of MDE expanded, a variety of different techniques were proposed to address these tasks in specific contexts. Of the various stages of consistency management, this work focuses on inconsistency handling in MDE, particularly in model repair techniques. This paper proposes a feature-based classification system for model repair techniques, based on an systematic literature review of the area. We expect this work to assist developers and researchers from different disciplines in comparing their work under a unifying framework, and aid MDE practitioners in selecting suitable model repair approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2620145","North Portugal Regional Operational Programme; European Regional Development Fund (ERDF); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7605502","Model-driven engineering, consistency management, inconsistency handling, model repair","Maintenance engineering;Unified modeling language;Taxonomy;Context;Feature extraction;Software engineering;Systematics","pattern classification;software maintenance","model repair approach;consistency management;model-driven engineering;MDE;feature-based classification system","","1","","91","","","","","","IEEE","IEEE Journals & Magazines"
"CHiP: A Configurable Hybrid Parallel Covering Array Constructor","H. Mercan; C. Yilmaz; K. Kaya","Computer sciences and engineering, Sabanci Universitesi, 52991 Istanbul, Istanbul Turkey 34956 (e-mail: hanefimercan@sabanciuniv.edu); Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Tuzla Turkey 34956 (e-mail: cyilmaz@sabanciuniv.edu); Computer Science and Engineering, Sabanci Universitesi, 52991 &#x0130;stanbul, Tuzla Turkey 34956 (e-mail: kaya@sabanciuniv.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","We present a configurable, hybrid, and parallel covering array constructor, called CHiP. CHiP is parallel in that it utilizes vast amount of parallelism provided by graphics processing units (GPUs). CHiP is hybrid in that it bundles the bests of two construction approaches for computing covering arrays; a metaheuristic search-based approach for efficiently covering a large portion of the required combinations and a constraint satisfaction-based approach for effectively covering the remaining hard-to-cover-by-chance combinations. CHiP is configurable in that a trade-off between covering array sizes and construction times can be made. We have conducted a series of experiments, in which we compared the efficiency and effectiveness of CHiP to those of a number of existing constructors by using both full factorial designs and well-known benchmarks. In these experiments, we report new upper bounds on covering array sizes, demonstrating the effectiveness of CHiP, and the first results for a higher coverage strength, demonstrating the scalability of CHiP.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2837759","Scientific and Technological Research Council of Turkey; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8360512","Covering arrays;parallel computing;graphics processing units;CUDA;metaheuristic search;constraint satisfaction problem","Simulated annealing;Graphics processing units;Parallel processing;Benchmark testing;Upper bound;Scalability","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Experimental evaluation of a reusability-oriented parallel programming environment","J. C. Browne; T. Lee; J. Werth","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; Dept. of Comput. Sci., Texas Univ., Austin, TX, USA","IEEE Transactions on Software Engineering","","1990","16","2","111","120","Reports on the initial experimental evaluation of ROPE (reusability-oriented parallel programming environment), a software component reuse system. ROPE helps the designer find and understand components by using a new classification method called structured relational classification. ROPE is part of a development environment for parallel programs which uses a declarative/hierarchical graphical programming interface. This interface allows use of components with different levels of abstraction, ranging from design units to actual code modules. ROPE supports reuse of all the component types defined in the development environment. Programs developed with the aid of ROPE were found to have error rates far less than those developed without ROPE.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44375","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44375","","Parallel programming;Error analysis;Software engineering;Software measurement;Computer displays;Concurrent computing;Software reusability;Software systems;Design for experiments;Libraries","computer graphics;parallel programming;performance evaluation;programming environments;software reusability;user interfaces","reusability-oriented parallel programming environment;experimental evaluation;ROPE;software component reuse system;structured relational classification;development environment;declarative/hierarchical graphical programming interface;design","","21","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Pictorial information retrieval using the random neural network","A. Stafylopatis; A. Likas","Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece; Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Greece","IEEE Transactions on Software Engineering","","1992","18","7","590","600","A technique is developed based on the use of a neural network model for performing information retrieval in a pictorial information system. The neural network provides autoassociative memory operation and allows the retrieval of stored symbolic images using erroneous or incomplete information as input. The network used is based on an adaptation of the random neural network model featuring positive and negative nodes and symmetrical behavior of positive and negative signals. The network architecture considered has hierarchical structure and allows two-level operation during learning and recall. An experimental software prototype, including an efficient graphical interface, has been implemented and tested. The performance of the system has been investigated through experiments under several schemes concerning storage and reconstruction of patterns. These schemes are either based on properties of the random network or constitute adaptations of known neural network techniques.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.148477","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=148477","","Information retrieval;Neural networks;Information systems;Image processing;Image databases;Management information systems;Image retrieval;Data mining;Image storage;Relational databases","computerised picture processing;content-addressable storage;graphical user interfaces;information retrieval;neural nets;software prototyping","pictorial information retrieval;random neural network;autoassociative memory operation;stored symbolic images;hierarchical structure;learning;recall;software prototype;graphical interface;performance;random network","","8","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Determining an optimal time interval for testing and debugging software","N. D. Singpurwalla","Sch. of Eng. & Appl. Sci., George Washington Univ., Washington, DC, USA","IEEE Transactions on Software Engineering","","1991","17","4","313","319","A decision-theoretic procedure for determining an optimal time interval for testing software prior to its release is proposed. The approach is based on the principles of decision-making under uncertainty and involves a maximization of expected utility. Two plausible forms for the utility function, one based on costs and the other involving the realized reliability of the software, are described. Using previous results on probabilistic models for software failure, the ensuing optimization problem (which can be addressed using numerical techniques) is outlined for the case of single-state testing. The sensitivity of the results to the various input parameters is discussed, and some directions for future research are outlined.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90431","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=90431","","Software testing;Software debugging;Cost function;Bayesian methods;Certification;Computer bugs;Decision making;Uncertainty;Utility theory;Computer science","decision theory;program debugging;program testing;programming theory","software testing;software debugging;decision theory;software reliability;optimal time interval;decision-making;uncertainty;maximization;expected utility;utility function;costs;probabilistic models;software failure;optimization problem;single-state testing","","45","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Maisie: a language for the design of efficient discrete-event simulations","R. L. Bagrodia; Wen-Toh Liao","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA; Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA","IEEE Transactions on Software Engineering","","1994","20","4","225","238","Maisie is a C-based discrete-event simulation language that was designed to cleanly separate a simulation model from the underlying algorithm (sequential or parallel) used for the execution of the model. With few modifications, a Maisie program may be executed by using a sequential simulation algorithm, a parallel conservative algorithm or a parallel optimistic algorithm. The language constructs allow the run-time system to implement optimizations that reduce recomputation and state saving overheads for optimistic simulations and synchronization overheads for conservative implementations. This paper presents the Maisie simulation language, describes a set of optimizations, and illustrates the use of the language in the design of efficient parallel simulations.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.277572","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=277572","","Computational modeling;Discrete event simulation;Computer simulation;Algorithm design and analysis;Parallel architectures;Protocols;Design optimization;Concurrent computing;Distributed computing;Computer science","discrete event simulation;simulation languages;optimisation;synchronisation;parallel algorithms;C language","Maisie;C-based discrete-event simulation language;simulation model/algorithm separation;sequential simulation algorithm;parallel conservative algorithm;parallel optimistic algorithm;language constructs;run-time system;optimizations;recomputation overheads;state saving overheads;synchronization overheads;distributed simulation;semantic rollback;lookahead optimization;interrogative simulation","","85","","35","","","","","","IEEE","IEEE Journals & Magazines"
"An Industrial Survey of Safety Evidence Change Impact Analysis Practice","J. L. de la Vara; M. Borg; K. Wnuk; L. Moonen","Computer Science Department, Carlos III University of Madrid, Avda. de la Universidad 30, 28911 Leganes, Madrid, Spain; Software and Systems Laboratory, SICS Swedish ICT AB, Ideon Science Park, Building Beta 2, Scheelevägen 17, Lund, Sweden; Software Engineering Research Lab, Blekinge Institute of Technology, Karlskrona, Sweden; Certus Centre for Software V&V, Simula Research Laboratory, P.O. Box 134, Lysaker, Norway","IEEE Transactions on Software Engineering","","2016","42","12","1095","1117","Context. In many application domains, critical systems must comply with safety standards. This involves gathering safety evidence in the form of artefacts such as safety analyses, system specifications, and testing results. These artefacts can evolve during a system's lifecycle, creating a need for change impact analysis to guarantee that system safety and compliance are not jeopardised. Objective. We aim to provide new insights into how safety evidence change impact analysis is addressed in practice. The knowledge about this activity is limited despite the extensive research that has been conducted on change impact analysis and on safety evidence management. Method. We conducted an industrial survey on the circumstances under which safety evidence change impact analysis is addressed, the tool support used, and the challenges faced. Results. We obtained 97 valid responses representing 16 application domains, 28 countries, and 47 safety standards. The respondents had most often performed safety evidence change impact analysis during system development, from system specifications, and fully manually. No commercial change impact analysis tool was reported as used for all artefact types and insufficient tool support was the most frequent challenge. Conclusion. The results suggest that the different artefact types used as safety evidence co-evolve. In addition, the evolution of safety cases should probably be better managed, the level of automation in safety evidence change impact analysis is low, and the state of the practice can benefit from over 20 improvement areas.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2553032","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7450627","Safety-critical system;safety evidence;change impact analysis;state of the practice;survey research","Safety;Market research;Best practices;Industries;Standards;Certification","safety-critical software;software standards","industrial survey;safety evidence change impact analysis;SECIA;safety-critical system;safety standard","","16","","71","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis of concurrency-coherency control protocols for distributed transaction processing systems with regional locality","B. Ciciani; D. M. Dias; P. S. Yu","Dept. of Comput. Sci., Rome Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","1992","18","10","899","914","A system structure and protocols for improving the performance of a distributed transaction processing system when there is some regional locality of data reference are presented. A distributed computer system is maintained at each region, and a central computer system with a replication of all databases at the distributed sites is introduced. It provides the advantage of distributed systems principally for local transactions, and has the advantage of centralized systems for transactions accessing nonlocal data. Specialized protocols keep the copies at the distributed and centralized systems consistent without incurring the overhead and delay of generalized protocols for fully replicated databases. The advantages achievable through this system structure and the tradeoffs between protocols for concurrency and coherency control of the duplicate copies of the databases are studied. An approximate analytic model is used to estimate the system performance. It is found that the performance is sensitive to the protocol and that substantial performance improvement can be obtained as compared with distributed systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.163606","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=163606","","Distributed control;Access protocols;Distributed computing;Distributed databases;Transaction databases;Delay;Concurrent computing;Control systems;Performance analysis;System performance","concurrency control;distributed databases;distributed processing;performance evaluation;protocols;transaction processing","concurrency-coherency control protocols;distributed transaction processing systems;performance;databases;delay;fully replicated databases;duplicate copies;approximate analytic model","","16","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Pert: The Application-Aware Tailoring of Java Object Persistence","P. Liu; C. Zhang","Hong Kong University of Science and Technology, Hong Kong; Hong Kong University of Science and Technology, Hong Kong","IEEE Transactions on Software Engineering","","2012","38","4","909","922","Persistence is a widely used technique which allows the objects that represent the results of lengthy computations to outlive the process that creates it in order to considerably speed up subsequent program executions. We observe that conventional persistence techniques usually do not consider the application contexts of the persistence operations, where not all of the object states need to be persisted. Leveraging this observation, we have designed and implemented a framework called Pert, which first performs static program analysis to estimate the actual usage of the persisted object, given the context of its usage in the program. The Pert runtime uses the statically computed information to efficiently make tailoring decisions to prune the redundant and unused object states during the persistence operations. Our evaluation result shows that the Pert-based optimization can speed up the conventional persistence operations by 1 to 45 times. The amount of persisted data is also dramatically reduced, as the result of the application-aware tailoring.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5963692","Object persistence;program analysis;performance optimization","Runtime;Anodes;Optimization;Context;Libraries;Java;Algorithm design and analysis","Java;optimisation","application aware tailoring;Java object persistence;lengthy computations;subsequent program executions;persistence techniques;static program analysis;Pert based optimization","","","","20","","","","","","IEEE","IEEE Journals & Magazines"
"On parallelization of static scheduling algorithms","M. -. Wu; W. Shu","Dept. of Electr. & Comput. Eng., Central Florida Univ., Orlando, FL, USA; NA","IEEE Transactions on Software Engineering","","1997","23","8","517","528","Most static algorithms that schedule parallel programs represented by macro dataflow graphs are sequential. This paper discusses the essential issues pertaining to parallelization of static scheduling and presents two efficient parallel scheduling algorithms. The proposed algorithms have been implemented on an Intel Paragon machine and their performances have been evaluated. These algorithms produce high-quality scheduling and are much faster than existing sequential and parallel algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/32.624307","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=624307","","Scheduling algorithm;Processor scheduling;Parallel algorithms;Concurrent computing;Performance evaluation;NP-complete problem;Scalability;Costs;Computational efficiency","processor scheduling;parallel algorithms;parallel programming;data flow graphs;software performance evaluation","static scheduling algorithms;algorithm parallelization;parallel program scheduling;macro dataflow graphs;parallel scheduling algorithms;Intel Paragon machine;performance evaluation;modified critical-path algorithm","","4","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Persistent caching: an implementation technique for complex objects with object identity","K. Kato; T. Masuda","Dept. of Inf. Sci., Tokyo Univ., Japan; Dept. of Inf. Sci., Tokyo Univ., Japan","IEEE Transactions on Software Engineering","","1992","18","7","631","645","Many recent complex object database systems support the concepts of object identity and object identifier. Following an object identifier to access the referenced object is called navigation operation and is an essential operation in dealing with complex objects. Navigation operation is a difficult operation to implement efficiently since every navigation operation inherently causes one disk access operation. A scheme to notably accelerate the navigation operation among a sea of complex objects, by increasing the effective number of objects in one disk page is proposed. The main concept of the presented technique is threefold. The first idea is to store a cached value within a complex object that is referencing another complex object. The second is that when the referenced object is to be updated the update propagation is delayed until the time when the cached value is referenced. The third is to utilize a hashed table on main memory to efficiently validate the consistency between the cached values and the original values.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.148481","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=148481","","Navigation;Data models;Knowledge management;Office automation;Computer aided manufacturing;CADCAM;Computer aided software engineering;Computer languages;Knowledge engineering;Identity management systems","buffer storage;database management systems;file organisation","persistent caching;implementation technique;complex objects;object identity;complex object database systems;object identifier;navigation operation;update propagation;hashed table","","9","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Metamorphic Testing of RESTful Web APIs","S. Segura; J. A. Parejo; J. Troya; A. Ruiz-Cortés","Department of Computer Languages and Systems, Universidad de Sevilla, Sevilla, Spain; Department of Computer Languages and Systems, Universidad de Sevilla, Sevilla, Spain; Department of Computer Languages and Systems, Universidad de Sevilla, Sevilla, Spain; Department of Computer Languages and Systems, Universidad de Sevilla, Sevilla, Spain","IEEE Transactions on Software Engineering","","2018","44","11","1083","1099","Web Application Programming Interfaces (APIs) allow systems to interact with each other over the network. Modern Web APIs often adhere to the REST architectural style, being referred to as RESTful Web APIs. RESTful Web APIs are decomposed into multiple resources (e.g., a video in the YouTube API) that clients can manipulate through HTTP interactions. Testing Web APIs is critical but challenging due to the difficulty to assess the correctness of API responses, i.e., the oracle problem. Metamorphic testing alleviates the oracle problem by exploiting relations (so-called metamorphic relations) among multiple executions of the program under test. In this paper, we present a metamorphic testing approach for the detection of faults in RESTful Web APIs. We first propose six abstract relations that capture the shape of many of the metamorphic relations found in RESTful Web APIs, we call these Metamorphic Relation Output Patterns (MROPs). Each MROP can then be instantiated into one or more concrete metamorphic relations. The approach was evaluated using both automatically seeded and real faults in six subject Web APIs. Among other results, we identified 60 metamorphic relations (instances of the proposed MROPs) in the Web APIs of Spotify and YouTube. Each metamorphic relation was implemented using both random and manual test data, running over 4.7K automated tests. As a result, 11 issues were detected (3 in Spotify and 8 in YouTube), 10 of them confirmed by the API developers or reproduced by other users, supporting the effectiveness of the approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2764464","European Commission (FEDER) and Spanish Government; Andalusian Government project COPAS; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8074764","Metamorphic testing;REST;RESTful Web services;web API","Testing;YouTube;Web services;Companies;Standards;Manuals;Indexes","application program interfaces;hypermedia;program testing;social networking (online);transport protocols;Web services","metamorphic relation output pattern;web application programming interfaces;RESTful web API;REST architectural style;HTTP interaction;oracle problem;abstract relation;MROP;YouTube;Spotify","","","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps","K. P. Moran; C. Bernal-C&#x00E1;rdenas; M. Curcio; R. Bonett; D. Poshyvanyk","Computer Science, College of William &amp; Mary, Williamsburg, Virginia United States 23185 (e-mail: kpmoran@cs.wm.edu); Computer Science, College of William and Mary, 8604 Williamsburg, Virginia United States (e-mail: cebernal@cs.wm.edu); Computer Science, College of William &amp; Mary, Williamsburg, Virginia United States (e-mail: mjcurcio@email.wm.edu); Computer Science, College of William &amp; Mary, Williamsburg, Virginia United States (e-mail: rfbonett@email.wm.edu); Computer Science, William and Mary, Williamsburg, Virginia United States 23188 (e-mail: denys@cs.wm.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application&#x0027;s inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled. We implemented this approach for Android in a system called ReDraw. Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91% and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw&#x0027;s potential to improve real development workflows.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2844788","Division of Computing and Communication Foundations; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8374985","GUI;CNN;Mobile;Prototyping;Machine-Learning;Mining Software Repositories","Graphical user interfaces;Software;Task analysis;Prototypes;Metadata;Androids;Humanoid robots","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"Data dependency graphs for Ada programs","L. E. Moser","Dept. of Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA","IEEE Transactions on Software Engineering","","1990","16","5","498","509","A compositional method of constructing data dependency graphs for Ada programs is presented. These graphs are useful in a program development environment for analyzing data dependencies and tracking information flow within a program. Graphs for primitive program statements are combined together to form graphs for larger program units. Composition rules are described for iteration, recursion, exception handling, and tasking, as well as for simpler Ada constructs. The correctness of the construction and the practicality of the technique are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.52773","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=52773","","Data analysis;Information analysis;Software reliability;Data security;Information security;Modems;Computer languages;Data structures;Performance analysis;National security","Ada;data structures;programming","composition rules;Ada programs;data dependency graphs;program development environment;tracking;information flow;primitive program statements;iteration;recursion;exception handling;tasking;correctness","","9","","19","","","","","","IEEE","IEEE Journals & Magazines"
"On some reliability estimation problems in random and partition testing","M. Z. Tsoukalas; J. W. Duran; S. C. Ntafos","Comput. Sci. Program, Univ. of Texas at Dallas, Richardson, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","1993","19","7","687","697","Studies have shown that random testing can be an effective testing strategy. One of the goals of testing is to estimate the reliability of the program from the test outcomes. The authors extend the Thayer-Lipow-Nelson reliability model (R. Thayer et al., 1978) to account for the cost of errors. They also compare random testing with partition testing by examining upper confidence bounds for the cost weighted performance of the two strategies.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.238569","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=238569","","Costs;Automatic testing;Software testing;Input variables;Documentation;Design methodology;Software performance;Programming;Computer science;Runtime","program testing;software reliability","random testing;effective testing strategy;test outcomes;Thayer-Lipow-Nelson reliability model;partition testing;upper confidence bounds;cost weighted performance","","48","","16","","","","","","IEEE","IEEE Journals & Magazines"
"CTK: configurable object abstractions for multiprocessors","D. M. Silva; K. Schwan; G. Eisenhauer","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","6","531","549","The Configuration Toolkit (CTK) is a library for constructing configurable object based abstractions that are part of multiprocessor programs or operating systems. The library is unique in its exploration of runtime configuration for attaining performance improvements: 1) its programming model facilitates the expression and implementation of program configuration; and 2) its efficient runtime support enables performance improvements by the configuration of program components during their execution. Program configuration is attained without compromising the encapsulation or the reuse of software abstractions. CTK programs are configured using attributes associated with object classes, object instances, state variables, operations, and object invocations. At runtime, such attributes are interpreted by policy classes, which may be varied separately from the abstractions with which they are associated. Using policies and attributes, an object's runtime behavior may be varied by: 1) changing its performance or reliability while preserving the implementation of its functional behavior, or 2) changing the implementation of its internal computational strategy. CTK's multiprocessor implementation is layered on a Cthreads-compatible programming library, which results in its portability to a wide variety of uni- and multiprocessor machines, including a Kendall Square KSR-2 Supercomputer, SGI machines, various SUN workstations, and as a native kernel on the GP1000 BBN Butterfly multiprocessor. The platforms evaluated in the paper are the KSR and SGI machines.","0098-5589;1939-3520;2326-3881","","10.1109/32.926175","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=926175","","Operating systems;Quality of service;Contracts;Runtime library;Dynamic programming;Resource management;Performance gain;Encapsulation;Supercomputers;Sun","bibliographies;configuration management;object-oriented programming;multiprocessing programs;software performance evaluation;software reusability;software libraries;multiprocessing systems","configurable object abstractions;operating systems;Configuration Toolkit;CTK library;multiprocessor programs;runtime configuration;performance improvements;programming model;program configuration;runtime support;program component configuration;software abstractions;CTK programs;object classes;object instances;state variables;object invocations;policy classes;runtime behavior;functional behavior;internal computational strategy;multiprocessor implementation;Cthreads-compatible programming library;multiprocessor machines;Kendall Square KSR-2 Supercomputer;SGI machines;SUN workstations;native kernel","","","","57","","","","","","IEEE","IEEE Journals & Magazines"
"A model for secure protocols and their compositions","N. Heintze; J. D. Tygar","AT&T Bell Labs., Murray Hill, NJ, USA; NA","IEEE Transactions on Software Engineering","","1996","22","1","16","30","The paper develops a foundation for reasoning about protocol security. We adopt a model-based approach for defining protocol security properties. This allows us to describe security properties in greater detail and precision than previous frameworks. Our model allows us to reason about the security of protocols, and considers issues of beliefs of agents, time, and secrecy. We prove a composition theorem which allows us to state sufficient conditions on two secure protocols A and B such that they may be combined to form a new secure protocol C. Moreover, we give counter-examples to show that when the conditions are not met, the protocol C may not be secure.","0098-5589;1939-3520;2326-3881","","10.1109/32.481514","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=481514","","Cryptographic protocols;Sufficient conditions;Computer security;Logic;Authentication;Clocks;Cryptography;Concrete;Concatenated codes;Computer science","protocols;inference mechanisms;software agents;knowledge based systems;belief maintenance;theorem proving;message authentication;cryptography;distributed processing","protocol security reasoning;secure protocol compositions;secure protocol model;protocol security properties;agent beliefs;time;secrecy;composition theorem proving","","26","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editors' prologue special issue on software design methods","G. D. Bergland; P. Zave","Department of Digital Systems Research in Murray Hill, NJ; SIGSOFT and ACM Computing Surveys","IEEE Transactions on Software Engineering","","1986","SE-12","2","185","191","We describe why these papers were chosen, and categorize them in terms of major contribution, underlying model of the software life-cycle, and applicability to various types of system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312934","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312934","","Object oriented modeling;Documentation;Software engineering;Software design;Tutorials;Computers","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Clarifying some fundamental concepts in software testing","A. S. Parrish; S. H. Zweben","Dept. of Comput. Sci., Alabama Univ., Tuscaloosa, AL, USA; NA","IEEE Transactions on Software Engineering","","1993","19","7","742","746","A software test data adequacy criterion is a means for determining whether a test set is sufficient, or adequate, for testing a given program. A set of properties that useful adequacy criteria should satisfy have been previously proposed (E. Weyuker, 1986; 1988). The authors identify some additional properties of useful adequacy criteria that are appropriate under certain realistic models of testing. They discuss modifications to the formal definitions of certain popular adequacy criteria to make the criteria consistent with these additional properties.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.238573","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=238573","","Software testing;System testing;Computer science;Information science;Performance analysis","formal verification;program testing","software testing;software test data adequacy criterion;test set;useful adequacy criteria;realistic models;formal definitions","","10","","13","","","","","","IEEE","IEEE Journals & Magazines"
"QoS Assurance for Dynamic Reconfiguration of Component-Based Software Systems","W. Li","Central Quneensland University, Rockhampton","IEEE Transactions on Software Engineering","","2012","38","3","658","676","A major challenge of dynamic reconfiguration is Quality of Service (QoS) assurance, which is meant to reduce application disruption to the minimum for the system's transformation. However, this problem has not been well studied. This paper investigates the problem for component-based software systems from three points of view. First, the whole spectrum of QoS characteristics is defined. Second, the logical and physical requirements for QoS characteristics are analyzed and solutions to achieve them are proposed. Third, prior work is classified by QoS characteristics and then realized by abstract reconfiguration strategies. On this basis, quantitative evaluation of the QoS assurance abilities of existing work and our own approach is conducted through three steps. First, a proof-of-concept prototype called the reconfigurable component model is implemented to support the representation and testing of the reconfiguration strategies. Second, a reconfiguration benchmark is proposed to expose the whole spectrum of QoS problems. Third, each reconfiguration strategy is tested against the benchmark and the testing results are evaluated. The most important conclusion from our investigation is that the classified QoS characteristics can be fully achieved under some acceptable constraints.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5740932","Change management;componentware;dynamic reconfiguration;modeling the QoS assurance process;system evolution.","Quality of service;Encryption;Protocols;Connectors;Receivers;Benchmark testing","quality of service;software quality","QoS assurance;dynamic reconfiguration;component based software systems;Quality of Service;application disruption;physical requirements;logical requirements;abstract reconfiguration;quantitative evaluation;reconfigurable component;reconfiguration benchmark","","19","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Aspect-Oriented Refactoring of Legacy Applications: An Evaluation","M. Mortensen; S. Ghosh; J. Bieman","Google, Boulder; Colorado State University, Fort Collins; Colorado State University, Fort Collins","IEEE Transactions on Software Engineering","","2012","38","1","118","140","The primary claimed benefits of aspect-oriented programming (AOP) are that it improves the understandability and maintainability of software applications by modularizing crosscutting concerns. Before there is widespread adoption of AOP, developers need further evidence of the actual benefits as well as costs. Applying AOP techniques to refactor legacy applications is one way to evaluate costs and benefits. We replace crosscutting concerns with aspects in three industrial applications to examine the effects on qualities that affect the maintainability of the applications. We study several revisions of each application, identifying crosscutting concerns in the initial revision and also crosscutting concerns that are added in later revisions. Aspect-oriented refactoring reduced code size and improved both change locality and concern diffusion. Costs include the effort required for application refactoring and aspect creation, as well as a decrease in performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.109","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5661792","Aspect-oriented programming;crosscutting concerns;legacy systems;refactoring;maintainability.","Software measurement;Maintenance engineering;Legacy systems;Java;Programming;Aspect-oriented programming","aspect-oriented programming;software maintenance","aspect-oriented refactoring;legacy applications;primary claimed benefits;aspect-oriented programming;software understandability;software maintainability;crosscutting concerns;AOP techniques;cost evaluation;benefits evaluation;code size;change locality;concern diffusion;application refactoring;aspect creation","","13","","37","","","","","","IEEE","IEEE Journals & Magazines"
"A Rigorous Framework for Specification, Analysis and Enforcement of Access Control Policies","A. Margheri; M. Masi; R. Pugliese; F. Tiezzi","University of Southampton, Southampton, United Kingdom; Tiani “Spirit” GmbH, Wien, Austria; Dipartimento di Statistica, Università degli Studi di Firenze, Firenze, Italy; Università di Camerino, Camerino, Italy","IEEE Transactions on Software Engineering","","2019","45","1","2","33","Access control systems are widely used means for the protection of computing systems. They are defined in terms of access control policies regulating the access to system resources. In this paper, we introduce a formally-defined, fully-implemented framework for specification, analysis and enforcement of attribute-based access control policies. The framework rests on FACPL, a language with a compact, yet expressive, syntax for specification of real-world access control policies and with a rigorously defined denotational semantics. The framework enables the automated verification of properties regarding both the authorisations enforced by single policies and the relationships among multiple policies. Effectiveness and performance of the analysis rely on a semantic-preserving representation of FACPL policies in terms of SMT formulae and on the use of efficient SMT solvers. Our analysis approach explicitly addresses some crucial aspects of policy evaluation, such as missing attributes, erroneous values and obligations, which are instead overlooked in other proposals. The framework is supported by Java-based tools, among which an Eclipse-based IDE offering a tailored development and analysis environment for FACPL policies and a Java library for policy enforcement. We illustrate the framework and its formal ingredients by means of an e-Health case study, while its effectiveness is assessed by means of performance stress tests and experiments on a well-established benchmark.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2765640","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8081817","Attribute-based access control;policy languages;policy analysis;SMT","Semantics;Authorization;Tools;Syntactics;Proposals;Java","authorisation;Java;programming language semantics","access control systems;attribute-based access control policies;real-world access control policies;rigorously defined denotational semantics;FACPL policies;authorisations;semantic-preserving representation;SMT formulae;SMT solvers;Java-based tools;Eclipse-based IDE","","1","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Counterexample Generation in Probabilistic Model Checking","T. Han; J. Katoen; D. Berteun","RWTH Aachen University, Aachen and University of Twente, Enschede; RWTH Aachen University, Aachen and University of Twente, Enschede; RWTH Aachen University, Aachen and University of Twente, Enschede","IEEE Transactions on Software Engineering","","2009","35","2","241","257","Providing evidence for the refutation of a property is an essential, if not the most important, feature of model checking. This paper considers algorithms for counterexample generation for probabilistic CTL formulae in discrete-time Markov chains. Finding the strongest evidence (i.e., the most probable path) violating a (bounded) until-formula is shown to be reducible to a single-source (hop-constrained) shortest path problem. Counterexamples of smallest size that deviate most from the required probability bound can be obtained by applying (small amendments to) k-shortest (hop-constrained) paths algorithms. These results can be extended to Markov chains with rewards, to LTL model checking, and are useful for Markov decision processes. Experimental results show that typically the size of a counterexample is excessive. To obtain much more compact representations, we present a simple algorithm to generate (minimal) regular expressions that can act as counterexamples. The feasibility of our approach is illustrated by means of two communication protocols: leader election in an anonymous ring network and the Crowds protocol.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.5","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4770111","Model checking;Diagnostics","Protocols;Logic;Biological system modeling;Quantum computing;Feedback;Biology computing;Distributed computing;Computer Society;Shortest path problem;Nominations and elections","decision theory;formal verification;Markov processes;probabilistic logic;probability;temporal logic;trees (mathematics)","counterexample generation;probabilistic model checking;property refutation;discrete-time Markov chain;single-source shortest path problem;k-shortest path algorithm;Markov decision process;linear temporal logic;computation tree logic","","38","","61","","","","","","IEEE","IEEE Journals & Magazines"
"Platform-Independent Dynamic Taint Analysis for JavaScript","R. Karim; F. Tip; A. Sochurkova; K. Sen","CSIC, Samsung Research America, Mountain View, California United States (e-mail: rezwana.k@samsung.com); College of Computer and Information Science, Northeastern University, Boston, Massachusetts United States 02115-5005 (e-mail: f.tip@northeastern.edu); N/A, Avast, Prague, N/A Czech Republic (e-mail: sochuale@fit.cvut.cz); EECS, University of California Berkeley, Berkeley, California United States (e-mail: ksen@berkeley.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Previous approaches to dynamic taint analysis for JavaScript are implemented directly in a browser or JavaScript engine, limiting their applicability to a single platform and requiring ongoing maintenance as platforms evolve, or they require nontrivial program transformations. We present an approach that relies on instrumentation to encode taint propagation as instructions for an abstract machine. Our approach has two key advantages: it is platform-independent and can be used with any existing JavaScript engine, and it can track taint on primitive values without requiring the introduction of wrapper objects. Furthermore, our technique enables multiple deployment scenarios by varying when and where the generated instructions are executed and it supports indirect taint sources, i.e., situations where taint enters an application via arguments passed to dynamically registered event-listener functions. We implemented the technique for the ECMAScript 5 language in a tool called Ichnaea, and evaluated it on 22 NPM modules containing several types of injection vulnerabilities, including 4 modules containing vulnerabilities that were not previously discovered and reported. On these modules, run-time overheads range from 3.17x to 38.42x, which is significantly better than a previous transformation-based technique. We also report on a case study that shows how Ichnaea can be used to detect privacy leaks in a Tizen web application for the Samsung Gear S2 smart watch.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2878020","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8511058","Taint analysis;dynamic analysis;JavaScript;platform-independent;instrumentation","Instruments;Engines;Browsers;Tools;Privacy;Gears;Data privacy","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Methodology for Business System Development","R. N. Mathur","Computer Sciences Corporation","IEEE Transactions on Software Engineering","","1987","SE-13","5","593","601","The methodology presented here is currently being used in the design of Standard Automated Financial System (STAFS), which is a large scale business system for use by the 14 Naval Laboratories across the nation. The system will be operational in 1987-1988 time-frame. The methodology presented is suitable for all transaction oriented systems. It enforces documentation of design at all levels of system development process allowing managers and users a high visibility into the design. It highlights the human engineering aspects of system design and utilizes thread definition, data designer, and structured design techniques. The methodology requires the use of system verification diagrams which specify the user requirements. The design produces an integrated database with security mechanisms to restrict unauthorized users from accessing data they are not authorized to use. The threads also serve as a useful tool for system testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233464","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702259","Decomposition;design;methodology;structured design;testing;threads;transaction","Transaction databases;System analysis and design;Documentation;Design methodology;Large-scale systems;Laboratories;Ergonomics;Data security;System testing","","Decomposition;design;methodology;structured design;testing;threads;transaction","","3","","2","","","","","","IEEE","IEEE Journals & Magazines"
"A Controlled Experiment for Evaluating the Impact of Coupling on the Maintainability of Service-Oriented Software","M. Perepletchikov; C. Ryan","RMIT University, Melbourne; RMIT University, Melbourne","IEEE Transactions on Software Engineering","","2011","37","4","449","465","One of the goals of Service-Oriented Computing (SOC) is to improve software maintainability as businesses become more agile, and thus underlying processes and rules change more frequently. This paper presents a controlled experiment examining the relationship between coupling in service-oriented designs, as measured using a recently proposed suite of SOC-specific coupling metrics and software maintainability in terms of the specific subcharacteristics of analyzability, changeability, and stability. The results indicate a statistically significant causal relationship between the investigated coupling metrics and the maintainability of service-oriented software. As such, the investigated metrics can facilitate coupling related design decisions with the aim of producing more maintainable service-oriented software products.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.61","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5482590","Services systems;design concepts;maintainability;product metrics;empirical studies.","Software maintenance;Programming;Software measurement;Logic;Software design;Stability analysis;Product design;Application software;Costs;Software metrics","service-oriented architecture;software maintenance;software metrics","software maintainability improvement;service-oriented computing;service-oriented designs;specific coupling metrics;specific subcharacteristics;statistically significant causal relationship;service-oriented software products","","21","","48","","","","","","IEEE","IEEE Journals & Magazines"
"System Structure Analysis: Clustering with Data Bindings","D. H. Hutchens; V. R. Basili","Department of Computer Science, Clemson University; NA","IEEE Transactions on Software Engineering","","1985","SE-11","8","749","757","This paper examines the use of cluster analysis as a tool for system modularization. Several clustering techniques are discussed and used on two medium-size systems and a group of small projects. The small projects are presented because they provide examples (that will fit into a paper) of certain types of phenomena. Data bindings between the routines of the system provide the basis for the bindings. It appears that the clustering of data bindings provides a meaningful view of system modularization.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232524","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702084","Cluster;coupling;data binding;module;measurement;system structure","Data analysis;Computer science;Documentation;Military computing;Proposals;Computerized monitoring;Fluid flow measurement;Stability analysis","","Cluster;coupling;data binding;module;measurement;system structure","","160","","18","","","","","","IEEE","IEEE Journals & Magazines"
"MADMatch: Many-to-Many Approximate Diagram Matching for Design Comparison","S. Kpodjedo; F. Ricca; P. Galinier; G. Antoniol; Y. Guéhéneuc","Ecole Polytechnique de Montreal, Montreal; Università di Genova, Genova; Ecole Polytechnique de Montreal, Montreal; Ecole Polytechnique de Montreal, Montreal; Ecole Polytechnique de Montreal, Montreal","IEEE Transactions on Software Engineering","","2013","39","8","1090","1111","Matching algorithms play a fundamental role in many important but difficult software engineering activities, especially design evolution analysis and model comparison. We present MADMatch, a fast and scalable many-to-many approximate diagram matching approach based on an error-tolerant graph matching (ETGM) formulation. Diagrams are represented as graphs, costs are assigned to possible differences between two given graphs, and the goal is to retrieve the cheapest matching. We address the resulting optimization problem with a tabu search enhanced by the novel use of lexical and structural information. Through several case studies with different types of diagrams and tasks, we show that our generic approach obtains better results than dedicated state-of-the-art algorithms, such as AURA, PLTSDiff, or UMLDiff, on the exact same datasets used to introduce (and evaluate) these algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6464271","Diagram differencing;search-based software engineering;approximate graph matching;identifier splitting","Unified modeling language;Algorithm design and analysis;Software;Scalability;Software algorithms;Software engineering;Optimization","graph theory;optimisation;search problems;software engineering","MADMatch approach;many-to-many approximate diagram matching approach;error-tolerant graph matching;ETGM;software engineering;design evolution analysis;model comparison;design comparison;optimization problem;tabu search;lexical information;structural information;AURA algorithm;PLTSDiff algorithm;UMLDiff algorithm","","6","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Requirements development in scenario-based design","J. M. Carroll; M. B. Rosson; G. Chin; J. Koenemann","Dept. of Comput. Sci., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","12","1156","1170","We describe and analyze the process of requirements development in scenario based design through consideration of a case study. In our project, a group of teachers and system developers initially set out to create a virtual physics laboratory. Our design work centered on the collaborative development of a series of scenarios describing current and future classroom activities. We observed classroom scenarios to assess needs and opportunities, and envisioned future scenarios to specify and analyze possible design moves. We employed claims analysis to evaluate design trade-offs implicit in these scenarios, to codify the specific advantages and disadvantages in achieving requirements. Through the course of this process, the nature of our project requirements has evolved, providing more information but also more kinds of information. We discuss the utility of managing requirements development through an evolving set of scenarios, and the generality of the scenario stages from this case study.","0098-5589;1939-3520;2326-3881","","10.1109/32.738344","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=738344","","Collaborative work;Physics;Laboratories;Design engineering;Writing;Humans;Vocabulary;Computer science","formal specification;systems analysis;physics computing;courseware;virtual reality;user interfaces","requirements development;scenario based design;case study;teachers;system developers;virtual physics laboratory;collaborative development;future classroom activities;classroom scenarios;future scenarios;claims analysis;design trade-offs;project requirements;evolving set;scenario stages","","42","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Searching for points-to analysis","G. Bruns; S. Chandra","AT&T, Lisle, IL, USA; NA","IEEE Transactions on Software Engineering","","2003","29","10","883","897","The points-to analysis problem is to find the pointer relationships that could arise during program execution. Many points-to analysis algorithms exist, each making a particular trade off between cost of the analysis and precision of the results. In this paper, we show how points-to analysis algorithms can be defined as transformed versions of an exact algorithm. We present a set of program transformations over a general program model and use them to define some existing points-to analysis algorithms. Doing so makes explicit the approximations involved in these algorithms. We also show how the transformations can be used to define new points-to analysis algorithms. Our transformations are generic and may be useful in the design of other program analysis algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237170","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237170","","Algorithm design and analysis;Reachability analysis;Costs;Iterative algorithms;Program processors;Optimizing compilers;Data analysis;Logic programming;Approximation algorithms;Merging","reachability analysis;program diagnostics","model checking;reachability analysis;program analysis;points-to analysis;pointer relationships;program execution","","1","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Distributed database management model and validation","M. Tsuchiya; M. P. Mariani; J. D. Brom","TRW Defense Systems Group, Redondo Beach, CA 90278; TRW Defense Systems Group, Colorado Springs, CO 80916; TRW Defense Systems Group, Colorado Springs, CO 80916","IEEE Transactions on Software Engineering","","1986","SE-12","4","511","520","The authors describe a simple, yet effective, distributed database model that simulates database usage and buffer management in the distributed data processing environment. The model is table driven such that database access requirements, file location, and other information defining the database environment are set up internally in several tables, and linked lists represent the directory and data blocks. Each database transaction is defined and represented by a transaction flow diagram (TFD), and a sequence of TFDs representing an operational scenario is input to the model. The model `executes' input TFDs by looking up tables, and performs buffer management for directory and file data while logging history and gathering various statistics on database usage and buffer management. The performance data are used for database access overhead measurement, database workload characterization, and buffer allocation. Disk access frequency and response time are used to validate the simulation results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312898","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312898","Buffer management;directory;disk access;distributed database management;performance;transaction flow diagram","Distributed databases;Data models;Space vehicles;Analytical models;Aerospace electronics;Time factors","database management systems;database theory;distributed processing","validation;distributed database model;database usage;buffer management;database access requirements;file location;database transaction;transaction flow diagram;database access overhead measurement;workload characterization;buffer allocation","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"The evolution support environment system","C. V. Ramamoorthy; Y. Usuda; A. Prakash; W. T. Tsai","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA","IEEE Transactions on Software Engineering","","1990","16","11","1225","1234","The evolution support environment (ESE) system, which provides a framework for capturing and making available semantic information about software components of an evolving software system, is described. The goal in the design of the ESE system was to provide integrated support for management of software architecture configuration, life-cycle configuration, and version control. Software architecture configuration management allows tracking of interconnections among software components that make up a system. Life-cycle management allows traceability among specifications, design, code, and test cases during software development. Adding version control allows specific versions of software objects and their associated objects, such as specifications and test cases, to be retrieved. The authors' experience with the use of the system is discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60311","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60311","","Software systems;Software testing;Software maintenance;Life testing;Software architecture;Control systems;Software development management;Environmental management;Programming environments;Computer bugs","programming environments","evolution support environment system;semantic information;software components;evolving software system;ESE system;integrated support;management;software architecture configuration;life-cycle configuration;version control;interconnections;traceability;specifications;design;code;test cases;software development;software objects","","13","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Sequences of Developer Interactions in Visual Studio for Usage Smells","K. Damevski; D. C. Shepherd; J. Schneider; L. Pollock","Department of Computer Science, Virginia Commonwealth University, Richmond, VA; ABB Corporate Research, Raleigh, NC; ABB Corporate Research, Baden-Dättwill, Switzerland; Department of Computer and Information Sciences, University of Delaware, Newark, DE","IEEE Transactions on Software Engineering","","2017","43","4","359","371","In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2592905","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7516714","IDE usage data;data mining;pattern mining;usability analysis","Data mining;Visualization;Usability;Data analysis;Debugging;Software engineering;Navigation","data mining;program debugging;software engineering","developer interactions sequences mining;visual studio;usage smells extraction;large-scale dataset mining;IDE interactions;frequent IDE usage pattern mining;time-ordered sequences identifies;active file navigation;code search;software development activities","","6","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Computing bounds for the performance indices of quasi-lumpable stochastic well-formed nets","G. Franceschinis; R. R. Muntz","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA; Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA","IEEE Transactions on Software Engineering","","1994","20","7","516","525","Structural symmetries in stochastic well-formed colored Petri nets (SWN's) lead to behavioral symmetries that can be exploited by using the symbolic reachability graph (SRG) construction algorithm. The SRC allows one to compute an aggregated reachability graph (RG) and a ""lumped"" continuous time Markov chain (CTMC) that contain all the information needed to study the qualitative properties and the performance of the modeled system, respectively. Some models exhibit qualitative behavioral symmetries that are not completely reflected at the CTMC level. We call them quasi-lumpable SWN models. In these cases, exact performance indices can be obtained by avoiding the aggregation of those markings that are qualitatively, but not quantitatively, equivalent. An alternative approach consists of aggregating all the qualitatively equivalent states and computing approximated performance indices. In this paper, a technique is proposed to compute bounds on the performance of SWN models of this kind, using the results we have presented elsewhere. The technique is based on the Courtois and Semal bounded aggregation method.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.297940","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=297940","","Stochastic processes;Performance analysis;Petri nets;Stochastic systems;Computer science;Roentgenium;Parametric statistics;Computational efficiency;Timing","Petri nets;stochastic processes;Markov processes;performance evaluation","performance indices;quasilumpable stochastic well-formed Nets;stochastic well-formed colored Petri nets;symbolic reachability graph;aggregated reachability graph;continuous time Markov chain;qualitative properties;quasilumpable SWN models;approximated performance indices;SWN models;bounded aggregation method","","7","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Scalar memory references in pipelined multiprocessors: a performance study","R. Ganesan; S. Weiss","Bell Atlantic, Beltsville, MD, USA; NA","IEEE Transactions on Software Engineering","","1992","18","1","78","86","Interleaved memories are essential in pipelined computers to attain high memory bandwidth. As a memory bank is accessed, a reservation is placed on the bank for the duration of the memory cycle, which is often considerably longer than the processor cycle time. This additional parameter, namely, the bank reservation time or the bank busy time, adds to the complexity of the memory model. For Markov models, exact solutions are not feasible even without this additional parameter due to the very large state space of the Markov chain. The authors develop a Markov model which explicitly tracks the bank reservation time. Because only one processor and the requested bank are modeled, the transition probabilities are not known and have to be approximated. The performance predicted by the model is in close agreement with simulation results.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.120318","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=120318","","Bandwidth;Predictive models;Computational modeling;State-space methods;Supercomputers;Performance analysis;Costs;Stochastic processes;Computer science;Vector processors","Markov processes;parallel machines;performance evaluation;pipeline processing;probability;storage management","scalar memory references;pipelined multiprocessors;pipelined computers;high memory bandwidth;memory bank;memory cycle;processor cycle time;bank reservation time;bank busy time;Markov models;state space;Markov chain;transition probabilities;simulation results","","","","29","","","","","","IEEE","IEEE Journals & Magazines"
"The dynamics of software project staffing: a system dynamics based simulation approach","T. K. Abdel-Hamid","Dept. of Adm. Sci., US Naval Postgraduate Sch., Monterey, CA, USA","IEEE Transactions on Software Engineering","","1989","15","2","109","119","The author focuses on the dynamics of software project staffing throughout the software-development lifecycle. The research vehicle is a comprehensive system-dynamics model of the software-development process. A detailed discussion of the model's structure as well as its behavior is provided. The results of a case study in which the model is used to simulate the staffing practices of an actual software project are then presented. The experiment produces some interesting insights into the policies (both explicit and implicit) for managing the human resource, and their impact on project behavior. The decision-support capability of the model to answer what-if questions is also demonstrated. In particular, the model is used to test the degree of interchangeability of men and months on the particular software project.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21738","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21738","","Vehicle dynamics;Project management;Programming;Human resource management;Art;Personnel;Costs;Vehicles;Software testing;Software engineering","DP management;personnel;software engineering","software project staffing;system dynamics;simulation approach;software-development lifecycle;research vehicle;software-development process;software project;human resource;decision-support;what-if questions","","83","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Variability in Software Systems—A Systematic Literature Review","M. Galster; D. Weyns; D. Tofan; B. Michalik; P. Avgeriou","Department of Computer Science and Software Engineering, University of Canterbury, Private Bag 4800, Christchurch, New Zealand; Department of Computer Science, Linnaeus University, Växjö, Sweden; Department of Mathematics and Computing Science, University of Groningen, Groningen 9700 AK, The Netherlands; Amartus, Poland; Department of Mathematics and Computing Science, University of Groningen, Groningen 9700 AK, The Netherlands","IEEE Transactions on Software Engineering","","2014","40","3","282","306","Context: Variability (i.e., the ability of software systems or artifacts to be adjusted for different contexts) became a key property of many systems. Objective: We analyze existing research on variability in software systems. We investigate variability handling in major software engineering phases (e.g., requirements engineering, architecting). Method: We performed a systematic literature review. A manual search covered 13 premium software engineering journals and 18 premium conferences, resulting in 15,430 papers searched and 196 papers considered for analysis. To improve reliability and to increase reproducibility, we complemented the manual search with a targeted automated search. Results: Software quality attributes have not received much attention in the context of variability. Variability is studied in all software engineering phases, but testing is underrepresented. Data to motivate the applicability of current approaches are often insufficient; research designs are vaguely described. Conclusions: Based on our findings we propose dimensions of variability in software engineering. This empirically grounded classification provides a step towards a unifying, integrated perspective of variability in software systems, spanning across disparate or loosely coupled research themes in the software engineering community. Finally, we provide recommendations to bridge the gap between research and practice and point to opportunities for future research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6682901","Variability;systematic review;software engineering","Decision support systems;Software systems;Systematics;Software engineering;Context;Manuals;Data collection","program testing;software product lines;software reliability;software reviews","software systems;variability handling;systematic literature review;manual search;software engineering journals;software reliability;reproducibility;targeted automated search;software engineering phase;software testing;software engineering community","","65","","60","","","","","","IEEE","IEEE Journals & Magazines"
"A Bayesian analysis of the logarithmic-Poisson execution time model based on expert opinion and failure data","S. Campodonico; N. D. Singpurwalla","Res. and Test Dept., Assoc. of American Railroads, Washington, DC, USA; NA","IEEE Transactions on Software Engineering","","1994","20","9","677","683","We propose a Bayesian approach for predicting the number of failures in a piece of software, using the logarithmic-Poisson model, a nonhomogeneous Poisson process (NHPP) commonly used for describing software failures. A similar approach can be applied to other forms of the NHPP. The key feature of the approach is that now we are able to use, in a formal manner, expert knowledge on software testing, as for example, published information on the empirical experiences of other researchers. This is accomplished by treating such information as expert opinion in the construction of a likelihood function which leads us to a joint distribution. The procedure is computationally intensive, but for the case of the logarithmic-Poisson model has been codified for use on a personal computer. We illustrate the working of the approach via some real live data on software testing. The aim is not to propose another model for software reliability assessment. Rather, we present a methodology that can be invoked with existing software reliability models.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.317426","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=317426","","Bayesian methods;Failure analysis;Software reliability;Predictive models;Software testing;Statistics;Microcomputers;Stochastic processes;Relays","software quality;software reliability;program testing;Bayes methods;maximum likelihood estimation","Bayesian analysis;logarithmic-Poisson execution time model;expert opinion;failure data;failure prediction;nonhomogeneous Poisson process;NHPP;software failures;expert knowledge;software testing;empirical experiences;likelihood function;joint distribution;personal computer;software reliability assessment;software reliability models","","21","","18","","","","","","IEEE","IEEE Journals & Magazines"
"The Common Ada Programming Support Environment (APSE) Interface Set (CAIS)","P. A. Oberndorf","US Naval Ocean Syst. Center, San Diego, CA, USA","IEEE Transactions on Software Engineering","","1988","14","6","742","748","The Common APSE Interface Set (CAIS) is discussed, along with its relationship to issues in the development of environments. The CAIS concepts and features are described, followed by a discussion of several ways in which the CAIS provides valuable capabilities for environment architectures and construction.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6154","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6154","","Computer aided instruction;Operating systems;Software tools;Government;Computer architecture;Software engineering;Kernel;Packaging;Defense industry","Ada;operating systems (computers);programming environments;software portability;software tools","operating system interfaces;software portability;Ada Programming Support Environment;Common APSE Interface Set;CAIS","","15","","7","","","","","","IEEE","IEEE Journals & Magazines"
"A development environment for complex distributed real-time applications","A. D. Stoyen; T. J. Marlowe; M. F. Younis; P. V. Petrov","Dept. of Comput. Sci., Nebraska Univ., Omaha, NE, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1999","25","1","50","74","Engineering of complex distributed real-time applications is one of the hardest tasks faced by the software profession today. All aspects of the process, from design to implementation, are made more difficult by the interaction of behavioral and platform constraints. Providing tools for this task is likewise not without major challenges. In this paper, we discuss a tool suite which supports the development of complex distributed real-time applications in a suitable high-level language (CRL). The suite's component tools include a compiler, a transformer-optimizer, an allocator-migrator, a schedulability analyzer, a debugger-monitor, a kernel, and a (simulated) network manager. The overall engineering approach supported by the suite is to provide as simple and natural an integrated development paradigm as possible. The suite tools address complexity due to distribution, scheduling, allocation and other sources in an integrated manner (largely) transparent to the developer. To reflect the needs of propagation of functional and nonfunctional requirements throughout the development process, a number of robust code transformation and communication mechanisms have been incorporated into the suite. To facilitate practical use of the suite, the developed programs compile-transform to a safe subset of C++ with appropriate libraries and runtime support. (In this safe subset (C++) the use of pointers is minimized. Aliases are not allowed.","0098-5589;1939-3520;2326-3881","","10.1109/32.748918","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=748918","","Real time systems;Application software;Runtime;Kernel;Resource management;Processor scheduling;Computer languages;Computer science;Computer Society;Process design","real-time systems;processor scheduling;software libraries;resource allocation;software tools;program compilers;distributed programming;program debugging;programming environments;object-oriented programming","complex distributed real-time application engineering;development environment;platform constraints;behavioral constraints;tool suite;high-level language;CRL;compiler;transformer optimizer;allocator migrator;schedulability analyzer;debugger monitor;kernel;network manager;integrated development paradigm;complexity;distribution;scheduling;allocation;functional requirements;nonfunctional requirements;robust code transformation mechanisms;robust code communication mechanisms;C++;libraries;runtime support","","1","","67","","","","","","IEEE","IEEE Journals & Magazines"
"AutoSense: A Framework for Automated Sensitivity Analysis of Program Data","B. Nongpoh; R. Ray; S. Dutta; A. Banerjee","Department of Computer Science & Engineering, National Institute of Technology Meghalaya, Shillong, India; Department of Computer Science & Engineering, National Institute of Technology Meghalaya, Shillong, India; Department of Computer Science & Engineering, Jadavpur University, Kolkata, India; Advanced Computing and Microelectronics Unit, Indian Statistical Institute, Kolkata, India","IEEE Transactions on Software Engineering","","2017","43","12","1110","1124","In recent times, approximate computing is being increasingly adopted across the computing stack, from algorithms to computing hardware, to gain energy and performance efficiency by trading accuracy within acceptable limits. Approximation aware programming languages have been proposed where programmers can annotate data with type qualifiers (e.g., precise and approx) to denote its reliability. However, programmers need to judiciously annotate so that the accuracy loss remains within the desired limits. This can be non-trivial for large applications where error resilient and non-resilient program data may not be easily identifiable. Mis-annotation of even one data as error resilient/insensitive may result in an unacceptable output. In this paper, we present AutoSense, a framework to automatically classify resilient (insensitive) program data versus the sensitive ones with probabilistic reliability guarantee. AutoSense implements a combination of dynamic and static analysis methods for data sensitivity analysis. The dynamic analysis is based on statistical hypothesis testing, while the static analysis is based on classical data flow analysis. Experimental results compare our automated data classification with reported manual annotations on popular benchmarks used in approximate computing literature. AutoSense achieves promising reliability results compared to manual annotations and earlier methods, as evident from the experimental results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2654251","National Institute of Technology Meghalaya and Visvesvaraya Ph.D. Scheme; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7820185","Approximate computing;sensitivity analysis;hypothesis testing;sequential probability ratio test","Quality of service;Sensitivity analysis;Approximate computing;Probabilistic logic;Sequential analysis","data flow analysis;pattern classification;probability;program diagnostics;program verification;sensitivity analysis;statistical analysis","approximate computing literature;AutoSense;automated sensitivity analysis;probabilistic reliability;energy efficiency;manual annotations;automated data classification;classical data flow analysis;dynamic analysis;data sensitivity analysis;static analysis;resilient program data;accuracy loss;approximation aware programming languages;trading accuracy;performance efficiency;computing hardware;computing stack","","2","","31","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Efficient database access from Prolog","S. Ceri; G. Gottlob; G. Wiederhold","Dept. of Math., Modena Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","1989","15","2","153","164","In designing the interface between a relational database and a Prolog interpreter, efficiency is a major issue. The authors present a method for loading into the memory-resident database of Prolog facts permanently stored in secondary storage. The rationale of the method is to save access to the database by never repeating the same query and by storing in main memory, in a compact and efficient way, information about the past interaction with the database. The authors discuss how to reduce subsumption rests required by the method to pattern matching in many relevant cases. They also describe a simulator of the method, which validates their approach, and they discuss the results of the simulation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21742","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21742","","Relational databases;Information retrieval;Pattern matching;Database systems;Transaction databases;Testing;Technology management;Database languages;Stress;Computer industry","logic programming;program interpreters;relational databases","Prolog;relational database;Prolog interpreter;memory-resident database;Prolog facts;secondary storage;subsumption rests;pattern matching","","16","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Representing and using nonfunctional requirements: a process-oriented approach","J. Mylopoulos; L. Chung; B. Nixon","Dept. of Comput. Sci., Toronto Univ., Ont., Canada; Dept. of Comput. Sci., Toronto Univ., Ont., Canada; Dept. of Comput. Sci., Toronto Univ., Ont., Canada","IEEE Transactions on Software Engineering","","1992","18","6","483","497","A comprehensive framework for representing and using nonfunctional requirements during the development process is proposed. The framework consists of five basic components which provide the representation of nonfunctional requirements in terms of interrelated goals. Such goals can be refined through refinement methods and can be evaluated in order to determine the degree to which a set of nonfunctional requirements is supported by a particular design. Evidence for the power of the framework is provided through the study of accuracy and performance requirements for information systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.142871","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=142871","","Software quality;Information systems;Software systems;Programming;Software engineering;Software measurement;Cost function;Power system reliability;Maintenance","formal specification;management information systems","nonfunctional requirements;process-oriented approach;development process;refinement methods;performance requirements;information systems","","391","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Statistical foundations of audit trail analysis for the detection of computer misuse","P. Helman; G. Liepins","Dept. of Comput. Sci., New Mexico Univ., Albuquerque, NM, USA; NA","IEEE Transactions on Software Engineering","","1993","19","9","886","901","We model computer transactions as generated by two stationary stochastic processes, the legitimate (normal) process N and the misuse process M. We define misuse (anomaly) detection to be the identification of transactions most likely to have been generated by M. We formally demonstrate that the accuracy of misuse detectors is bounded by a function of the difference of the densities of the processes N and M over the space of transactions. In practice, detection accuracy can be far below this bound, and generally improves with increasing sample size of historical (training) data. Careful selection of transaction attributes also can improve detection accuracy; we suggest several criteria for attribute selection, including adequate sampling rate and separation between models. We demonstrate that exactly optimizing even the simplest of these criteria is NP-hard, thus motivating a heuristic approach. We further differentiate between modeling (density estimation) and nonmodeling approaches.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.241771","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=241771","","Monitoring;Laboratories;System testing;Intrusion detection;Physics computing;Computer science;Stochastic processes;Detectors;Space stations;Sampling methods","auditing;computer crime;security of data;stochastic processes;transaction processing","audit trail analysis;computer misuse;computer transactions;stationary stochastic processes;misuse detectors;detection accuracy;transaction attributes;NP-hard;heuristic approach;density estimation;modeling;statistical foundations;system security","","55","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Improving speed and productivity of software development: a global survey of software developers","J. D. Blackburn; G. D. Scudder; L. N. Van Wassenhove","Owen Graduate Sch. of Manage., Vanderbilt Univ., Nashville, TN, USA; NA; NA","IEEE Transactions on Software Engineering","","1996","22","12","875","885","Time is an essential measure of performance in software development because time delays tend to fall directly to the bottom line. To address this issue, this research seeks to distinguish time-based software development practices: those managerial actions that result in faster development speed and higher productivity. This study is based upon a survey of software management practices in Western Europe and builds upon an earlier study we carried out in the United States and Japan (Integrated Manufacturing Systems, vol. 7, no. 2, 1996). We measure the extent to which managers in the USA, Japan and Europe differ in their management of software projects and also determine the tools, technology and practices that separate fast and slow developers in Western Europe.","0098-5589;1939-3520;2326-3881","","10.1109/32.553636","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=553636","","Productivity;Programming;Software development management;Europe;Software measurement;Project management;Technology management;Time measurement;Delay effects;Research and development management","software engineering;software development management;delays;human resource management;project management","software development speed;software development productivity;global survey;software developers;performance measurement;time delays;time-based software development practices;managerial actions;Western Europe;USA;Japan;software project management;software engineering;global performance comparisons;empirical research","","69","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A hierarchical knowledge based system for airplane classification","D. I. Moldovan; C. -. Wu","Dept. of Electr. Eng.-Syst., Univ. of Southern California, Los Angeles, CA, USA; NA","IEEE Transactions on Software Engineering","","1988","14","12","1829","1834","Airplane classification is used as an application domain to illustrate how hierarchical reasoning on large knowledge bases can be implemented. The knowledge base is organized as a two-dimensional hierarchy: one dimension corresponds to the levels of complexity often seen in computer vision, and the other dimension corresponds to the complexity of hypothesis used in the reasoning process. Reasoning proceeds top-down, from more abstract levels with fewer details toward levels with more details. Whenever possible, with the help of domain knowledge, decision is taken at a higher level, which significantly reduces processing time. A software package called RuBICS (Rule-Based Image Classification System) is described, and some examples of airplane classification are shown.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9066","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9066","","Knowledge based systems;Airplanes;Military aircraft;Air traffic control;Shape;Computer vision;Software packages;Image classification;Control system synthesis;Image recognition","aerospace computing;aircraft;computer vision;knowledge based systems;knowledge engineering","hierarchical knowledge based system;airplane classification;hierarchical reasoning;complexity;computer vision;reasoning process;RuBICS;Rule-Based Image Classification System","","5","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic Analysis for Diagnosing Integration Faults","L. Mariani; F. Pastore; M. Pezze","University of Milano Bicocca, Milan; University of Milano Bicocca, Milan; University of Milan Bicocca, Milano","IEEE Transactions on Software Engineering","","2011","37","4","486","508","Many software components are provided with incomplete specifications and little access to the source code. Reusing such gray-box components can result in integration faults that can be difficult to diagnose and locate. In this paper, we present Behavior Capture and Test (BCT), a technique that uses dynamic analysis to automatically identify the causes of failures and locate the related faults. BCT augments dynamic analysis techniques with model-based monitoring. In this way, BCT identifies a structured set of interactions and data values that are likely related to failures (failure causes), and indicates the components and the operations that are likely responsible for failures (fault locations). BCT advances scientific knowledge in several ways. It combines classic dynamic analysis with incremental finite state generation techniques to produce dynamic models that capture complementary aspects of component interactions. It uses an effective technique to filter false positives to reduce the effort of the analysis of the produced data. It defines a strategy to extract information about likely causes of failures by automatically ranking and relating the detected anomalies so that developers can focus their attention on the faults. The effectiveness of BCT depends on the quality of the dynamic models extracted from the program. BCT is particularly effective when the test cases sample the execution space well. In this paper, we present a set of case studies that illustrate the adequacy of BCT to analyze both regression testing failures and rare field failures. The results show that BCT automatically filters out most of the false alarms and provides useful information to understand the causes of failures in 69 percent of the case studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.93","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5611554","Dynamic Analysis;diagnosis;fault localization;false positive filters;regression failure analysis;field failure analysis.","Automata;Monitoring;Analytical models;Engines;Software;Testing;Inference algorithms","fault diagnosis;object-oriented programming;program testing;software fault tolerance","software components;Integration Faults;behavior capture and test technique;BCT;dynamic analysis;model-based monitoring;incremental finite state generation techniques","","33","","71","","","","","","IEEE","IEEE Journals & Magazines"
"Response-time bounds of EQL rule-based programs under rule priority structure","Rwo-Hsi Wang; A. K. Mok","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; Dept. of Comput. Sci., Texas Univ., Austin, TX, USA","IEEE Transactions on Software Engineering","","1995","21","7","605","614","A key index of the performance of a rule based program used in real time monitoring and control is its response time, defined by the longest program execution time before a fixed point of the program is reached from a start state. Previous work in computing the response time bounds for rule based programs effectively assumes that all rules take the same amount of firing time. It is also assumed that if two rules are enabled, then either one of them may be scheduled first for firing. These assumptions can result in loose bounds, especially in the case programmers choose to impose a priority structure on the set of rules. We remove the uniform firing cost assumption and discuss how to get tighter bounds by taking rule priority information into account. We show that the rule suppression relation we previously introduced can be extended to incorporate rule priority information. A bound derivation algorithm for programs whose potential trigger relations satisfy an acyclicity condition is presented, followed by its correctness proof and an analysis example.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.392981","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=392981","","Delay;Monitoring;Algorithm design and analysis;Processor scheduling;Knowledge based systems;Timing;Time factors;Artificial intelligence;Equations;State-space methods","knowledge based systems;logic programming;real-time systems;software performance evaluation;program verification;decision support systems","response-time bounds;EQL rule based programs;EQL rule-based programs;rule priority structure;rule based program;real time monitoring;program execution time;response time bounds;priority structure;uniform firing cost assumption;rule priority information;rule suppression relation;bound derivation algorithm;potential trigger relations;acyclicity condition;correctness proof;real time decision systems;timing analysis;priority;equational rule based program","","2","","12","","","","","","IEEE","IEEE Journals & Magazines"
"A specificational approach to high level program monitoring and measuring","Y. Liao; D. Cohen","NEC Systems Lab., Princeton, NJ, USA; NA","IEEE Transactions on Software Engineering","","1992","18","11","969","978","Program monitoring and measuring is the activity of collecting information about the execution characteristics of a program. Although this activity is occasionally supported by special-purpose hardware, it is normally done by adding instrumentation code to the program so that it collects interesting data as it runs. Unfortunately, this alteration is itself a difficult task involving all the complexities of programming. Given some questions to be answered, the programmer must determine what data must be collected, determine where in the program those data can be collected, and add code to the program to collect that data and to process it to produce the desired results. The goal of the work described is to automate the process. A high-level program monitoring and measuring system is presented. The system provides a high-level specification language to let programmers specify what they want to know about their program's execution. It automatically generates an augmented program whose execution produces both the results of the original program and answers to the specified questions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.177366","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=177366","","Monitoring;Instruments;Programming profession;Costs;Hardware;Specification languages;Debugging;Animation;Current measurement;Time measurement","automatic programming;formal specification;software metrics;specification languages;system monitoring","program measurement;specificational approach;high level program monitoring;execution characteristics;special-purpose hardware;instrumentation code;complexities;high-level specification language;augmented program","","15","","21","","","","","","IEEE","IEEE Journals & Magazines"
"A Markov chain model for statistical software testing","J. A. Whittaker; M. G. Thomason","Software Eng. Technol. Inc., Knoxville, TN, USA; NA","IEEE Transactions on Software Engineering","","1994","20","10","812","824","Statistical testing of software establishes a basis for statistical inference about a software system's expected field quality. This paper describes a method for statistical testing based on a Markov chain model of software usage. The significance of the Markov chain is twofold. First, it allows test input sequences to be generated from multiple probability distributions, making it more general than many existing techniques. Analytical results associated with Markov chains facilitate informative analysis of the sequences before they are generated, indicating how the test is likely to unfold. Second, the test input sequences generated from the chain and applied to the software are themselves a stochastic model and are used to create a second Markov chain to encapsulate the history of the test, including any observed failure information. The influence of the failures is assessed through analytical computations on this chain. We also derive a stopping criterion for the testing process based on a comparison of the sequence generating properties of the two chains.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.328991","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=328991","","Software testing;Statistical analysis;History;Probability distribution;Software quality;Failure analysis;Software systems;Stochastic processes;Performance evaluation","Markov processes;program testing;software quality;probability","Markov chain model;statistical software testing;statistical inference;software field quality;software usage;test input sequences;multiple probability distributions;stochastic model;software failures;testing process;sequence generating properties","","225","","28","","","","","","IEEE","IEEE Journals & Magazines"
"The FreeBSD project: a replication case study of open source development","T. T. Dinh-Trong; J. M. Bieman","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA","IEEE Transactions on Software Engineering","","2005","31","6","481","494","Case studies can help to validate claims that open source software development produces higher quality software at lower cost than traditional commercial development. One problem inherent in case studies are external validity - we do not know whether or not results from one case study apply to another development project. We gain or lose confidence in case study results when similar case studies are conducted on other projects. This case study of the FreeBSD project, a long-lived open source project, provides further understanding of open source development. The paper details a method for mining repositories and querying project participants to retrieve key process information. The FreeBSD development process is fairly well-defined with proscribed methods for determining developer responsibilities, dealing with enhancements and defects, and managing releases. Compared to the Apache project, FreeBSD uses 1) a smaller set of core developers - developers who control the code base - that implement a smaller percentage of the system, 2) a larger set of top developers to implement 80 percent of the system, and 3) a more well-defined testing process. FreeBSD and Apache have a similar ratio of core developers to people involved in adapting and debugging the system and people who report problems. Both systems have similar defect densities and the developers are also users in both systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463231","Index Terms- Software engineering process;process measurement;qualitative process analysis;testing and debugging;reliability;maintenance process;maintainability;open source software;measurement;defect density;code ownership;FreeBSD.","Computer aided software engineering;Open source software;Linux;Kernel;Costs;Debugging;Density measurement;Software measurement;Maintenance;Software quality","public domain software;software development management;project management;program debugging;software reliability;software maintenance","FreeBSD project;replication case study;open source development;software engineering process;process measurement;qualitative process analysis;software testing;software debugging;software reliability;software maintenance process;open source software;defect density;code ownership;information retrievel","","56","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Improving the reliability of function point measurement: an empirical study","C. F. Kemerer; B. S. Porter","MIT, Cambridge, MA, USA; NA","IEEE Transactions on Software Engineering","","1992","18","11","1011","1024","One measure of the size and complexity of information systems that is growing in acceptance and adoption is function points, a user-oriented, nonsource line of code metric of the systems development product. Previous research has documented the degree of reliability of function points as a metric. This research extends that work by (a) identifying the major sources of variation through a survey of current practice, and (b) estimating the magnitude of the effect of these sources of variation using detailed case study data from commercial systems. The results of this research show that a relatively small number of factors has the greatest potential for affecting reliability, and recommendations are made for using these results to improve the reliability of function point counting in organizations.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.177370","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=177370","","Productivity;Software measurement;Management information systems;Size measurement;Programming;Software maintenance;Area measurement;Costs;Information systems;Application software","software metrics;software quality;software reliability","software complexity measures;reliability;function point measurement;size;complexity;information systems;user-oriented;nonsource line of code metric;systems development product;function point counting","","43","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Development of Veda, a prototyping tool for distributed algorithms","C. Jard; J. -. Monin; R. Groz","IRISA, CNRS, Rennes, France; NA; NA","IEEE Transactions on Software Engineering","","1988","14","3","339","352","The development of a simulator, called Veda, is described. Veda is a software tool to help designers in protocol modeling and validation. It is oriented towards the rapid prototyping of distributed algorithms. Algorithms are described using an ISO (International Organisation for Standardization) formal description technique, called Estelle. The development of Veda and its internal structure is presented, emphasizing the use of Prolog as a software engineering tool. Typical uses of Veda are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4654","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4654","","Prototypes;Distributed algorithms;Protocols;Virtual prototyping;Software prototyping;Software tools;Software engineering;Costs;Software algorithms;Hardware","distributed processing;protocols;software tools","Veda;prototyping tool;distributed algorithms;simulator;software tool;ISO;International Organisation for Standardization;Estelle;Prolog","","29","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Test selection based on communicating nondeterministic finite-state machines using a generalized Wp-method","Gang Luo; G. von Bochmann; A. Petrenko","Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada; Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada; Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada","IEEE Transactions on Software Engineering","","1994","20","2","149","162","Presents a method of generating test sequences for concurrent programs and communication protocols that are modeled as communicating nondeterministic finite-state machines (CNFSMs). A conformance relation, called trace-equivalence, is defined within this model, serving as a guide to test generation. A test generation method for a single nondeterministic finite-state machine (NFSM) is developed, which is an improved and generalized version of the Wp-method that generates test sequences only for deterministic finite-state machines. It is applicable to both nondeterministic and deterministic finite-state machines. When applied to deterministic finite-state machines, it yields usually smaller test suites with full fault coverage than the existing methods that also provide full fault coverage, provided that the number of states in implementation NFSMs are bounded by a known integer. For a system of CNFSMs, the test sequences are generated in the following manner: a system of CNFSMs is first reduced into a single NFSM by reachability analysis; then the test sequences are generated from the resulting NFSM using the generalized Wp-method.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.265636","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=265636","","Protocols;Software testing;System testing;Concurrent computing;Communication system software;Computer science;Senior members;Reachability analysis;Software engineering","program testing;multiprocessing programs;protocols;finite state machines;conformance testing;specification languages;software engineering;programming theory","test selection;communicating nondeterministic finite-state machines;generalized Wp-method;test sequence generation;concurrent programs;communication protocols;conformance relation;trace-equivalence;deterministic finite-state machines;test suites;fault coverage;reachability analysis;protocol conformance testing;protocol engineering;SDL;software engineering;software testing","","119","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Recovery point selection on a reverse binary tree task model","S. -. Chen; W. T. Tsai; M. B. Thuraisingham","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","8","963","976","An analysis is conducted of the complexity of placing recovery points where the computation is modeled as a reverse binary tree task model. The objective is to minimize the expected computation time of a program in the presence of faults. The method can be extended to an arbitrary reverse tree model. For uniprocessor systems, an optimal placement algorithm is proposed. For multiprocessor systems, a procedure for computing their performance is described. Since no closed form solution is available, an alternative measurement is proposed that has a closed form formula. On the basis of this formula, algorithms are devised for solving the recovery point placement problem. The estimated formula can be extended to include communication delays where the algorithm devised still applies.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31353","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31353","","Binary trees;Testing;Computational modeling;Delay estimation;Computer science;Multiprocessing systems;Closed-form solution;Sequential analysis;Fault detection","computational complexity;fault tolerant computing;multiprocessing systems;trees (mathematics)","performance computation procedure;computation time minimization;recovery point selection;reverse binary tree task model;arbitrary reverse tree model;uniprocessor systems;optimal placement algorithm;multiprocessor systems;closed form solution;closed form formula;recovery point placement problem;communication delays","","3","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A survey of software design techniques","S. S. Yau; J. J. -. Tsai","Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60201; Department of Electrical Engineering and Computer Science, University of Illinois at Chicago, Chicago, IL 60680","IEEE Transactions on Software Engineering","","1986","SE-12","6","713","721","Software design is the process which translates requirements into a detailed design representation of a software system. It is argued that good software design is the key to reliable and understandable software. Important techniques for software design, including architectural and detailed design stages, are surveyed. Recent advances in distributed software system design methodologies are also reviewed. To ensure software quality, various design verification and validation techniques are discussed. In addition, current software metrics and error-resistant software design methodologies are considered. Future research in software design is considered.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312969","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312969","Design methodologies;design representation;design verification and validation;distributed software system design;error-resistant software design;software design technique;software metrics","Software systems;Software design;Design methodology;Programming;Data structures;Formal specifications","software engineering","software design techniques;design representation;software system;software design;design stages;distributed software system;design methodologies;current software metrics;error-resistant software design;software design","","23","","","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Workflow Models from Web Applications","M. Schur; A. Roth; A. Zeller","SAP SE; SAP SE, Germany; Saarland University–Chair for Software Engineering, Germany","IEEE Transactions on Software Engineering","","2015","41","12","1184","1201","Modern business applications predominantly rely on web technology, enabling software vendors to efficiently provide them as a service, removing some of the complexity of the traditional release and update process. While this facilitates shorter, more efficient and frequent release cycles, it requires continuous testing. Having insight into application behavior through explicit models can largely support development, testing and maintenance. Model-based testing allows efficient test creation based on a description of the states the application can be in and the transitions between these states. As specifying behavior models that are precise enough to be executable by a test automation tool is a hard task, an alternative is to extract them from running applications. However, mining such models is a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present Process Crawler (ProCrawl), a tool to mine behavior models from web applications that support multi-user workflows. ProCrawl incrementally learns a model by generating program runs and observing the application behavior through the user interface. In our evaluation on several real-world web applications, ProCrawl extracted models that concisely describe the implemented workflows and can be directly used for model-based testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2461542","German Federal Ministry of Education and Research (BMBF); European Research Council (ERC); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7169616","Specification mining;dynamic analysis;model-based testing;web system testing;Specification mining;dynamic analysis;model-based testing;web system testing","Web services;Software engineering;Data models;Data mining;Automation;Browsers","data mining;Internet;program testing;system monitoring;user interfaces","user interface;multiuser workflow;ProCrawl;Process Crawler;test automation tool;behavior model;test creation;model-based testing;continuous testing;software vendor;Web technology;Web applications;mining workflow model","","2","","47","","","","","","IEEE","IEEE Journals & Magazines"
"How We Refactor, and How We Know It","E. Murphy-Hill; C. Parnin; A. P. Black","North Carolina State University, Raleigh; Georgia Institute of Technology, Atlanta; Portland State University, Portland","IEEE Transactions on Software Engineering","","2012","38","1","5","18","Refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. However, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. In this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers. To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. Finally, we interviewed the Eclipse and Mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6112738","Refactoring;refactoring tools;floss refactoring;root-canal refactoring.","Java;Software tools;Refactoring","software maintenance;software tools","research and development;refactoring tools;randomly sampled code;Eclipse developers;Mylyn developers","","125","","22","","","","","","IEEE","IEEE Journals & Magazines"
"User interface evaluation and empirically-based evolution of a prototype experience management tool","C. B. Seaman; M. G. Mendonca; V. R. Basili; Y. M. Kim","Dept. of Inf. Syst., Maryland Univ., Baltimore, MD, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","9","838","850","Experience management refers to the capture, structuring, analysis, synthesis, and reuse of an organization's experience in the form of documents, plans, templates, processes, data, etc. The problem of managing experience effectively is not unique to software development, but the field of software engineering has had a high-level approach to this problem for some time. The Experience Factory is an organizational infrastructure whose goal is to produce, store, and reuse experiences gained in a software development organization. This paper describes The Q-Labs Experience Management System (Q-Labs EMS), which is based on the Experience Factory concept and was developed for use in a multinational software engineering consultancy. A critical aspect of the Q-Labs EMS project is its emphasis on empirical evaluation as a major driver of its development and evolution. The initial prototype requirements were grounded in the organizational needs and vision of Q-Labs, as were the goals and evaluation criteria later used to evaluate the prototype. However, the Q-Labs EMS architecture, data model, and user interface were designed to evolve, based on evolving user needs. This paper describes this approach, including the evaluation that was conducted of the initial prototype and its implications for the further development of systems to support software experience management.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232288","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232288","","User interfaces;Prototypes;Software prototyping;Software development management;Medical services;Engineering management;Programming;Software engineering;Production facilities;Computer architecture","user interfaces;software reusability;knowledge management;data models;software performance evaluation;systems re-engineering","user interface evaluation;reuse;prototype experience management tool;organization experience;software development;software engineering;Experience Factory;software experience management;Q-Labs Experience Management System;Q-Labs EMS;empirical evaluation;data model","","8","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Coping with Existing Systems in Information Systems Development","F. Zickert; R. Beck","Goethe University Frankfurt, Frankfurt; Goethe University Frankfurt, Frankfurt","IEEE Transactions on Software Engineering","","2012","38","5","1027","1039","Determining how to cope with existing systems is an important issue for information systems development (ISD). In this paper, we investigate how well different ISD patterns are suited for coping with existing systems. Empirical results, gathered from three software development projects undertaken by a financial institution, suggest propositions regarding how ISD patterns and existing systems affect the characteristics of objective ISD complexity, which in turn determine overall experienced complexity. Existing systems increase complexity due to conflicting interdependencies, but ISD patterns that reduce this complexity, such as those that employ bottom-up or concurrent consideration patterns, are best suited for coping with existing systems. In contrast, top-down and iterative focusing patterns, as classically used in new development, increase the complexity associated with conflicting interdependency, which makes them particularly unsuited for coping with existing systems in ISD.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5999674","Complexity measures;improvements;software maintenance;software engineering process","Complexity theory;Information systems;Maintenance engineering;Software maintenance;File systems;Programming;Servers","financial data processing;information systems;investment;project management;software maintenance;software metrics","information systems development;ISD patterns;software development projects;financial institution;complexity reduction;bottom-up patterns;concurrent consideration patterns;top-down focusing patterns;iterative focusing patterns;information systems portfolio;complexity measures;software maintenance","","","","73","","","","","","IEEE","IEEE Journals & Magazines"
"A Theoretical and Empirical Analysis of the Role of Test Sequence Length in Software Testing for Structural Coverage","A. Arcuri","Simula Research Laboratory, Lysaker","IEEE Transactions on Software Engineering","","2012","38","3","497","519","In the presence of an internal state, often a sequence of function calls is required to test software. In fact, to cover a particular branch of the code, a sequence of previous function calls might be required to put the internal state in the appropriate configuration. Internal states are not only present in object-oriented software, but also in procedural software (e.g., static variables in C programs). In the literature, there are many techniques to test this type of software. However, to the best of our knowledge, the properties related to the choice of the length of these sequences have received only a little attention in the literature. In this paper, we analyze the role that the length plays in software testing, in particular branch coverage. We show that, on “difficult” software testing benchmarks, longer test sequences make their testing trivial. Hence, we argue that the choice of the length of the test sequences is very important in software testing. Theoretical analyses and empirical studies on widely used benchmarks and on an industrial software are carried out to support our claims.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5750005","Evolutionary testing;object-oriented software;state problem;search-based software engineering;software testing;length;test sequence.","Software;Containers;Software testing;Algorithm design and analysis;Search problems;Software algorithms","C language;object-oriented programming;program testing","empirical analysis;theoretical analysis;test sequence length;software testing;structural coverage;internal state;function calls;object-oriented software;procedural software;C programs;static variables;test sequences","","11","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Guidelines for Eliciting Usability Functionalities","N. Juristo; A. Moreno; M. Sanchez-Segura","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","11","744","758","Like any other quality attribute, usability imposes specific constraints on software components. Features that raise the software system's usability have to be considered from the earliest development stages. But, discovering and documenting usability features is likely to be beyond the usability knowledge of most requirements engineers, developers, and users. We propose an approach based on developing specific guidelines that capitalize upon key elements recurrently intervening in the usability features elicitation and specification process. The use of these guidelines provides requirements analysts with a knowledge repository. They can use this repository to ask the right questions and capture precise usability requirements information.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70741","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339231","Usability requirements;usability features elicitation;Requirements/Specifications;Elicitation methods","Guidelines;Usability;Software systems;Application software;Costs;Software architecture;Software quality;Knowledge engineering;Programming;Design engineering","formal specification;software quality;systems analysis","usability functionalities;quality attribute;software components;requirements engineering;specification process;knowledge repository","","53","","56","","","","","","IEEE","IEEE Journals & Magazines"
"A temporal approach for testing distributed systems","A. Khoumsi","Sherbrooke Univ., Que., Canada","IEEE Transactions on Software Engineering","","2002","28","11","1085","1103","This paper deals with testing distributed software systems. In the past, two important problems have been determined for executing tests using a distributed test architecture: controllability and observability problems. A coordinated test method has subsequently been proposed to solve these two problems. In the present article: 1) we show that controllability and observability are indeed resolved if and only if the test system respects timing constraints, even when the system under test is non-real-time; 2) we determine these timing constraints; 3) we determine other timing constraints which optimize the duration of test execution; 4) we show that the communication medium used by the test system does not necessarily have to be FIFO; and 5) we show that the centralized test method can be considered just as a particular case of the proposed coordinated test method.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049406","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049406","","System testing;Timing;Controllability;Observability;Software testing;Constraint optimization;Fault detection;Software systems;Computer architecture;Error correction","program testing;timing;distributed programming","distributed software system testing;distributed test architecture;controllability;observability;temporal approach;coordinated test method;timing constraints;communication medium;centralized test method","","31","","10","","","","","","IEEE","IEEE Journals & Magazines"
"An approach to experimental evaluation of real-time fault-tolerant distributed computing schemes","K. H. Kim","Dept. of Electr. Eng., California Univ., Irvine, CA, USA","IEEE Transactions on Software Engineering","","1989","15","6","715","725","A test-based approach to the evaluation of fault-tolerant distributed-computing schemes is discussed. The approach is based on experimental incorporation of system structuring and design techniques into real-time distributed computing testbeds centered around tightly coupled microcomputer networks. The effectiveness of this approach has been experimentally confirmed. Primary advantages of the testbed-based approach include the relatively high accuracy of the data obtained on timing and logical complexity, as well as the relatively high degree of assurance that can be obtained on the practical effectiveness of the scheme evaluated. Various design issues encountered in the course of establishing the basic microcomputer network testbed facilities are discussed, along with their augmentation to support some experiments. The shortcomings of the testbeds that have been recognized are also discussed together with the desired extensions of the testbeds. Some of the desired extensions are beyond the state-of-the-art in microcomputer network implementation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24725","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24725","","Fault tolerance;Distributed computing;Microcomputers;System testing;Real time systems;Costs;Workstations;Laboratories;Logic testing;Timing","computer networks;fault tolerant computing;microcomputer applications;performance evaluation;program testing;real-time systems","experimental evaluation;real-time fault-tolerant distributed computing schemes;test-based approach;system structuring;design techniques;real-time distributed computing testbeds;tightly coupled microcomputer networks;timing;logical complexity;practical effectiveness;design issues;microcomputer network testbed facilities;microcomputer network implementation","","11","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Software Numerical Instability Detection and Diagnosis by Combining Stochastic and Infinite-Precision Testing","E. Tang; X. Zhang; N. T. Müller; Z. Chen; X. Li","State Key Laboratory for Novel Software Technology and Software Institute of Nanjing University, Jiangsu, China; Department of Computer Science, Purdue University, 305 North University Street, West Lafayette, IN; Abteilung Informatik, University of Trier, Trier, Germany; State Key Laboratory for Novel Software Technology and Software Institute of Nanjing University, Jiangsu, China; State Key Laboratory for Novel Software Technology and Software Institute of Nanjing University, Jiangsu, China","IEEE Transactions on Software Engineering","","2017","43","10","975","994","Numerical instability is a well-known problem that may cause serious runtime failures. This paper discusses the reason of instability in software development process, and presents a toolchain that not only detects the potential instability in software, but also diagnoses the reason for such instability. We classify the reason of instability into two categories. When it is introduced by software requirements, we call the instability caused by problem . In this case, it cannot be avoided by improving software development, but requires inspecting the requirements, especially the underlying mathematical properties. Otherwise, we call the instability caused by practice. We design our toolchain as four loosely-coupled tools, which combine stochastic arithmetic with infinite-precision testing. Each tool in our toolchain can be configured with different strategies according to the properties of the analyzed software. We evaluate our toolchain on subjects from literature. The results show that it effectively detects and separates the instabilities caused by problems from others. We also conduct an evaluation on the latest version of GNU Scientific Library, and the toolchain finds a few real bugs in the well-maintained and widely deployed numerical library. With the help of our toolchain, we report the details and fixing advices to the GSL buglist.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2642956","National Basic Research Program of China 973 Program; NSF Award; National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7792694","Numerical analysis;infinite-precision arithmetic;stochastic arithmetic;software testing","Software;Software algorithms;Algorithm design and analysis;Libraries;Computer bugs;Software testing","fault diagnosis;formal specification;mathematics computing;numerical stability;program debugging;program testing;software quality;software reliability;software tools;stochastic processes","software requirements;toolchain;loosely-coupled tools;stochastic arithmetic;numerical library;software development process;potential instability;mathematical properties;infinite-precision testing;software numerical instability detection;software numerical instability diagnosis;runtime failures;GNU scientific library;GSL buglist","","","","64","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"State transition analysis: a rule-based intrusion detection approach","K. Ilgun; R. A. Kemmerer; P. A. Porras","Adv. Comput. Commun., Santa Barbara, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","1995","21","3","181","199","The paper presents a new approach to representing and detecting computer penetrations in real time. The approach, called state transition analysis, models penetrations as a series of state changes that lead from an initial secure state to a target compromised state. State transition diagrams, the graphical representation of penetrations, identify precisely the requirements for and the compromise of a penetration and present only the critical events that must occur for the successful completion of the penetration. State transition diagrams are written to correspond to the states of an actual computer system, and these diagrams form the basis of a rule based expert system for detecting penetrations, called the state transition analysis tool (STAT). The design and implementation of a Unix specific prototype of this expert system, called USTAT, is also presented. This prototype provides a further illustration of the overall design and functionality of this intrusion detection approach. Lastly, STAT is compared to the functionality of comparable intrusion detection tools.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.372146","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=372146","","Intrusion detection;Expert systems;Data analysis;Prototypes;Data security;Computer security;Software;Computer science;Information analysis;Research and development","safety systems;security of data;authorisation;access control;expert systems;real-time systems","state transition analysis;rule-based intrusion detection approach;computer penetrations;state changes;state transition diagrams;graphical representation;critical events;rule based expert system;STAT;Unix specific prototype;USTAT;intrusion detection tools","","288","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A layout algorithm for data flow diagrams","C. Batini; E. Nardelli; R. Tamassia","Dipartimento di Informatica e Sistemistica, Universit&#x00E1; di Roma &#x201C;La Sapienza,&#x201D; Via Buonarroti 12, 00185 Rome, Italy; Dipartimento di Informatica e Sistemistica, Universit&#x00E1; di Roma &#x201C;La Sapienza,&#x201D; Via Buonarroti 12, 00185 Rome, Italy; Dipartimento di Informatica e Sistemistica, Universit&#x00E1; di Roma &#x201C;La Sapienza,&#x201D; Via Buonarroti 12, 00185 Rome, Italy","IEEE Transactions on Software Engineering","","1986","SE-12","4","538","546","A layout algorithm is presented that allows the automatic drawing of data flow diagrams, a diagrammatic representation widely used in the functional analysis of information systems. A grid standard is defined for such diagrams, and aesthetics for good readability are identified. The layout algorithm receives as input an abstract graph specifying connectivity relations between the elements of the diagram, and produces as output a corresponding diagram according to the aesthetics. The basic strategy is to build incrementally the layout; first, a good topology is constructed with few crossings between edges; subsequently, the shape of the diagram is determined in terms of angles appearing along edges. and finally dimensions are given to the graph, obtaining a grid skeleton for the diagram.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312901","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312901","Database design;design tools;functional analysis;layout algorithms","Layout;Standards;Algorithm design and analysis;Shape;Planarization;Skeleton;Minimization","flowcharting;software tools;systems analysis","layout algorithm;data flow diagrams;functional analysis;information systems;readability;abstract graph;connectivity relations;aesthetics;grid skeleton","","28","","","","","","","","IEEE","IEEE Journals & Magazines"
"Specifying and verifying requirements of real-time systems","A. P. Ravn; H. Rischel; K. M. Hansen","Dept. of Comput. Sci., Tech. Univ. of Denmark, Lyngby, Denmark; Dept. of Comput. Sci., Tech. Univ. of Denmark, Lyngby, Denmark; Dept. of Comput. Sci., Tech. Univ. of Denmark, Lyngby, Denmark","IEEE Transactions on Software Engineering","","1993","19","1","41","55","An approach to specification of requirements and verification of design for real-time systems is presented. A system is defined by a conventional mathematical model for a dynamic system where application specific states denote functions of real time. Specifications are formulas in duration calculus, a real-time interval logic, where predicates define durations of states. Requirements define safety and functionality constraints on the system or a component. A top-level design is given by a control law: a predicate that defines an automation controlling the transition between phases of operation. Each phase maintains certain relations among the system states; this is analogous to the control functions known from conventional control theory. The top-level design is decomposed into an architecture for a distributed system with specifications for sensor, actuator, and program components. Programs control the distributed computation through synchronous events. Sensors and actuators relate events with system states. Verification is a deduction showing that a design implies requirements.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.210306","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=210306","","Real time systems;Automatic control;Sensor systems;Actuators;Mathematical model;Calculus;Logic;Safety;Design automation;Control systems","formal specification;formal verification;real-time systems;temporal logic","real-time systems;specification of requirements;verification of design;mathematical model;duration calculus;real-time interval logic;top-level design;control law;sensor;actuator;distributed computation;synchronous events","","91","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Design by Contract to Improve Software Vigilance","Y. Le Traon; B. Baudry; J. -. Jezequel","France Te´le´com R&D, 2, Avenue Pierre Marzin, 22 307 Lannion Cedex, France; IRISA, Universite´ de Rennes1, Campus Universitaire de Beaulieu, 35042 Rennes Cedex, France; IRISA, Universite´ de Rennes1, Campus Universitaire de Beaulieu, 35042 Rennes Cedex, France","IEEE Transactions on Software Engineering","","2006","32","8","571","586","Design by contract is a lightweight technique for embedding elements of formal specification (such as invariants, pre and postconditions) into an object-oriented design. When contracts are made executable, they can play the role of embedded, online oracles. Executable contracts allow components to be responsive to erroneous states and, thus, may help in detecting and locating faults. In this paper, we define vigilance as the degree to which a program is able to detect an erroneous state at runtime. Diagnosability represents the effort needed to locate a fault once it has been detected. In order to estimate the benefit of using design by contract, we formalize both notions of vigilance and diagnosability as software quality measures. The main steps of measure elaboration are given, from informal definitions of the factors to be measured to the mathematical model of the measures. As is the standard in this domain, the parameters are then fixed through actual measures, based on a mutation analysis in our case. Several measures are presented that reveal and estimate the contribution of contracts to the overall quality of a system in terms of vigilance and diagnosability","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703388","Object-oriented design methods;programming by contract;diagnostics;metrics.","Contracts;Fault detection;Embedded software;Formal specifications;Mathematical model;Object oriented modeling;Runtime;Software quality;Software measurement;Measurement standards","formal specification;object-oriented methods;object-oriented programming;program debugging;program diagnostics;program verification;software fault tolerance;software metrics;software quality","design by contract;software vigilance improvement;formal specification;object-oriented design method;embedded online oracle;executable contract;software fault detection;program runtime erroneous state detection;software diagnosability;software quality measure;mathematical model;mutation analysis;software metrics","","28","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Preventing Temporal Violations in Scientific Workflows: Where and How","X. Liu; Y. Yang; Y. Jiang; J. Chen","Swinburne University of Technology, Melbourne; Anhui University, Hefei and Swinburne University of Technology, Melbourne; Hefei University of Technology, Hefei and University of Pittsburgh, Pittsburgh; University of Technology, Sydney and Swinburne University of Technology, Melbourne","IEEE Transactions on Software Engineering","","2011","37","6","805","825","Due to the dynamic nature of the underlying high-performance infrastructures for scientific workflows such as grid and cloud computing, failures of timely completion of important scientific activities, namely, temporal violations, often take place. Unlike conventional exception handling on functional failures, nonfunctional QoS failures such as temporal violations cannot be passively recovered. They need to be proactively prevented through dynamically monitoring and adjusting the temporal consistency states of scientific workflows at runtime. However, current research on workflow temporal verification mainly focuses on runtime monitoring, while the adjusting strategy for temporal consistency states, namely, temporal adjustment, has so far not been thoroughly investigated. For this issue, two fundamental problems of temporal adjustment, namely, where and how, are systematically analyzed and addressed in this paper. Specifically, a novel minimum probability time redundancy-based necessary and sufficient adjustment point selection strategy is proposed to address the problem of where and an innovative genetic-algorithm-based effective and efficient local rescheduling strategy is proposed to tackle the problem of how. The results of large-scale simulation experiments with generic workflows and specific real-world applications demonstrate that our temporal adjustment strategy can remarkably prevent the violations of both local and global temporal constraints in scientific workflows.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.99","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5645643","Workflow management;exception handling;reliability;software verification;statistical methods.","Decision support systems;Software reliability;Quality of service;Workflow management software","genetic algorithms;middleware;quality of service;workflow management software","temporal violation prevention;scientific workflows;grid computing;cloud computing;nonfunctional QoS failures;workflow temporal verification;adjustment point selection;rescheduling strategy;generic workflows","","30","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Abstract data views: an interface specification concept to enhance design for reuse","D. D. Cowan; C. J. P. Lucena","Dept. of Comput. Sci., Waterloo Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","1995","21","3","229","243","The abstract data view (ADV) design model was originally created to specify clearly and formally the separation of the user interface from the application component of a software system, and to provide a systematic design method that is independent of specific application environments. Such a method should lead to a high degree of reuse of designs for both interface and application components. The material presented, extends the concept of ADV's to encompass the general specification of interfaces between application components in the same or different computing environments. This approach to specifying interfaces clearly separates application components from each other, since they do not need to know how they are used, or how they obtain services from other application components. Thus, application components called abstract data objects (ADOs), are designed to minimize knowledge of the environment in which they are used and should be more amenable to reuse.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.372150","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=372150","","User interfaces;Application software;Data structures;Design methodology;File systems;Counting circuits;Knowledge engineering;Computer science;Mechanical systems;Couplings","abstract data types;user interfaces;formal specification;software reusability;data structures","abstract data view;interface specification concept;design for reuse;ADV design model;user interface;systematic design method;specific application environments;application components;interactive applications;end user programming;script languages","","45","","69","","","","","","IEEE","IEEE Journals & Magazines"
"Software Architecture Reconstruction: A Process-Oriented Taxonomy","S. Ducasse; D. Pollet","INRIA, Lille Nord Europe; University of Lille 1, Francr","IEEE Transactions on Software Engineering","","2009","35","4","573","591","To maintain and understand large applications, it is important to know their architecture. The first problem is that unlike classes and packages, architecture is not explicitly represented in the code. The second problem is that successful applications evolve over time, so their architecture inevitably drifts. Reconstructing the architecture and checking whether it is still valid is therefore an important aid. While there is a plethora of approaches and techniques supporting architecture reconstruction, there is no comprehensive software architecture reconstruction state of the art and it is often difficult to compare the approaches. This paper presents a state of the art in software architecture reconstruction approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815276","Software architecture reconstruction.","Software architecture;Taxonomy;Computer architecture;Application software;Data mining;Programming;Europe;Packaging;Cognitive science;Bridges","software architecture;software maintenance;software packages","software architecture reconstruction;process-oriented taxonomy;software development","","126","","181","","","","","","IEEE","IEEE Journals & Magazines"
"QoS-aware middleware for Web services composition","Liangzhao Zeng; B. Benatallah; A. H. H. Ngu; M. Dumas; J. Kalagnanam; H. Chang","IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","5","311","327","The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online business-to-business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different quality of service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are described and compared: one based on local (task-level) selection of services and the other based on global allocation of tasks to services using integer programming.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291834","Web services;quality of service;service composition;integer programming.","Middleware;Web services;Quality of service;Linear programming;Web and internet services;Financial management;Availability;Computer science;Computer Society;Online Communities/Technical Collaboration","electronic commerce;middleware;quality of service;Internet;integer programming","Web manual interaction;online business-to-business collaboration;B2B;Web services;quality of service;QoS;middleware platform;integer programming;service composition","","1609","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Implementing Fault-Tolerant Distributed Objects","K. P. Birman; T. A. Joseph; T. Raeuchle; A. El Abbadi","Department of Computer Science, Cornell University; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","6","502","508","This paper describes a technique for implementing k-resilient objects–distributed objects that remain available, and whose operations are guaranteed to progress to completion, despite up to k site failures. The implementation is derived from the object specification automatically, and does not require any information beyond what would be required for a nonresilient nondistributed implementation. It is therefore unnecessary for an applications programmer to have knowledge of the complex protocols nonnally employed to implement fault-tolerant objects. Our technique is used in ISIS, a system being developed at Cornell to support resilient objects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232242","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702047","Abstract data types;availability;checkpoint/restart;concurrency;consistency;distributed databases;distributed systems;fault-tolerance;recovery;reliability","Fault tolerance;Protocols;Intersymbol interference;Fault tolerant systems;Programming profession;Availability;Concurrent computing;Distributed databases;Software systems;Computer science","","Abstract data types;availability;checkpoint/restart;concurrency;consistency;distributed databases;distributed systems;fault-tolerance;recovery;reliability","","49","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Analysis of Object-Oriented Design Metrics for Predicting High and Low Severity Faults","Yuming Zhou; Hareton Leung","IEEE Computer Society; NA","IEEE Transactions on Software Engineering","","2006","32","10","771","789","In the last decade, empirical studies on object-oriented design metrics have shown some of them to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented design metrics and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented design metrics, specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these design metrics are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these design metrics are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.102","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717471","Object-oriented;faults;fault-proneness;metrics;prediction;cross validation.","Object oriented modeling;Predictive models;Logistics;Learning systems;NASA;Computer Society;Software systems;Fault detection;Decision making;Programming","object-oriented programming;regression analysis;software fault tolerance;software metrics","object-oriented design metrics;fault-proneness prediction;object-oriented software system;fault severity;logistic regression method;machine learning method;public domain NASA data set;fault-prone classes","","152","","31","","","","","","IEEE","IEEE Journals & Magazines"
"A distributed deadlock detection and resolution algorithm and its correctness proof","A. K. Elmagarmid; N. Soundararajan; M. T. Liu","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; NA; NA","IEEE Transactions on Software Engineering","","1988","14","10","1443","1452","The key idea of the algorithm is to let one transaction controller be in charge of all transactions in a set of interacting transactions. Two transactions are interacting if they are both interested in (accessing) the same resource. In addition, the controller is in charge of all the resources allocated to any of the transactions in the set. Having one controller in charge of all the transactions in a set of interacting transactions and all the resources allocated to them makes it easier to detect deadlocks and avoid them. The main problem dealt with is how a controller takes charge of another transaction when the transaction tries to access one of the resources currently in the control of the controller and how a controller releases a transaction back to its original controller when the transaction is no longer interested in any of the resources in which one or more of the other transactions are also interested. Communicating sequential processes (CSP) is used to code the algorithm. The correctness of the algorithm is proved in a semiformal manner.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6189","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6189","","System recovery;Resource management;Radio control;Computer science;Information science;Control systems","distributed processing;program verification;programming theory;system recovery;transaction processing","transaction processing;communicating sequential processes;distributed deadlock detection;correctness proof;transaction controller","","11","","4","","","","","","IEEE","IEEE Journals & Magazines"
"The Role of Deliberate Artificial Design Elements in Software Engineering Experiments","J. Hannay; M. Jørgensen","NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","242","259","Increased realism in software engineering experiments is often promoted as an important means of increasing generalizability and industrial relevance. In this context, artificiality, e.g., the use of constructed tasks in place of realistic tasks, is seen as a threat. In this paper, we examine the opposite view that deliberately introduced artificial design elements may increase knowledge gain and enhance both generalizability and relevance. In the first part of this paper, we identify and evaluate arguments and examples in favor of and against deliberately introducing artificiality into software engineering experiments. We find that there are good arguments in favor of deliberately introducing artificial design elements to 1) isolate basic mechanisms, 2) establish the existence of phenomena, 3) enable generalization from particularly unfavorable to more favorable conditions (persistence of phenomena), and 4) relate experiments to theory. In the second part of this paper, we summarize a content analysis of articles that report software engineering experiments published over a 10-year period from 1993 to 2002. The analysis reveals a striving for realism and external validity, but little awareness of for what and when various degrees of artificiality and realism are appropriate. Furthermore, much of the focus on realism seems to be based on a narrow understanding of the nature of generalization. We conclude that an increased awareness and deliberation as to when and for what purposes both artificial and realistic design elements are applied is valuable for better knowledge gain and quality in empirical software engineering experiments. We also conclude that time spent on studies that have obvious threats to validity that are due to artificiality might be better spent on studies that investigate research questions for which artificiality is a strength rather than a weakness. However, arguments in favor of artificial design elements should not be used to justify studies that are badly designed or that have research questions of low relevance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4453833","Software Engineering;Surveys of historical development of one particular area;Software Engineering;Surveys of historical development of one particular area","Software design;Software engineering;Computer industry;Computer Society;Engineering drawings;Displays;Programming;Probability distribution","knowledge acquisition;software engineering","empirical software engineering experiment;content analysis;realistic design element;artificial design element;knowledge acquisition","","28","","110","","","","","","IEEE","IEEE Journals & Magazines"
"Identification of dynamic comprehension processes during large scale maintenance","A. Von Mayrhauser; A. M. Vans","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA","IEEE Transactions on Software Engineering","","1996","22","6","424","437","We present results of observing professional maintenance engineers working with industrial code at actual maintenance tasks. Protocol analysis is used to explore how code understanding might differ for small versus large scale code. The experiment confirms that cognition processes work at all levels of abstraction simultaneously as programmers build a mental model of the code. Analysis focused on dynamic properties and processes of code understanding. Cognition processes emerged at three levels of aggregation representing lower and higher level strategies of understanding. They show differences in what triggers them and how they achieve their goals. Results are useful for defining information which maintenance engineers need for their work and for documentation and development standards.","0098-5589;1939-3520;2326-3881","","10.1109/32.508315","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=508315","","Large-scale systems;Cognition;Protocols;Software maintenance;Programming profession;Switches;Maintenance engineering;Cognitive science;Documentation;Standards development","software maintenance;reverse engineering","dynamic comprehension processes;large scale maintenance;professional maintenance engineers;industrial code;protocol analysis;code understanding;cognition processes;abstraction;dynamic properties;aggregation","","74","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A Distributed Algorithm for Constructing Minimal Spanning Trees","Y. K. Dalal","Metaphor Computer Systems","IEEE Transactions on Software Engineering","","1987","SE-13","3","398","405","Most algorithms for constructing minimal spanning trees are sequential in operation. Distributed algorithms for constructing these trees operate both concurrently and asynchronously, and are useful in store-and-forward packet-switching computer-communication networks where there is typically no single source of control. The difficulties in designing such algorithms arise from communication and synchronization problems. This paper discusses these problems and describes the first distributed algorithm for constructing minimal spanning trees. This algorithm and the principles and techniques underlying its design will find application in large communication networks and large multiprocessor computer systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233171","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702226","Broadcast routing;computer-communication networks;distributed algorithms;minimal spanning tree;protocols;store-and-forward packet-switching;synchronization","Distributed algorithms;Computer networks;Costs;Algorithm design and analysis;Delay estimation;Nearest neighbor searches;Distributed computing;Application software;Communication networks;Routing","","Broadcast routing;computer-communication networks;distributed algorithms;minimal spanning tree;protocols;store-and-forward packet-switching;synchronization","","12","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic QoS Adaptation for Mobile Middleware","S. Chuang; A. T. S. Chan","The Hong Kong Polytechnic University, Hong Kong; The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Software Engineering","","2008","34","6","738","752","Computation and networking resources in mobile operating environments are much scarcer and more dynamic than in desktop operating environments. Mobile applications can leverage on the benefits of adaptive computing to optimize the QoS delivery based on contextual situations. Fuzzy control models have been successfully applied to various distributed network QoS management systems. However, existing models are either application-specific or limited to abstract modeling and simple conceptual scenarios which do not take into account overall model scalability. Specifically, the large number of QoS parameters in a mobile operating environment causes an exponential increase in the number of rules correspondingly increases the demand for processing power to infer the rules. Hierarchical fuzzy systems were introduced to reduce the number of rules using hierarchical fuzzy control, in which correlated linguistic variables are hierarchically inferred and grouped into abstract linguistic variables. In this paper, we propose a mobile QoS management framework that uses a hierarchical fuzzy control model to support a highly extensible and structured adaptation paradigm. The proposed framework integrates several levels of QoS abstractions derived from user-perceived requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4547430","Mobile Computing;Domain-specific architectures;Mobile Computing;Domain-specific architectures","Middleware;Quality of service;Fuzzy control;Mobile computing;Power system management;Power system modeling;Quality management;Battery management systems;Energy management;Computer network management","fuzzy control;fuzzy systems;middleware;mobile computing;quality of service","dynamic QoS adaptation;mobile middleware;adaptive computing;fuzzy control;distributed network QoS management systems;mobile operating environment;fuzzy systems","","13","","35","","","","","","IEEE","IEEE Journals & Magazines"
"The generic consensus service","R. Guerraoui; A. Schiper","Dept. de Syst. de Commun., Ecole Polytech. Federale de Lausanne, Switzerland; NA","IEEE Transactions on Software Engineering","","2001","27","1","29","41","This paper describes a modular approach for the construction of fault-tolerant agreement protocols. The approach is based on a generic consensus service. Fault-tolerant agreement protocols are built using a client-server interaction, where the clients are the processes that must solve the agreement problem and the servers implement the consensus service. This service is accessed through a generic consensus filter, customized for each specific agreement problem. We illustrate our approach on the construction of various fault-tolerant agreement protocols, such as nonblocking atomic commitment, group membership, view synchronous communication, and total order multicast. Through a systematic reduction to consensus, we provide a simple way to solve agreement problems. In addition to its modularity, our approach enables efficient implementations of agreement protocols and precise characterization of the assumptions underlying their liveness and safety properties.","0098-5589;1939-3520;2326-3881","","10.1109/32.895986","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=895986","","Multicast protocols;Fault tolerance;Access protocols;Broadcasting;Filters;File servers;Fault tolerant systems;Resilience;Modular construction;Safety","protocols;client-server systems;multicast communication;computer network reliability","generic consensus service;fault-tolerant agreement protocols;client-server interaction;generic consensus filter;nonblocking atomic commitment;group membership;view synchronous communication;total order multicast;liveness;safety properties","","50","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Lightweight extraction of object models from bytecode","D. Jackson; A. Waingold","Lab. for Comput. Sci., MIT, Cambridge, MA, USA; NA","IEEE Transactions on Software Engineering","","2001","27","2","156","169","A program's object model captures the essence of its design. For some programs, no object model was developed during design; for others, an object model exists but may be out-of-sync with the code. This paper describes a tool that automatically extracts an object model from the class-files of a Java program. Unlike existing tools, it handles container classes by inferring the types of elements stored in a container and eliding the container itself. This feature is crucial for obtaining models that show the structure of the abstract state and bear some relation to conceptual models. Although the tool performs only a simple, heuristic analysis that is almost entirely local, the resulting object model is surprisingly accurate. The paper explains what object models are and why they are useful; describes the analysis, its assumptions, and limitations; evaluates the tool for accuracy, and illustrates its use on a suite of sample programs.","0098-5589;1939-3520;2326-3881","","10.1109/32.908960","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=908960","","Java;Containers;Performance analysis;Computer Society;Reverse engineering;Feature extraction;Computer science","Java;object-oriented programming;abstract data types;reverse engineering","lightweight object model extraction;bytecode;class-files;Java program;container classes;abstract state;heuristic analysis","","17","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Survey of Program Comprehension through Dynamic Analysis","B. Cornelissen; A. Zaidman; A. van Deursen; L. Moonen; R. Koschke","Delft University of Technology, The Netherlands; Delft University of Technology, The Netherlands; Delft University of Technology, The Netherlands; Simula Research Laboratory, Norway; University of Bremen, Germany","IEEE Transactions on Software Engineering","","2009","35","5","684","702","Program comprehension is an important activity in software maintenance, as software must be sufficiently understood before it can be properly modified. The study of a program's execution, known as dynamic analysis, has become a common technique in this respect and has received substantial attention from the research community, particularly over the last decade. These efforts have resulted in a large research body of which currently there exists no comprehensive overview. This paper reports on a systematic literature survey aimed at the identification and structuring of research on program comprehension through dynamic analysis. From a research body consisting of 4,795 articles published in 14 relevant venues between July 1999 and June 2008 and the references therein, we have systematically selected 176 articles and characterized them in terms of four main facets: activity, target, method, and evaluation. The resulting overview offers insight in what constitutes the main contributions of the field, supports the task of identifying gaps and opportunities, and has motivated our discussion of several important research directions that merit additional consideration in the near future.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815280","Survey;program comprehension;dynamic analysis.","Computer Society;Software maintenance;Software systems;Documentation;Software engineering;Data analysis;Information analysis;Availability;Runtime;Virtual machining","reverse engineering;software maintenance;system monitoring","program comprehension;dynamic analysis;software maintenance;systematic literature survey","","163","","154","","","","","","IEEE","IEEE Journals & Magazines"
"Randomized Byzantine Agreement","K. J. Perry","I. B. M. Thomas J. Watson Research Center","IEEE Transactions on Software Engineering","","1985","SE-11","6","539","546","A randomized model of distributed computation was recently presented by Rabin [ 81. This model admits a solution to the Byzantine Agreement Problem for systems of n asynchronous processes where no more than t are faulty. The algorithm described by Rabin produces agreement in an expected number of rounds which is a small constant independent of n and t. Using the same model, we present an algorithm of similar complexity which is able to tolerate a greater portion of malicious processes. The algorithm is also applicable, with minor changes, to systems of synchronous processes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232246","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702051","Distributed computing;distributed database systems;fault-tolerance;protocols;reliability","Protocols;Computational modeling;Distributed computing;Indexes;Database systems;Fault tolerant systems;Computer science;Application software;Fault tolerance","","Distributed computing;distributed database systems;fault-tolerance;protocols;reliability","","5","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Prolog-based meta-rules for relational database representation and manipulation","T. Niemi; K. Jarvelin","Tampere Univ., Finland; Tampere Univ., Finland","IEEE Transactions on Software Engineering","","1991","17","8","762","788","A Prolog-based experimental system for relational databases that is not defined from the viewpoint of any specific relational topic is proposed. The idea is that the experimental system can be used in many different contexts such as query optimization, data restructuring and database design. The definition is based entirely on the theoretical foundations of the relational model. The experimental system offers a well-defined environment for studying how other systems can be integrated with relational databases. The use of the experimental system in the context of different approaches to deductive databases is considered.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.83913","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=83913","","Relational databases;Deductive databases;Knowledge representation;Database languages;Logic programming;Application software;Data models;Knowledge based systems;Environmental management;Design automation","database theory;deductive databases;knowledge based systems;PROLOG;relational databases","Prolog-based meta-rules;relational database representation;Prolog-based experimental system;query optimization;data restructuring;database design;theoretical foundations;well-defined environment;deductive databases","","2","","70","","","","","","IEEE","IEEE Journals & Magazines"
"Measuring process consistency: implications for reducing software defects","M. S. Krishnan; M. I. Kellner","Sch. of Bus., Michigan Univ., Ann Arbor, MI, USA; NA","IEEE Transactions on Software Engineering","","1999","25","6","800","815","In this paper, an empirical study that links software process consistency with product defects is reported. Various measurement issues such as validity, reliability, and other challenges in measuring process consistency at the project level are discussed. A measurement scale for software process consistency is introduced. An empirical study that uses this scale to measure consistency in achieving the CMM goal questions in various key process areas (KPAs) in 45 projects at a leading software vendor is reported. The results of this analysis indicate that consistent adoption of practices specified in the CMM is associated with a lower number of defects. Even a relatively modest improvement in the consistency of implementing these practices is associated with a significant reduction in field defects.","0098-5589;1939-3520;2326-3881","","10.1109/32.824401","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=824401","","Software measurement;Coordinate measuring machines;Software engineering;Software maintenance;Costs;Software performance;Software development management;Software quality;Software systems;Scheduling","software process improvement;software metrics;software reliability","process consistency measurement;software defect reduction;software process consistency;product defects;software validity;software reliability;measurement scale;CMM;software vendor","","57","","53","","","","","","IEEE","IEEE Journals & Magazines"
"An approach to developing domain requirements as a core asset based on commonality and variability analysis in a product line","Mikyeong Moon; Keunhyuk Yeom; Heung Seok Chae","Dept. of Comput. Sci. & Eng., Pusan Nat. Univ., South Korea; Dept. of Comput. Sci. & Eng., Pusan Nat. Univ., South Korea; Dept. of Comput. Sci. & Eng., Pusan Nat. Univ., South Korea","IEEE Transactions on Software Engineering","","2005","31","7","551","569","The methodologies of product line engineering emphasize proactive reuse to construct high-quality products more quickly that are less costly. Requirements engineering for software product families differs significantly from requirements engineering for single software products. The requirements for a product line are written for the group of systems as a whole, with requirements for individual systems specified by a delta or an increment to the generic set. Therefore, it is necessary to identify and explicitly denote the regions of commonality and points of variation at the requirements level. In this paper, we suggest a method of producing requirements that will be a core asset in the product line. We describe a process for developing domain requirements where commonality and variability in a domain are explicitly considered. A CASE environment, named DREAM, for managing commonality and variability analysis of domain requirements is also described. We also describe a case study for an e-travel system domain where we found that our approach to developing domain requirements based on commonality and variability analysis helped to produce domain requirements as a core asset for product lines.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492371","Index Terms- Requirement engineering;product-line;core asset;commonality;variability;domain analysis;reuse.","Design engineering;Computer architecture;Moon;Computer Society;Computer aided software engineering;Environmental management;Software systems;Control systems;Testing;Costs","computer aided software engineering;software reusability;software quality;software architecture;formal specification;travel industry","product line engineering;software reuse;variability analysis;commonality analysis;high-quality software product;requirements engineering;software product family;CASE environment;DREAM;e-travel system domain","","69","","29","","","","","","IEEE","IEEE Journals & Magazines"
"On the value of static analysis for fault detection in software","J. Zheng; L. Williams; N. Nagappan; W. Snipes; J. P. Hudepohl; M. A. Vouk","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","4","240","253","No single software fault-detection technique is capable of addressing all fault-detection concerns. Similarly to software reviews and testing, static analysis tools (or automated static analysis) can be used to remove defects prior to release of a software product. To determine to what extent automated static analysis can help in the economic production of a high-quality product, we have analyzed static analysis faults and test and customer-reported failures for three large-scale industrial software systems developed at Nortel Networks. The data indicate that automated static analysis is an affordable means of software fault detection. Using the orthogonal defect classification scheme, we found that automated static analysis is effective at identifying assignment and checking faults, allowing the later software production phases to focus on more complex, functional, and algorithmic faults. A majority of the defects found by automated static analysis appear to be produced by a few key types of programmer errors and some of these types have the potential to cause security vulnerabilities. Statistical analysis results indicate the number of automated static analysis faults can be effective for identifying problem modules. Our results indicate static analysis tools are complementary to other fault-detection techniques for the economic production of a high-quality software product.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628970","Code inspections;walkthroughs.","Fault detection;Software tools;Failure analysis;Automatic testing;Software testing;Fault diagnosis;Production systems;System testing;Large-scale systems;Computer industry","program diagnostics;fault diagnosis;software quality","software fault-detection;static analysis tool;automated static analysis;Nortel Networks;orthogonal defect classification scheme;programmer error;security vulnerability;high-quality software product;code inspection;industrial software system","","95","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Implementing remote evaluation","J. W. Stamos; D. K. Gifford","Lab. for Comput. Sci., MIT, Cambridge, MA, USA; Lab. for Comput. Sci., MIT, Cambridge, MA, USA","IEEE Transactions on Software Engineering","","1990","16","7","710","722","Remote evaluation (REV) is a construct for building distributed systems that involves sending executable code from one computer to another computer via a communication network. How REV can reduce communication and improve performance for certain classes of distributed applications is explained. Implementation issues are discussed. REV is incorporated into a high-level programming language by defining its syntax and its semantics. The compile-time and run-time support for REV is discussed in both heterogeneous and homogeneous systems and compared to that needed by a remote procedure call implementation. Sample performance measurements are included. Experience with a prototype REV implementation is summarized.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.56097","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=56097","","Network servers;Buildings;Application software;High performance computing;Laboratories;Computer science;Programming profession;Computer networks;Communication networks;Computer languages","computer networks;distributed processing;performance evaluation","remote evaluation implementation;compile time support;heterogeneous systems;distributed systems;executable code;communication network;distributed applications;high-level programming language;syntax;semantics;run-time support;homogeneous systems;remote procedure call implementation;performance measurements","","38","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Bayesian Networks For Evidence-Based Decision-Making in Software Engineering","A. T. Misirli; A. B. Bener","Department of Information Processing Science, University of Oulu, Finland; Mechanical and Industrial Engineering Department, Ryerson University, Toronto, CA","IEEE Transactions on Software Engineering","","2014","40","6","533","554","Recommendation systems in software engineering (SE) should be designed to integrate evidence into practitioners experience. Bayesian networks (BNs) provide a natural statistical framework for evidence-based decision-making by incorporating an integrated summary of the available evidence and associated uncertainty (of consequences). In this study, we follow the lead of computational biology and healthcare decision-making, and investigate the applications of BNs in SE in terms of 1) main software engineering challenges addressed, 2) techniques used to learn causal relationships among variables, 3) techniques used to infer the parameters, and 4) variable types used as BN nodes. We conduct a systematic mapping study to investigate each of these four facets and compare the current usage of BNs in SE with these two domains. Subsequently, we highlight the main limitations of the usage of BNs in SE and propose a Hybrid BN to improve evidence-based decision-making in SE. In two industrial cases, we build sample hybrid BNs and evaluate their performance. The results of our empirical analyses show that hybrid BNs are powerful frameworks that combine expert knowledge with quantitative data. As researchers in SE become more aware of the underlying dynamics of BNs, the proposed models will also advance and naturally contribute to evidence based-decision-making.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2321179","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6808495","Evidence-based decision-making;Bayesian networks;Bayesian statistics;software reliability;software metrics;post-release defects","Software engineering;Decision making;Bayes methods;Software;Medical services;Systematics;Buildings","belief networks;decision making;software metrics;software reliability","Bayesian networks;evidence-based decision-making;software engineering;recommendation systems;SE;natural statistical framework;associated uncertainty;computational biology;health care decision-making;systematic mapping study;hybrid BN node;software reliability;software metrics","","18","","81","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting the Combined Selection of Model-Based Testing Techniques","A. C. Dias-Neto; G. H. Travassos","Institute of Computing, Federal University of Amazonas, Manaus, Brazil; Systems Engineering and Computer Science Programme at COPPE, Federal University of Rio de Janeiro, Rio de Janeiro , Brazil","IEEE Transactions on Software Engineering","","2014","40","10","1025","1041","The technical literature on model-based testing (MBT) offers us several techniques with different characteristics and goals. Contemporary software projects usually need to make use of different software testing techniques. However, a lack of empirical information regarding their scalability and effectiveness is observed. It makes their application difficult in real projects, increasing the technical difficulties to combine two or more MBT techniques for the same software project. In addition, current software testing selection approaches offer limited support for the combined selection of techniques. Therefore, this paper describes the conception and evaluation of an approach aimed at supporting the combined selection of MBT techniques for software projects. It consists of an evidence-based body of knowledge with 219 MBT techniques and their corresponding characteristics and a selection process that provides indicators on the level of adequacy (impact indicator) amongst MBT techniques and software projects characteristics. Results from the data analysis indicate it contributes to improve the effectiveness and efficiency of the selection process when compared to another selection approach available in the technical literature. Aiming at facilitating its use, a computerized infrastructure, evaluated into an industrial context and evolved to implement all the facilities needed to support such selection approach, is presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2312915","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6776497","Software testing;model-based testing;software technology selection;recommendation system;experimental software engineering","Software testing;Context;Computational modeling;Organizations;Software quality","program testing;project management;software development management","model-based testing techniques;technical literature;software projects;software testing techniques;MBT techniques;software testing selection;impact indicator;software project characteristics;data analysis;computerized infrastructure","","7","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Generating Test Data from OCL Constraints with Search Techniques","S. Ali; M. Zohaib Iqbal; A. Arcuri; L. C. Briand","Simula Research Lab, Norway; National University of Computer and Emerging Sciences, Islamabad and University of Luxembourg, Luxembourg; Simula Research Lab, Norway; University of Luxembourg, Luxembourg","IEEE Transactions on Software Engineering","","2013","39","10","1376","1402","Model-based testing (MBT) aims at automated, scalable, and systematic testing solutions for complex industrial software systems. To increase chances of adoption in industrial contexts, software systems can be modeled using well-established standards such as the Unified Modeling Language (UML) and the Object Constraint Language (OCL). Given that test data generation is one of the major challenges to automate MBT, we focus on test data generation from OCL constraints in this paper. This endeavor is all the more challenging given the numerous OCL constructs and operations that are designed to facilitate the definition of constraints. Though search-based software testing has been applied to test data generation for white-box testing (e.g., branch coverage), its application to the MBT of industrial software systems has been limited. In this paper, we propose a set of search heuristics targeted to OCL constraints to guide test data generation and automate MBT in industrial applications. We evaluate these heuristics for three search algorithms: Genetic Algorithm, (1+1) Evolutionary Algorithm, and Alternating Variable Method. We empirically evaluate our heuristics using complex artificial problems, followed by empirical analyses of the feasibility of our approach on one industrial system in the context of robustness testing. Our approach is also compared with the most widely referenced OCL solver (UMLtoCSP) in the literature and shows to be significantly more efficient.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6491405","OCL;search-based testing;test data generation;empirical evaluation;search-based software engineering;model-based testing","Unified modeling language;Search problems;Software algorithms;Genetic algorithms;Standards;Software testing","genetic algorithms;program testing;specification languages","test data generation;OCL constraints;search techniques;model-based testing;MBT;industrial software systems;object constraint language;unified modeling language;UML;OCL constructs;OCL operations;search-based software testing;white-box testing;genetic algorithm;one-plus-one evolutionary algorithm;alternating variable method;empirical analysis;robustness testing context;UMLtoCSP OCL solver","","45","","75","","","","","","IEEE","IEEE Journals & Magazines"
"Profiling an incremental data flow analysis algorithm","B. G. Ryder; W. Landi; H. D. Pande","Dept. of Comput. Sci., Rutgers Univ., New Brunswick, NJ, USA; Dept. of Comput. Sci., Rutgers Univ., New Brunswick, NJ, USA; Dept. of Comput. Sci., Rutgers Univ., New Brunswick, NJ, USA","IEEE Transactions on Software Engineering","","1990","16","2","129","140","Incremental data flow analysis algorithms have been designed to deal efficiently with change in evolving software systems. These algorithms document the current state of a software system by incorporating change effects into previously derived information describing the definition and use of data in the system. Unfortunately, the performance of these algorithms cannot, in general, be characterized by analytic predictions of their expected behavior. It is possible, however, to observe their performance empirically and predict their average behavior. The authors report on experiments on the empirical profiling of a general-purpose, incremental data flow analysis algorithm. The algorithm, dominator based and coded in C, was applied to statistically significant numbers of feasible, random software systems of moderate size. The experimental results, with quantifiable confidence limits, substantiate the claim that incremental analyses are viable and grow more valuable as a software system grows in size.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44377","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44377","","Data analysis;Algorithm design and analysis;Software systems;Software algorithms;Performance analysis;Information analysis;Application software;Computer science;Computer industry","parallel programming;program testing","incremental data flow analysis algorithm;evolving software systems;empirical profiling","","9","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Algorithmic transformations for neural computing and performance of supervised learning on a dataflow machine","S. T. Kim; K. Suwunboriruksa; S. Herath; A. Jayasumana; J. Herath","Dept. of Electr. & Comput. Eng., Drexel Univ., Philadelphia, PA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1992","18","7","613","623","Reprogrammable dataflow neural classifiers are proposed as an alternative to traditional implementations. In general, these classifiers are based on functional languages, neural-dataflow transformations, dataflow algorithmic transformations, and dataflow multiprocessors. An experimental approach is used to investigate the performance of a large-scale fine-grained dataflow classifier architecture. In this study, the functional descriptions of high level data dependency of a supervised learning algorithm are transformed into a machine executable low-level dataflow graph. The tagged token dataflow algorithmic transformation is applied to exploit the parallelism. Dataflow neural classifiers are used to implement the learning algorithm. No attempt is made to optimize the granularity of the high-level language programming blocks to balance the computation and communication. The proposed classifier architecture is more versatile than other existing architectures. Performance results show the effectiveness of dataflow neural classifiers.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.148479","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=148479","","Supervised learning;Large-scale systems;Sensor arrays;Hamming distance;Parallel processing;Computer architecture;Intelligent sensors;Intelligent actuators;Military computing;Neurons","computerised pattern recognition;learning systems;neural nets;parallel architectures;performance evaluation","algorithmic transformations;reprogrammable dataflow neural classifiers;neural computing;performance;supervised learning;dataflow machine;functional languages;neural-dataflow transformations;dataflow algorithmic transformations;dataflow multiprocessors;high level data dependency;machine executable low-level dataflow graph;tagged token dataflow algorithmic transformation;granularity","","3","","16","","","","","","IEEE","IEEE Journals & Magazines"
"A method for software reliability analysis and prediction application to the TROPICO-R switching system","K. Kanoun; M. R. de Bastos Martini; J. M. de Souza","LAAS-CNRS, Toulouse, France; NA; NA","IEEE Transactions on Software Engineering","","1991","17","4","334","344","An evaluation method which allows existing reliability growth models to provide better predictions of software behavior is presented. The method is primarily based on the analysis of the trend exhibited by the data collected on the program (which is determined by reliability growth tests). Reliability data are then partitioned according to the trend, and two types of reliability growth models can be applied: when the data exhibit reliability decrease followed by reliability growth, an S-shaped model can be applied, and in case of reliability growth, most of the other existing reliability growth models can be applied. The hyperexponential model is shown to allow prediction of the software residual failure rate in operation, and this failure rate is used as a qualification index for the software product. The method is illustrated through its application to the Brazilian electronic switching system TROPICO-R.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90433","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=90433","","Software reliability;Application software;Predictive models;Testing;Maintenance;Electronic switching systems;Switching systems;Software systems;Failure analysis;Software development management","electronic switching systems;reliability theory;software reliability","software reliability;prediction application;TROPICO-R switching system;reliability growth models;software behavior;reliability decrease;reliability growth;S-shaped model;hyperexponential model;software residual failure rate;qualification index;Brazilian electronic switching system","","45","","33","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of testing and integration strategies using artificial software systems","J. A. Solheim; J. H. Rowland","Emporia State Univ., KS, USA; NA","IEEE Transactions on Software Engineering","","1993","19","10","941","949","There has been much discussion about the merits of various testing and integration strategies. Top-down, bottom-up, big-bang, and sandwich integration strategies are advocated by various authors. Also, some authors insist that modules be unit tested, while others believe that unit testing diverts resources from more effective verification processes. This article addresses the ability of the aforementioned integration strategies to detect defects, and produce reliable systems. It also explores the efficacy of spot unit testing, and compares phased and incremental versions of top-down and bottom-up integration strategies. Relatively large artificial software systems were constructed using a code generator with ten basic module templates. These systems were seeded with known defects and tested using the above testing and integration strategies. A number of experiments were then conducted using a simulator whose validity was established by comparing results against these artificial systems. The defect detection ability and resulting system reliability were measured for each strategy. Results indicated that top-down integration strategies are generally most effective in terms of defect correction. Top-down and big-bang strategies produced the most reliable systems. Results favored neither those strategies that incorporate spot unit testing nor those that do not; also, results favored neither phased nor incremental strategies.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.245736","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=245736","","Software testing;System testing;Software systems;Software engineering;Milling machines;Software reliability;Reliability engineering;Phase measurement;Software measurement","program testing;software reliability","artificial software systems;top-down strategies;code generator;sandwich integration strategies;verification processes;reliable systems;spot unit testing;bottom-up integration;system reliability;defect correction;big-bang strategies","","10","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Using term rewriting to verify software","S. Antoy; J. Gannon","Dept. of Comput. Sci., Portland State Univ., OR, USA; NA","IEEE Transactions on Software Engineering","","1994","20","4","259","274","This paper describes a uniform approach to the automation of verification tasks associated with while statements, representation functions for abstract data types, generic program units, and abstract base classes. Program units are annotated with equations containing symbols defined by algebraic axioms. An operation's axioms are developed by using strategies that guarantee crucial properties such as convergence and sufficient completeness. Sets of axioms are developed by stepwise extensions that preserve these properties. Verifications are performed with the aid of a program that incorporates term rewriting, structural induction, and heuristics based on ideas used in the Boyer-Moore prover. The program provides valuable mechanical assistance: managing inductive arguments and providing hints for necessary lemmas, without which formal proofs would be impossible. The successes and limitations of our approaches are illustrated with examples from each domain.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.277574","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=277574","","Equations;Computer science;Concrete;Automation;Convergence;Tail;Abstracts;Turing machines","abstract data types;rewriting systems;program verification;theorem proving;software tools","term rewriting;verification tasks;while statements;representation functions;abstract data types;generic program units;abstract base classes;algebraic axioms;convergence;sufficient completeness;structural induction;Boyer-Moore prover;mechanical assistance","","8","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic feature learning for predicting vulnerable software components","H. K. Dam; T. Tran; T. T. M. Pham; S. W. Ng; J. Grundy; A. Ghose","School of Computing and Information Technology, University of Wollongong, Wollongong, New South Wales Australia (e-mail: hoa@uow.edu.au); Centre for Pattern Recognition and Data Analytics, Deakin University, Waurn Ponds, Victoria Australia 3216 (e-mail: truyen.tran@deakin.edu.au); 434468, Deakin University, Waurn Ponds, Victoria Australia 3216 (e-mail: phtra@deakin.edu.au); School of Computing and Information Technology, University of Wollongong, Wollongong, New South Wales Australia (e-mail: swn881@uowmail.edu.au); Faculty of Information Technology, Monash University, Melbourne, Victoria Australia (e-mail: john.grundy@monash.edu); School of Computing and Information Technology, University of Wollongong, 8691 Wollongong, New South Wales Australia (e-mail: aditya@uow.edu.au)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Code flaws or vulnerabilities are prevalent in software systems and can potentially cause a variety of problems including deadlock, hacking, information loss and system failure. A variety of approaches have been developed to try and detect the most likely locations of such code vulnerabilities in large code bases. Most of them rely on manually designing code features (e.g. complexity metrics or frequencies of code tokens) that represent the characteristics of the potentially problematic code to locate. However, all suffer from challenges in sufficiently capturing both semantic and syntactic representation of source code, an important capability for building accurate prediction models. In this paper, we describe a new approach, built upon the powerful deep learning Long Short Term Memory model, to automatically learn both semantic and syntactic features of code. Our evaluation on 18 Android applications and the Firefox application demonstrates that the prediction power obtained from our learned features is better than what is achieved by state of the art vulnerability prediction models, for both within-project prediction and cross-project prediction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2881961","Samsung; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8540022","Software vulnerability prediction;Mining software engineering repositories;Empirical software engineering","Semantics;Software systems;Predictive models;Security;Feature extraction;System recovery","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Views for Multilevel Database Security","D. E. Denning; S. G. Akl; M. Heckman; T. F. Lunt; M. Morgenstern; P. G. Neumann; R. R. Schell","SRI International; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","2","129","140","Because views on relational database systems mathematically define arbitrary sets of stored and derived data, they have been proposed as a way of handling context-and content-dependent classification, dynamic classification, inference, aggregation, and sanitization in multilevel database systems. This paper describes basic view concepts for a multilevel-secure relational database model that addresses the above issues. All data entering the database are labeled according to views called classification constraints, which specify access classes for related data. In addition, views called aggregation constraints restrict access to aggregates of information. All data accesses are confined to a third set of views called access views.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232889","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702194","Classification;multilevel security;protection;relational databases;security;views","Data security;Relational databases;Database systems;National security;Aggregates;Multilevel systems;Protection;Information security;Computer security","","Classification;multilevel security;protection;relational databases;security;views","","9","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Tradeoffs in the design of efficient algorithm-based error detection schemes for hypercube multiprocessors","V. Balasubramanian; P. Banerjee","Dept. of Electr. Eng., Illinois Univ., Urbana, IL, USA; Dept. of Electr. Eng., Illinois Univ., Urbana, IL, USA","IEEE Transactions on Software Engineering","","1990","16","2","183","196","The authors provide an in-depth study of the various issues and tradeoffs available in algorithm-based error detection, as well as a general methodology for evaluating the schemes. They illustrate the approach on an extremely useful computation in the field of numerical linear algebra: QR factorization. They have implemented and investigated numerous ways of applying algorithm-based error detection using different system-level encoding strategies for QR factorization. Specifically, schemes based on the checksum and sum-of-squares (SOS) encoding techniques have been developed. The results of studies performed on a 16-processor Intel iPSC-2/D4/MX hypercube multiprocessor are reported. It is shown that, in general, the SOS approach gives much better coverage (85-100%) for QR factorization while maintaining low overheads (below 10%).<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44381","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44381","","Algorithm design and analysis;Hypercubes;Fault detection;Parallel algorithms;Hardware;Concurrent computing;Electrical fault detection;Costs;Computer errors;Computer architecture","encoding;error detection;linear algebra;multiprocessing systems;software engineering","hypercube multiprocessors;algorithm-based error detection;numerical linear algebra;QR factorization;encoding;checksum;sum-of-squares;16-processor Intel iPSC-2/D4/MX","","15","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Toward a Smell-Aware Bug Prediction Model","F. Palomba; M. Zanoni; F. A. Fontana; A. De Lucia; R. Oliveto","Delft University of Technology, Delft, The Netherlands; University of Milano-Bicocca, Milano, MI, Italy; University of Milano-Bicocca, Milano, MI, Italy; University of Salerno, Fisciano, SA, Italy; University of Molise, Campobasso, Italy","IEEE Transactions on Software Engineering","","2019","45","2","194","218","Code smells are symptoms of poor design and implementation choices. Previous studies empirically assessed the impact of smells on code quality and clearly indicate their negative impact on maintainability, including a higher bug-proneness of components affected by code smells. In this paper, we capture previous findings on bug-proneness to build a specialized bug prediction model for smelly classes. Specifically, we evaluate the contribution of a measure of the severity of code smells (i.e., code smell intensity) by adding it to existing bug prediction models based on both product and process metrics, and comparing the results of the new model against the baseline models. Results indicate that the accuracy of a bug prediction model increases by adding the code smell intensity as predictor. We also compare the results achieved by the proposed model with the ones of an alternative technique which considers metrics about the history of code smells in files, finding that our model works generally better. However, we observed interesting complementarities between the set of <italic>buggy and smelly</italic> classes correctly classified by the two models. By evaluating the actual information gain provided by the intensity index with respect to the other metrics in the model, we found that the intensity index is a relevant feature for both product and process metrics-based models. At the same time, the metric counting the average number of code smells in previous versions of a class considered by the alternative model is also able to reduce the entropy of the model. On the basis of this result, we devise and evaluate a <italic>smell-aware</italic> combined bug prediction model that included product, process, and smell-related features. We demonstrate how such model classifies bug-prone code components with an F-Measure at least 13 percent higher than the existing state-of-the-art models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2770122","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8097044","Code smells;bug prediction;empirical study;mining software repositories","Computer bugs;Measurement;Predictive models;Indexes;Software;Complexity theory;Entropy","","","","2","","132","","","","","","IEEE","IEEE Journals & Magazines"
"Performance analysis of concurrency control using locking with deferred blocking","P. S. Yu; D. M. Dias","Res. Div., IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; Res. Div., IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","1993","19","10","982","996","The concurrency control (CC) method employed can be critical to the performance of transaction processing systems. Conventional locking suffers from the blocking phenomenon, where waiting transactions continue to hold locks and block other transactions from progressing. In a high data contention environment, as an increasing number of transactions wait, a larger number of lock requests get blocked and fewer lock requests can get through. The proposed scheme reduces the blocking probability by deferring the blocking behavior of transactions to the later stages of their execution. By properly balancing the blocking and abort effects, the proposed scheme can lead to better performance than either the conventional locking or the optimistic concurrency control (OCC) schemes at all data and resource contention levels. We consider both static and dynamic approaches to determine when to switch from the nonblocking phase to the blocking phase. An analytical model is developed to estimate the performance of this scheme and determine the optimal operating or switching point. The accuracy of the analytic model is validated through a detailed simulation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.245740","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=245740","","Performance analysis;Concurrency control;Broadcasting;Switches;Analytical models;Database systems;Throughput;Transaction databases;Control system analysis","concurrency control;distributed databases;performance evaluation;transaction processing","concurrency control;locking;deferred blocking;transaction processing systems;performance analysis;high data contention environment;blocking probability;optimistic concurrency control;resource contention;switching point;simulation","","8","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Monitoring Software Development Through Dynamic Variables","C. W. Doerflinger; V. R. Basili","Texas Instruments Incorporated; NA","IEEE Transactions on Software Engineering","","1985","SE-11","9","978","985","This paper describes research conducted by the Software Engineering Laboratory (SEL) on the use of dynamic variables as a tool to monitor software development. The intent of the project is to identify project independent measures which may be used in a management tool for monitoring software development. This study examines several Fortran projects with similar profiles. The staff was experienced in developing these types of projects. The projects developed serve similar functions. Because these projects are similar we believe some underlying relationships exist that are invariant between the projects. These relationships, once well defined, may be used to compare the development of different projects to determine whether they are evolving the same way previous projects in this environment evolved.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232833","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702118","Database;management tool;measurement;monitoring software development","Programming;Project management;Aerodynamics;Software engineering;Software measurement;NASA;Laboratories;Software development management;Computerized monitoring;Computer science","","Database;management tool;measurement;monitoring software development","","5","","9","","","","","","IEEE","IEEE Journals & Magazines"
"A protocol modeling and verification approach based on a specification language and Petri nets","T. Suzuki; S. M. Shatz; T. Murata","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA","IEEE Transactions on Software Engineering","","1990","16","5","523","536","An approach for automated modeling and verification of communication protocols is presented. A language that specifies the input/output behavior of protocol entities is introduced as the starting point of the approach, and verification of the linguistic specifications is discussed. Rules for conversion of the specifications into a Petri net model (based on a timed Petri net) are presented and illustrated by examples. This leads to a second level of verification on the net model. The approach is illustrated by its application to a part of the LAPD protocol.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.52775","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=52775","","Protocols;Specification languages;Petri nets;Software engineering;Concurrent computing;Computer science;Automata;Time factors","Petri nets;program verification;protocols;specification languages","conversion rules;specification language;Petri nets;automated modeling;verification;communication protocols;input/output behavior;linguistic specifications;timed Petri net;LAPD protocol","","28","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Refactoring Inspection Support for Manual Refactoring Edits","E. L. G. Alves; M. Song; T. Massoni; P. D. L. Machado; M. Kim","Computer Science Department, Federal University of Campina Grande, Campina Grande, Brazil; Computer Science Department, The University of Nebraska at Omaha, Omaha, NE; Computer Science Department, Universidade Federal de Campina Grande, Campina Grande, PB, Brazil; Systems and Computing Department, Federal University of Campina Grande, Campina Grande, Brazil; Computer Science Department, University of California, Los Angeles, CA","IEEE Transactions on Software Engineering","","2018","44","4","365","383","Refactoring is commonly performed manually, supported by regression testing, which serves as a safety net to provide confidence on the edits performed. However, inadequate test suites may prevent developers from initiating or performing refactorings. We propose RefDistiller, a static analysis approach to support the inspection of manual refactorings. It combines two techniques. First, it applies predefined templates to identify potential missed edits during manual refactoring. Second, it leverages an automated refactoring engine to identify extra edits that might be incorrect. RefDistiller also helps determine the root cause of detected anomalies. In our evaluation, RefDistiller identifies 97 percent of seeded anomalies, of which 24 percent are not detected by generated test suites. Compared to running existing regression test suites, it detects 22 times more anomalies, with 94 percent precision on average. In a study with 15 professional developers, the participants inspected problematic refactorings with RefDistiller versus testing only. With RefDistiller, participants located 90 percent of the seeded anomalies, while they located only 13 percent with testing. The results show RefDistiller can help check the correctness of manual refactorings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2679742","National Science Foundation; Google Faculty Award; National Institute of Science and Technology for Software Engineering; CNPq/Brasil; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7874212","Refactoring;refactoring anomalies;code inspection","Manuals;Inspection;Testing;Computer bugs;Transforms;Engines;Detectors","program diagnostics;program testing;regression analysis;software maintenance","RefDistiller;seeded anomalies;refactoring inspection support;manual refactoring edits;regression testing;inadequate test suites;potential missed edits;automated refactoring engine;extra edits;generated test suites;running existing regression test suites;problematic refactorings","","1","","49","","","","","","IEEE","IEEE Journals & Magazines"
"User-process communication performance in networks of computers","L. -. Cabrera; E. Hunter; M. J. Karels; D. A. Hosher","Dept. of Comput. Sci., IBM Almaden Res. Center, San Jose, CA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1988","14","1","38","53","The authors present a study of the performance achieved by user processes when using the IPC mechanisms as implemented in Berkeley Unix 4.2BSD in Ethernet based environments. The authors assess not only the impact that different processors, network hardware interfaces, and Ethernets have on the communication across machines, but also the effect of the loading of the hosts and communication media that participate in the interprocess communication mechanism. The measurements highlight the ultimate bounds on performance that may be achieved by user process applications communicating across machines, and serve as a guide in designing performance-critical applications. A detailed timing analysis is presented of the dynamic behavior of the TCP/IP and the UDP/IP network communication protocols' implementation in Berkeley Unix 4.2BSD.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4621","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4621","","Intelligent networks;Computer networks;Protocols;Ethernet networks;Hardware;Telecommunication network reliability;Operating systems;Computer science;Application software;Timing","local area networks;network operating systems;protocols","network protocols;user processes;IPC mechanisms;Berkeley Unix 4.2BSD;Ethernet;network hardware interfaces;communication media;interprocess communication mechanism;performance-critical applications;timing analysis;TCP/IP;UDP/IP","","43","","26","","","","","","IEEE","IEEE Journals & Magazines"
"API-Based and Information-Theoretic Metrics for Measuring the Quality of Software Modularization","S. Sarkar; G. M. Rama; A. C. Kak","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","1","14","32","We present in this paper a new set of metrics that measure the quality of modularization of a non-object-oriented software system. We have proposed a set of design principles to capture the notion of modularity and defined metrics centered around these principles. These metrics characterize the software from a variety of perspectives: structural, architectural, and notions such as the similarity of purpose and commonality of goals. (By structural, we are referring to intermodule coupling-based notions, and by architectural, we mean the horizontal layering of modules in large software systems.) We employ the notion of API (application programming interface) as the basis for our structural metrics. The rest of the metrics we present are in support of those that are based on API. Some of the important support metrics include those that characterize each module on the basis of the similarity of purpose of the services offered by the module. These metrics are based on information-theoretic principles. We tested our metrics on some popular open-source systems and some large legacy-code business applications. To validate the metrics, we compared the results obtained on human-modularized versions of the software (as created by the developers of the software) with those obtained on randomized versions of the code. For randomized versions, the assignment of the individual functions to modules was randomized","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256942","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027146","Metrics/measurement;modules and interfaces;information theory;distribution;maintenance and enhancement;maintainability;coupling;layered architecture.","Software measurement;Software quality;Software systems;Application software;Open source software;Data structures;Software maintenance;System testing;Information theory;Computer architecture","application program interfaces;information theory;object-oriented programming;software architecture;software metrics;software quality","information-theoretic metrics;quality measurement;software modularization;nonobject-oriented software system;application programming interface;open-source systems;legacy-code business applications;human-modularized versions;randomized versions","","51","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental Test Generation for Software Product Lines","E. Uzuncaova; S. Khurshid; D. Batory","Microsoft, Redmond; University of Texas at Austin, Austin; University of Texas at Austin, Austin","IEEE Transactions on Software Engineering","","2010","36","3","309","322","Recent advances in mechanical techniques for systematic testing have increased our ability to automatically find subtle bugs, and hence, to deploy more dependable software. This paper builds on one such systematic technique, scope-bounded testing, to develop a novel specification-based approach for efficiently generating tests for products in a software product line. Given properties of features as first-order logic formulas in Alloy, our approach uses SAT-based analysis to automatically generate test inputs for each product in a product line. To ensure soundness of generation, we introduce an automatic technique for mapping a formula that specifies a feature into a transformation that defines incremental refinement of test suites. Our experimental results using different data structure product lines show that an incremental approach can provide an order of magnitude speedup over conventional techniques. We also present a further optimization using dedicated integer constraint solvers for feature properties that introduce integer constraints, and show how to use a combination of solvers in tandem for solving Alloy formulas.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5456077","Software/program verification;testing and debugging;software engineering.","Software testing;Automatic testing;System testing;Computer bugs;Logic testing;Data structures;Software quality;Automatic logic units;Acoustic testing;Constraint optimization","computability;data structures;program testing;program verification","incremental test generation;software product lines;mechanical techniques;systematic testing;scope-bounded testing;specification-based approach;first-order logic formulas;SAT-based analysis;data structure product lines;dedicated integer constraint solvers;Alloy formulas;program verification","","32","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Software Reliability Analysis Using Weakest Preconditions in Linear Assignment Programs","H. Luo; X. Liu; X. Chen; T. Long; R. Jiang","Department of Measurement and Control Engineering, School of Manufacturing Science and Engineering, Sichuan University, Chengdu, P.R.China; School of Computer Science, McGill University, Montreal H3A0E9, Canada; School of Computer Science, McGill University, Montreal H3A0E9, Canada; Department of Control Engineering, Chengdu University of Information Technology, Shuangliu, P.R.China; School of Electrical Engineering and Information, Sichuan University, Chengdu, P.R.China","IEEE Transactions on Software Engineering","","2016","42","9","866","885","Weakest preconditions derived from triple axiomatic semantics have been widely used to prove the correctness of programs. They can also be applied to evaluate the reliability of software. However, deducing a weakest precondition, as well as determining its propagation path, encounters challenges such as unknown constraint conditions, symbol computation and means of representation. To address these challenges, in this paper, we utilize the disjunctive normal form of if-else branch structure to capture reasonable propagation paths of the weakest precondition. Meanwhile, by removing the sequential dependencies, we demonstrate how to get the weakest precondition of loop-structure by leveraging program function. Moreover, we extensively explore three modeling characteristics (i.e., path extension, innermost connection and condition leap) for deducing the weakest precondition of structured programs. Finally, taking the definition of program node and storage structure of weakest precondition as bases, we design a serial of modeling algorithms. Based on symbol computation and recursive call technology with Depth-First Search (DFS), our algorithms can not only be used to deduce the weakest precondition, but also to capture the propagate path of the weakest precondition. Experiments illustrate the efficacy and effectiveness of our proposed models and designed deductive algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2521379","Research Foundation of Young Teachers in Sichuan University of P.R. China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7398131","Weakest precondition;path extension;innermost connection;condition leap;node;cell-structure","Algorithm design and analysis;Semantics;Software reliability;Computational modeling;Computer bugs;Cognition","linear programming;search problems;software reliability","software reliability analysis;weakest preconditions;linear assignment program;triple axiomatic semantics;program correctness;propagation path;if-else branch structure;loop-structure precondition;path extension characteristic;innermost connection characteristic;condition leap characteristic;symbol computation;recursive call technology;depth-first search;DFS;deductive algorithms","","1","","31","","","","","","IEEE","IEEE Journals & Magazines"
"The transformation schema: An extension of the data flow diagram to represent control and timing","P. T. Ward","Yourdon, Inc., New York, NY 10036","IEEE Transactions on Software Engineering","","1986","SE-12","2","198","210","The data flow diagram has been extensively used to model the data transformation aspects of proposed systems. However, previous definitions of the data flow diagram have not provided a comprehensive way to represent the interaction between the timing and control aspects of a system and its data transformation behavior. An extension of the data flow diagram called the transformation schema is described. This schema provides a notation and formation rules for building a comprehensive system model, and a set of execution rules to allow prediction of the behavior over time of a system modeled in this way. The notation and formation rules allow depiction of a system as a network of potentially concurrent `centers of activity' (transformations), and of data repositories (stores), linked by communication paths (flows). The execution rules provide a qualitative prediction rather than a quantitative one, describing the acceptance of inputs and the production of outputs by the transformations but not input and output values.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312936","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312936","Concurrent systems;data flow diagram;requirements modeling;software design;systems design;transformation schema","Delay;Data models;Production;Buffer storage;Transforms;Automata","flowcharting;programming","software engineering;transformation schema;data flow diagram;control;timing;notation;formation rules;system model;data repositories;communication paths","","63","","","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic model transformations using extended UML object diagrams in modeling environments","D. Milicev","Sch. of Electr. Eng., Belgrade Univ., Serbia","IEEE Transactions on Software Engineering","","2002","28","4","413","431","One of the most important features of modeling tools is generation of output. The output may be documentation, source code, net list, or any other presentation of the system being constructed. The process of output generation may be considered as automatic creation of a target model from a model in the source modeling domain. This translation does not need to be accomplished in a single step. Instead, a tool may generate multiple intermediate models as other views to the system. These models may be used either as better descriptions of the system, or as a descent down the abstraction levels of the user-defined model, gradually leading to the desired implementation. If the modeling domains have their metamodels defined in terms of object-oriented concepts, the models consist of instances of the abstractions from the metamodels and links between them. A new technique for specifying the mapping between different modeling domains is proposed in the paper. It uses UML object diagrams that show the instances and links of the target model that should be created during automatic translations. The diagrams are extended with the proposed concepts of conditional, repetitive, parameterized, and polymorphic model creation, implemented by the standard UML extensibility mechanisms. Several examples from different engineering domains are provided, illustrating the applicability and benefits of the approach. The first experimental results show that the specifications may lead to better reuse and shorter production time when developing customized output generators.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995438","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995438","","Unified modeling language","specification languages;formal specification;object-oriented programming;diagrams;software tools;software reusability","automatic model transformations;UML object diagrams;modeling tools;documentation;source code;net list;user-defined model;metamodels;object-oriented concepts;polymorphic model;extensibility mechanisms;experimental results;software reuse;domain-specific modeling;Unified Modeling Language;model-based output generation","","25","","32","","","","","","IEEE","IEEE Journals & Magazines"
"A rational design process: How and why to fake it","D. L. Parnas; P. C. Clements","Department of Computer Science, University of Victoria, Victoria, B. C. V8W 2Y2, Canada; Computer Science and Systems Branch, Naval Research Laboratory, Washington, DC 20375; Computer Science and Systems Branch, Naval Research Laboratory, Washington, DC 20375","IEEE Transactions on Software Engineering","","1986","SE-12","2","251","257","Many have sought a software design process that allows a program to be derived systematically from a precise statement of requirements. It is proposed that, although designing a real product in that way will not be successful, it is possible to produce documentation that makes it appear that the software was designed by such a process. The ideal process and the documentation that it requires are described. The authors explain why one should attempt to design according to the ideal process and why one should produce the documentation that would have been produced by that process. The contents of each of the required documents are outlined.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312940","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312940","Programming methods;software design;software documentation;software engineering","Documentation;Software design;Computers;Maintenance engineering;Mathematical model;Data structures","software engineering;system documentation","rational design process;statement of requirements;documentation","","103","","","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Studies of Pair Programming for CS/SE Teaching in Higher Education: A Systematic Literature Review","N. Salleh; E. Mendes; J. Grundy","International Islamic University of Malaysia, Kuala Lumpur and University of Auckland, Auckland; University of Auckland, Auckland; Swinburne University of Technology, Hawthorn","IEEE Transactions on Software Engineering","","2011","37","4","509","525","The objective of this paper is to present the current evidence relative to the effectiveness of pair programming (PP) as a pedagogical tool in higher education CS/SE courses. We performed a systematic literature review (SLR) of empirical studies that investigated factors affecting the effectiveness of PP for CS/SE students and studies that measured the effectiveness of PP for CS/SE students. Seventy-four papers were used in our synthesis of evidence, and 14 compatibility factors that can potentially affect PP's effectiveness as a pedagogical tool were identified. Results showed that students' skill level was the factor that affected PP's effectiveness the most. The most common measure used to gauge PP's effectiveness was time spent on programming. In addition, students' satisfaction when using PP was overall higher than when working solo. Our meta-analyses showed that PP was effective in improving students' grades on assignments. Finally, in the studies that used quality as a measure of effectiveness, the number of test cases succeeded, academic performance, and expert opinion were the quality measures mostly applied. The results of this SLR show two clear gaps in this research field: 1) a lack of studies focusing on pair compatibility factors aimed at making PP an effective pedagogical tool and 2) a lack of studies investigating PP for software design/modeling tasks in conjunction with programming tasks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5482588","Empirical studies;pair programming;systematic review.","Programming profession;Education;Educational programs;Computer science;Performance evaluation;Time measurement;Testing;Software design;Collaborative work;Algorithm design and analysis","educational technology;further education;software prototyping;teaching","pair programming;CS/SE teaching;higher education;systematic literature review;PP","","88","","75","","","","","","IEEE","IEEE Journals & Magazines"
"Quantitative Evaluation of Model-Driven Performance Analysis and Simulation of Component-Based Architectures","F. Brosig; P. Meier; S. Becker; A. Koziolek; H. Koziolek; S. Kounev","Department of Computer Science, University of Würzburg, Am Hubland, W&#x00FC;rzburg, Germany; Codecentric AG, Elsenheimerstr. 55a, M&#x00FC;nchen, Germany; Department of Software Engineering, University of Paderborn, Zukunftsmeile 1, Paderborn, Germany; Karlsruhe Institute of Technology (KIT), Am Fasanengarten 5, Karlsruhe, Germany; ABB Corporate Research, Wallstadter Str. 59, Ladenburg, Germany; Department of Computer Science, University of Würzburg, Am Hubland, W&#x00FC;rzburg, Germany","IEEE Transactions on Software Engineering","","2015","41","2","157","175","During the last decade, researchers have proposed a number of model transformations enabling performance predictions. These transformations map performance-annotated software architecture models into stochastic models solved by analytical means or by simulation. However, so far, a detailed quantitative evaluation of the accuracy and efficiency of different transformations is missing, making it hard to select an adequate transformation for a given context. This paper provides an in-depth comparison and quantitative evaluation of representative model transformations to, e.g., queueing petri nets and layered queueing networks. The semantic gaps between typical source model abstractions and the different analysis techniques are revealed. The accuracy and efficiency of each transformation are evaluated by considering four case studies representing systems of different size and complexity. The presented results and insights gained from the evaluation help software architects and performance engineers to select the appropriate transformation for a given context, thus significantly improving the usability of model transformations for performance prediction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2362755","DFG; Collaborative Research Center “On-The-Fly Computing”; DFG; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6920061","D.2.11 Software architectures;D.2.10.h Quality analysis and evaluation;D.2.2 Design tools and techniques;Software architectures;quality analysis and evaluation;design tools and techniques","Unified modeling language;Analytical models;Predictive models;Phase change materials;Accuracy;Stochastic processes;Software architecture","object-oriented programming;software architecture;software performance evaluation;stochastic processes","quantitative evaluation;model-driven performance analysis;component-based architectures;performance predictions;transformations map performance-annotated software architecture models;stochastic models;representative model transformations;semantic gaps;source model abstractions","","22","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Software Reliability Models: Assumptions, Limitations, and Applicability","A. L. Goel","Department of Electrical and Computer Engineering and the School of Computer and Information Science, Syracuse University","IEEE Transactions on Software Engineering","","1985","SE-11","12","1411","1423","A number of analytical models have been proposed during the past 15 years for assessing the reliability of a software system. In this paper we present an overview of the key modeling approaches, provide a critical analysis of the underlying assumptions, and assess the limitations and applicability of these models during the software development cycle. We also propose a step-by-step procedure for fitting a model and illustrate it via an analysis of failure data from a medium-sized real-time command and control software system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232177","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701963","Estimation;failure count models;fault seeding;input domain models;model fitting;NHPP;software reliability;times between failures","Software reliability;Software systems;Hardware;Programming profession;Software testing;Analytical models;Data analysis;Failure analysis;Real time systems;Command and control systems","","Estimation;failure count models;fault seeding;input domain models;model fitting;NHPP;software reliability;times between failures","","476","","49","","","","","","IEEE","IEEE Journals & Magazines"
"On the Effectiveness of Contracts as Test Oracles in the Detection and Diagnosis of Functional Faults in Concurrent Object-Oriented Software","W. Araujo; L. C. Briand; Y. Labiche","Juniper Networks, 1194 N Mathilda Ave, Sunnyvale, CA 94089; SnT Centre, University of Luxembourg, 6, rue Richard Coudenhove-Kalergi, L-1359 Luxembourg-Kirchberg, Luxembourg; Systems and Computer Engineering Department, Carleton University, 1125 Colonel By Drive, Ottawa, Canada","IEEE Transactions on Software Engineering","","2014","40","10","971","992","Design by contract (DbC) is a software development methodology that focuses on clearly defining the interfaces between components to produce better quality object-oriented software. Though there exists ample support for DbC for sequential programs, applying DbC to concurrent programs presents several challenges. Using Java as the target programming language, we tackle such challenges by augmenting the Java Modelling Language (JML) and modifying the JML compiler (jmlc) to generate runtime assertion checking code to support DbC in concurrent programs. We applied our solution in a carefully designed case study on a highly concurrent industrial software system from the telecommunications domain to assess the effectiveness of contracts as test oracles in detecting and diagnosing functional faults in concurrent software. Based on these results, clear and objective requirements are defined for contracts to be effective test oracles for concurrent programs whilst balancing the effort to design them. Effort is measured indirectly through the contract complexity measure (CCM), a measure we define. Main results include that contracts of a realistic level of completeness and complexity can detect around 76 percent of faults and reduce the diagnosis effort for such faults tenfold. We, therefore, show that DbC can be applied to concurrent software and can be a valuable tool to improve the economics of software engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339829","Juniper Networks; Luxembourg's National Research Fund; NSERC Discovery; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6857355","Software/Program Verification¿Programming by contract;Software Quality/SQA¿Measurement applied to SQA and V&V;Concurrent programming;object-oriented programming","Contracts;Java;Concurrent computing;Programming;Software;Message systems;Interference","concurrency control;contracts;fault diagnosis;Java;object-oriented methods;parallel programming;program compilers;program testing;software quality;software reliability","design by contract;software development methodology;object-oriented software quality;DbC;sequential programs;concurrent programs;target programming language;Java modelling language;JML compiler;JMLC;runtime assertion checking code;concurrent industrial software system;telecommunication domain;test oracles;functional fault diagnosing;functional fault detection;concurrent software;contract complexity measure;CCM;software engineering","","4","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Synthesizing Code for Resource Controllers","K. Ramamritham","Department of Computer and Information Science, University of Massachusetts","IEEE Transactions on Software Engineering","","1985","SE-11","8","774","783","A distributed system is viewed as a set of objects and processes utilizing the objects. If a shared object, known as a resource, is accessed concurrently, some mechanism is necessary to control use of the resource in order to satisfy the consistency and fairness requirements associated with the resource. These mechanisms are termed resource controllers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232526","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702086","Shared resources;specification;synchronization;synthesis;temporal logic","Object oriented modeling;Control system synthesis;Logic;Safety;Network synthesis;Software systems;Computer architecture;Communication networks;Computer languages;Control systems","","Shared resources;specification;synchronization;synthesis;temporal logic","","1","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Data Quality: Some Comments on the NASA Software Defect Datasets","M. Shepperd; Q. Song; Z. Sun; C. Mair","Brunel University, Uxbridge; Xi'an Jiaotong University, Xi'an; Xi'an Jiaotong University, Xi'an; Southampton Solent University, Southampton","IEEE Transactions on Software Engineering","","2013","39","9","1208","1215","Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6464273","Empirical software engineering;data quality;machine learning;defect prediction","NASA;Software;PROM;Educational institutions;Sun;Communities;Abstracts","data analysis;learning (artificial intelligence);pattern classification;software reliability","data quality;NASA software defect dataset;National Aeronautics and Space Administration;data replication;machine learning;software module classification;defect-prone classification;not-defect-prone classification;IEEE Transactions on Software Engineering;data preprocessing;dataset provenance","","98","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Making use of scenarios for validating analysis and design","W. Dzida; R. Freitag","Syst. Design Technol. Inst., Nat. Res. Center for Inf. Technol., St. Augustin, Germany; NA","IEEE Transactions on Software Engineering","","1998","24","12","1182","1196","Scenarios can help remedy the most serious obstacle in the design process that is a chronic lack of knowledge of the application domain. Moreover, scenarios can be employed in analysis and design to serve both illustrating the context of an envisaged usage (user's perspective) and demonstrating the design proposal in terms of the intended usage (analyst's perspective). In contrasting both perspectives by means of a dialectic process a synthesis can be achieved that incorporates a shared understanding. Validation is a process to achieve such an understanding. The semantic structure of types of scenarios is investigated thus illustrating how a context of use analysis according to ISO 9241-11 can be exploited for validation purposes. The role of scenarios in usability engineering is contrasted with traditional concepts of systems analysis as an attempt to narrow the bridge between software engineering and usability engineering.","0098-5589;1939-3520;2326-3881","","10.1109/32.738346","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=738346","","Proposals;Design engineering;Process design;Usability;Software engineering;Mirrors;ISO standards;Bridges;Software prototyping;Prototypes","systems analysis;formal verification;formal specification;ISO standards;software standards;user interfaces","scenarios;analysis validation;design validation;envisaged usage;user perspective;design proposal;intended usage;analyst perspective;dialectic process;shared understanding;semantic structure;use analysis;ISO 9241-11;usability engineering;systems analysis;software engineering","","25","","45","","","","","","IEEE","IEEE Journals & Magazines"
"Using redundancies to find errors","Yichen Xie; D. Engler","Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA","IEEE Transactions on Software Engineering","","2003","29","10","915","928","Programmers generally attempt to perform useful work. If they performed an action, it was because they believed it served some purpose. Redundant operations violate this belief. However, in the past, redundant operations have been typically regarded as minor cosmetic problems rather than serious errors. This paper demonstrates that, in fact, many redundancies are as serious as traditional hard errors (such as race conditions or null pointer dereferences). We experimentally test this idea by writing and applying five redundancy checkers to a number of large open source projects, finding many errors. We then show that, even when redundancies are harmless, they strongly correlate with the presence of traditional hard errors. Finally, we show how flagging redundant operations gives a way to detect mistakes and omissions in specifications. For example, a locking specification that binds shared variables to their protecting locks can use redundancies to detect missing bindings by flagging critical sections that include no shared state.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237172","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237172","","Redundancy;Writing;Programming profession;Testing;Protection;Software quality;System recovery;Computer languages;Robustness;Debugging","program compilers;redundancy;software quality","compilation;error detection;program redundancy;software quality;conceptual errors","","14","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Comparative analysis of different models of checkpointing and recovery","V. F. Nicola; J. M. van Spanje","Dept. of Comput. Sci., Duke Univ., Durham, NC, USA; NA","IEEE Transactions on Software Engineering","","1990","16","8","807","821","Different checkpointing strategies are combined with recovery models of different refinement levels in the database systems. The complexity of the resulting model increases with its accuracy in representing a realistic system. Three different analytic approaches are used depending on the complexity of the model: analytic, numerical and simulation. A Markovian queuing model is developed, resulting in a combined Poisson and load-dependent checkpointing strategy with stochastic recovery. A state-space analysis approach is used to derive semianalytic expressions for the performance variables in terms of a set of unknown boundary state probabilities. An efficient numerical algorithm for evaluating unknown probabilities is outlined. The validity of the numerical solution is checked against simulation results and shown to be of acceptable accuracy, particularly in the stable operating range. Simulations have shown that realistic load-dependent checkpointing results in performance close to the optimal deterministic checkpointing. Furthermore, the stochastic recovery model is an accurate representation of a realistic recovery.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.57620","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=57620","","Checkpointing;Database systems;Frequency;Stochastic processes;Analytical models;Queueing analysis;Performance analysis;Availability;Delay;Computer science","computational complexity;database management systems;database theory;probability;queueing theory;system recovery","DBMS;checkpointing strategies;recovery models;refinement levels;realistic system;analytic approaches;simulation;Markovian queuing model;Poisson;load-dependent checkpointing strategy;stochastic recovery;state-space analysis approach;semianalytic expressions;performance variables;unknown boundary state probabilities;numerical algorithm;numerical solution;stable operating range;optimal deterministic checkpointing","","37","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Extending objects to support multiple interfaces and access control","B. Hailpern; H. Ossher","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","1990","16","11","1247","1257","A mechanism, called views, that allows programmers to specify multiple interfaces for objects and to control explicitly access to each interface is described. This mechanism provides a simple and flexible means of specifying enforceable access restrictions at many levels of granularity. It also results in system organization that supports browsing based on a number of different criteria. Views is defined, some examples of its uses are given, the impact of views on system organization is discussed, and five approaches to implementing views are outlined.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60313","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60313","","Access control;Encapsulation;Senior members;Control systems;Protection;Data structures","data structures;object-oriented programming","multiple interfaces;access control;views;objects;enforceable access restrictions;granularity;system organization;browsing","","24","","30","","","","","","IEEE","IEEE Journals & Magazines"
"A characterization of independence for competing Markov chains with applications to stochastic Petri nets","R. J. Boucherie","Dept. of Econ., Amsterdam Univ., Netherlands","IEEE Transactions on Software Engineering","","1994","20","7","536","544","This paper shows that some of the recently obtained product form results for stochastic Petri nets can be obtained as a special case of a simple exclusion mechanism for the product process of a collection of Markov chains.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.297942","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=297942","","Stochastic processes;Petri nets;Resource management;Routing;Fires;Fellows;Traffic control;Equations;Sufficient conditions","Petri nets;Markov processes","competing Markov chains;stochastic Petri nets;simple exclusion mechanism;independence;resource sharing","","31","","23","","","","","","IEEE","IEEE Journals & Magazines"
"A System for Profiling and Monitoring Database Access Patterns by Application Programs for Anomaly Detection","L. Bossi; E. Bertino; S. R. Hussain","Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN","IEEE Transactions on Software Engineering","","2017","43","5","415","431","Database Management Systems (DBMSs) provide access control mechanisms that allow database administrators (DBAs) to grant application programs access privileges to databases. Though such mechanisms are powerful, in practice finer-grained access control mechanism tailored to the semantics of the data stored in the DMBS is required as a first class defense mechanism against smart attackers. Hence, custom written applications which access databases implement an additional layer of access control. Therefore, securing a database alone is not enough for such applications, as attackers aiming at stealing data can take advantage of vulnerabilities in the privileged applications and make these applications to issue malicious database queries. An access control mechanism can only prevent application programs from accessing the data to which the programs are not authorized, but it is unable to prevent misuse of the data to which application programs are authorized for access. Hence, we need a mechanism able to detect malicious behavior resulting from previously authorized applications. In this paper, we present the architecture of an anomaly detection mechanism, DetAnom, that aims to solve such problem. Our approach is based the analysis and profiling of the application in order to create a succinct representation of its interaction with the database. Such a profile keeps a signature for every submitted query and also the corresponding constraints that the application program must satisfy to submit the query. Later, in the detection phase, whenever the application issues a query, a module captures the query before it reaches the database and verifies the corresponding signature and constraints against the current context of the application. If there is a mismatch, the query is marked as anomalous. The main advantage of our anomaly detection mechanism is that, in order to build the application profiles, we need neither any previous knowledge of application vulnerabilities nor any example of possible attacks. As a result, our mechanism is able to protect the data from attacks tailored to database applications such as code modification attacks, SQL injections, and also from other data-centric attacks as well. We have implemented our mechanism with a software testing technique called concolic testing and the PostgreSQL DBMS. Experimental results show that our profiling technique is close to accurate, requires acceptable amount of time, and the detection mechanism incurs low runtime overhead.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2598336","Northrop Grumman Systems Corporation; Department of Homeland Security (DHS); Science and Technology Directorate; Homeland Security Advanced Research Projects Agency; Cyber Security Division; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7534833","Database;insider attacks;anomaly detection;application profile;SQL injection","Databases;Access control;Software testing;Software;Engines","authorisation;database management systems;digital signatures;program diagnostics;program testing;query processing;software architecture","database access patterns profiling;database access patterns monitoring;application programs;anomaly detection mechanism architecture;database management systems;DBMS;database administrator;DBA;defense mechanism;smart attackers;access control mechanism;malicious behavior detection;DetAnom;signature;query submission;software testing technique;concolic testing;PostgreSQL","","4","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Specifying Ada server tasks with executable formal grammars","D. Hemmendinger","Dept. of Comput. Sci., Union Coll., Schenectady, NY, USA","IEEE Transactions on Software Engineering","","1990","16","7","741","754","The author shows how a class of concurrent programming problems can be specified with formal grammars. These grammars, more powerful than path expressions, translate readily into Ada server tasks using the rendezvous and select-statement, though they may also be applied to other synchronization constructs. The grammars may be used to clarify informal specifications, to compare different specifications, and to analyze the behavior of implementations of such specifications. They may also be easily converted into Prolog programs that can be executed to generate the strings of events accepted by a grammar or by the Ada task being modeled. The automated translation from Ada to such grammars, and from grammatical specifications to Ada is discussed. The former facilitates the analysis of Ada programs; the latter yields Ada code of high quality.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.56100","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=56100","","Formal specifications;Concurrent computing;Programming profession;Automata;Formal languages;Computer science;Delay","Ada;formal specification;grammars;parallel programming","Ada server tasks specification;executable formal grammars;concurrent programming problems;synchronization constructs;Prolog programs","","4","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Seven layers of knowledge representation and reasoning in support of software development","C. Rich; Y. A. Feldman","Mitsubishi Electr. Res. Lab., Cambridge, MA, USA; NA","IEEE Transactions on Software Engineering","","1992","18","6","451","469","The authors' experience in the Programmer's Apprentice project in applying knowledge representation and automated reasoning to support software development is summarized. A system, called Cake, is described that comprises seven layers of knowledge representation and reasoning facilities: truth maintenance, Boolean constraint propagation, equality, types, algebra, frames, and Plan Calculus. Sessions with two experimental software development tools implemented using Cake, the Requirements Apprentice and the Debugging Assistant, are also included.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.142869","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=142869","","Knowledge representation;Programming;Debugging;Calculus;Software tools;Laboratories;Knowledge engineering;Algebra;Software maintenance;Software prototyping","inference mechanisms;knowledge representation;software engineering;software tools","knowledge representation;reasoning;software development;Programmer's Apprentice;Cake;truth maintenance;Boolean constraint propagation;equality;types;algebra;frames;Plan Calculus;software development tools;Requirements Apprentice;Debugging Assistant","","33","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Predicate logic for software engineering","D. L. Parnas","Dept. of Electr. & Comput. Eng., McMaster Univ., Hamilton, Ont., Canada","IEEE Transactions on Software Engineering","","1993","19","9","856","862","The interpretations of logical expressions found in most introductory textbooks are not suitable for use in software engineering applications because they do not deal with partial functions. More advanced papers and texts deal with partial functions in a variety of complex ways. This paper proposes a very simple change to the classic interpretation of predicate expressions, one that defines their value for all values of all variables, yet is almost identical to the standard definitions. It then illustrates the application of this interpretation in software documentation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.241769","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=241769","","Logic;Software engineering;Application software;Documentation;Design engineering;Mathematics;Programming profession;Calculus;Proposals;Solids","formal logic;software engineering","software engineering;logical expressions;partial functions;predicate expressions;predicate logic;software documentation;tabular expressions","","29","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Requirements specification for process-control systems","N. G. Leveson; M. P. E. Heimdahl; H. Hildreth; J. D. Reese","Dept. of Comput. Sci. & Eng., Washington Univ., Seattle, WA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1994","20","9","684","707","The paper describes an approach to writing requirements specifications for process-control systems, a specification language that supports this approach, and an example application of the approach and the language on an industrial aircraft collision avoidance system (TCAS II). The example specification demonstrates: the practicality of writing a formal requirements specification for a complex, process-control system; and the feasibility of building a formal model of a system using a specification language that is readable and reviewable by application experts who are not computer scientists or mathematicians. Some lessons learned in the process of this work, which are applicable both to forward and reverse engineering, are also presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.317428","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=317428","","Software safety;Writing;Reverse engineering;Control systems;Costs;Software systems;Software prototyping;System testing;Computer science;Specification languages","process control;process computer control;formal specification;specification languages;position control;aerospace computing;aircraft instrumentation","requirements specification;process-control systems;specification language;example application;industrial aircraft collision avoidance system;TCAS II;example specification;formal requirements specification;formal model;reverse engineering","","252","","26","","","","","","IEEE","IEEE Journals & Magazines"
"The TAME project: towards improvement-oriented software environments","V. R. Basili; H. D. Rombach","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1988","14","6","758","773","Experience from a dozen years of analyzing software engineering processes and products is summarized as a set of software engineering and measurement principles that argue for software engineering process models that integrate sound planning and analysis into the construction process. In the TAME (Tailoring A Measurement Environment) project at the University of Maryland, such an improvement-oriented software engineering process model was developed that uses the goal/question/metric paradigm to integrate the constructive and analytic aspects of software development. The model provides a mechanism for formalizing the characterization and planning tasks, controlling and improving projects based on quantitative analysis, learning in a deeper and more systematic way about the software process and product, and feeding the appropriate experience back into the current and future projects. The TAME system is an instantiation of the TAME software engineering process model as an ISEE (integrated software engineering environment). The first in a series of TAME system prototypes has been developed. An assessment of experience with this first limited prototype is presented including a reassessment of its initial architecture.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6156","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6156","","Software engineering;Software measurement;Process planning;Computer architecture;NASA;Computer science;Personnel;Programming;Software prototyping;Prototypes","programming environments;software engineering","TAME;process models;University of Maryland;improvement-oriented software engineering;ISEE;integrated software engineering environment","","599","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Toxic Code Snippets on Stack Overflow","C. Ragkhitwetsagul; J. Krinke; M. Paixao; G. Bianco; R. Oliveto","Computer Science, Faculty of Information and Communication Technology, Mahidol University, Salaya, Nakhon Pathom Thailand 73170 (e-mail: cragkhit@gmail.com); Computer Science, University College London, London, London United Kingdom of Great Britain and Northern Ireland WC1E 6BT (e-mail: j.krinke@ucl.ac.uk); Computer Science, Universidade Estadual do Ceara, 67843 Fortaleza, Ceara Brazil 60740-000 (e-mail: matheus.paixao@uece.br); Computer Science, Universita degli Studi del Molise, 18960 Campobasso, Campobasso Italy (e-mail: giuseppe@corsi.it); Department of Bioscience and Territory, University of Molise, Pesche, Isernia Italy 86090 (e-mail: rocco.oliveto@unimol.it)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Online code clones are code fragments that are copied from software projects or online sources to Stack Overflow as examples. Due to an absence of a checking mechanism after the code has been copied to Stack Overflow, they can become toxic code snippets, e.g., they suffer from being outdated or violating the original software license. We present a study of online code clones on Stack Overflow and their toxicity by incorporating two developer surveys and a large-scale code clone detection. A survey of 201 high-reputation Stack Overflow answerers (33% response rate) showed that 131 participants (65%) have ever been notified of outdated code and 26 of them (20%) rarely or never fix the code. 138 answerers (69%) never check for licensing conflicts between their copied code snippets and Stack Overflow?s CC BY-SA 3.0. A survey of 87 Stack Overflow visitors shows that they experienced several issues from Stack Overflow answers: mismatched solutions, outdated solutions, incorrect solutions, and buggy code. 85% of them are not aware of CC BY-SA 3.0 license enforced by Stack Overflow, and 66% never check for license conflicts when reusing code snippets. Our clone detection found online clone pairs between 72,365 Java code snippets on Stack Overflow and 111 open source projects in the curated Qualitas corpus. We analysed 2,289 non-trivial online clone candidates. Our investigation revealed strong evidence that 153 clones have been copied from a Qualitas project to Stack Overflow. We found 100 of them (66%) to be outdated, of which 10 were buggy and harmful for reuse. Furthermore, we found 214 code snippets that could potentially violate the license of their original software and appear 7,112 times in 2,427 GitHub projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2900307","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8643998","Code Clone Detection;Stack Overflow;Outdated Code;Software Licensing","Cloning;Licenses;Software;Programming;Computer bugs;Security;Tutorials","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Extending Ina Jo with temporal logic","J. M. Wing; M. R. Nixon","Dept. of Comput. Sci., Carnegie-Mellon Univ., Pittsburgh, PA, USA; NA","IEEE Transactions on Software Engineering","","1989","15","2","181","197","The authors give both informal and formal descriptions of both the current Ina Jo specification language and Ina Jo enhanced with temporal logic. They include details of a simple example to demonstrate the use of the proof system and details of an extended example to demonstrate the expressiveness of the enhanced language. The authors discuss their language design goals, decisions, and their implications.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21744","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21744","","Formal specifications;Concurrent computing;Specification languages;Security;Logic design;Programming;Software tools;Computer science;Trademarks;Logic functions","specification languages","Ina Jo;temporal logic;specification language;proof system;expressiveness;language design goals;decisions","","5","","39","","","","","","IEEE","IEEE Journals & Magazines"
"A Classification Framework for Software Component Models","I. Crnkovic; S. Sentilles; A. Vulgarakis; M. R. V. Chaudron","Mälardalen University, Västerås; Mälardalen University, Västerås; Mälardalen University, Västerås; Universiteit Leiden, Leiden","IEEE Transactions on Software Engineering","","2011","37","5","593","615","In the last decade, a large number of different software component models have been developed, with different aims and using different principles and technologies. This has resulted in a number of models which have many similarities, but also principal differences, and in many cases unclear concepts. Component-based development has not succeeded in providing standard principles, as has, for example, object-oriented development. In order to increase the understanding of the concepts and to differentiate component models more easily, this paper identifies, discusses, and characterizes fundamental principles of component models and provides a Component Model Classification Framework based on these principles. Further, the paper classifies a large number of component models using this framework.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.83","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5587419","Software components;software component models;component lifecycle;extra-functional properties;component composition.","Data models;Bismuth;Packaging","object-oriented programming;pattern classification","software component models;component based development;object oriented development;component model classification framework","","83","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Guaranteeing real-time requirements with resource-based calibration of periodic processes","R. Gerber; Seongsoo Hong; M. Saksena","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","1995","21","7","579","592","The paper presents a comprehensive design methodology for guaranteeing end to end requirements of real time systems. Applications are structured as a set of process components connected by asynchronous channels, in which the end points are the system's external inputs and outputs. Timing constraints are then postulated between these inputs and outputs; they express properties such as end to end propagation delay, temporal input sampling correlation, and allowable separation times between updated output values. The automated design method works as follows: First new tasks are created to correlate related inputs, and an optimization algorithm, whose objective is to minimize CPU utilization, transforms the end to end requirements into a set of intermediate rate constraints on the tasks. If the algorithm fails, a restructuring tool attempts to eliminate bottlenecks by transforming the application, which is then resubmitted into the assignment algorithm. The final result is a schedulable set of fully periodic tasks, which collaboratively maintain the end to end constraints.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.392979","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=392979","","Calibration;Timing;Real time systems;Design methodology;Constraint optimization;Design optimization;Temperature;Computer science;USA Councils;Performance analysis","real-time systems;formal specification;systems analysis;scheduling;operating systems (computers)","real-time requirements;resource-based calibration;periodic processes;comprehensive design methodology;end to end requirements guarantees;real time systems;process components;asynchronous channels;timing constraints;end to end propagation delay;temporal input sampling correlation;allowable separation times;updated output values;automated design method;optimization algorithm;CPU utilization;intermediate rate constraints;restructuring tool;assignment algorithm;schedulable set;fully periodic tasks;end to end constraint maintenance","","91","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting software designers with integrated domain-oriented design environments","G. Fischer; A. Girgensohn; K. Nakakoji; D. Redmiles","Dept. of Comput. Sci., Colorado Univ., Boulder, CO, USA; Dept. of Comput. Sci., Colorado Univ., Boulder, CO, USA; Dept. of Comput. Sci., Colorado Univ., Boulder, CO, USA; Dept. of Comput. Sci., Colorado Univ., Boulder, CO, USA","IEEE Transactions on Software Engineering","","1992","18","6","511","522","An approach that embeds human-computer cooperative problem-solving tools into knowledge-based design environments that work in conjunction with human software designers in specific application domains is described. This human-centered approach takes advantage of peoples' ability to understand and incrementally reformulate their problems, while allowing them to contribute to the gradual improvement of the underlying knowledge base. The notion of evolution circumvents the inability of the original builders of a design environment to anticipate all future needs and knowledge for complete coverage of a domain. The access and development of knowledge is supported in a cycle of location, comprehension, and modification. Modification includes the evolution of the knowledge base and tools. A framework for building such tools and mechanisms is described and illustrated in terms of three systems: CATALOGEXPLORER, EXPLAINER, and MODIFIER. User studies of these systems demonstrate the promise and the limitations of the design environment approach.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.142873","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=142873","","Software design;Humans;Design engineering;Application software;Software tools;Knowledge engineering;Automatic programming;Problem-solving;Embedded software;Buildings","expert systems;programming environments;user interfaces","problem reformulation;software designers;domain-oriented design environments;human-computer cooperative problem-solving tools;knowledge-based design environments;location;comprehension;modification;CATALOGEXPLORER;EXPLAINER;MODIFIER","","32","","57","","","","","","IEEE","IEEE Journals & Magazines"
"More experience with data flow testing","E. J. Weyuker","Dept. of Comput. Sci., New York Univ., NY, USA","IEEE Transactions on Software Engineering","","1993","19","9","912","919","Experience is provided about the cost and effectiveness of the Rapps-Weyuker data flow testing criteria. This experience is based on studies using a suite of well-known numerical programs, and supplements an earlier study (Weyuker 1990) using different types of programs. The conclusions drawn in the earlier study involving cost are confirmed in this study. New observations about tester variability and cost assessment, as well as fault detection, are also provided.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.241773","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=241773","","Costs;Fault detection;Software testing;Guidelines;Software tools;Computer bugs;NASA;Computer science;Software systems","program testing;software cost estimation","data flow testing;Rapps-Weyuker data flow testing criteria;numerical programs;tester variability;cost assessment;fault detection;software testing;data adequacy","","52","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Function points analysis: an empirical study of its measurement processes","A. Abran; P. N. Robillard","Quebec Univ., Montreal, Que., Canada; NA","IEEE Transactions on Software Engineering","","1996","22","12","895","910","Function point analysis (FPA) was initially designed on the basis of expert judgments, without explicit reference to any theoretical foundation. From the point of view of the measurement scales used in its measurement process, FPA constitutes a potpourri of scales not admissible without the transformations imbedded in the implicit models of expert judgments. The results of this empirical study demonstrate that in a homogeneous environment not burdened with major differences in productivity factors there is a clear relationship between FPA's primary components and work-effort. This empirical study also indicates that there is such a relationship for each step of the FPA measurement process prior to the mixing of scales and the assignments of weights. Comparisons with FPA productivity models based on weights confirm, on the one hand, that the weights do not add information and, on the other, that the weights are fairly robust and can be used when little historical data is available. The full data set is provided for future studies.","0098-5589;1939-3520;2326-3881","","10.1109/32.553638","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=553638","","Size measurement;Productivity;Application software;Artificial intelligence;Software engineering;Software measurement;Robustness;Measurement standards;Standards development;Standards publication","software cost estimation;software metrics;human resource management","function points analysis;measurement processes;measurement scales;homogeneous environment;productivity factors;work-effort;weight assignments","","55","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Programmer-transparent coordination of recovering concurrent processes: philosophy and rules for efficient implementation","K. H. Kim","Dept. of Electr. Eng., California Univ., Irvine, CA, USA","IEEE Transactions on Software Engineering","","1988","14","6","810","821","An approach to coordination of cooperating concurrent processes, each capable of error direction and recovery, is presented. Error detection, rollback, and retry in a process are specified by a well-structured language construct called recovery block. Recovery points of processes must be properly coordinated to prevent a disastrous avalanche of process rollbacks. The approach relies on an intelligent processor system (that runs processes) capable of establishing and discarding the recovery points of interacting processes in a well coordinated manner such that a process never makes two consecutive rollbacks without making a retry between the two, and every process rollback becomes a minimum-distance rollback. Following a discussion of the underlying philosophy of the author's approach, basic rules of reducing storage and time overhead in such a processor system are discussed. Examples are drawn from the systems in which processes communicate through monitors.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6160","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6160","","Military computing;Computer errors;Intelligent systems;Process design","data structures;error detection;fault tolerant computing;multiprocessing programs;programming theory;supervisory programs;system recovery","programmer transparent coordination;system recovery;storage reduction;data structures;recovering concurrent processes;error direction;language construct;recovery block;intelligent processor system;process rollback;minimum-distance rollback;time overhead","","30","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Hipikat: a project memory for software development","D. Cubranic; G. C. Murphy; J. Singer; K. S. Booth","Dept. of Comput. Sci., Victoria Univ., BC, Canada; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","6","446","465","Sociological and technical difficulties, such as a lack of informal encounters, can make it difficult for new members of noncollocated software development teams to learn from their more experienced colleagues. To address this situation, we have developed a tool, named Hipikat that provides developers with efficient and effective access to the group memory for a software development project that is implicitly formed by all of the artifacts produced during the development. This project memory is built automatically with little or no change to existing work practices. After describing the Hipikat tool, we present two studies investigating Hipikat's usefulness in software modification tasks. One study evaluated the usefulness of Hipikat's recommendations on a sample of 20 modification tasks performed on the Eclipse Java IDE during the development of release 2.1 of the Eclipse software. We describe the study, present quantitative measures of Hipikat's performance, and describe in detail three cases that illustrate a range of issues that we have identified in the results. In the other study, we evaluated whether software developers who are new to a project can benefit from the artifacts that Hipikat recommends from the project memory. We describe the study, present qualitative observations, and suggest implications of using project memory as a learning aid for project newcomers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.71","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463229","Index Terms- Software development teams;project memory;software artifacts;recommender system;user studies.","Programming;Virtual groups;Software systems;Computer Society;Software tools;Performance evaluation;Java;Software performance;Recommender systems;Employee welfare","software tools;project management;Java;programming environments;software performance evaluation","Hipikat tool;project memory;software development;software artifacts;recommerider system;user studies;software development project;software modification task;Eclipse Java IDE;Eclipse software;learning aid","","129","","44","","","","","","IEEE","IEEE Journals & Magazines"
"CSP methods for identifying atomic actions in the design of fault tolerant concurrent systems","A. M. Tyrrell; G. F. Carpenter","Dept. of Electron., York Univ., Heslington, UK; NA","IEEE Transactions on Software Engineering","","1995","21","7","629","639","Limiting the extent of error propagation when faults occur and localizing the subsequent error recovery are common concerns in the design of fault tolerant parallel processing systems. Both activities are made easier if the designer associates fault tolerance mechanisms with the underlying atomic actions of the system. With this in mind, the paper has investigated two methods for the identification of atomic actions in parallel processing systems described using CSP. Explicit trace evaluation forms the basis of the first algorithm, which enables a designer to analyze interprocess communications and thereby locate atomic action boundaries in a hierarchical fashion. The second method takes CSP descriptions of the parallel processes and uses structural arguments to infer the atomic action boundaries. This method avoids the difficulties involved with producing full trace sets, but does incur the penalty of a more complex algorithm.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.392983","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=392983","","Fault diagnosis;Fault tolerant systems;Fault tolerance;Algorithm design and analysis;Real time systems;Fault detection;Protection;Electronic switching systems;Parallel processing;Distributed processing","communicating sequential processes;parallel programming;software fault tolerance;program diagnostics","CSP methods;atomic actions;fault tolerant concurrent systems design;error propagation;error recovery;fault tolerant parallel processing systems;fault tolerance mechanisms;underlying atomic actions;explicit trace evaluation;interprocess communications;CSP descriptions;structural arguments;full trace sets;communicating sequential processes","","2","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Projecting software defects from analyzing Ada designs","W. W. Agresti; W. M. Evanco","Mitre Corp., McLean, VA, USA; Mitre Corp., McLean, VA, USA","IEEE Transactions on Software Engineering","","1992","18","11","988","997","Models for projecting software defects from analyses of Ada designs are described. The research is motivated by the need for technology to analyze designs for their likely effect on software quality. The models predict defect density based on product and process characteristics. Product characteristics are extracted from a static analysis of Ada subsystems, focusing on context coupling, visibility, and the import-export of declarations. Process characteristics provide for effects of reuse level and extent of changes. Multivariate regression analyses were conducted with empirical data from industry/government-developed projects: 16 Ada subsystems totaling 149000 source lines of code. The resulting models explain 63-74% of the variation in defect density of the subsystems. Context coupling emerged as a consistently significant variable in the models.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.177368","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=177368","","Software quality;Software design;Predictive models;Software reliability;Maintenance;Q factor;Government;Data mining;Multivariate regression;Context modeling","Ada;software metrics;software quality;software reliability;statistical analysis","software defects projection;context coupling;Ada designs;software quality;defect density;process characteristics;static analysis;visibility;import-export of declarations;reuse level;regression analyses","","50","","33","","","","","","IEEE","IEEE Journals & Magazines"
"How accurate is scientific software?","L. Hatton; A. Roberts","Programming Res. Ltd., Hersham, UK; NA","IEEE Transactions on Software Engineering","","1994","20","10","785","797","This paper describes some results of what, to the authors' knowledge, is the largest N-version programming experiment ever performed. The object of this ongoing four-year study is to attempt to determine just how consistent the results of scientific computation really are, and, from this, to estimate accuracy. The experiment is being carried out in a branch of the earth sciences known as seismic data processing, where 15 or so independently developed large commercial packages that implement mathematical algorithms from the same or similar published specifications in the same programming language (Fortran) have been developed over the last 20 years. The results of processing the same input dataset, using the same user-specified parameters, for nine of these packages is reported in this paper. Finally, feedback of obvious flaws was attempted to reduce the overall disagreement. The results are deeply disturbing. Whereas scientists like to think that their code is accurate to the precision of the arithmetic used, in this study, numerical disagreement grows at around the rate of 1% in average absolute difference per 4000 fines of implemented code, and, even worse, the nature of the disagreement is nonrandom. Furthermore, the seismic data processing industry has better than average quality standards for its software development with both identifiable quality assurance functions and substantial test datasets.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.328993","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=328993","","Data processing;Packaging;Geoscience;Computer languages;Feedback;Arithmetic;Computer industry;Software standards;Standards development;Programming","programming;seismology;geophysics computing;software packages;software quality","scientific software;N-version programming experiment;scientific computation;seismic data processing;large commercial packages;mathematical algorithms;programming language;Fortran;input dataset;seismic data processing industry;quality standards;software development;quality assurance","","58","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Modular Software Model Checking for Distributed Systems","W. Leungwattanakit; C. Artho; M. Hagiya; Y. Tanabe; M. Yamamoto; K. Takahashi","Department of Mathematics and Informatics, Faculty of Science, Chiba University, 1-3-3 Yayoicho, Inage Ward, Chiba, Japan; Research Institute for Secure Systems at the National Institute of Advanced Industrial Science and Technology (AIST), Amagasaki, Japan; Department of Computer Science, University of Tokyo, Science Building No. 7, 7-3-1 Hongo, Tokyo, Japan; National Institute of Informatics (NII), 2-1-2 Hitotsubashi, Japan; Department of Mathematics and Informatics, Faculty of Science, Chiba University, 1-3-3 Yayoicho, Inage Ward, Chiba, Japan; Research Institute for Secure Systems at the National Institute of Advanced Industrial Science and Technology (AIST), Amagasaki, Japan","IEEE Transactions on Software Engineering","","2014","40","5","483","501","Distributed systems are complex, being usually composed of several subsystems running in parallel. Concurrent execution and inter-process communication in these systems are prone to errors that are difficult to detect by traditional testing, which does not cover every possible program execution. Unlike testing, model checking can detect such faults in a concurrent system by exploring every possible state of the system. However, most model-checking techniques require that a system be described in a modeling language. Although this simplifies verification, faults may be introduced in the implementation. Recently, some model checkers verify program code at runtime but tend to be limited to stand-alone programs. This paper proposes cache-based model checking, which relaxes this limitation to some extent by verifying one process at a time and running other processes in another execution environment. This approach has been implemented as an extension of Java PathFinder, a Java model checker. It is a scalable and promising technique to handle distributed systems. To support a larger class of distributed systems, a checkpointing tool is also integrated into the verification system. Experimental results on various distributed systems show the capability and scalability of cache-based model checking.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6645368","Software model checking;software verification;distributed systems;checkpointing","Model checking;Software;Java;Checkpointing;Synchronization;Scalability;Message systems","cache storage;checkpointing;concurrency control;distributed processing;Java;program testing;program verification","modular software model checking;distributed systems;parallel subsystems;concurrent execution;interprocess communication;program execution;fault detection;model-checking techniques;modeling language;program code verification;stand-alone programs;cache-based model checking;execution environment;Java PathFinder extension;Java model checker;checkpointing tool","","7","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Toward understanding the rhetoric of small source code changes","R. Purushothaman; D. E. Perry","Server Operating Syst. Group, Dell Comput. Corp., Round Rock, TX, USA; NA","IEEE Transactions on Software Engineering","","2005","31","6","511","526","Understanding the impact of software changes has been a challenge since software systems were first developed. With the increasing size and complexity of systems, this problem has become more difficult. There are many ways to identify the impact of changes on the system from the plethora of software artifacts produced during development, maintenance, and evolution. We present the analysis of the software development process using change and defect history data. Specifically, we address the problem of small changes by focusing on the properties of the changes rather than the properties of the code itself. Our study reveals that 1) there is less than 4 percent probability that a one-line change introduces a fault in the code, 2) nearly 10 percent of all changes made during the maintenance of the software under consideration were one-line changes, 3) nearly 50 percent of the changes were small changes, 4) nearly 40 percent of changes to fix faults resulted in further faults, 5) the phenomena of change differs for additions, deletions, and modifications as well as for the number of lines affected, and 6) deletions of up to 10 lines did not cause faults.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.74","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463233","Index Terms- Source code changes;software faults;one-line changes;fault probabilities.","Rhetoric;Software maintenance;Risk management;Software systems;Programming;Costs;Computer Society;History;Computer architecture;Life testing","software fault tolerance;software maintenance;reverse engineering","source code changes;software faults;one-line changes;fault probabilities;software system;plethora;software artifacts;software development process;defect history data;software maintenance","","71","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Program slicing with dynamic points-to sets","M. Mock; D. C. Atkinson; C. Chambers; S. J. Eggers","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","8","657","678","Program slicing is a potentially useful analysis for aiding program understanding. However, in reality even slices of small programs are often too large to be useful. Imprecise pointer analyses have been suggested as one cause of this problem. In this paper, we use dynamic points-to data, which represents optimistic pointer information, to obtain a bound on the best case slice size improvement that can be achieved with improved pointer precision. Our experiments show that slice size can be reduced significantly for programs that make frequent use of calls through function pointers because for them the dynamic pointer data results in a considerably smaller call graph, which leads to fewer data dependences. Programs without or with only few calls through function pointers, however, show considerably less improvement. We discovered that C programs appear to have a significant fraction of direct and nonspurious pointer data dependences so that reducing spurious dependences via pointers is only of limited benefit. Consequently, to make slicing useful in general for such programs, improvements beyond better pointer analyses are necessary. On the other hand, since we show that collecting dynamic function pointer information can be performed with little overhead (average slowdown of 10 percent for our benchmarks), dynamic pointer information may be a practical approach to making slicing of programs with frequent function pointer use more successful in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.94","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498771","Index Terms- Dynamic analysis;points-to analysis;program slicing.","Data analysis;Computer Society;Application software;Programming profession;Debugging;Software maintenance;Software testing;Reverse engineering;Computer languages;Information analysis","program slicing;reverse engineering;software cost estimation","program slicing;optimistic pointer information;function pointers;call graph;data dependences;C programs;dynamic function pointer information;dynamic points-to set analysis;program understanding","","20","","61","","","","","","IEEE","IEEE Journals & Magazines"
"RELAI Testing: A Technique to Assess and Improve Software Reliability","D. Cotroneo; R. Pietrantuono; S. Russo","Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione (DIETI), Università di Napoli Federico II, Via Claudio 21, Naples, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione (DIETI), Università di Napoli Federico II, Via Claudio 21, Naples, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione (DIETI), Università di Napoli Federico II, Via Claudio 21, Naples, Italy","IEEE Transactions on Software Engineering","","2016","42","5","452","475","Testing software to assess or improve reliability presents several practical challenges. Conventional operational testing is a fundamental strategy that simulates the real usage of the system in order to expose failures with the highest occurrence probability. However, practitioners find it unsuitable for assessing/achieving very high reliability levels; also, they do not see the adoption of a “real” usage profile estimate as a sensible idea, being it a source of non-quantifiable uncertainty. Oppositely, debug testing aims to expose as many failures as possible, but regardless of their impact on runtime reliability. These strategies are used either to assess or to improve reliability, but cannot improve and assess reliability in the same testing session. This article proposes Reliability Assessment and Improvement (RELAI) testing, a new technique thought to improve the delivered reliability by an adaptive testing scheme, while providing, at the same time, a continuous assessment of reliability attained through testing and fault removal. The technique also quantifies the impact of a partial knowledge of the operational profile. RELAI is positively evaluated on four software applications compared, in separate experiments, with techniques conceived either for reliability improvement or for reliability assessment, demonstrating substantial improvements in both cases.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2491931","European Commission; FP7 Marie Curie Industry-Academia Partnerships and Pathways (IAPP); MIUR; SVEVIA; COSMIC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7299696","Software Testing;Reliability;Operational Testing;Random Testing;Sampling;Operational Profile;Software testing;reliability;operational testing;random testing;sampling;operational profile","Testing;Software reliability;Software;Uncertainty;Estimation error","probability;program debugging;program testing;software reliability","RELAI testing;software testing;operational testing;nonquantifiable uncertainty;debug testing;software failures;runtime reliability;reliability assessment and improvement testing;software reliability;adaptive testing scheme;continuous reliability assessment;fault removal;software applications","","5","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Verifying Linearizability via Optimized Refinement Checking","Y. Liu; W. Chen; Y. A. Liu; J. Sun; S. J. Zhang; J. S. Dong","Nanyang Technological University, Singapore; Microsoft Research Asia, Beijing; State University of New York at Stony Brook, Stony Brook; Singapore University of Technology and Design, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore","IEEE Transactions on Software Engineering","","2013","39","7","1018","1039","Linearizability is an important correctness criterion for implementations of concurrent objects. Automatic checking of linearizability is challenging because it requires checking that: (1) All executions of concurrent operations are serializable, and (2) the serialized executions are correct with respect to the sequential semantics. In this work, we describe a method to automatically check linearizability based on refinement relations from abstract specifications to concrete implementations. The method does not require that linearization points in the implementations be given, which is often difficult or impossible. However, the method takes advantage of linearization points if they are given. The method is based on refinement checking of finite-state systems specified as concurrent processes with shared variables. To tackle state space explosion, we develop and apply symmetry reduction, dynamic partial order reduction, and a combination of both for refinement checking. We have built the method into the PAT model checker, and used PAT to automatically check a variety of implementations of concurrent objects, including the first algorithm for scalable nonzero indicators. Our system is able to find all known and injected bugs in these implementations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.82","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363443","Linearizability;refinement;model checking;PAT","History;Sun;Educational institutions;Optimization;Electronic mail;Semantics","concurrency control;formal specification;program debugging;program verification;programming language semantics;refinement calculus","linearizability verification;optimized refinement checking;concurrent objects;automatic linearizability checking;concurrent operations;serialized executions;sequential semantics;refinement relations;linearization points;finite-state systems;shared variables;symmetry reduction;dynamic partial-order reduction;PAT model checker;scalable nonzero indicators;bug detection","","11","","62","","","","","","IEEE","IEEE Journals & Magazines"
"GossipKit: A Unified ComponentFramework for Gossip","F. Taïani; S. Lin; G. S. Blair","University of Rennes 1 / IRISA Rennes, France; SAP Labs, China; University of Lancaster, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","2","123","136","Although the principles of gossip protocols are relatively easy to grasp, their variety can make their design and evaluation highly time consuming. This problem is compounded by the lack of a unified programming framework for gossip, which means developers cannot easily reuse, compose, or adapt existing solutions to fit their needs, and have limited opportunities to share knowledge and ideas. In this paper, we consider how component frameworks, which have been widely applied to implement middleware solutions, can facilitate the development of gossip-based systems in a way that is both generic and simple. We show how such an approach can maximize code reuse, simplify the implementation of gossip protocols, and facilitate dynamic evolution and redeployment.Also known as “epidemic” protocols.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6645372","Distributed systems;components;frameworks;protocols","Protocols;Peer-to-peer computing;Programming;Wireless sensor networks;Ad hoc networks;Software;Assembly","distributed processing;object-oriented programming;protocols","GossipKit;unified component framework;gossip protocols;protocol design;protocol evaluation;unified programming framework;middleware solutions;gossip-based systems;code reuse;epidemic protocols","","2","","77","","","","","","IEEE","IEEE Journals & Magazines"
"MAP 2.1 conformance testing tools","R. S. Matthews; K. H. Muralidhar; S. Sparks","Commun. & Distributed Syst. Lab., Ind. Technol. Inst., Ann Arbor, MI, USA; Commun. & Distributed Syst. Lab., Ind. Technol. Inst., Ann Arbor, MI, USA; Commun. & Distributed Syst. Lab., Ind. Technol. Inst., Ann Arbor, MI, USA","IEEE Transactions on Software Engineering","","1988","14","3","363","374","The major components of the MAP 2.1 conformance test system are described. Protocol conformance testing and program testing are compared. The scope and process of dynamic conformance testing is reviewed. The architecture of the tests system is presented, first in terms of the developing ISO (International Organization for Standardization) framework, and then in terms of run-time components. Several specific tools which comprise the test system are described. These tools include test engines, a high-level control tool, monitoring and analysis tools, and document handling tools. Benefits and limitations of the test tools are examined. Conclusions and suggestions for future efforts are provided.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4656","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4656","","System testing;Software testing;Software tools;Open systems;Communication system control;Manufacturing automation;Access protocols;Runtime;Engines;Level control","manufacturing computer control;program testing;protocols;software tools","MAP 2.1 conformance testing tools;program testing;ISO;run-time components;test engines;high-level control tool;monitoring;document handling tools","","7","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Software defect association mining and defect correction effort prediction","Qinbao Song; M. Shepperd; M. Cartwright; C. Mair","Dept. of Comput. Sci. & Technol., Xi'an Jiaotong Univ., China; NA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","2","69","82","Much current software defect prediction work focuses on the number of defects remaining in a software system. In this paper, we present association rule mining based methods to predict defect associations and defect correction effort. This is to help developers detect software defects and assist project managers in allocating testing resources more effectively. We applied the proposed methods to the SEL defect data consisting of more than 200 projects over more than 15 years. The results show that, for defect association prediction, the accuracy is very high and the false-negative rate is very low. Likewise, for the defect correction effort prediction, the accuracy for both defect isolation effort prediction and defect correction effort prediction are also high. We compared the defect correction effort prediction method with other types of methods - PART, C4.5, and Naive Bayes - and show that accuracy has been improved by at least 23 percent. We also evaluated the impact of support and confidence levels on prediction accuracy, false-negative rate, false-positive rate, and the number of rules. We found that higher support and confidence levels may not result in higher prediction accuracy, and a sufficient number of rules is a precondition for high prediction accuracy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599417","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599417","Software defect prediction;defect association;defect isolation effort;defect correction effort.","Software systems;Accuracy;Software quality;Data mining;Resource management;Job shop scheduling;Inspection;Association rules;Project management;Software development management","program testing;data mining;software quality;software process improvement","software defect association prediction;defect correction effort prediction;SEL defect data;association rule mining;software quality","","80","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Concurrency control in distributed databases through time intervals and short-term locks","U. Halici; A. Dogac","Dept. of Electr. & Electron. Eng., Middle East Technic. Univ., Ankara, Turkey; NA","IEEE Transactions on Software Engineering","","1989","15","8","994","1003","A method for concurrency control in distributed database management systems that increases the level of concurrent execution of transactions, called ordering by serialization numbers (OSN), is proposed. The OSN method works in the certifier model and uses time-interval techniques in conjunction with short-term locks to provide serializability and prevent deadlocks. The scheduler is distributed, and the standard transaction execution policy is assumed, that is, the read and write operations are issued continuously during transaction execution. However, the write operations are copied into the database only when the transaction commits. The amount of concurrency provided by the OSN method is demonstrated by log classification. It is shown that the OSN method provides more concurrency than basic timestamp ordering and two-phase locking methods and handles successfully some logs which cannot be handled by any of the past methods. The complexity analysis of the algorithm indicates that the method works in a reasonable amount of time.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31355","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31355","","Concurrency control;Distributed databases;Concurrent computing;Transaction databases;System recovery;Costs;Testing;Algorithm design and analysis;Performance analysis;Communication system control","concurrency control;distributed databases","concurrent transaction execution;distributed scheduler;distributed databases;concurrency control;distributed database management systems;ordering by serialization numbers;OSN method;certifier model;time-interval techniques;short-term locks;serializability;deadlocks;standard transaction execution policy;concurrency;log classification;timestamp ordering;two-phase locking;complexity analysis","","7","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Facilitating Coordination between Software Developers: A Study and Techniques for Timely and Efficient Recommendations","K. Blincoe; G. Valetto; D. Damian","Software Engineering Global Interaction Lab, Victoria, BC, Canada; Fondazione Bruno Kessler, Trento, Italy; Software Engineering Global Interaction Lab, Victoria, BC, Canada","IEEE Transactions on Software Engineering","","2015","41","10","969","985","When software developers fail to coordinate, build failures, duplication of work, schedule slips and software defects can result. However, developers are often unaware of when they need to coordinate, and existing methods and tools that help make developers aware of their coordination needs do not provide timely or efficient recommendations. We describe our techniques to identify timely and efficient coordination recommendations, which we developed and evaluated in a study of coordination needs in the Mylyn software project. We describe how data obtained from tools that capture developer actions within their Integrated Development Environment (IDE) as they occur can be used to timely identify coordination needs; we also describe how properties of tasks coupled with machine learning can focus coordination recommendations to those that are more critical to the developers to reduce information overload and provide more efficient recommendations. We motivate our techniques through developer interviews and report on our quantitative analysis of coordination needs in the Mylyn project. Our results suggest that by leveraging IDE logging facilities, properties of tasks and machine learning techniques awareness tools could make developers aware of critical coordination needs in a timely way. We conclude by discussing implications for software engineering research and tool design.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2431680","US National Science Foundation (NSF); NECSIS; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7105409","Human Factors in Software Design;Management;Metrics/Measurement;Productivity;Programming Teams;Computer-supported cooperative work;human factors in software design;management;metrics/measurement;productivity;programming teams","Software;Encoding;Interviews;Statistical analysis;Manuals;Accuracy;Correlation","groupware;learning (artificial intelligence);programming environments;project management;software tools","software developers;coordination recommendation;Mylyn software project;integrated development environment;coordination needs quantitative analysis;IDE logging facilities;task properties;machine learning technique awareness tools;software engineering research;tool design","","3","","70","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing Ordered Throughput Using Autonomic Cloud Bursting Schedulers","S. Kailasam; N. Gnanasambandam; J. Dharanipragada; N. Sharma","Indian Institute of Technology Madras, Chennai; Xerox Research Center Webster, New York; Indian Institute of Technology Madras, Chennai; Xerox Research Center Webster, New York","IEEE Transactions on Software Engineering","","2013","39","11","1564","1581","Optimizing ordered throughput not only improves the system efficiency but also makes cloud bursting transparent to the user. This is critical from the perspective of user fairness in customer-facing systems, correctness in stream processing systems, and so on. In this paper, we consider optimizing ordered throughput for near real-time, data-intensive, independent computations using cloud bursting. Intercloud computation of data-intensive applications is a challenge due to large data transfer requirements, low intercloud bandwidth, and best-effort traffic on the Internet. The system model we consider is comprised of two processing stages. The first stage uses cloud bursting opportunistically for parallel processing, while the second stage (sequential) expects the output of the first stage to be in the same order as the arrival sequence. We propose three scheduling heuristics as part of an autonomic cloud bursting approach that adapt to changing workload characteristics, variation in bandwidth, and available resources to optimize ordered throughput. We also characterize the operational regimes for cloud bursting as stabilization mode versus acceleration mode, depending on the workload characteristics like the size of data to be transferred for a given compute load. The operational regime characterization helps in deciding how many instances can be optimally utilized in the external cloud.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6520852","Cloud bursting;ordered throughput;autonomic;data-intensive","Cloud computing;Optimization;Scheduling","cloud computing;fault tolerant computing;parallel processing;scheduling","external cloud;operational regime characterization;data size;acceleration mode;stabilization mode;bandwidth variation;workload characteristics;scheduling heuristics;arrival sequence;parallel processing;Internet traffic;intercloud bandwidth;large data transfer requirements;data-intensive applications;intercloud computation;near real-time data-intensive independent computation;stream processing system correctness;customer-facing systems;user fairness;system efficiency improvement;autonomic cloud bursting schedulers;ordered throughput optimization","","13","","41","","","","","","IEEE","IEEE Journals & Magazines"
"The Evolution of Wang Institute's Master of Software Engineering Program","M. A. Ardis","Software Engineering Institute, Carnegie-Mellon University","IEEE Transactions on Software Engineering","","1987","SE-13","11","1149","1155","Master of Software Engineering (MSE) programs are relatively new. Starting such a program is expensive in terms of human and capital resources. Some of the costs are: preparation of new course materials, acquisition of sophisticated equipment and software, and maintenance of a low student/faculty ratio. In addition, MSE students and faculty have special needs, such as technical background and familiarity with current industrial practices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232863","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702161","Computer science education;Master of Software Engineering;professional-degree programs;programming methods;project management;software tools","Software engineering;Software maintenance;Educational institutions;Information technology;Buildings;Humans;Costs;Refining;Educational programs;Programming profession","","Computer science education;Master of Software Engineering;professional-degree programs;programming methods;project management;software tools","","3","","6","","","","","","IEEE","IEEE Journals & Magazines"
"How Do Users Revise Answers on Technical Q&A Websites? A Case Study on Stack Overflow","S. Wang; T. P. Chen; A. E. Hassan","School of Computing, Queen's University, 4257 Kingston, Ontario Canada (e-mail: shaowei@cs.queensu.ca); Computer Science and Software Engineering, Concordia University, 5618 Montreal, Quebec Canada H3G 2W1 (e-mail: peterc@encs.concordia.ca); School of Computing, Queen's University, Kingston, Ontario Canada K7L 3N6 (e-mail: ahmed@cs.queensu.ca)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","To ensure the quality of its shared knowledge, Stack Overflow encourages users to revise answers through a badge system, which is based on quantitative measures (e.g., a badge is awarded after revising more than 500 answers). Prior studies show that badges can positively steer the user behavior on Stack Overflow (e.g., increasing user participation). However, little is known whether revision-related badges have a negative impact on the quality of revisions since some studies show that certain users may game incentive systems to gain rewards. In this study, we analyze 3,871,966 revision records that are collected from 2,377,692 Stack Overflow answers. We find that: 1) Users performed a much larger than usual revisions on the badge-awarding days compared to normal days; 25% of the users did not make any more revisions once they received their first revision-related badge. 2) Performing more revisions than usual in a single day increased the likelihood of such revisions being rolled back (e.g., due to undesired or incorrect revisions). 3) Users were more likely to perform text and small revisions if they performed many revisions in a single day. Our findings are concurred by the Stack Overflow community, and they highlight the need for changes to the current badge system in order to provide a better balance between the quality and quantity of revisions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2874470","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8485395","Stack Overflow;Incentive System;Badge;Answer Revision","","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"The Impact of Correlated Metrics on the Interpretation of Defect Models","J. Jiarpakdee; C. Tantithamthavorn; A. E. Hassan","Faculty of Information Technology, Monash University, 2541 Clayton, Victoria Australia (e-mail: jirayus.jiar@gmail.com); Faculty of Information Technology, Monash University, 2541 Clayton, Victoria Australia (e-mail: chakkrit.tantithamthavorn@monash.edu); School of Computing, Queen's University, Kingston, Ontario Canada K7L 3N6 (e-mail: ahmed@cs.queensu.ca)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Defect models are analytical models for building empirical theories related to software quality. Prior studies often derive knowledge from such models using interpretation techniques, e.g., ANOVA Type-I. Recent work raises concerns that correlated metrics may impact the interpretation of defect models. Yet, the impact of correlated metrics in such models has not been investigated. In this paper, we investigate the impact of correlated metrics on the interpretation of defect models and the improvement of the interpretation of defect models when removing correlated metrics. Through a case study of 14 publicly-available defect datasets, we find that (1) correlated metrics have the largest impact on the consistency, the level of discrepancy, and the direction of the ranking of metrics, especially for ANOVA techniques. On the other hand, we find that removing all correlated metrics (2) improves the consistency of the produced rankings regardless of the ordering of metrics (except for ANOVA Type-I); (3) improves the consistency of ranking of metrics among the studied interpretation techniques; (4) impacts the model performance by less than 5 percentage points. Thus, when one wishes to derive sound interpretation from defect models, one must (1) mitigate correlated metrics especially for ANOVA analyses; and (2) avoid using ANOVA Type-I even if all correlated metrics are removed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2891758","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8608002","Software Quality Assurance;Defect models;Hypothesis Testing;Correlated Metrics;Model Specification","Measurement;Analytical models;Analysis of variance;Complexity theory;Software quality;Computer bugs;Correlation","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Experimentation in software engineering","V. R. Basili; R. W. Selby; D. H. Hutchens","Department of Computer Science, University of Maryland, College Park, MD 20742; Department of Computer SCience, University of Maryland, College Park, MD 20742; Department of Information and Computer Science, University of California, Irvine, CA 92717; Department of Computer Science, Clemson University, Clemson, SC 29634","IEEE Transactions on Software Engineering","","1986","SE-12","7","733","743","A framework is presented for analyzing most of the experimental work performed in software engineering over the past several years. The framework of experimentation consists of four categories corresponding to phases of the experimentation process: definition, planning, operation, and interpretation. A variety of experiments are described within the framework and their contribution to the software engineering discipline is discussed. Some recommendations for the application of the experimental process in software engineering are included.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312975","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312975","Controlled experiment;data collection and analysis;empirical study;experimental design;software metrics;software technology measurement and evaluation","Software;Testing;Software engineering;Measurement;Educational institutions;Programming;Debugging","software engineering","software engineering;experimental process","","61","","","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing the Defect Reduction Benefits of Code Inspection and Test-Driven Development","J. W. Wilkerson; J. F. Nunamaker; R. Mercer","Pennsylvania State University, Erie, Erie; University of Arizona, Tucson; University of Arizona, Tucson","IEEE Transactions on Software Engineering","","2012","38","3","547","560","This study is a quasi experiment comparing the software defect rates and implementation costs of two methods of software defect reduction: code inspection and test-driven development. We divided participants, consisting of junior and senior computer science students at a large Southwestern university, into four groups using a two-by-two, between-subjects, factorial design and asked them to complete the same programming assignment using either test-driven development, code inspection, both, or neither. We compared resulting defect counts and implementation costs across groups. We found that code inspection is more effective than test-driven development at reducing defects, but that code inspection is also more expensive. We also found that test-driven development was no more effective at reducing defects than traditional programming methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5750007","Agile programming;code inspections and walk throughs;reliability;test-driven development;testing strategies;empirical study.","Inspection;Software;Testing;Java;Writing;Programming profession","program testing;system recovery","defect reduction benefits;code inspection;test driven development;quasi experiment;software defect rates;software defect reduction;senior computer science students;junior computer science students;programming assignment","","7","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Detection and Resolution of Lexical Ambiguity in Process Models","F. Pittke; H. Leopold; J. Mendling","Institute for Information Business, Vienna, WU, Austria; Department of Computer Science, VU University Amsterdam, The Netherlands; Institute for Information Business, Vienna, WU, Austria","IEEE Transactions on Software Engineering","","2015","41","6","526","544","System-related engineering tasks are often conducted using process models. In this context, it is essential that these models do not contain structural or terminological inconsistencies. To this end, several automatic analysis techniques have been proposed to support quality assurance. While formal properties of control flow can be checked in an automated fashion, there is a lack of techniques addressing textual quality. More specifically, there is currently no technique available for handling the issue of lexical ambiguity caused by homonyms and synonyms. In this paper, we address this research gap and propose a technique that detects and resolves lexical ambiguities in process models. We evaluate the technique using three process model collections from practice varying in size, domain, and degree of standardization. The evaluation demonstrates that the technique significantly reduces the level of lexical ambiguity and that meaningful candidates are proposed for resolving ambiguity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2396895","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7027184","Identification of Lexical Ambiguity;Resolution of Lexical Ambiguity;Business Process Models;Identification of lexical ambiguity;resolution of lexical ambiguity;business process models","Unified modeling language;Object oriented modeling;Context;Business;Natural languages;Manuals;Vectors","computational linguistics;grammars;natural language processing;software engineering","automatic detection;automatic resolution;lexical ambiguity;system-related engineering task;structural inconsistency;terminological inconsistency;automatic analysis technique;quality assurance;control flow;automated fashion;textual quality;homonyms;synonyms;process model collection","","22","","106","","","","","","IEEE","IEEE Journals & Magazines"
"A layered architecture for uniform version management","B. Westfechtel; B. P. Munch; R. Conradi","Dept. of Comput. Sci. III, Aachen Univ. of Technol., Germany; NA; NA","IEEE Transactions on Software Engineering","","2001","27","12","1111","1133","Version management is a key part of software configuration management. A big variety of version models has been realized in both commercial systems and research prototypes. These version models differ with respect to the objects put under version control (files, directories, entities, objects), the organization of versions (version graphs versus multidimensional version spaces), the granularity of versioning (whole software products versus individual components), emphasis on states versus emphasis on changes (state-versus change-based versioning), rules for version selection, etc. We present a uniform version model-and its support architecture-for software configuration management. Unlike other unification approaches, such as UML for object-oriented modeling, we do not assemble all the concepts having been introduced in previous systems. Instead, we define a base model that is built on a small number of concepts. Specific version models may be expressed in terms of this base model. Our approach to uniform version management is distinguished by its underlying layered architecture. Unlike the main stream of software configuration management systems, our instrumentable version engine is completely orthogonal to the data model used for representing software objects and their relationships. In addition, we introduce version rules at the bottom of the layered architecture and employ them as a uniform mechanism for expressing different version models. This contrasts to the main stream solution, where a specific version model-usually version graphs-is deeply built into the system and version rules are dependent on this model.","0098-5589;1939-3520;2326-3881","","10.1109/32.988710","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988710","","Object oriented modeling;Computer architecture;Software prototyping;Prototypes;Multidimensional systems;Unified modeling language;Assembly systems;Software systems;Instruments;Engines","configuration management;software architecture","uniform version management;layered architecture;support architecture;software configuration management;granularity;states;uniform version model;instrumentable version engine","","31","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Uncertainty Analysis in Software Reliability Modeling by Bayesian Analysis with Maximum-Entropy Principle","Y. Dai; M. Xie; Q. Long; S. Ng","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","11","781","795","In software reliability modeling, the parameters of the model are typically estimated from the test data of the corresponding component. However, the widely used point estimators are subject to random variations in the data, resulting in uncertainties in these estimated parameters. Ignoring the parameter uncertainty can result in grossly underestimating the uncertainty in the total system reliability. This paper attempts to study and quantify the uncertainties in the software reliability modeling of a single component with correlated parameters and in a large system with numerous components. Another characteristic challenge in software testing and reliability is the lack of available failure data from a single test, which often makes modeling difficult. This lack of data poses a bigger challenge in the uncertainty analysis of the software reliability modeling. To overcome this challenge, this paper proposes utilizing experts' opinions and historical data from previous projects to complement the small number of observations to quantify the uncertainties. This is done by combining the maximum-entropy principle (MEP) into the Bayesian approach. This paper further considers the uncertainty analysis at the system level, which contains multiple components, each with its respective model/parameter/ uncertainty, by using a Monte Carlo approach. Some examples with different modeling approaches (NHPP, Markov, Graph theory) are illustrated to show the generality and effectiveness of the proposed approach. Furthermore, we illustrate how the proposed approach for considering the uncertainties in various components improves a large-scale system reliability model.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70739","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339233","Software Reliability;Uncertainty analysis;Bayesian method;Monte Carlo;Markov model;Graph theory","Uncertainty;Software reliability;Bayesian methods;Parameter estimation;Software testing;Uncertain systems;Monte Carlo methods;Graph theory;Reliability theory;Predictive models","Bayes methods;maximum entropy methods;Monte Carlo methods;parameter estimation;program testing;software reliability","uncertainty analysis;software reliability modeling;Bayesian approach;maximum-entropy principle;point estimators;parameter estimation;parameter uncertainty;system reliability;software testing;Monte Carlo approach","","44","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Combining Perceptions and Prescriptions in Requirements Engineering Process Assessment: An Industrial Case Study","N. P. Napier; L. Mathiassen; R. D. Johnson","Georgia Gwinnett College, Lawrenceville; Georgia State University, Atlanta; University of Pretoria, Pretoria","IEEE Transactions on Software Engineering","","2009","35","5","593","606","Requirements engineering (RE) is a key discipline in software development and several methods are available to help assess and improve RE processes. However, these methods rely on prescriptive models of RE; they do not, like other disciplines within software engineering, draw directly on stakeholder perceptions and subjective judgments. Given this backdrop, we present an empirical study in RE process assessment. Our aim was to investigate how stakeholder perceptions and process prescriptions can be combined during assessments to effectively inform RE process improvement. We first describe existing methods for RE process assessment and the role played by stakeholder perceptions and subjective judgments in the software engineering and management literature. We then present a method that combines perceptions and prescriptions in RE assessments together with an industrial case study in which the method was applied and evaluated over a three-year period at TelSoft. The data suggest that the combined method led to a comprehensive and rich assessment and it helped TelSoft consider RE as an important and integral part of the broader engineering context. This, in turn, led to improvements that combined plan-driven and adaptive principles for RE. Overall, the combined method helped TelSoft move from Level 1 to Level 2 in RE maturity, and the employees perceived the resulting engineering practices to be improved. Based on these results, we suggest that software managers and researchers combine stakeholder perceptions and process prescriptions as one way to effectively balance the specificity, comparability, and accuracy of software process assessments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4967614","Process implementation and change;qualitative process analysis;requirements engineering process;software management;software process.","Software engineering;Computer industry;Programming;Engineering management;Software quality;Project management;Risk management;Educational institutions;Data engineering;Quality management","formal specification;formal verification;project management;software development management;software maintenance;software process improvement;software quality;statistical analysis;systems analysis","requirements engineering process assessment;industrial case study;software development;RE prescription model;software engineering;stakeholder perception;subjective judgment;empirical study;software RE process improvement;software project management;TelSoft;plan-driven principle;adaptive principle;software process change;qualitative process analysis","","17","","56","","","","","","IEEE","IEEE Journals & Magazines"
"A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches","S. Herbold; A. Trautsch; J. Grabowski","University of Goettingen, Göttingen, Germany; University of Goettingen, Göttingen, Germany; University of Goettingen, Göttingen, Germany","IEEE Transactions on Software Engineering","","2018","44","9","811","833","Cross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article, we provide a benchmark for CPDP. We replicate 24 approaches proposed by researchers between 2008 and 2015 and evaluate their performance on software products from five different data sets. Based on our benchmark, we determined that an approach proposed by Camargo Cruz and Ochimizu (2009) based on data standardization performs best and is always ranked among the statistically significant best results for all metrics and data sets. Approaches proposed by Turhan et al. (2009), Menzies et al. (2011), and Watanabe et al. (2008) are also nearly always among the best results. Moreover, we determined that predictions only seldom achieve a high performance of 0.75 recall, precision, and accuracy. Thus, CPDP still has not reached a point where the performance of the results is sufficient for the application in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2724538","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7972992","Cross-project defect prediction;benchmark;comparison;replication","Benchmark testing;Prediction methods;Software;Quality assurance;Measurement;Correlation","project management;quality assurance;software metrics;software quality","benchmark cross-Project Defect Prediction approaches;CPDP;quality assurance;software projects;software products;data standardization;performance metrics;data sets","","3","","90","","","","","","IEEE","IEEE Journals & Magazines"
"Perturbation techniques for detecting domain errors","S. J. Zeil","Dept. of Comput. Sci., Old Dominion Univ., Norfolk, VA, USA","IEEE Transactions on Software Engineering","","1989","15","6","737","746","Perturbation testing is an approach to software testing which focuses on faults within arithmetic expressions appearing throughout a program. This approach is expanded to permit analysis of individual test points rather than entire paths, and to concentrate on domain errors. Faults are modeled as perturbing functions drawn from a vector space of potential faults and added to the correct form of an arithmetic expression. Sensitivity measures are derived which limit the possible size of those faults that would go undetected after the execution of a given test set. These measures open up an interesting view of testing, in which attempts are made to reduce the volume of possible faults which, were they present in the program being tested, would have escaped detection on all tests performed so far. The combination of these measures with standard optimization techniques yields a novel test-data-generation method called arithmetic fault detection.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24727","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24727","","Perturbation methods;Software testing;Error correction;Fault detection;Size measurement;Volume measurement;Measurement standards;Optimization methods;Floating-point arithmetic;Computer science","error detection;perturbation techniques;program testing","error detection;sensitivity measures;software testing;arithmetic expressions;individual test points;domain errors;perturbing functions;vector space;potential faults;test set;standard optimization techniques;novel test-data-generation method;arithmetic fault detection","","22","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Approximate analysis of reader/writer queues","T. Johnson","Dept. of Comput. & Inf. Sci., Florida Univ., Gainesville, FL, USA","IEEE Transactions on Software Engineering","","1995","21","3","209","218","We analyze the performance of queues that serve readers and writers. Readers are served concurrently, while writers require exclusive service. We approximately analyze a first-come-first-serve (FCFS) reader/writer queue, and derive simple formulae for computing waiting times and capacity under the assumption of Poisson arrivals and exponential service. We extend the analysis to handle a one writer queue, and a queue that includes write intention locks. The simple analyses that we present can be used as rules of thumb for designing concurrent systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.372148","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=372148","","Queueing analysis;Databases;Performance analysis;Concurrency control;Predictive models;Data analysis;Algorithm design and analysis;Operating systems;Concurrent computing;Delay","queueing theory;concurrency control;resource allocation","approximate analysis;reader/writer queues;exclusive service;first-come-first-serve;FCFS reader/writer queue;waiting times;Poisson arrivals;exponential service;one writer queue;write intention locks;concurrent systems design;performance analysis;queuing system;resource allocation;concurrency control algorithms","","12","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Load-leveling in fault-tolerant distributed computing systems","L. M. Patnaik; K. V. Iyer","School of Automation, Indian Institute of Science, Bangalore 560012, India; Vikram Sarabhai Space Centre, Trivandrum 695022, India","IEEE Transactions on Software Engineering","","1986","SE-12","4","554","560","Assuming a horizontally distributed computing system, formulations of the edge-failure and node-failure recovery problems from the standpoint of load-leveling are presented. The conditions for the existence of solutions to these problems are examined and simple algorithms are proposed for these problems. In connection with the node-failure recovery problem, the concept of a node-failure metric to characterize different possible solutions is introduced, exploiting the notion of the strength of processors. A possible application of the recovery methods in the context of reconfiguration of distributed database systems is suggested.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312903","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312903","Distributed computing systems;fault-recovery;horizontal distribution;load-leveling;work load allocation","Program processors;Resource management;Measurement;Context;Distributed computing;Throughput;Arrays","distributed processing;fault tolerant computing;system recovery","load leveling;fault-tolerant distributed computing systems;horizontally distributed computing system;edge-failure;node-failure recovery;recovery methods;reconfiguration","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Designing Autonomic Management Systems by Using Reactive Control Techniques","N. Berthier; É. Rutten; N. De Palma; S. M. Gueye","ERODS team, University of Grenoble LIG Bât. C, 220 rue de la Chimie, St Martin d'Hères, France; LIG/INRIA Grenoble - Rhône-Alpes, Inovallée, 655 av. de l'Europe, Montbonnot, St Ismier, France; ERODS team, University of Grenoble, LIG Bât. C, 220 rue de la Chimie, St Martin d'Hères, France; ERODS team, University of Grenoble, LIG Bât. C, 220 rue de la Chimie, St Martin d'Hères, France","IEEE Transactions on Software Engineering","","2016","42","7","640","657","The ever growing complexity of software systems has led to the emergence of automated solutions for their management. The software assigned to this work is usually called an Autonomic Management System (AMS). It is ordinarily designed as a composition of several managers, which are pieces of software evaluating the dynamics of the system under management through measurements (e.g., workload, memory usage), taking decisions, and acting upon it so that it stays in a set of acceptable operating states. However, careless combination of managers may lead to inconsistencies in the taken decisions, and classical approaches dealing with these coordination problems often rely on intricate and ad hoc solutions. To tackle this problem, we take a global view and underscore that AMSs are intrinsically reactive, as they react to flows of monitoring data by emitting flows of reconfiguration actions. Therefore we propose a new approach for the design of AMSs, based on synchronous programming and discrete controller synthesis techniques. They provide us with high-level languages for modeling the system to manage, as well as means for statically guaranteeing the absence of logical coordination problems. Hence, they suit our main contribution, which is to obtain guarantees at design time about the absence of logical inconsistencies in the taken decisions. We detail our approach, illustrate it by designing an AMS for a realistic multi-tier application, and evaluate its practicality with an implementation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2510004","French ANR project Ctrl-Green; ANR INFRA; MINALOGIC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7360217","Autonomic computing;coordination;discrete control;reactive programming","Software;Programming;Automata;Sensor systems;Actuators","software management;software performance evaluation","autonomic management systems;reactive control techniques;software systems;AMS;software evaluation;software measurements;ad hoc solutions;monitoring data flow;emitting flows;synchronous programming;discrete controller synthesis techniques;logical coordination problems;realistic multitier application","","2","","47","","","","","","IEEE","IEEE Journals & Magazines"
"On satisfying timing constraints in hard-real-time systems","J. Xu; D. L. Parnas","Dept. of Comput. Sci., York Univ., North York, Ont., Canada; NA","IEEE Transactions on Software Engineering","","1993","19","1","70","84","The authors explain why pre-run-time scheduling is essential if one wishes to guarantee that timing constraints will be satisfied in a large complex hard-real-time system. They examine some of the major concerns in pre-run-time scheduling and consider what formulations of mathematical scheduling problems can be used to address those concerns. This work provides a guide to the available algorithms.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.210308","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=210308","","Timing;Processor scheduling;Application software;Military computing;Software safety;Control systems;Error correction;Scheduling algorithm;Computer applications;Degradation","operating systems (computers);real-time systems;scheduling","timing constraints;hard-real-time systems;pre-run-time scheduling;mathematical scheduling problems","","115","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Concurrent Broadcast for Information Dissemination","D. M. Topkis","Graduate School of Administration, University of California","IEEE Transactions on Software Engineering","","1985","SE-11","10","1107","1112","Concurrent broadcast involves the dissemination of a database, consisting of messages initially distributed among the nodes of a network, so that a copy of the entire database eventually resides at each node. One application is the dissemination of network status information for adaptive routing in a communications network. This paper examines the time complexity and communication complexity of several distributed procedures for concurrent broadcast. The procedures do not use information depending on the network topology. The worst-case time complexity of a flooding procedure for concurrent broadcast is shown to be linear in the number of nodes plus the number of messages, and no other procedure for concurrent broadcast has a better worst-case time complexity. A variant of flooding is proposed to eliminate redundant message receipts from the flooding process by real-time signaling between neighbors concerning messages residing at each. This variant can reduce communication complexity, while having a worst-case time complexity similar in form to that of the flooding procedure. Special properties of concurrent broadcast in a tree are also given. The present time complexity results can be used to bound the time during which inconsistent databases may reside at different nodes, to evaluate and compare procedures for (or including) concurrent broadcast, and to schedule a sequence of instances of concurrent broadcast so that the instances do not overlap and there is no need for sequence numbers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231858","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701926","Adaptive routing;broadcast;communication complexity;concurrent broadcast;distributed computation;distributed database;information dissemination;time complexity","Broadcasting;Routing;Databases;Complexity theory;Floods;Centralized control;Communication networks;Network topology;Signal processing;Concurrent computing","","Adaptive routing;broadcast;communication complexity;concurrent broadcast;distributed computation;distributed database;information dissemination;time complexity","","6","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Automatically generating test data from a Boolean specification","E. Weyuker; T. Goradia; A. Singh","AT&T Bell Labs., Murray Hill, NJ, USA; NA; NA","IEEE Transactions on Software Engineering","","1994","20","5","353","363","This paper presents a family of strategies for automatically generating test data for any implementation intended to satisfy a given specification that is a Boolean formula. The fault detection effectiveness of these strategies is investigated both analytically and empirically, and the costs, assessed in terms of test set size, are compared.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.286420","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=286420","","Automatic testing;Genetic mutations;Fault detection;Software testing;Formal specifications;System testing;Computer bugs;Costs;Aircraft;Collision avoidance","program testing;formal specification;Boolean algebra","test data generation;Boolean specification;fault detection effectiveness;costs;test set size;automatic test case generation;black-box testing;software testing","","139","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Partition strategy for distributed query processing in fast local networks","C. T. Yu; K. -. Guh; D. Brill; A. L. P. Chen","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","6","780","793","A partition-and-replicate strategy for processing distributed queries referencing no fragmented relation is sketched. An algorithm is given to determine which relation and which copy of the relation is to be partitioned into fragments, how the relation is to be partitioned, and where the fragments are to be sent for processing. Simulation results show that the partition strategy is useful for processing queries in fast local network environments. The results also show that the number of partitions does not need to be large. The use of semijoins in the partition strategy is discussed. A necessary and sufficient condition for a semijoin to yield an improvement is provided.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24731","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24731","","Query processing;Intelligent networks;Relational databases;Partitioning algorithms;Data communication;Costs;Parallel processing;Sufficient conditions;Information science;Performance analysis","database theory;distributed databases;local area networks;query languages","simulation results;distributed query processing;partition-and-replicate strategy;fragmented relation;fast local network environments;semijoins","","22","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Validating Second-Order Mutation at System Level","P. Reales Mateo; M. Polo Usaola; J. L. Fernández Alemán","University of Castilla-La Mancha, Ciudad Real; University of Castilla-La Mancha, Ciudad Real; University of Murcia, Murcia","IEEE Transactions on Software Engineering","","2013","39","4","570","587","Mutation has been recognized to be an effective software testing technique. It is based on the insertion of artificial faults in the system under test (SUT) by means of a set of mutation operators. Different operators can mutate each program statement in several ways, which may produce a huge number of mutants. This leads to very high costs for test case execution and result analysis. Several works have approached techniques for cost reduction in mutation testing, like n-order mutation where each mutant contains n artificial faults instead of one. There are two approaches to n-order mutation: increasing the effectiveness of mutation by searching for good n-order mutants, and decreasing the costs of mutation testing by reducing the mutants set through the combination of the first-order mutants into n-order mutants. This paper is focused on the second approach. However, this second use entails a risk: the possibility of leaving undiscovered faults in the SUT, which may distort the perception of the test suite quality. This paper describes an empirical study of different combination strategies to compose second-order mutants at system level as well as a cost-risk analysis of n-order mutation at system level.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6216382","Empirical evaluation;high-order mutation;mutation testing","Algorithm design and analysis;Concrete;Educational institutions;Benchmark testing;Optimization;Software testing","program testing;software fault tolerance","second order mutation;system level;software testing technique;system under test;SUT;mutation operators;test case execution;cost risk analysis","","7","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting Scope Tracking and Visualization for Very Large-Scale Requirements Engineering-Utilizing FSC+, Decision Patterns, and Atomic Decision Visualizations","K. Wnuk; T. Gorschek; D. Callele; E. Karlsson; E. Åhlin; B. Regnell","Software Engineering Research Lab (SERL), Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden; Software Engineering Research Lab (SERL), Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Add a Lot, Sweden; Sony Mobile Communications, Lund, Sweden; Department of Computer Science, Lund University, Lund, Sweden","IEEE Transactions on Software Engineering","","2016","42","1","47","74","Deciding the optimal project scope that fulfills the needs of the most important stakeholders is challenging due to a plethora of aspects that may impact decisions. Large companies that operate in rapidly changing environments experience frequently changing customer needs which force decision makers to continuously adjust the scope of their projects. Change intensity is further fueled by fierce market competition and hard time-to-market deadlines. Staying in control of the changes in thousands of features becomes a major issue as information overload hinders decision makers from rapidly extracting relevant information. This paper presents a visual technique, called Feature Survival Charts+ (FSC+), designed to give a quick and effective overview of the requirements scoping process for Very Large-Scale Requirements Engineering (VLSRE). FSC+ were applied at a large company with thousands of features in the database and supported the transition from plan-driven to a more dynamic and change-tolerant release scope management process. FSC+ provides multiple views, filtering, zooming, state-change intensity views, and support for variable time spans. Moreover, this paper introduces five decision archetypes deduced from the dataset and subsequently analyzed and the atomic decision visualization that shows the frequency of various decisions in the process. The capabilities and usefulness of FSC+, decision patterns (state changes that features undergo) and atomic decision visualizations are evaluated through interviews with practitioners who found utility in all techniques and indicated that their inherent flexibility was necessary to meet the varying needs of the stakeholders.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2445347","IKNOWDM project; Knowledge Foundation in Sweden; SCALARE ITEA2 project; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7123669","- D.2.1Requirements/Specifications D.2.9.d Initiation and scope definition D.2.9 Management;Requirements/specifications;initiation and scope definition;management","Power capacitors;Companies;Planning;Visualization;Software;Software engineering;Electronic mail","data visualisation;formal specification","scope tracking;very large-scale requirements engineering visualization;FSC+;decision patterns;atomic decision visualizations;Feature Survival Charts+;requirements scoping process;very large-scale requirements engineering;VLSRE;change-tolerant release scope management process","","4","","108","","","","","","IEEE","IEEE Journals & Magazines"
"Avoiding packaging mismatch with flexible packaging","R. DeLine","Carnegie Mellon Univ., Pittsburgh, PA, USA","IEEE Transactions on Software Engineering","","2001","27","2","124","143","To integrate a software component into a system, it must interact properly with the system's other components. Unfortunately, the decisions about how a component is to interact with other components are typically committed long before the moment of integration and are difficult to change. This paper introduces the flexible packaging method, which allows a component developer to defer some decisions about component interaction until system integration time. The method divides the component's source into two pieces: the ware, which encapsulates the component's functionality; and the packager, which encapsulates the details of interaction. Both the ware and the packager are independently reusable. A ware, as a reusable part, allows a given piece of functionality to be employed in systems in different architectural styles. A packager, as a reusable part, encapsulates conformance to a component standard, like an ActiveX control or an ODBC database accessor. Because the packager's source code is often formulaic, a tool is provided to generate the packager's source from a high-level description of the intended interaction, a description written in the architectural description language UniCon. The method and tools are evaluated with a series of experiments in which three wares and nine types of packaging are combined to form thirteen components.","0098-5589;1939-3520;2326-3881","","10.1109/32.908958","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=908958","","Packaging;Software packages;Software systems;Databases;Architecture description languages;Software architecture;Filters;System testing;Heart;Maintenance engineering","software reusability;object-oriented programming;software architecture;program processors","packaging mismatch;flexible packaging;software component;ware;component functionality;packager;ActiveX control;ODBC database accessor;architectural description language;UniCon;software packaging;system integration;software architecture","","9","","32","","","","","","IEEE","IEEE Journals & Magazines"
"An Efficient and Scalable Approach to Correct Class Model Refinement","W. Shen; K. Wang; A. Egyed","Western Michigan University, Kalamazoo; Siemens PLM Software, Ann Arbor; Johannes Kepler University, Linz","IEEE Transactions on Software Engineering","","2009","35","4","515","533","Today, programmers benefit immensely from Integrated Development Environments (IDEs), where errors are highlighted within seconds of their introduction. Yet, designers rarely benefit from such an instant feedback in modeling tools. This paper focuses on the refinement of UML-style class models with instant feedback on correctness. Following the Model-Driven Architecture (MDA) paradigm, we strongly believe in the benefit of maintaining high-level and low-level models separately to 1) document the lower level model and 2) continuously ensure the correctness of the low-level model during later evolution (i.e., high- or low-level models may be evolved independently). However, currently the refinement and subsequent evolution lack automated support, let alone an instant feedback on their correctness (i.e., consistency). Traditional approaches to consistency checking fail here because of the computational cost of comparing class models. Our proposed instant approach first transforms the low-level model into an intermediate model that is then easier comparable with the high-level model. The key to computational scalability is the separation of transformation and comparison so that each can react optimally to changes-changes that could happen concurrently in both the high- and low-level class models. We evaluate our approach on eight third-party design models. The empirical data show that the separation of transformation and comparison results in a 6 to 11-fold performance gain and a ninefold reduction in producing irrelevant feedback. While this work emphasizes the refinement of class models, we do believe that the concepts are more generally applicable to other kinds of modeling languages, where transformation and subsequent comparison are computationally expensive.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815278","Class models;consistency checking;refinement;separation of concerns;and UML.","Feedback;Programming profession;Unified modeling language;Software systems;Error correction;Computational efficiency;Concurrent computing;Scalability;Performance gain;System recovery","software architecture;Unified Modeling Language","class model refinement;integrated development environment;UML-style class models;Unified Modeling Language;model-driven architecture;computational scalability","","7","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Lessons from using Z to specify a software tool","M. Neil; G. Ostrolenk; M. Tobin; M. Southworth","Centre for Software Reliability, City Univ., London, UK; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","1","15","23","The authors were recently involved in the development of a COBOL parser (G. Ostrolenk et al., 1994), specified formally in Z. The type of problem tackled was well suited to a formal language. The specification process was part of a life cycle characterized by the front loading of effort in the specification stage and the inclusion of a statistical testing stage. The specification was found to be error dense and difficult to comprehend. Z was used to specify inappropriate procedural rather than declarative detail. Modularity and style problems in the Z specification made it difficult to review. In this sense, the application of formal methods was not successful. Despite these problems the estimated fault density for the product was 1.3 faults per KLOC, before delivery, which compares favorably with IBM's Cleanroom method. This was achieved, despite the low quality of the Z specification, through meticulous and effort intensive reviews. However, because the faults were in critical locations, the reliability of the product was assessed to be unacceptably low. This demonstrates the necessity of assessing reliability as well as ""correctness"" during system testing. Overall, the experiences reported in the paper suggest a range of important lessons for anyone contemplating the practical application of formal methods.","0098-5589;1939-3520;2326-3881","","10.1109/32.663995","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=663995","","Software tools;Statistical analysis;Productivity;Formal languages;Computer Society;System testing;Rail transportation;Telecommunication switching;Power generation","formal specification;specification languages;formal languages;COBOL;program compilers;software performance evaluation;software reliability","software tool specification;COBOL parser;formal language;specification process;life cycle;front loading;statistical testing stage;Z specification;fault density;critical locations;reliability;correctness;system testing;formal methods","","1","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Support for Distributed Transactions in the TABS Prototype","A. Z. Spector; J. Butcher; D. S. Daniels; D. J. Duchamp; J. L. Eppinger; C. E. Fineman; A. Heddaya; P. M. Schwarz","Department of Computer Science, Carnegie-Mellon University; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","6","520","530","The TABS prototype is an experimental facility that provides operating system-level support for distributed transactions that operate on shared abstract types. The facility is designed to simplify the construction of highly available and reliable distributed applications. This paper describes the TABS system model, the TABS prototype's structure, and certain aspects of its operation. The paper concludes with a discussion of the status of the project and a preliminary evaluation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232244","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702049","Availability;distributed databases;distributed systems;overating systems organization;reliability;transaction-based systems;virtual memory","Prototypes;Buildings;Laboratories;Computer science;Concurrent computing;Jacobian matrices;Programming profession;Distributed databases;Transaction databases;Computerized monitoring","","Availability;distributed databases;distributed systems;overating systems organization;reliability;transaction-based systems;virtual memory","","15","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Stability and Distributed Scheduling Algorithms","J. A. Stankovic","Department of Computer Science, Carnegie-Mellon University","IEEE Transactions on Software Engineering","","1985","SE-11","10","1141","1152","Many distributed scheduling algorithms have been developed and reported in the current literature. However, very few of them explicitly treat stability issues. This paper first discusses stability issues for distributed scheduling algorithms in general terms. Two very different distributed scheduling algorithms which contain explicit mechanisms for stability are then presented and evaluated with respect to individual specific stability issues. One of the agorithms is based on stochastic learning automata and the other on bidding. The results indicate how very specific the treatment of stability is to the algorithm and environnent under consideration.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231862","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701930","Bidding;distributed computing;real time;stability;stochastic learning automata","Stability;Scheduling algorithm;Learning automata;Stochastic processes;Distributed computing;Analytical models;Testing;Feedback;Algorithm design and analysis;Computational modeling","","Bidding;distributed computing;real time;stability;stochastic learning automata","","24","","17","","","","","","IEEE","IEEE Journals & Magazines"
"MODEST: A Compositional Modeling Formalism for Hard and Softly Timed Systems","H. Bohnenkamp; P. R. D'Argenio; H. Hermanns; J. -. Katoen","Software Modeling and Verification Group, Informatik 2, University (RWTH) Aachen, 52056 Aachen, Germany; Computer Science Group, FaMAF, Universidad Nacional de Co´rdoba, Ciudad Universitaria, 5000—Cordoba, Argentina; Dependable Systems and Software Group, Department of Computer Science, Saarland University, 66123 Saarbrucken, Germany; Software Modeling and Verification Group, Informatik 2, University (RWTH) Aachen, 52056 Aachen, Germany","IEEE Transactions on Software Engineering","","2006","32","10","812","830","This paper presents MODEST (modeling and description language for stochastic timed systems), a formalism that is intended to support 1) the modular description of reactive systems' behavior while covering both 2) functional and 3) nonfunctional system aspects such as timing and quality-of-service constraints in a single specification. The language contains, features such as simple and structured data types, structuring mechanisms like parallel composition and abstraction, means to control the granularity of assignments, exception handling, and nondeterministic and random branching and timing. MODEST can be viewed as an overarching notation for a wide spectrum of models, ranging from labeled transition systems to timed automata (and probabilistic variants thereof), as well as prominent stochastic processes such as (generalized semi-) Markov chains and decision processes. The paper describes the design rationales and details of the syntax and semantics","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.104","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717473","Modeling formalism;compositionality;formal semantics;timed automata;stochastic processes.","Embedded software;Learning automata;Stochastic systems;Stochastic processes;Timing;Costs;Algebra;Software systems;Robustness;Unified modeling language","formal languages;formal specification;Markov processes;programming language semantics;specification languages;stochastic automata","compositional modeling formalism;description language;stochastic timed system;modular description;reactive system;nonfunctional system;quality-of-service constraint;formal specification;stochastic process;Markov chain;decision process;formal semantics","","63","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Verisim: formal analysis of network simulations","K. Bhargavan; C. A. Gunter; Moonjoo Kim; Insup Lee; D. Obradovic; O. Sokolsky; M. Viswanathan","Dept. of Comput. & Inf. Sci., Pennsylvania Univ., Philadelphia, PA, USA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","2","129","145","Network protocols are often analyzed using simulations. We demonstrate how to extend such simulations to check propositions expressing safety properties of network event traces in an extended form of linear temporal logic. Our technique uses the INS simulator together with a component of the MaC system to provide a uniform framework. We demonstrate its effectiveness by analyzing simulations of the ad hoc on-demand distance vector (AODV) routing protocol for packet radio networks. Our analysis finds violations of significant properties and we discuss the faults that cause them. Novel aspects of our approach include modest integration costs with other simulation objectives such as performance evaluation, greatly increased flexibility in specifying properties to be checked and techniques for analyzing complex traces of alarms raised by the monitoring software.","0098-5589;1939-3520;2326-3881","","10.1109/32.988495","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988495","","Analytical models;Discrete event simulation;Safety;Logic;Vectors;Routing protocols;Packet radio networks;Costs;Performance analysis;Monitoring","protocols;telecommunication network routing;temporal logic;packet radio networks;program debugging;formal verification;digital simulation;telecommunication computing;system monitoring","formal analysis;network simulations;network protocols;Verisim;safety properties;network event traces;linear temporal logic;MaC system;ad hoc on-demand distance vector routing protocol;packet radio networks;integration costs;performance evaluation;complex alarm traces;monitoring software","","41","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A Survey on Load Testing of Large-Scale Software Systems","Z. M. Jiang; A. E. Hassan","Department of Electrical Engineering and Computer ScienceSoftware Construction, AnaLytics and Evaluation (SCALE) Lab, York University, Toronto, ON, Canada; Software Analysis and Intelligence (SAIL) Lab, School of Computing, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2015","41","11","1091","1118","Many large-scale software systems must service thousands or millions of concurrent requests. These systems must be load tested to ensure that they can function correctly under load (i.e., the rate of the incoming requests). In this paper, we survey the state of load testing research and practice. We compare and contrast current techniques that are used in the three phases of a load test: (1) designing a proper load, (2) executing a load test, and (3) analyzing the results of a load test. This survey will be useful for load testing practitioners and software engineering researchers with interest in the load testing of large-scale software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2445340","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7123673","Software Testing;Load Testing;Software Quality;Software testing;load testing;software quality;large-scale software systems;survey","Testing;Stress;Computer bugs;Robustness;Software systems;Stress measurement","program testing;software quality","load testing;large-scale software systems;concurrent requests;load testing research;contrast current techniques;software engineering researchers","","19","","196","","","","","","IEEE","IEEE Journals & Magazines"
"Software reuse research: status and future","W. B. Frakes; Kyo Kang","Comput. Sci. Dept., Virginia Tech, Falls Church, VA, USA; NA","IEEE Transactions on Software Engineering","","2005","31","7","529","536","This paper briefly summarizes software reuse research, discusses major research contributions and unsolved problems, provides pointers to key publications, and introduces four papers selected from The Eighth International Conference on Software Reuse (ICSR8).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.85","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492369","Index Terms- Software reuse;domain engineering;research;metrics;architectures;generators;finance.","Software quality;Software reusability;Software engineering;Computer science;Finance;Productivity;Reliability engineering;Software systems;Software libraries;Software architecture","software reusability;research and development;software metrics;software architecture","software reuse research;software engineering;software metrics;software architecture","","231","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A Framework for Temporal Verification Support in Domain-Specific Modelling","B. Meyers; H. Vangheluwe; J. Denil; R. Salay","Codesign & Optimization, Flanders Make vzw, Leuven, Vlaams Brabant Belgium (e-mail: bart.meyers@flandersmake.be); Computer Science and Mathematics, Universiteit Antwerpen, 26660 Antwerpen, Antwerpen Belgium (e-mail: hv@cs.mcgill.ca); Computer Science and Mathematics, Universiteit Antwerpen, 26660 Antwerpen, Antwerpen Belgium (e-mail: joachim.denil@uantwerpen.be); Computer Science, University of Toronto, 7938 Toronto, Ontario Canada M5S 3G4 (e-mail: rsalay@cs.toronto.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","In Domain-Specific Modelling (DSM) the general goal is to provide Domain-Specific Modelling Languages (DSMLs) for domain users to model systems using concepts and notations they are familiar with, in their problem domain. Verifying whether a model satisfies a set of requirements is considered to be an important challenge in DSM, but is nevertheless mostly neglected. We present a solution in the form of ProMoBox, a framework that integrates the definition and verification of temporal properties in discrete-time behavioural DSMLs, whose semantics can be described as a schedule of graph rewrite rules. Thanks to the expressiveness of graph rewriting, this covers a very large class of problems. With ProMoBox, the domain user models not only the system with a DSML, but also its properties, input model, run-time state and output trace. A DSML is thus comprised of five sublanguages, which share domain-specific syntax, and are generated from a single metamodel. Generic transformations to and from a verification backbone ensure that both the language engineer and the domain user are shielded from underlying notations and techniques. We explicitly model the ProMoBox framework's process in the paper. Furthermore, we evaluate ProMoBox to assert that it supports the specification and verification of properties in a highly flexible and automated way.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2859946","Flanders Make vzw; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8419296","","Syntactics;Semantics;Computational modeling;Schedules;Tools;Model checking;Electronic mail","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A Technique for Estimating Performance of Fault-Tolerant Programs","R. D. Schlichting","Department of Computer Science, University of Arizona","IEEE Transactions on Software Engineering","","1985","SE-11","6","555","563","A technique is presented for estimating the performance of programs written for execution on fail-stop processors. It is based on modeling the program as a discrete-time Markov chain and then using z-transforms to derive a probability distribution for time to completion.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232493","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702053","Fail-stop processors;fault-tolerant computing;Markov chains;performance evaluation;z-transforms","Fault tolerance;Probability distribution;Fault tolerant systems;Software performance;Distributed computing;Random variables;Writing;Computer crashes;Control theory","","Fail-stop processors;fault-tolerant computing;Markov chains;performance evaluation;z-transforms","","4","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Combining static concurrency analysis with symbolic execution","M. Young; R. N. Taylor","Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA; Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA","IEEE Transactions on Software Engineering","","1988","14","10","1499","1511","Static concurrency analysis detects anomalous synchronization patterns in concurrent programs, but may also report spurious errors involving infeasible execution paths. Integrated application of static concurrency analysis and symbolic execution sharpens the results of the former without incurring the full costs of the latter when applied in isolation. Concurrency analysis acts as a path selection mechanism for symbolic execution, while symbolic execution acts as a pruning mechanism for concurrency analysis. Methods of combining the techniques follow naturally from explicit characterization and comparison of the state spaces explored by each, suggesting a general approach for integrating state-based program analysis techniques in a software development environment.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6195","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6195","","Concurrent computing;Pattern analysis;State-space methods;History;Costs;Programming;Computer errors;Error correction;System recovery;Algorithm design and analysis","parallel programming;program testing","program testing;static concurrency analysis;symbolic execution;synchronization patterns;concurrent programs;path selection mechanism;concurrency analysis;program analysis;software development environment","","30","","36","","","","","","IEEE","IEEE Journals & Magazines"
"An information retrieval approach for automatically constructing software libraries","Y. S. Maarek; D. M. Berry; G. E. Kaiser","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA","IEEE Transactions on Software Engineering","","1991","17","8","800","813","A technology for automatically assembling large software libraries which promote software reuse by helping the user locate the components closest to her/his needs is described. Software libraries are automatically assembled from a set of unorganized components by using information retrieval techniques. The construction of the library is done in two steps. First, attributes are automatically extracted from natural language documentation by using an indexing scheme based on the notions of lexical affinities and quantity of information. Then a hierarchy for browsing is automatically generated using a clustering technique which draws only on the information provided by the attributes. Due to the free-text indexing scheme, tools following this approach can accept free-style natural language queries.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.83915","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=83915","","Information retrieval;Software libraries;Indexing;Documentation;Programming profession;Productivity;Assembly;Natural languages;Computer science;Data mining","automatic programming;information retrieval systems;natural languages;software reusability;subroutines","information retrieval approach;large software libraries;software reuse;attributes;natural language documentation;indexing scheme;lexical affinities;browsing;clustering technique;free-text indexing scheme;free-style natural language queries","","136","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Extending Abstract Interpretation to Dependency Analysis of Database Applications","A. Jana; R. Halder; A. Kalahasti; S. Ganni; A. Cortesi","Computer Science & Engineering, Indian Institute of Technology Patna Department of Computer Science and Engineering, 250392 Patna, Bihar India (e-mail: janaangshuman@gmail.com); Computer Science & Engineering, Indian Institute of Technology Patna Department of Computer Science and Engineering, 250392 Patna, Bihar India (e-mail: halder@iitp.ac.in); Computer Science & Engineering, Indian Institute of Technology Patna Department of Computer Science and Engineering, 250392 Patna, Bihar India (e-mail: kalahasti.cs13@iitp.ac.in); Computer Science & Engineering, Indian Institute of Technology Patna Department of Computer Science and Engineering, 250392 Patna, Bihar India (e-mail: ganni.cs13@iitp.ac.in); Computer Science, University Ca' Foscari di Venezia, Venezia, VE Italy 30170 (e-mail: cortesi@unive.it)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Dependency information (data- and/or control-dependencies) among program variables and program statements is playing crucial roles in a wide range of software-engineering activities, e.g. program slicing, security analyses, debugging, code-optimization, code-reuse, code-understanding, etc. In spite of intensive research in this area, covering several mainstream languages, researchers have paid limited attention towards database applications embedding queries and data-manipulation commands. As a result, the existing approaches are practically infeasible when to apply on data-intensive codes supporting databases. This paper extends the Abstract Interpretation framework for static analysis to the case of database applications, providing a semantics-based dependency analysis tunable with respect to precision. More specifically, we instantiate dependency computation by using various relational and non-relational abstract domains, yielding to a detailed comparative analysis with respect to precision and efficiency. We present experimental evaluation results to establish the effectiveness of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2861707","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8423692","Dependency Graphs;Static Analysis;Relational Databases;Structured Query Languages","Databases;Semantics;Static analysis;Security;Syntactics;Open source software;Debugging","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Build-level components","M. de Jonge","Philips Res. Lab., Eindhoven, Netherlands","IEEE Transactions on Software Engineering","","2005","31","7","588","600","Reuse between software systems is often not optimal. An important reason is that while at the functional level well-known modularization principles are applied for structuring functionality in modules, this is not the case at the build level for structuring files in directories. This leads to a situation where files are entangled in directory hierarchies and build processes, making it hard to extract functionality and to make functionality suitable for reuse. Consequently, software may not come available for reuse at all, or only in rather large chunks of functionality, which may lead to extra software dependencies. In this paper, we propose to improve this situation by applying component-based software engineering (CBSE) principles to the build level. We discuss how existing software systems break CBSE principles, we introduce the notion of build-level components, and we define rules for developing such components. To make our techniques feasible, we define a reengineering process for semiautomatically transforming existing software systems into build-level components. Our techniques are demonstrated in two case studies where we decouple the source tree of Graphviz into 46 build-level components and analyze the source tree of Mozilla.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492373","Index Terms- CBSE;software component;software reuse;software construction;software engineering;source tree composition;build level.","Software systems;Software engineering;Tree graphs;Assembly systems;Functional programming;Debugging;Component architectures","object-oriented programming;software reusability;systems re-engineering","build-level components;software system reusability;component-based software engineering;reengineering process;Graphviz source tree;Mozilla source tree;software component;software construction;software engineering","","19","","34","","","","","","IEEE","IEEE Journals & Magazines"
"ASCENT: An Algorithmic Technique for Designing Hardware and Software in Tandem","J. White; B. Doughtery; D. C. Schmidt","Virginia Tech, Blacksburg; Vanderbilt University, Nashville; Vanderbilt University, Nashville","IEEE Transactions on Software Engineering","","2010","36","6","838","851","Search-based software engineering is an emerging paradigm that uses automated search algorithms to help designers iteratively find solutions to complicated design problems. For example, when designing a climate monitoring satellite, designers may want to use the minimal amount of computing hardware to reduce weight and cost while supporting the image processing algorithms running onboard. A key problem in these situations is that the hardware and software designs are locked in a tightly coupled cost-constrained producer/consumer relationship that makes it hard to find a good hardware/software design configuration. Search-based software engineering can be used to apply algorithmic techniques to automate the search for hardware/software designs that maximize the image processing accuracy while respecting cost constraints. This paper provides the following contributions to research on search-based software engineering: 1) We show how a cost-constrained producer/consumer problem can be modeled as a set of two multidimensional multiple-choice knapsack problems (MMKPs), 2) we present a polynomial-time search-based software engineering technique, called the Allocation-baSed Configuration Exploration Technique (ASCENT), for finding near optimal hardware/software codesign solutions, and 3) we present empirical results showing that ASCENT's solutions average over 95 percent of the optimal solution's value.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5539763","Algorithms;computer aided software engineering;optimization methods;distributed computing.","Software algorithms;Software design;Algorithm design and analysis;Hardware;Software engineering;Iterative algorithms;Costs;Image processing;Monitoring;Satellites","hardware-software codesign;knapsack problems;software engineering","ASCENT;hardware-software codesign;automated search algorithm;climate monitoring satellite;computing hardware;image processing algorithm;hardware/software design configuration;cost-constrained producer/consumer problem;multidimensional multiple choice knapsack problem;polynomial time search-based software engineering;allocation-based configuration exploration technique","","7","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Threat-driven modeling and verification of secure software using aspect-oriented Petri nets","D. Xu; K. E. Nygard","Dept. of Comput. Sci., North Dakota State Univ., Fargo, ND, USA; Dept. of Comput. Sci., North Dakota State Univ., Fargo, ND, USA","IEEE Transactions on Software Engineering","","2006","32","4","265","278","Design-level vulnerabilities are a major source of security risks in software. To improve trustworthiness of software design, this paper presents a formal threat-driven approach, which explores explicit behaviors of security threats as the mediator between security goals and applications of security features. Security threats are potential attacks, i.e., misuses and anomalies that violate the security goals of systems' intended functions. Security threats suggest what, where, and how security features for threat mitigation should be applied. To specify the intended functions, security threats, and threat mitigations of a security design as a whole, we exploit aspect-oriented Petri nets as a unified formalism. Intended functions and security threats are modeled by Petri nets, whereas threat mitigations are modeled by Petri net-based aspects due to the incremental and crosscutting nature of security features. The unified formalism facilitates verifying correctness of security threats against intended functions and verifying absence of security threats from integrated functions and threat mitigations. As a result, our approach can make software design provably secured from anticipated security threats and, thus, reduce significant design-level vulnerabilities. We demonstrate our approach through a systematic case study on the threat-driven modeling and verification of a real-world shopping cart application.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628972","Software security;modeling;verification;threat modeling;Petri nets;aspect-oriented Petri nets;aspect-oriented software development.","Petri nets;Application software;Software design;Programming;Computer security;Protection;Cryptography;Intrusion detection","security of data;object-oriented programming;formal specification;formal verification;Petri nets","design-level vulnerability;software security;software design;security threat;aspect-oriented Petri net;formal verification;real-world shopping cart application;threat-driven modeling","","75","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating software design processes by analyzing change data over time","L. J. Chmura; A. F. Norcio; T. J. Wicinski","US Naval Res. Lab., Washington, DC, USA; US Naval Res. Lab., Washington, DC, USA; NA","IEEE Transactions on Software Engineering","","1990","16","7","729","740","An analysis is presented of early design and code change data from the software cost reduction (SCR) project, a well-reported effort conducted at the US Naval Research Laboratory from 1978 to 1988. The analyses are mostly time-based studies of the change data and relationships between the data and SCR personnel activity data. Some analyses of the change data show patterns consistent with a major goal of the SCR project: the design and development of easy-to-change software. Specifically, most changes took a day or less to uncover and resolve; the majority of changes updated at most one module. Moreover, these percentages remained fairly stable. No positive relationship appeared between error-correction effort and the number of days that an error remained in the SCR design documentation. Other analyses suggest that consistency may have been temporary. For example, the analyses suggest a stepwise growth in average change effort, and an increasing percentage of changes resulted in module interface updates. Certain specific ratios between SCR change data and personnel activity data may be possible indicators of design incompleteness.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.56099","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=56099","","Software design;Data analysis;Thyristors;Laboratories;Personnel;Electrical capacitance tomography;Software engineering;Costs;Statistical analysis;Guidelines","software engineering","software design processes evaluation;early design and code change data;software cost reduction;error-correction effort;stepwise growth;module interface updates","","6","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Virtual benchmarking and model continuity in prototyping embedded multiprocessor signal processing systems","R. S. Janka; L. M. Wills; L. B. Baumstark","Cadence Design Systems Inc., Atlanta, GA, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","9","832","846","The complexity of hardware/software codesign of embedded real-time signal processing systems can be reduced by rapid system prototyping (RSP). However, existing RSP frameworks do not provide a sound specification and design methodology (SDM) because they require the designer to choose the implementation target before specification and design exploration and they do not work together coherently across development stages. This paper presents a new SDM, called MAGIC, that allows the designer to capture an executable specification model for use in design exploration to find the optimal multiprocessor technology before committing to that technology. MAGIC uses a technique called ""virtual benchmarking,"" for early validation of promising architectures. The MAGIC SDM also exploits emerging open-standards computation and communication middleware to establish model continuity between RSP frameworks. This methodology has been validated through the specification and design of a moderately complex system representative of the signal processing domain: the RASSP Synthetic Aperture Radar benchmark. In this case study, MAGIC achieves three orders of magnitude speedup over existing virtual prototyping approaches and demonstrates the ability to evaluate competitive technologies prior to implementation. Transfer of this methodology to the system-on-a-chip domain using Cadence's Virtual Component Codesign infrastructure is also discussed with promising results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033224","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033224","","Virtual prototyping;Signal processing;Software prototyping;Hardware;Embedded software;Real time systems;Design methodology;Computer architecture;Middleware;Signal design","hardware-software codesign;embedded systems;radar computing;formal specification;multiprocessing systems;software prototyping;signal processing;synthetic aperture radar;radar signal processing;application program interfaces","hardware/software codesign;embedded real-time multiprocessor signal processing systems;rapid system prototyping;specification and design methodology;MAGIC;executable specification model;optimal multiprocessor technology;virtual benchmarking;early validation;architectures;open-standards computation;communication middleware;RASSP Synthetic Aperture Radar benchmark;system-on-a-chip domain;Cadence Virtual Component Codesign infrastructure;model continuity","","5","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Lock Conversion in Non-Two-Phase Locking Protocols","C. Mohan; D. Fussell; Z. M. Kedem; A. Silberschatz","IBM Research Laboratory; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","1","15","22","A locking protocol is a set of rules governing the manner in which the database entities may be accessed. Such a protocol usually employs several kinds of locks. Most of the previous work in this area has assumed that once a transaction acquires a particular kind of lock on a data item it is not allowed to convert this lock to another kind. In this paper we perform a systematic study of the consequences of allowing lock conversions in non-two-phase locking protocols, and show how this leads to increased concurrency and affects deadlock-freedom. The non-two-phase protocols that we study are the very general guard protocols defined for databases in which a directed acyclic graph structure can be superimposed on the data items. We present very natural generalizations of these protocols, including correctness proofs, and develop deadlock removal methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231533","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701894","Concurrency;consistency;database systems;deadlocks;locking protocols;rollbacks;serializability;transactions","System recovery;Access protocols;Transaction databases;Concurrent computing;Computer science;Database systems;Information retrieval;Concurrency control;Laboratories;Proposals","","Concurrency;consistency;database systems;deadlocks;locking protocols;rollbacks;serializability;transactions","","8","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Object-oriented software evolution","K. J. Lieberherr; C. Xiao","Coll. of Comput. Sci., Northeastern Univ., Boston, MA, USA; Coll. of Comput. Sci., Northeastern Univ., Boston, MA, USA","IEEE Transactions on Software Engineering","","1993","19","4","313","343","The authors review propagation patterns for describing object-oriented software at a higher level of abstraction than one used by today's programming languages. A propagation pattern defines a family of programs from which one can select a member by giving a class dictionary graph that details the structure of behavior through part-of and inheritance relationships between classes. Three concepts are introduced: evolution histories, growth-plans and a propagation-directive calculus. Evolution histories describe a sequence of development phases of an object-oriented program, each phase being executable and therefore testable. To keep the programs flexible and short, they are described in terms of propagation patterns. Each phase of an evolution history is tested in small steps that are constrained by class dictionary graphs belonging to a growth-plan. Propagation directives are useful for describing both propagation patterns and growth-plans and are therefore endowed with sufficient expressiveness by being given a formal calculus applicable to object-oriented programming in general. A propagation directive is a succinct description of a family of submodels for a given family of data models.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.223802","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=223802","","Object oriented programming;History;Testing;Dictionaries;Calculus;Propagation delay;Books;Computer languages;Data models;Software performance","object-oriented programming","object-oriented software evolution;propagation patterns;class dictionary graph;inheritance relationships;evolution histories;growth-plans;propagation-directive calculus;formal calculus;submodels;data models","","33","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Integrating Technical Debt Management and Software Quality Management Processes: A Normative Framework and Field Tests","N. Ramasubbu; C. F. Kemerer","Katz Graduate School of Business, University of Pittsburgh, Pittsburgh, Pennsylvania United States 15260 (e-mail: narayanr@pitt.edu); KGSB, University of Pittsburgh, Pittsburgh, Pennsylvania United States 15260 (e-mail: ckemerer@katz.pitt.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Despite the increasing awareness of the importance of managing technical debt in software product development, systematic processes for implementing technical debt management in software production have not been readily available. In this paper we report on the development and field tests of a normative process framework that systematically incorporates steps for managing technical debt in commercial software production. The framework integrates processes required for technical debt management with existing software quality management processes prescribed by the project management body of knowledge (PMBOK), and it contributes to the further development of software-specific extensions to the PMBOK. We partnered with three commercial software product development organizations to implement the framework in real-world software production settings. All three organizations, irrespective of their varying software process maturity levels, were able to adopt the proposed framework and integrate the prescribed technical debt management processes with their existing software quality management processes. Our longitudinal observations and case-study interviews indicate that the organizations were able to accrue economic benefits from the adoption and use of the integrated framework.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2774832","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8114229","Technical debt;software quality;software maintenance;software engineering economics;cost of quality;software product development;software process;software extension to PMBOK;case study","Software quality;Business;Economics;Product development;Tools;Systematics","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Achieving dependability throughout the development process: a distributed software experiment","J. P. J. Kelly; S. C. Murphy","Dept. of Electr. & Comput. Eng., California Univ., Santa Barbara, CA, USA; NA","IEEE Transactions on Software Engineering","","1990","16","2","153","165","Distributed software engineering techniques and methods for improving the specification and testing phases are considered. To examine these issues, an experiment was performed using the design diversity approach in the specification, design, implementation, and testing of distributed software. In the experiment, three diverse formal specifications were used to produce multiple independent implementations of a distributed communication protocol in Ada. The problems encountered in building complex concurrent processing systems in Ada were also studied. Many pitfalls were discovered in mapping the formal specifications into Ada implementations.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44379","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44379","","Software testing;Automatic testing;Fault tolerance;Software engineering;Formal specifications;Hardware;Fault detection;Software performance;Protocols;Buildings","Ada;computer communications software;data communication systems;distributed processing;formal specification;program testing;protocols","distributed software engineering;B/B testing;automated testing;software testing;dependability;multiple independent implementations;distributed communication protocol;Ada;complex concurrent processing systems","","15","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Interprocess communication dependency on network load","A. Braccini; A. Del Bimbo; E. Vicario","Dipartimento di Sistemi e Inf., Firenze Univ., Italy; Dipartimento di Sistemi e Inf., Firenze Univ., Italy; Dipartimento di Sistemi e Inf., Firenze Univ., Italy","IEEE Transactions on Software Engineering","","1991","17","4","357","369","Results of an analysis of the communication performance as perceived by the application layer in the presence of background load are presented. The analysis was carried out on two distinct protocol suites, TCP/IP and XNS, on a single Ethernet LAN with personal computer workstations. Service times for a reliable transfer and an unreliable transmission as perceived by the application layer were measured. An appropriate model of the reliable transfer process was used to derive service time analytical estimates. Owing to the close relation between measured and estimated values, the model was used to identify the major sources of delay and to evaluate the effects of possible improvements. The most important causes affecting the transport layer performance as background load increases are identified, and different strategies are examined in order to reduce their impact. The improvement margin offered by the implementation of a lightweight transport protocol specifically tailored to the requirements of a bulk data transfer over an Ethernet is addressed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90435","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=90435","","Application software;Ethernet networks;Delay estimation;Performance analysis;Protocols;TCPIP;Local area networks;Microcomputers;Workstations;Delay effects","local area networks;performance evaluation;protocols","interprocess communication dependency;network load;communication performance;application layer;background load;protocol suites;TCP/IP;XNS;Ethernet LAN;personal computer workstations;reliable transfer;unreliable transmission;service time analytical estimates;transport protocol;bulk data transfer","","5","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Provable improvements on branch testing","P. G. Frankl; E. J. Weyuker","Dept. of Comput. Sci., Polytechnic Univ., Brooklyn, NY, USA; NA","IEEE Transactions on Software Engineering","","1993","19","10","962","975","This paper compares the fault-detecting ability of several software test data adequacy criteria. It has previously been shown that if C/sub 1/ properly covers C/sub 2/, then C/sub 1/ is guaranteed to be better at detecting faults than C/sub 2/, in the following sense: a test suite selected by independent random selection of one test case from each subdomain induced by C/sub 1/ is at least as likely to detect a fault as a test suite similarly selected using C/sub 2/. In contrast, if C/sub 1/ subsumes but does not properly cover C/sub 2/, this is not necessarily the case. These results are used to compare a number of criteria, including several that have been proposed as stronger alternatives to branch testing. We compare the relative fault-detecting ability of data flow testing, mutation testing, and the condition-coverage techniques, to branch testing, showing that most of the criteria examined are guaranteed to be better than branch testing according to two probabilistic measures. We also show that there are criteria that can sometimes be poorer at detecting faults than substantially less expensive criteria.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.245738","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=245738","","Fault detection;Software testing;Genetic mutations;Concrete;Space technology;Computer science;Fluid flow measurement;Costs;NASA;Software systems","program debugging;program testing;programming theory","branch testing;fault-detecting ability;software test data adequacy;test suite;independent random selection;data flow testing;mutation testing;condition-coverage techniques;probabilistic measure;software testing","","63","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Modular verification of data abstractions with shared realizations","G. W. Ernst; R. J. Hookway; W. F. Ogden","Dept. of Comput. Eng. & Sci., Case Western Reserve Univ., Cleveland, OH, USA; NA; NA","IEEE Transactions on Software Engineering","","1994","20","4","288","307","Presents a method for the modular specification and verification of data abstractions in which multiple abstract objects share a common realization level data structure. Such shared realizations are an important implementation technique for data abstractions, because they provide for efficient use of memory; i.e., they allow the amount of memory allocated to the realization of an abstract object to be dynamic, so that only the amount of memory needed for its realization is allocated to it at any one time. To be explicit, an example of this kind of data abstraction is given. Although a number of programming languages provide good support for shared realizations, there has been limited research on its specification and verification. An important property of The authors' method is that it allows data abstractions to be dealt with modularly; i.e., each data abstraction can be specified and verified individually. Its abstract specification is made available for use by other program modules, but all of its implementation details are hidden, which simplifies the verification of code that uses the abstraction. The authors have developed semantics for data abstractions and their method of specification, and have used it to prove that their verification method is logically sound and relatively complete in the sense of Cook (1978). The use of shared realizations impacts specification and verification in several related ways. The manipulation of one abstract object may inadvertently produce a side effect on other abstract objects. Without shared realizations, such unwanted side effects can be prevented by scoping rules, but this is not possible with shared realizations. Instead, the absence of such side effects must be explicitly proven by the verification method. This requires the specification language to provide for quantification over the currently active (allocated) instances of an abstract type that is not necessary for the specification of less advanced implementations of data abstractions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.277576","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=277576","","Data structures;Computer languages;Specification languages;Application software;National security;Information science;Aggregates;Runtime","data structures;program verification;specification languages","modular verification;data abstractions;shared realizations;modular specification;realization level data structure;abstract specification;semantics;quantification","","7","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Muse—A Computer Assisted Verification System","J. D. Halpern; S. Owre; N. Proctor; W. F. Wilson","Department of Government Research and Development, Sytek, Inc.; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","2","151","156","Muse is a verification system which extends the collection of tools developed by SRI International for their Hierarchical Development Methodology (HDM). It enhances the SRI system by providing a capability for proving invariants and constraints for the state machine described by a specification written in SPECIAL (the specification language of HDM). In particular, it enables one to use the HDM system to meet the requirements for formal verification in a National Computer Security Center A1 evaluation of a secure operating system. In addition to the tools provided by SRI, Muse has a parser, a facility to handle multiple modules, a formula generator, and a theorem prover. The theorem prover has a number of interesting features designed to facilitate human direction of the proving process. In concept, it is open-ended. We introduce the notion of a theorem prover kernel as a device for ensuring the logical soundness of the prover in the face of continual improvements to its functionality.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.226477","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702196","Formal specification;formal verification;HDM;interactive theorem proving;Muse;National Computer Security Center;software verification;SPECIAL;theorem prover","Formal verification;Computer security;Multilevel systems;Operating systems;Specification languages;Humans;Kernel;Software;Formal specifications;NASA","","Formal specification;formal verification;HDM;interactive theorem proving;Muse;National Computer Security Center;software verification;SPECIAL;theorem prover","","2","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Precise Calling Context Encoding","W. N. Sumner; Y. Zheng; D. Weeratunge; X. Zhang","Purdue University, West Lafayette; Purdue University, West Lafayette; Purdue University, West Lafayette; Purdue University, West Lafayette","IEEE Transactions on Software Engineering","","2012","38","5","1160","1177","Calling contexts (CCs) are very important for a wide range of applications such as profiling, debugging, and event logging. Most applications perform expensive stack walking to recover contexts. The resulting contexts are often explicitly represented as a sequence of call sites and hence are bulky. We propose a technique to encode the current calling context of any point during an execution. In particular, an acyclic call path is encoded into one number through only integer additions. Recursive call paths are divided into acyclic subsequences and encoded independently. We leverage stack depth in a safe way to optimize encoding: If a calling context can be safely and uniquely identified by its stack depth, we do not perform encoding. We propose an algorithm to seamlessly fuse encoding and stack depth-based identification. The algorithm is safe because different contexts are guaranteed to have different IDs. It also ensures contexts can be faithfully decoded. Our experiments show that our technique incurs negligible overhead (0-6.4 percent). For most medium-sized programs, it can encode all contexts with just one number. For large programs, we are able to encode most calling contexts to a few numbers. We also present our experience of applying context encoding to debugging crash-based failures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5963696","Calling context;context sensitivity;profiling;path encoding;calling context encoding;call graph","Context;Encoding;Instruments;Image edge detection;Runtime;Decoding;Software algorithms","optimisation;program compilers;program debugging","precise calling context encoding;CC;profiling;event logging;stack walking;context recovery;call sites;recursive call paths;acyclic subsequences;encoding optimization;stack depth-based identification;ID;medium-sized programs;crash-based failure debugging","","6","","54","","","","","","IEEE","IEEE Journals & Magazines"
"A case study of CES: a distributed collaborative editing system implemented in Argus","I. Greif; A. Seliger; W. Weihl","Lab. for Comput. Sci., MIT, Cambridge, MA, USA; Lab. for Comput. Sci., MIT, Cambridge, MA, USA; Lab. for Comput. Sci., MIT, Cambridge, MA, USA","IEEE Transactions on Software Engineering","","1992","18","9","827","839","Experience implementing CES, a distributed collaborative editing system, is described. CES was written in Argus, a language that was designed to support the construction of reliable distributed programs, and exhibits a number of requirements typical of distributed applications. The authors' experience illustrates numerous areas in which the support provided by Argus for meeting those requirements was quite helpful, but also identifies several areas in which the support provided by Argus was inadequate. Some of the problems arise because of the distinction in Argus (and in other systems) between locally and remotely accessible data and the mechanisms provided for implementing each. Others arise because of limitations of the mechanisms for building user-defined data types. The authors discuss the problems they encountered, including the implications for other systems. They also suggest solutions to the problems, or in some cases further research directed at finding solutions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.159831","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=159831","","Computer aided software engineering;Collaboration;Collaborative work;Computer science;Writing;Intelligent networks;Collaborative tools;Books;Computerized monitoring;Application software","data structures;groupware;parallel languages;text editing","distributed collaborative editing system;Argus;reliable distributed programs;distributed applications;remotely accessible data;user-defined data types","","18","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Measurement and analysis of workload effects on fault latency in real-time systems","M. H. Woodbury; K. G. Shin","Dept. of Electr. Eng. & Comput. Sci., Michigan Univ., Ann Arbor, MI, USA; Dept. of Electr. Eng. & Comput. Sci., Michigan Univ., Ann Arbor, MI, USA","IEEE Transactions on Software Engineering","","1990","16","2","212","216","The authors demonstrate the need to address fault latency in highly reliable real-time control computer systems. It is noted that the effectiveness of all known recovery mechanisms is greatly reduced in the presence of multiple latent faults. The presence of multiple latent faults increases the possibility of multiple errors, which could result in coverage failure. The authors present experimental evidence indicating that the duration of fault latency is dependent on workload. A synthetic work generator is used to vary the workload, and a hardware fault injector is applied to inject transient faults of varying durations. This method makes it possible to derive the distribution of fault latency duration. Experimental results obtained from the fault-tolerant multiprocessor at the NASA Airlab are presented and discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44383","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44383","","Delay;Real time systems;Computer errors;Hardware;Control systems;NASA;Laboratories;Time sharing computer systems;Fault tolerance;Physics computing","control systems;fault tolerant computing;multiprocessing systems;program testing;real-time systems;software engineering;system recovery","workload effects;fault latency;real-time systems;control computer systems;recovery mechanisms;multiple latent faults;coverage failure;synthetic work generator;hardware fault injector;fault-tolerant multiprocessor;NASA Airlab","","6","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying high performance ERP projects","E. Stensrud; I. Myrtveit","Norwegian Sch. of Manage., Sandvika, Norway; Norwegian Sch. of Manage., Sandvika, Norway","IEEE Transactions on Software Engineering","","2003","29","5","398","416","Learning from high performance projects is crucial for software process improvement. Therefore, we need to identify outstanding projects that may serve as role models. It is common to measure productivity as an indicator of performance. It is vital that productivity measurements deal correctly with variable returns to scale and multivariate data. Software projects generally exhibit variable returns to scale, and the output from ERP projects is multivariate. We propose to use data envelopment analysis variable returns to scale (DEA VRS) to measure the productivity of software projects. DEA VRS fulfills the two requirements stated above. The results from this empirical study of 30 ERP projects extracted from a benchmarking database in Accenture identified six projects as potential role models. These projects deserve to be studied and probably copied as part of a software process improvement initiative. The results also suggest that there is a 50 percent potential for productivity improvement, on average. Finally, the results support the assumption of variable returns to scale in ERP projects. We recommend DEA VRS be used as the default technique for appropriate productivity comparisons of individual software projects. Used together with methods for hypothesis testing, DEA VRS is also a useful technique for assessing the effect of alleged process improvements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199070","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199070","","Enterprise resource planning;Productivity;Software performance;Data envelopment analysis;Software measurement;Best practices;Software engineering;Project management;Performance evaluation;Data mining","software process improvement;enterprise resource planning;data envelopment analysis;project management;software management;management science","high-performance ERP project identification;data envelopment analysis variable returns;software process improvement;multivariate data;software projects;DEA VRS;software project productivity measurement;benchmarking database;Accenture;enterprise resource planning","","47","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Time and Probability-Based Information Flow Analysis","R. Lanotte; A. Maggiolo-Schettini; A. Troina","Universit&#x0E0; dell'Insubria, Como; Universit&#x0E0; di Pisa, Pisa; Universit&#x0E0; di Torino, Torino","IEEE Transactions on Software Engineering","","2010","36","5","719","734","In multilevel systems, it is important to avoid unwanted indirect information flow from higher levels to lower levels, namely, the so-called covert channels. Initial studies of information flow analysis were performed by abstracting away from time and probability. It is already known that systems that are proven to be secure in a possibilistic framework may turn out to be insecure when time or probability is considered. Recently, work has been done in order to consider also aspects either of time or of probability, but not both. In this paper, we propose a general framework based on Probabilistic Timed Automata, where both probabilistic and timing covert channels can be studied. We define a Noninterference security property and a Nondeducibility on Composition security property, which allow expressing information flow in a timed and probabilistic setting. We then compare these properties with analogous ones defined in contexts where either time or probability or neither of them are taken into account. This permits a classification of the properties depending on their discerning power. As an application, we study a system with covert channels that we are able to discover by applying our techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383372","Probabilistic timed automata;multilevel security;information flow analysis;weak bisimulation.","Information analysis;Information security;Automata;Clocks;Multilevel systems;Timing;Communication system control;Performance analysis;Power system security;Control systems","data flow analysis;probabilistic automata;probability;security of data;software agents;software engineering","probability based information flow analysis;multilevel system;indirect unwanted information flow;covert channel;probabilistic timed automata;probabilistic covert channel;timing covert channel;noninterference security property;nondeducibility;composition security property","","7","","45","","","","","","IEEE","IEEE Journals & Magazines"
"Systematic program development","R. G. Dromey","Dept. of Comput. Sci., Wollongong Univ., NSW, Australia","IEEE Transactions on Software Engineering","","1988","14","1","12","29","A constructive method of program development is presented. It is based on a simple strategy for problem decomposition that is claimed to be more supportive of goal-oriented programming than the Wirth-Dijkstra top-down refinement method. With the proposed method, a program is developed by making a sequence of refinements, each of which can establish the postcondition for a corresponding sequence of progressively weaker preconditions until a mechanism has been composed that will establish the postcondition for the original given precondition for the problem. The strategy can minimize case analysis, simplify constructive program proofs, and ensure a correspondence between program structure and data structure.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4619","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4619","","Data structures;Process design;Formal specifications;Pressing;Proposals;Information technology;Australia;Partitioning algorithms","programming;software engineering","program development;problem decomposition;goal-oriented programming;refinements;postcondition;case analysis;constructive program proofs;program structure;data structure","","3","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Reduction methods for real-time systems using Delay Time Petri Nets","E. Y. T. Juan; J. J. P. Tsai; T. Murata; Yi Zhou","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","5","422","448","We present a new net-reduction methodology to facilitate the analysis of real-time systems using Delay Time Petri Nets (DTPNs). Net reduction is one of the most important techniques for reducing the state-explosion problem of Petri nets. However, the application of net reduction to current timed-extensions of Petri nets (such as Merlin's Time PNs) is very limited due to the difficulty faced in the preservation of timing constraints. To overcome this problem, we introduce DTPNs which are inspired by Merlin's (1976) Time PNs, Senac's (1994) Hierarchical Time Stream PNs, and Little's (1991) Timed PNs. We show that DTPNs are much more suitable for net reduction. Then, we present a new set of DTPN reduction rules for the analysis of schedule and deadlock analysis. Our work is distinct from the others since our goal is to analyze real-time systems and the reduction methods we propose preserve both timing properties (schedule) and deadlock. To evaluate our framework, we have implemented an automated analysis tool whose main functions include net reduction and class-graph generation. The experimental results show that our net-reduction methodology leads to a significant contribution to the efficient analysis of real-time systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.922714","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=922714","","Real time systems;Delay systems;Delay effects;Petri nets;Timing;System recovery;Control systems;Aerospace control;Aerospace electronics;Formal verification","real-time systems;Petri nets;scheduling;systems analysis","real-time systems;Delay Time Petri Nets;net-reduction methodology;state-explosion problem;timing constraints;Time Petri nets;Hierarchical Time Stream Petri nets;schedule analysis;deadlock analysis;timing;class-graph generation;experiment","","37","","34","","","","","","IEEE","IEEE Journals & Magazines"
"The Requirements Apprentice: automated assistance for requirements acquisition","H. B. Reubenstein; R. C. Waters","Mitre Corp., Bedford, MA, USA; NA","IEEE Transactions on Software Engineering","","1991","17","3","226","240","An automated tool called the Requirements Apprentice (RA) which assists a human analyst in the creation and modification of software requirements is presented. Unlike most other requirements analysis tools, which start from a formal description language, the focus of the RA is on the transition between informal and formal specifications. The RA supports the earliest phases of creating a requirement, in which ambiguity, contradiction, and incompleteness are inevitable. From an artificial intelligence perspective, the central problem the RA faces is one of knowledge acquisition. The RA develops a coherent internal representation of a requirement from an initial set of disorganized imprecise statements. To do so, the RA relies on a variety of techniques, including dependency-directed reasoning, hybrid knowledge representations and the reuse of common forms (cliches). An annotated transcript showing an interaction with a working version of the RA is given.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.75413","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=75413","","Programming;Artificial intelligence;Knowledge acquisition;Humans;Knowledge representation;Helium;Software systems;Buildings;Knowledge engineering;Terrorism","formal specification;knowledge acquisition;knowledge representation;software tools","Requirements Apprentice;requirements acquisition;automated tool;human analyst;software requirements;formal specifications;ambiguity;contradiction;incompleteness;artificial intelligence;knowledge acquisition;coherent internal representation;disorganized imprecise statements;dependency-directed reasoning;hybrid knowledge representations","","127","","49","","","","","","IEEE","IEEE Journals & Magazines"
"A framework for the automated drawing of data structure diagrams","C. Ding; P. Mateti","Dept. of Comput. Eng. & Sci., Case Western Reserve Univ., Cleveland, OH, USA; NA","IEEE Transactions on Software Engineering","","1990","16","5","543","557","Data structure diagrams are two-dimensional figures made up of lines that aim to pictorially indicate the interrelationships of the elements of a data structure. The various rules and factors of aesthetics that go into the way data structure diagrams are drawn are collected together. The various subjective factors are formulated into computable objectives and numeric parameters. These are distilled from a large number of data structure drawings found in various textbooks. The rules used have not reached a level of acceptance comparable to that of the relevant rules in engineering graphics. The internal architecture of a (sub)system that helps draw data structure diagrams is outlined.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.52777","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=52777","","Data structures;Programming profession;Engineering drawings;Debugging;Books;Displays;Computer science;Animation;Reactive power;Data engineering","computer graphics;data structures;diagrams;software engineering","pictorial indication;automated drawing;data structure diagrams;two-dimensional figures;lines;interrelationships;rules;factors of aesthetics;computable objectives;numeric parameters","","23","","54","","","","","","IEEE","IEEE Journals & Magazines"
"A Uniform Representation of Hybrid Criteria for Regression Testing","S. Sampath; R. Bryce; A. M. Memon","University of Maryland, Baltimore; University of North Texas, Denton; University of Maryland, Baltimore","IEEE Transactions on Software Engineering","","2013","39","10","1326","1344","Regression testing tasks of test case prioritization, test suite reduction/minimization, and regression test selection are typically centered around criteria that are based on code coverage, test execution costs, and code modifications. Researchers have developed and evaluated new individual criteria; others have combined existing criteria in different ways to form what we--and some others--call hybrid criteria. In this paper, we formalize the notion of combining multiple criteria into a hybrid. Our goal is to create a uniform representation of such combinations so that they can be described unambiguously and shared among researchers. We envision that such sharing will allow researchers to implement, study, extend, and evaluate the hybrids using a common set of techniques and tools. We precisely formulate three hybrid combinations, Rank, Merge, and Choice, and demonstrate their usefulness in two ways. First, we recast, in terms of our formulations, others' previously reported work on hybrid criteria. Second, we use our previous results on test case prioritization to create and evaluate new hybrid criteria. Our findings suggest that hybrid criteria of others can be described using our Merge and Rank formulations, and that the hybrid criteria we developed most often outperformed their constituent individual criteria.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6484067","Test case prioritization;test criteria;hybrid test criteria;web testing;GUI testing","Testing;Fault detection;Educational institutions;Genetic algorithms;Vectors;Loss measurement;Minimization","program testing;regression analysis","uniform representation;hybrid test criteria;regression testing;rank-merge-and-choice hybrid combination;test case prioritization;merge-and-rank formulations","","13","","74","","","","","","IEEE","IEEE Journals & Magazines"
"NDT. A Model-Driven Approach for Web Requirements","M. J. Escalona; G. Aragón","University of Seville, Seville; Everis, Seville","IEEE Transactions on Software Engineering","","2008","34","3","377","390","Web engineering is a new research line in software engineering that covers the definition of processes, techniques, and models suitable for Web environments in order to guarantee the quality of results. The research community is working in this area and, as a very recent line, they are assuming the Model-Driven paradigm to support and solve some classic problems detected in Web developments. However, there is a lack in Web requirements treatment. This paper presents a general vision of Navigational Development Techniques (NDT), which is an approach to deal with requirements in Web systems. It is based on conclusions obtained in several comparative studies and it tries to fill some gaps detected by the research community. This paper presents its scope, its most important contributions, and offers a global vision of its associated tool: NDT-Tool. Furthermore, it analyzes how Web Engineering can be applied in the enterprise environment. NDT is being applied in real projects and has been adopted by several companies as a requirements methodology. The approach offers a Web requirements solution based on a Model-Driven paradigm that follows the most accepted tendencies by Web engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4497213","Software engineering for Internet projects;Surveys of historical development of one particular area;Requirements/Specifications;Software engineering for Internet projects;Surveys of historical development of one particular area;Requirements/Specifications","Model driven engineering;Software engineering;Navigation;Application software;Standards development;Companies;Internet;Software systems;Systems engineering and theory;Proposals","Internet;software engineering;Web design","Web requirements;Web engineering;software engineering;model-driven paradigm;Web developments;navigational development techniques;NDT-Tool","","42","","57","","","","","","IEEE","IEEE Journals & Magazines"
"On the exact and approximate throughput analysis of closed queuing networks with blocking","I. F. Akyildiz","Sch. of Inf. & Comput. Sci., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Transactions on Software Engineering","","1988","14","1","62","70","A type of blocking is investigated in which, on completion of its service, a job attempts to enter a new station. If, at that moment, the destination station is full, the job is forced to reside in the server of the source station until a place becomes available in the destination station. The server of the source station remains blocked during this period of time. This model is known as a queuing network with transfer blocking. The state space of queuing networks with blocking is reduced by considering finite capacities of the stations. A nonblocking queuing network with the appropriate total number of jobs is derived. The state space of this network is equal to the state space of the blocking queuing network. The transformation of state space is exact for two-station networks and approximate for three-or-more station cases. The approximation has been validated by executing several examples, including stress tests. In all investigated network models, the approximate throughput results deviate, on the average, less than 3% from the simulation results.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4623","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4623","","Throughput;Queueing analysis;Network servers;State-space methods;Computer networks;Application software;Algorithm design and analysis;Occupational stress;Testing;Performance analysis","performance evaluation;queueing theory;virtual machines","exact throughput;approximate throughput analysis;closed queuing networks;blocking;source station;destination station;transfer blocking;state space;two-station networks;simulation results","","44","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Principles and an Industrial Case Study in Retrieving Equivalent Requirements via Natural Language Processing Techniques","D. Falessi; G. Cantone; G. Canfora","Simula Research Laboratory, Lysaker and University of Rome "TorVergata", Rome; University of Rome "TorVergata", Rome; University of Sannio, Benevento","IEEE Transactions on Software Engineering","","2013","39","1","18","44","Though very important in software engineering, linking artifacts of the same type (clone detection) or different types (traceability recovery) is extremely tedious, error-prone, and effort-intensive. Past research focused on supporting analysts with techniques based on Natural Language Processing (NLP) to identify candidate links. Because many NLP techniques exist and their performance varies according to context, it is crucial to define and use reliable evaluation procedures. The aim of this paper is to propose a set of seven principles for evaluating the performance of NLP techniques in identifying equivalent requirements. In this paper, we conjecture, and verify, that NLP techniques perform on a given dataset according to both ability and the odds of identifying equivalent requirements correctly. For instance, when the odds of identifying equivalent requirements are very high, then it is reasonable to expect that NLP techniques will result in good performance. Our key idea is to measure this random factor of the specific dataset(s) in use and then adjust the observed performance accordingly. To support the application of the principles we report their practical application to a case study that evaluates the performance of a large number of NLP techniques for identifying equivalent requirements in the context of an Italian company in the defense and aerospace domain. The current application context is the evaluation of NLP techniques to identify equivalent requirements. However, most of the proposed principles seem applicable to evaluating any estimation technique aimed at supporting a binary decision (e.g., equivalent/nonequivalent), with the estimate in the range [0,1] (e.g., the similarity provided by the NLP), when the dataset(s) is used as a benchmark (i.e., testbed), independently of the type of estimator (i.e., requirements text) and of the estimation method (e.g., NLP).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.122","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6112783","Empirical software engineering;traceability recovery;natural language processing;equivalent requirements;metrics and measurement","Natural language processing;Context;Semantics;Measurement;Matrix decomposition;Monitoring;Thesauri","information retrieval;natural language processing;program diagnostics;software engineering","industrial case study;equivalent requirement retrieval;natural language processing techniques;software engineering;traceability recovery;clone detection;NLP techniques;reliable evaluation procedures;Italian company;aerospace domain;defense domain;estimation technique;binary decision","","40","","116","","","","","","IEEE","IEEE Journals & Magazines"
"An overview of JSD","J. R. Cameron","Michael Jackson Systems, Limited, London WIN 5AF, England","IEEE Transactions on Software Engineering","","1986","SE-12","2","222","240","The Jackson System Development (JSD) method addresses most of the software lifecycle. JSD specifications consist mainly of a distributed network of processes that communicate by message-passing and read-only inspection of each other's data. A JSD specification is therefore directly executable, at least in principle. Specifications are developed middle-out from an initial set of `model' processes. The model processes define a set of events, which limit the scope of the system, define its semantics, and form the basis for defining data and outputs. Implementation often involves reconfiguring or transforming the network to run on a smaller number of real or virtual processors. The main phase of JSD are introduced and illustrated by a small example system. The rationale for the approach is discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312938","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312938","Design methodology;system design;systems analysis","Libraries;Films;Data models;Databases;Synchronization;Contracts;Program processors","software engineering","JSD;Jackson System Development;software lifecycle;distributed network;message-passing;read-only inspection;JSD specification;semantics","","32","","","","","","","","IEEE","IEEE Journals & Magazines"
"A Study of Uncertainty in Software Cost and Its Impact on Optimal Software Release Time","B. Yang; H. Hu; L. Jia","University of Electronic Science and Technology of China, Chengdu; University of Electronic Science and Technology of China, Chengdu; Xi'an Jiaotong University, Xi'an","IEEE Transactions on Software Engineering","","2008","34","6","813","825","For a software development project, management often faces the dilemma of when to stop testing the software and release it for operation, which requires careful decision making as it has great impact on both software reliability and project cost. In most existing research on the optimal software release problem, the cost considered was the Expected Cost (EC) of the project. However, what concerns management is the Actual Cost (AC) of the project rather than the EC. Treatment (such as minimization) of the EC may not ensure the desired low level of the AC due to the uncertainty (variability) involved in the AC. In this paper, we study the uncertainty in software cost and its impact on optimal software release time in detail. The uncertainty is quantified by the variance of the AC and several risk functions. A risk-control approach to the optimal software release problem is proposed. New formulations of the problem which are extensions of current formulations are developed and solution procedures are established. Several examples are presented. Results reveal that it seems crucial to take into account the uncertainty in software cost in the optimal software release problem; otherwise, unsafe decisions may be reached which could be a false dawn to management.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4553721","Cost estimation;Time estimation;Project control and modeling;Reliability;Cost estimation;Time estimation;Project control and modeling;Reliability","Uncertainty;Cost function;Software testing;Programming;Project management;Decision making;Software reliability;Software safety;Software development management;Software quality","project management;software cost estimation;software development management;software reliability","optimal software release time;nonhomogeneous Poisson process;software reliability;actual cost;expected cost;risk-control approach;software cost uncertainty","","36","","36","","","","","","IEEE","IEEE Journals & Magazines"
"The Use of Summation to Aggregate Software Metrics Hinders the Performance of Defect Prediction Models","F. Zhang; A. E. Hassan; S. McIntosh; Y. Zou","School of Computing, Queen's University, Kingston, ON, Canada; School of Computing, Queen's University, Kingston, ON, Canada; Department of Electrical and Computer Engineering, McGill University, Montréal, QC, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2017","43","5","476","491","Defect prediction models help software organizations to anticipate where defects will appear in the future. When training a defect prediction model, historical defect data is often mined from a Version Control System (VCS, e.g., Subversion), which records software changes at the file-level. Software metrics, on the other hand, are often calculated at the class- or method-level (e.g., McCabe's Cyclomatic Complexity). To address the disagreement in granularity, the class- and method-level software metrics are aggregated to file-level, often using summation (i.e., McCabe of a file is the sum of the McCabe of all methods within the file). A recent study shows that summation significantly inflates the correlation between lines of code (Sloc) and cyclomatic complexity (Cc) in Java projects. While there are many other aggregation schemes (e.g., central tendency, dispersion), they have remained unexplored in the scope of defect prediction. In this study, we set out to investigate how different aggregation schemes impact defect prediction models. Through an analysis of 11 aggregation schemes using data collected from 255 open source projects, we find that: (1) aggregation schemes can significantly alter correlations among metrics, as well as the correlations between metrics and the defect count; (2) when constructing models to predict defect proneness, applying only the summation scheme (i.e., the most commonly used aggregation scheme in the literature) only achieves the best performance (the best among the 12 studied configurations) in 11 percent of the studied projects, while applying all of the studied aggregation schemes achieves the best performance in 40 percent of the studied projects; (3) when constructing models to predict defect rank or count, either applying only the summation or applying all of the studied aggregation schemes achieves similar performance, with both achieving the closest to the best performance more often than the other studied aggregation schemes; and (4) when constructing models for effort-aware defect prediction, the mean or median aggregation schemes yield performance values that are significantly closer to the best performance than any of the other studied aggregation schemes. Broadly speaking, the performance of defect prediction models are often underestimated due to our community's tendency to only use the summation aggregation scheme. Given the potential benefit of applying additional aggregation schemes, we advise that future defect prediction models should explore a variety of aggregation schemes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2599161","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7539677","Defect prediction;aggregation scheme;software metrics","Predictive models;Correlation;Software metrics;Indexes;Software;Data models","data aggregation;data mining;Java;public domain software;software metrics","software metrics aggregation;defect prediction models;software organizations;historical defect data mining;version control system;software changes recording;McCabe cyclomatic complexity;granularity disagreement;class-level software metrics;method-level software metrics;lines of code;Sloc;Cc;Java projects;open source projects;effort-aware defect prediction;summation","","6","","87","","","","","","IEEE","IEEE Journals & Magazines"
"Existence dependency: The key to semantic integrity between structural and behavioral aspects of object types","M. Snoeck; G. Dedene","Univ. Libre de Bruxelles, Belgium; NA","IEEE Transactions on Software Engineering","","1998","24","4","233","251","In object-oriented conceptual modeling, the generalization/specialization hierarchy and the whole/part relationship are prevalent classification schemes for object types. This paper presents an object-oriented conceptual model where, in the end, object types are classified according to two relationships only. Existence dependency and generalization/specialization. Existence dependency captures some of the interesting semantics that are usually associated with the concept of aggregation (also called composition or Part Of relation), but in contrast with the latter concept, the semantics of existence dependency are very precise and its use clear cut. The key advantage of classifying object types according to existence dependency are the simplicity of the concept, its absolute unambiguity, and the fact that it enables to check conceptual schemes for semantic integrity and consistency. We will first define the notion of existence dependency and claim that it is always possible to classify objects according to this relationship, thus removing the necessity for the Part Of relation and other kinds of associations between object types. The second claim of this paper is that existence dependency is the key to semantic integrity checking to a level unknown to current object-oriented analysis methods. In other words: Existence dependency allows us to track and solve inconsistencies in an object-oriented conceptual schema.","0098-5589;1939-3520;2326-3881","","10.1109/32.677182","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=677182","","Object oriented modeling;Lattices;Automata;Quality control","object-oriented methods;software engineering","semantic integrity;object types;existence dependency;object-oriented conceptual modeling;software engineering;conceptual model;aggregation;composition;quality;consistency checking","","52","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Effects of Personality on Pair Programming","J. E. Hannay; E. Arisholm; H. Engvik; D. I. K. Sjoberg","Simula Research Laboratory, Lysaker and University of Oslo, Oslo; Simula Research Laboratory, Lysaker and University of Oslo, Oslo; University of Oslo, Oslo; University of Oslo, Oslo","IEEE Transactions on Software Engineering","","2010","36","1","61","80","Personality tests in various guises are commonly used in recruitment and career counseling industries. Such tests have also been considered as instruments for predicting the job performance of software professionals both individually and in teams. However, research suggests that other human-related factors such as motivation, general mental ability, expertise, and task complexity also affect the performance in general. This paper reports on a study of the impact of the Big Five personality traits on the performance of pair programmers together with the impact of expertise and task complexity. The study involved 196 software professionals in three countries forming 98 pairs. The analysis consisted of a confirmatory part and an exploratory part. The results show that: (1) Our data do not confirm a meta-analysis-based model of the impact of certain personality traits on performance and (2) personality traits, in general, have modest predictive value on pair programming performance compared with expertise, task complexity, and country. We conclude that more effort should be spent on investigating other performance-related predictors such as expertise, and task complexity, as well as other promising predictors, such as programming skill and learning. We also conclude that effort should be spent on elaborating on the effects of personality on various measures of collaboration, which, in turn, may be used to predict and influence performance. Insights into such malleable, rather than static, factors may then be used to improve pair programming performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5089333","Pair programming;personality;Big Five;expertise;task complexity;performance.","Programming profession;Collaborative work;Keyboards;Books;Recruitment;Engineering profession;Employee welfare;Software testing;Instruments;Software performance","human factors;personnel;programming;recruitment;team working","personality tests;pair programming;recruitment industries;career counseling industries;job performance;software professionals;personality traits;task complexity;meta-analysis-based model;performance-related predictors","","47","","121","","","","","","IEEE","IEEE Journals & Magazines"
"Consistency issues in distributed checkpoints","J. -. Helary; R. H. B. Netzer; M. Raynal","IRISA, Rennes, France; NA; NA","IEEE Transactions on Software Engineering","","1999","25","2","274","281","A global checkpoint is a set of local checkpoints, one per process. The traditional consistency criterion for global checkpoints states that a global checkpoint is consistent if it does not include messages received and not sent. The paper investigates other consistency criteria, transitlessness, and strong consistency. A global checkpoint is transitless if it does not exhibit messages sent and not received. Transitlessness can be seen as a dual of traditional consistency. Strong consistency is the addition of transitlessness to traditional consistency. The main result of the paper is a statement of the necessary and sufficient condition answering the following question: ""given an arbitrary set of local checkpoints, can this set be extended to a global checkpoint that satisfies P"" (where P is traditional consistency, transitlessness, or strong consistency). From a practical point of view, this condition, when applied to transitlessness, is particularly interesting as it helps characterize which messages do not need to be recorded by checkpointing protocols.","0098-5589;1939-3520;2326-3881","","10.1109/32.761450","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=761450","","Sufficient conditions;Checkpointing;Protocols;Computer Society;Fault tolerant systems;Heart;System recovery;Distributed computing;Computational modeling","distributed algorithms;program diagnostics;data integrity;software fault tolerance","consistency issues;distributed checkpoints;global checkpoint;local checkpoints;consistency criterion;transitlessness;strong consistency;sufficient condition;arbitrary set;traditional consistency;checkpointing protocols;message recording;distributed systems;fault tolerance;rollback recovery","","25","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Extraction and Clustering of Requirements Glossary Terms","C. Arora; M. Sabetzadeh; L. Briand; F. Zimmer","SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Alphonse Weicker, Luxembourg; SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Alphonse Weicker, Luxembourg; SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Alphonse Weicker, Luxembourg; SES Techcom, Betzdorf, Luxembourg","IEEE Transactions on Software Engineering","","2017","43","10","918","945","A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary helps mitigate imprecision and ambiguity. A key step in building a glossary is to decide upon the terms to include in the glossary and to find any related terms. Doing so manually is laborious, particularly for large requirements documents. In this article, we develop an automated approach for extracting candidate glossary terms and their related terms from natural language requirements documents. Our approach differs from existing work on term extraction mainly in that it clusters the extracted terms by relevance, instead of providing a flat list of terms. We provide an automated, mathematically-based procedure for selecting the number of clusters. This procedure makes the underlying clustering algorithm transparent to users, thus alleviating the need for any user-specified parameters. To evaluate our approach, we report on three industrial case studies, as part of which we also examine the perceptions of the involved subject matter experts about the usefulness of our approach. Our evaluation notably suggests that: (1) Over requirements documents, our approach is more accurate than major generic term extraction tools. Specifically, in our case studies, our approach leads to gains of 20 percent or more in terms of recall when compared to existing tools, while at the same time either improving precision or leaving it virtually unchanged. And, (2) the experts involved in our case studies find the clusters generated by our approach useful as an aid for glossary construction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2635134","Luxembourg’s National Research Fund; European Research Council; European Union’s Horizon 2020 research and innovation program; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7765062","Requirements glossaries;term extraction;natural language processing;clustering;case study research","Terminology;Servers;Pipelines;Natural languages;Monitoring;Software;Clustering algorithms","natural language processing;pattern clustering;text analysis","natural language requirements documents;mathematically-based procedure;user-specified parameters;candidate glossary terms;technical terms;clustering algorithm;automated approach;software requirements document;requirements glossary terms;glossary construction;generic term extraction tools","","2","","100","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Crossover Designs in Software Engineering Experiments: Benefits and Perils","S. Vegas; C. Apa; N. Juristo","Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, Madrid, Spain; Instituto de Computación, Facultad de Ingeniería, Montevideo, Uruguay; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, Madrid, Spain","IEEE Transactions on Software Engineering","","2016","42","2","120","135","In experiments with crossover design subjects apply more than one treatment. Crossover designs are widespread in software engineering experimentation: they require fewer subjects and control the variability among subjects. However, some researchers disapprove of crossover designs. The main criticisms are: the carryover threat and its troublesome analysis. Carryover is the persistence of the effect of one treatment when another treatment is applied later. It may invalidate the results of an experiment. Additionally, crossover designs are often not properly designed and/or analysed, limiting the validity of the results. In this paper, we aim to make SE researchers aware of the perils of crossover experiments and provide risk avoidance good practices. We study how another discipline (medicine) runs crossover experiments. We review the SE literature and discuss which good practices tend not to be adhered to, giving advice on how they should be applied in SE experiments. We illustrate the concepts discussed analysing a crossover experiment that we have run. We conclude that crossover experiments can yield valid results, provided they are properly designed and analysed, and that, if correctly addressed, carryover is no worse than other validity threats.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2467378","Spanish Ministry of Economy and Competitiveness research; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7192651","Data analysis;crossover designs;carryover;Experimental software engineering;controlled experiment;data analysis;crossover design;carryover","Software engineering;Animals;Atmospheric measurements;Particle measurements;Psychology;US Government agencies;Information processing","software engineering","crossover design;software engineering experiments;SE research;crossover experiment","","7","","38","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of software design practices","D. N. Card; V. E. Church; W. W. Agresti","System Sciences Division, Computer Sciences Corporation, Silver Spring, MD 20910; System Sciences Division, Computer Sciences Corporation, Silver Spring, MD 20910; System Sciences Division, Computer Sciences Corporation, Silver Spring, MD 20910","IEEE Transactions on Software Engineering","","1986","SE-12","2","264","271","Results of an empirical study of software design practices in one specific environment are reported. The practices examined affect module size, module strength, data coupling, descendant span, unreferenced variables, and software reuse. Measures characteristic of these practices were extracted from 887 Fortran modules developed for five flight dynamics software projects monitored by the Software Engineering Laboratory. The relationship of these measures to cost and fault rate was analyzed using a contingency table procedure. The results show that some recommended design practices, despite their intuitive appeal, are ineffective in this environment, whereas others are very effective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312942","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312942","Coupling;fault rate;module cost;reuse;size;Software Engineering Laboratory;strength;unreferenced variables","Couplings;Correlation;Software reusability;Computers;Software design;Monitoring","software engineering","software design practices;module size;module strength;data coupling;descendant span;unreferenced variables;software reuse;Fortran modules;flight dynamics software projects;Software Engineering Laboratory;cost;fault rate;contingency table","","16","","","","","","","","IEEE","IEEE Journals & Magazines"
"An experimental comparison of the effectiveness of branch testing and data flow testing","P. G. Frankl; S. N. Weiss","Dept. of Comput. Sci., Polytech. Univ., Brooklyn, NY, USA; NA","IEEE Transactions on Software Engineering","","1993","19","8","774","787","An experiment comparing the effectiveness of the all-uses and all-edges test data adequacy criteria is discussed. The experiment was designed to overcome some of the deficiencies of previous software testing experiments. A large number of test sets was randomly generated for each of nine subject programs with subtle errors. For each test set, the percentages of executable edges and definition-use associations covered were measured, and it was determined whether the test set exposed an error. Hypothesis testing was used to investigate whether all-uses adequate test sets are more likely to expose errors than are all-edges adequate test sets. Logistic regression analysis was used to investigate whether the probability that a test set exposes an error increases as the percentage of definition-use associations or edges covered by it increases. Error exposing ability was shown to be strongly positively correlated to percentage of covered definition-use associations in only four of the nine subjects. Error exposing ability was also shown to be positively correlated to the percentage of covered edges in four different subjects, but the relationship was weaker.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.238581","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=238581","","Software testing;Data analysis;Computer science;Performance evaluation;Random number generation;Logistics;Regression analysis;Genetic mutations;Acoustic testing;Error correction","errors;program testing","error exposing ability;branch testing;data flow testing;all-edges test data adequacy criteria;software testing experiments;executable edges;definition-use associations;all-uses adequate test sets;regression analysis","","145","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Software Reliability Growth Modeling: Models and Applications","S. Yamada; S. Osaki","Graduate School of Systems Science, Okayama University of Science; NA","IEEE Transactions on Software Engineering","","1985","SE-11","12","1431","1437","This paper summarizes existing software reliability growth models (SRGM's) described by nonhomogeneous Poisson processes. The SRGM's are classified in terms of the software reliability growth index of the error detection rate per error. The maximum-likelihood estimations based on the SRGM's are discussed for software reliability data analysis and software reliability evaluation. Using actual software error data observed by software testing, application examples of the existing SRGM's are illustrated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232179","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701965","Error detection rate per error;maximum-likelihood estimation;nonhomogeneous Poisson processes;software error;software reliability analysis;software reliability growth models","Software reliability;Application software;Software testing;Software systems;Maximum likelihood estimation;Software measurement;Error correction;Data analysis;Computer errors;Programming","","Error detection rate per error;maximum-likelihood estimation;nonhomogeneous Poisson processes;software error;software reliability analysis;software reliability growth models","","208","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying extended entity-relationship object structures in relational schemas","V. M. Markowitz; J. A. Makowsky","Dept. of Comput. Sci. Res., Lawrence Berkeley Lab., CA, USA; NA","IEEE Transactions on Software Engineering","","1990","16","8","777","790","Relational schemas consisting of relation-schemes, key dependencies and key-based inclusion dependencies (referential integrity constraints) are considered. Schemas of this form are said to be entity-relationship (EER)-convertible if they can be associated with an EER schema. A procedure that determines whether a relational schema is EER-convertible is developed. A normal form is proposed for relational schemas representing EER object structures. For EER-convertible relational schemas, the corresponding normalization procedure is presented. The procedures can be used for analyzing the semantics of existing relational databases and for converting relational database schemas into object-oriented database schemas.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.57618","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=57618","","Relational databases;Erbium;Data models;Object oriented modeling;Object oriented databases;Information systems;Laboratories;Transaction databases","database management systems;database theory;object-oriented programming;relational databases","extended entity-relationship object structures;relation-schemes;key dependencies;key-based inclusion dependencies;referential integrity constraints;EER schema;EER-convertible;normal form;relational schemas;EER object structures;normalization procedure;semantics;relational database schemas;object-oriented database schemas","","78","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A contingency approach to estimating record selectivities","P. -. Chu","Coll. of Bus., Ohio State Univ., Columbus, OH, USA","IEEE Transactions on Software Engineering","","1991","17","6","544","552","An approach to estimating record selectivity rooted in the theory of fitting a hierarchy of models in discrete data analysis is presented. In contrast to parametric methods, this approach does not presuppose a distribution pattern to which the actual data conform; it searches for one that fits the actual data. This approach makes use of parsimonious models wherever appropriate in order to minimize the storage requirement without sacrificing accuracy. Two-dimensional cases are used as examples to illustrate the proposed method. It is demonstrated that the technique of identifying a good-fitting and parsimonious model can drastically reduce storage space and that the implementation of this technique requires little extra processing effort. The case of perfect or near-perfect association and the idea of keeping information about salient cells of a table are discussed. A strategy to reduce storage requirement in cases in which a good-fitting and parsimonious model is not available is proposed. Hierarchical models for three-dimensional cases are presented, along with a description of the W.E. Deming and F.F. Stephan (1940) iterative proportional fitting algorithm which fits hierarchical models of any dimensions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.87280","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=87280","","Histograms;Cost function;Data analysis;Parameter estimation;Query processing;Relational databases;Data models;Information retrieval;Database systems;Shape","information retrieval systems;relational databases;storage management","contingency approach;record selectivities;discrete data analysis;parsimonious models;storage requirement;storage space;near-perfect association;storage requirement;three-dimensional cases;iterative proportional fitting algorithm;hierarchical models","","4","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Query Processing in a Fragmented Relational Distributed System: Mermaid","C. T. Yu; C. C. Chang; M. Templeton; D. Brill; E. Lund","Department of Electrical Engineering and Computer Science, University of Illinois; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","8","795","810","This paper describes the query optimizer of the Mermaid system which provides a user with a unified view of multiple preexisting databases which may be stored under different DBMS's. The algorithm is designed for databases which may contain replicated or fragmented relations and for users who are primarily making interactive, ad hoc queries. Although the implementation of the algorithm is a front-end system, not an integrated distributed DBMS, it should be applicable to a distributed DBMS also.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232528","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702088","Algorithm;distributed query processing;dynamic estimation;semijoin","Query processing;Relational databases;Transaction databases;Algorithm design and analysis;Delay;Spatial databases;Costs;Parallel processing;Assembly;Database languages","","Algorithm;distributed query processing;dynamic estimation;semijoin","","19","","45","","","","","","IEEE","IEEE Journals & Magazines"
"""Sampling"" as a Baseline Optimizer for Search-based Software Engineering","J. Chen; V. Nair; R. Krishna; T. Menzies","Computer science, North Carolina State University, 6798 Raleigh, North Carolina United States 27695 (e-mail: jchen37@ncsu.edu); Computer science, North Carolina State University, 6798 Raleigh, North Carolina United States (e-mail: vivekaxl@gmail.com); Computer science, North Carolina State University, 6798 Raleigh, North Carolina United States (e-mail: rkrish11@ncsu.edu); Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, West Virginia United States 26501 (e-mail: tim@menzies.us)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Increasingly, Software Engineering (SE) researchers use search-based optimization techniques to solve SE problems with multiple conflicting objectives. These techniques often apply CPU-intensive evolutionary algorithms to explore generations of mutations to a population of candidate solutions. An alternative approach, proposed in this paper, is to start with a very large population and sample down to just the better solutions. We call this method ""SWAY"", short for ""the sampling way"". This paper compares SWAY versus state-of-the-art search-based SE tools using seven models: five software product line models; and two other software process control models (concerned with project management, effort estimation, and selection of requirements) during incremental agile development. For these models, the experiments of this paper show that SWAY is competitive with corresponding state-of-the-art evolutionary algorithms while requiring orders of magnitude fewer evaluations. Considering the simplicity and effectiveness of SWAY, we, therefore, propose this approach as a baseline method for search-based software engineering models, especially for models that are very slow to execute.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2790925","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8249828","Search-based SE;Sampling;Evolutionary Algorithms","Software engineering;Optimization;Software;Numerical models;Evolutionary computation;Estimation;Sociology","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"Automatic Compiler Production: The Front End","S. P. Reiss","Department of Computer Science, Brown University","IEEE Transactions on Software Engineering","","1987","SE-13","6","609","627","This paper describes a system for automatically producing complete compiler front ends from simple, nonprocedural specifications of the source language. This system is based on a detailed model of a compiler front end that is presented first. The system itself is then described using a Pascal subset as an example. This work was part of a larger project aimed at producing complete compilers in a similar fashion.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233472","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702267","Compiler-compilers;compiler generation;data types;semantics;symbols","Production systems;Buildings;Robustness;Computer science","","Compiler-compilers;compiler generation;data types;semantics;symbols","","","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Starvation and critical race analyzers for Ada","G. M. Karam; R. J. A. Buhr","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada","IEEE Transactions on Software Engineering","","1990","16","8","829","843","Starvation and critical race analysis tools for Ada designs are described. These tools are part of a temporal analysis toolset that includes an operational specification language, a language interpreter, and a deadlock analyzer for Ada. The starvation analyzer is based on a set-theoretic model of starvation. It uses a proof tree produced by the deadlock analyzer to define the possible computation space of the design. A preprocessing phase of the starvation tool optimizes the analysis so that the resulting analysis is efficient. Unlike livelock analysis in state machines, the starvation analyzer does not require a priori specification of home states to discern liveness. The critical race analysis tool provides semiautomatic proof of critical races by identifying nondeterministic rendezvous (races) from the proof tree generated by the deadlock analyzer, and then assisting the human operator in identifying which of these constitute critical races. Several design examples are used to demonstrate the capabilities of the two analysis methods.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.57622","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=57622","","System recovery;Algorithm design and analysis;Information analysis;Specification languages;Automatic testing;Delay;Software testing;Humans;System analysis and design","Ada;program interpreters;programming;software tools;specification languages;system recovery","race analyzers;critical race analysis tools;Ada designs;temporal analysis toolset;operational specification language;language interpreter;deadlock analyzer;starvation analyzer;set-theoretic model;deadlock analyzer;computation space;preprocessing phase;starvation tool;liveness;semiautomatic proof;nondeterministic rendezvous;human operator;design examples","","19","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Formal analysis of the alternating bit protocol by temporal Petri nets","I. Suzuki","Dept. of Electr. Eng. & Comput. Sci., Wisconsin Univ., Milwaukee, WI, USA","IEEE Transactions on Software Engineering","","1990","16","11","1273","1281","Temporal Petri nets are Petri nets in which certain restrictions on the firings of transitions are represented by formulas containing temporal operators. The use of temporal Petri nets for formal specification and verification of the alternating bit protocol is discussed. The temporal Petri net which models the protocol is analyzed formally using the existing theory of omega -regular expressions and Buchi-automata.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60315","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60315","","Protocols;Petri nets;Transmission lines;Safety;Formal specifications;Logic;Timing;Computer simulation;Transmission line theory","automata theory;formal specification;Petri nets;program verification;programming theory","formal analysis;formal verification;alternating bit protocol;temporal Petri nets;firings;transitions;formulas;temporal operators;formal specification;omega -regular expressions;Buchi-automata","","41","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient Dynamic Updates of Distributed Components Through Version Consistency","L. Baresi; C. Ghezzi; X. Ma; V. P. L. Manna","Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy; State Key Laboratory for Novel Software Technology and the Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing University, Nanjing, Jiangsu, China; Holst Centre/imec the Netherlands, Eindhoven, AE, The Netherlands","IEEE Transactions on Software Engineering","","2017","43","4","340","358","Modern component-based distributed software systems are increasingly required to offer non-stop service and thus their updates must be carried out at runtime. Different authors have already proposed solutions for the safe management of dynamic updates. Our contribution aims at improving their efficiency without compromising safety. We propose a new criterion, called version consistency, which defines when a dynamic update can be safely and efficiently applied to the components that execute distributed transactions. Version consistency ensures that distributed transactions be served as if they were operated on a single coherent version of the system despite possible concurrent updates. The paper presents a distributed algorithm for checking version consistency efficiently, formalizes the proposed approach by means of a graph transformation system, and verifies its correctness through model checking. The paper also presents ConUp, a novel prototype framework that supports the approach and offers a viable, concrete solution for the use of version consistency. Both the approach and ConUp are evaluated on a significant third-party application. Obtained results witness the benefits of the proposed solution with respect to both timeliness and disruption.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2592913","The 973 Program of China; NSFC; EEB-Edifici a zero consumo energetico in distretti urbani intelligenti; Italian Technology Cluster For Smart Communities; Telecom Italia; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7516718","Component-based distributed system;dynamic update;version-consistency","Portals;Runtime;Software systems;Safety;Model checking;Concrete","distributed programming;formal verification;graph theory","distributed components;version consistency;distributed software systems;dynamic update;graph transformation system","","2","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Flow control for limited buffer multicast","P. B. Danzig","Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA","IEEE Transactions on Software Engineering","","1994","20","1","1","12","Analyses a multiround flow control algorithm that attempts to minimize the time required to multicast a message to a group of recipients and receive responses directly from each group member. Such a flow control algorithm may be necessary because the hurry of responses to the multicast can overflow the buffer space of the process that issued the multicast. The condition that each recipient directly respond to the multicast prevents the use of reliable multicast protocols based on software combining trees or negative-acknowledgments. The flow control analysed algorithm directs the responding processes to hold their responses for some period of time, called the backoff time, before sending them to the originator. The backoff time depends on the number of recipients that respond, the originator's available buffer space and buffer service time distribution, and the number of times that the originator is willing to retransmit its message. This paper develops an approximate analysis of the service time distribution of the limited-buffer preemptive queuing process that occurs within the protocol processing layers of a multiprogrammed operating system. It then uses this model to calculate multicast backoff times. The paper reports experimental verification of the accuracy of this service time model and discusses its application to the multicast flow control problem.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.263751","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=263751","","Multicast algorithms;Multicast protocols;Communication system control;Operating systems;Intersymbol interference;Algorithm design and analysis;Queueing analysis;Communication channels;Throughput;System recovery","telecommunications control;protocols;storage management;queueing theory;buffer storage;multiprogramming;network operating systems","multiround flow control algorithm;limited buffer multicast;buffer overflow;response holding;backoff time;recipients;available buffer space;buffer service time distribution;message retransmission;approximate analysis;service time distribution;limited-buffer preemptive queuing process;protocol processing layers;multiprogrammed operating system;accuracy","","16","","29","","","","","","IEEE","IEEE Journals & Magazines"
"An Autonomous Engine for Services Configuration and Deployment","F. Cuadrado; J. C. Duenas; R. Garcia-Carmona","Queen Mary University of London, London; Universidad Politécnica de Madrid, Madrid; Universidad Politécnica de Madrid, Madrid","IEEE Transactions on Software Engineering","","2012","38","3","520","536","The runtime management of the infrastructure providing service-based systems is a complex task, up to the point where manual operation struggles to be cost effective. As the functionality is provided by a set of dynamically composed distributed services, in order to achieve a management objective multiple operations have to be applied over the distributed elements of the managed infrastructure. Moreover, the manager must cope with the highly heterogeneous characteristics and management interfaces of the runtime resources. With this in mind, this paper proposes to support the configuration and deployment of services with an automated closed control loop. The automation is enabled by the definition of a generic information model, which captures all the information relevant to the management of the services with the same abstractions, describing the runtime elements, service dependencies, and business objectives. On top of that, a technique based on satisfiability is described which automatically diagnoses the state of the managed environment and obtains the required changes for correcting it (e.g., installation, service binding, update, or configuration). The results from a set of case studies extracted from the banking domain are provided to validate the feasibility of this proposal.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728830","Autonomic systems;model-based management;satisfiability;service configuration.","Runtime;Containers;Servers;Web services;Context;Business;Engines","computability;fault tolerant computing;service-oriented architecture","autonomous engine;services configuration;runtime management;service-based systems;dynamically composed distributed services;distributed elements;runtime resources;automated closed control loop;generic information model;runtime elements;service dependencies;business objectives;satisfiability;banking domain","","9","","38","","","","","","IEEE","IEEE Journals & Magazines"
"From live sequence charts to state machines and back: a guided tour","Y. Bontemps; P. Heymans; P. -. Schobbens","SmalS-MvM/eGov, Bruxelles, Belgium; NA; NA","IEEE Transactions on Software Engineering","","2005","31","12","999","1014","The problem of relating state-based intraagent (or intraobject) behavioral descriptions with scenario-based interagent (interobject) descriptions has recently focused much interest among the software engineering community. This paper compiles the results of our investigation of this problem. As interagent formalism, we adopt a simple variant of live sequence charts. For the intraagent perspective, we consider a game-theoretic foundation, looking at agents as ""strategies,"" which encompasses the popular ""state-based"" paradigm. Three classes of relationships between models are studied: scenario checking (called eLSC checking), synthesis, and verification. We set a formally defined theoretical stage that allows us to express these three problems very simply, to discuss their complexity, and to describe optimal solutions. Our study reveals the intrinsic high computational difficulty of these tasks. Consequently, many related problems and solutions are surveyed, some of which can be the basis for practical solutions. In this, we also offer a panorama of current research and directions for the future.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.137","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566603","Index Terms- Requirements engineering;life cycle;program verification.","Systems engineering and theory;Humans;Automotive engineering;Hardware;Computer Society;Software engineering;Concurrent computing;Distributed computing;Computer industry;Embedded software","program verification;systems analysis;software agents;flowcharting;finite state machines","live sequence charts;state machines;guided tour;state-based intraagent description;scenario-based interagent description;software engineering;game-theoretic foundation;scenario checking;eLSC checking;requirements engineering;life cycle;program verification","","27","","72","","","","","","IEEE","IEEE Journals & Magazines"
"A Message-Based Approach to Discrete-Event Simulation","R. L. Bagrodia; K. M. Chandy; J. Misra","Department of Computer Sciences, University of Texas at Austin; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","6","654","665","This paper develops a message-based approach to discrete-event simulation. Although message-based simulators have the same expressive power as traditional discrete-event simulation lanuages, they provide a more natural environment for simulating distributed systems. In message-based simulations, a physical system is modeled by a set of message-communicating processes. The events in the system are modeled by message-communications. The paper proposes the entity construct to represent a message-communicating process operating in simulated time. A general wait until construct is used for process scheduling and message-communication. Based on these two notions, the paper proposes a language fragment comprising a small set of primitives. The language fragment can be implemented in any general-purpose, sequential programming language to construct a message-based simulator. We give an example of a message-based simulation language, called MAY, developed by implementing the language fragment in Fortran. MAY is in the public domain and is available on request.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233203","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702271","Discrete-event simulation;distributed system;entity;message;message-based simulation","Discrete event simulation;Computer languages;Power system modeling;Computational modeling;Trademarks;Virtual manufacturing;Packaging machines","","Discrete-event simulation;distributed system;entity;message;message-based simulation","","35","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Translating SQL Into Relational Algebra: Optimization, Semantics, and Equivalence of SQL Queries","S. Ceri; G. Gottlob","Dipartimento di Elettronica, Politecnico di Milano; NA","IEEE Transactions on Software Engineering","","1985","SE-11","4","324","345","In this paper, we present a translator from a relevant subset of SQL into relational algebra. The translation is syntax-directed, with translation rules associated with grammar productions; each production corresponds to a particular type of SQL subquery.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232223","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702016","Program translation;query equivalence;query languages;query optimization;relational algebra;relational database model;SQL","Algebra;Database languages;Query processing;Production;Calculus;Database systems;Relational databases;Formal languages;Proposals","","Program translation;query equivalence;query languages;query optimization;relational algebra;relational database model;SQL","","11","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Integrated environments for formally well-founded design and simulation of concurrent systems","A. Giacalone; S. A. Smolka","Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA; Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA","IEEE Transactions on Software Engineering","","1988","14","6","787","802","An ongoing project concerned with the development of environments that support the specification and design of concurrent systems is reported. The project has two key aspects: an existing and working system, Clara, that supports Milner's CCS as a specification and design language; and the development of general techniques for computer-aided generation of Clara-like environments for other concurrent languages. The Clara environment is emphasized. It has two main components: support for the usage of formal techniques in the design process, and a rich and highly interactive simulation facility. A further distinguishing feature is the environment's graphical user interface which is based on a pictorial version of CCS. The semantics of CCS is defined nonprocedurally in two phases: an operational semantics given as a set of inference rules, and an algebraic semantics represented by a set of equational rules.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6158","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6158","","Carbon capture and storage;Concurrent computing;User interfaces;Computational modeling;Calculus;Application software;Computer science;Process design;Graphical user interfaces;Equations","computer graphics;parallel programming;programming environments;programming theory;simulation languages;specification languages;user interfaces","programming environments;parallel programming;specification language;concurrent systems;Clara;CCS;concurrent languages;formal techniques;interactive simulation;graphical user interface;operational semantics;inference rules;algebraic semantics;equational rules","","8","","51","","","","","","IEEE","IEEE Journals & Magazines"
"TACO: Efficient SAT-Based Bounded Verification Using Symmetry Breaking and Tight Bounds","J. P. Galeotti; N. Rosner; C. G. López Pombo; M. F. Frias","Universidad de Buenos Aires and CONICET, Argentina; Universidad de Buenos Aires, Argentina; Universidad de Buenos Aires and CONICET, Argentina; Instituto Tecnológico de Buenos Aires and CONICET, Argentina","IEEE Transactions on Software Engineering","","2013","39","9","1283","1307","SAT-based bounded verification of annotated code consists of translating the code together with the annotations to a propositional formula, and analyzing the formula for specification violations using a SAT-solver. If a violation is found, an execution trace exposing the failure is exhibited. Code involving linked data structures with intricate invariants is particularly hard to analyze using these techniques. In this paper, we present Translation of Annotated COde (TACO), a prototype tool which implements a novel, general, and fully automated technique for the SAT-based analysis of JML-annotated Java sequential programs dealing with complex linked data structures. We instrument code analysis with a symmetry-breaking predicate which, on one hand, reduces the size of the search space by ignoring certain classes of isomorphic models and, on the other hand, allows for the parallel, automated computation of tight bounds for Java fields. Experiments show that the translations to propositional formulas require significantly less propositional variables, leading to an improvement of the efficiency of the analysis of orders of magnitude, compared to the noninstrumented SAT--based analysis. We show that in some cases our tool can uncover bugs that cannot be detected by state-of-the-art tools based on SAT-solving, model checking, or SMT-solving.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6482141","Static analysis;SAT-based code analysis;Alloy;KodKod;DynAlloy","Metals;Java;Cost accounting;Instruments;Analytical models;Contracts;Context","computability;formal specification;formal verification;program diagnostics;program interpreters","TACO tool;translation-of-annotated code tool;SAT-based bounded verification;satisfiability;code translation;specification violation;JML-annotated Java sequential program;data structure;code analysis;symmetry-breaking predicate;isomorphic model;automated tight bound computation;model checking;SAT-solving;SMT-solving","","10","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Proof procedure and answer extraction in Petri net model of logic programs","G. Peterka; T. Murata","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA","IEEE Transactions on Software Engineering","","1989","15","2","209","217","A proof procedure and answer extraction in a high-level Petri net model of logic programs is discussed. The logic programs are restricted to the Horn clause subset of first-order predicate logic and finite problems. The logic program is modeled by a high-level Petri net and the execution of the logic program or the answer extraction process in predicate calculus corresponds to a firing sequence which fires the goal transition in the net. For the class of the programs described above, the goal transition is potentially firable if an only if there exists a nonnegative T-invariant which includes the goal transition in its support. This is the main result proved. Three examples are given to illustrate in detail the above results.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21746","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21746","","Logic programming;Fires;Petri nets;Calculus;Computer science;Artificial intelligence;Automatic logic units;Sufficient conditions;Terminology","logic programming;Petri nets;programming theory;theorem proving","answer extraction;Petri net model;logic programs;proof procedure;Horn clause subset;first-order predicate logic;firing sequence;goal transition","","61","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Does reviewer recommendation help developers?","V. Kovalenko; N. Tintarev; E. Pasynkov; C. Bird; A. Bacchelli","Software Technology, Technische Universiteit Delft, 2860 Delft, Zuid-Holland Netherlands (e-mail: v.v.kovalenko@tudelft.nl); Web Information Systems, Technische Universiteit Delft, 2860 Delft, Zuid-Holland Netherlands (e-mail: n.tintarev@tudelft.nl); Upsource team, JetBrains GmbH, Munich, Bayern Germany (e-mail: evgeny.pasynkov@jetbrains.com); Microsoft Research, Microsoft Corportation, Redmond, Washington United States 98052 (e-mail: cbird@microsoft.com); Department of Informatics, University of Zurich, Zürich, Zürich Switzerland (e-mail: bacchelli@ifi.uzh.ch)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Selecting reviewers for code changes is a critical step for an efficient code review process. Recent studies propose automated reviewer recommendation algorithms to support developers in this task. However, the evaluation of recommendation algorithms, when done apart from their target systems and users (i.e. code review tools and change authors), leaves out important aspects: perception of recommendations, influence of recommendations on human choices, and their effect on user experience. This study is the first to evaluate a reviewer recommender in vivo. We compare historical reviewers and recommendations for over 21,000 code reviews performed with a deployed recommender in a company environment and set to measure the influence of recommendations on users' choices, along with other performance metrics. Having found no evidence of influence, we turn to the users of the recommender. Through interviews and a survey we find that, though perceived as relevant, reviewer recommendations rarely provide additional value for the respondents. We confirm this finding with a larger study at another company. The confirmation of this finding brings up a case for more user-centric approaches to designing and evaluating the recommenders. Finally, we investigate information needs of developers during reviewer selection and discuss promising directions for the next generation of reviewer recommendation tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2868367","Schweizerischer Nationalfonds zur Forderung der Wissenschaftlichen Forschung; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8453850","Code Review;Reviewer Recommendation;Empirical Software Engineering","Tools;Recommender systems;Companies;Measurement;Software;In vivo;Software engineering","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Designing and prototyping data-intensive applications in the Logres and Algres programming environment","F. Cacace; S. Ceri; L. Tanca; S. Crespi-Reghizzi","Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy; Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy; Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy; Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy","IEEE Transactions on Software Engineering","","1992","18","6","534","546","The authors present an environment and a methodology for the design and rapid prototyping of data-intensive software applications, i.e., applications which perform substantial retrieval and update activity on persistent data. In the approach, the application is formally specified using Logres, a database language which combines object-oriented data modeling and rule-based programming. These specifications are translated into Algres, an extended relational algebra, thus yielding a rapid executable prototype. Algres programs embedded into a conventional programming language interface may be converted to conventional programs operating on a commercial relational system. This methodology helps automate the conversion from declarative requirements to imperative code, performing several tasks fully automatically and reducing the probability of human errors, while integrity constraints and application specifications are expressed in a declarative language, at a very high level of abstraction.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.142875","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=142875","","Prototypes;Application software;Software prototyping;Design methodology;Software performance;Information retrieval;Database languages;Object oriented modeling;Object oriented programming;Algebra","formal specification;programming environments;software prototyping","prototyping;data-intensive applications;Logres;Algres;programming environment;rapid prototyping;persistent data;database language;object-oriented data modeling;rule-based programming;relational algebra;declarative requirements;imperative code;human errors;integrity constraints;application specifications","","3","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental LL(1) parsing in language-based editors","J. J. Shilling","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Transactions on Software Engineering","","1993","19","9","935","940","This paper introduces an efficient incremental LL(1) parsing algorithm for use in language-based editors that use the structure recognition approach. It is designed to parse user input at intervals of very small granularity and to limit the amount of incremental parsing needed when changes are made internal to the editing buffer. The algorithm uses the editing focus as a guide in restricting parsing. It has been implemented in the Fred language-based editor.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.241775","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=241775","","Production;Programming;Feedback;Heuristic algorithms","grammars;program compilers;software tools;text editing","incremental LL(1) parsing;language-based editors;structure recognition approach;granularity;Fred language-based editor;software development environments","","1","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Software reliability trend analyses from theoretical to practical considerations","K. Kanoun; J. C. Laprie","Lab. d'Autom. et d'Anal. des Syst., CNRS, Toulouse, France; Lab. d'Autom. et d'Anal. des Syst., CNRS, Toulouse, France","IEEE Transactions on Software Engineering","","1994","20","9","740","747","This paper addresses the problem of reliability growth characterization and analysis. It is intended to show how reliability trend analyses can help the project manager in controlling the progress of the development activities and in appreciating the efficiency of the test programs. Reliability trend change may result from various reasons, some of them are desirable and expected (such as reliability growth due to fault removal) and some of them are undesirable (such as slowing down Of the testing effectiveness). Identification in time of the latter allows the project manager to take the appropriate decisions very quickly in order to avoid problems which may manifest later. The notions of reliability growth over a given interval and local reliability trend change are introduced through the subadditive property, allowing: better definition and understanding of the reliability growth phenomena; the already existing trend tests are then revisited using these concepts. Emphasis is put on the way trend tests can be used to help the management of the testing and validation process and on practical results that can be derived from their use; it is shown that, for several circumstances, trend analyses give information of prime importance to the developer.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.317434","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=317434","","Software reliability;Software testing;Project management;Software development management;Information analysis;Application software;Reliability engineering;Reliability theory;Software systems;Predictive models","software reliability;project management","software reliability trend analyses;software reliability growth characterization;reliability trend analyses;project manager;software testing;software validation","","21","","22","","","","","","IEEE","IEEE Journals & Magazines"
"An Approach to the Modeling of Software Testing with Some Applications","T. Downs","Department of Electrical Engineering, University of Queensland","IEEE Transactions on Software Engineering","","1985","SE-11","4","375","386","In this paper, an approach to the modeling of software testing is described. A major aim of this approach is to allow the assessment of the effects of different testing (and debugging) strategies in different situations. It is shown how the techniques developed can be used to estimate, prior to the commencement of testing, the optimum allocation of test effort for software which is to be nonuniformly executed in its operational phase. In addition, the question of application of statistical models in cases where the data environment undergoes changes is discussed. Finally, two models are presented for the assessment of the effects of imperfections in the debugging process.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232227","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702020","Computer performance modeling;reliability growth;software reliability;software testing;stochastic models","Software testing;Application software;Debugging;Software systems;Australia;Software reliability;Mathematical model;Software performance;Phase estimation;Telecommunications","","Computer performance modeling;reliability growth;software reliability;software testing;stochastic models","","12","","11","","","","","","IEEE","IEEE Journals & Magazines"
"An evaluation of relational join algorithms in a pipelined query processing environment","K. P. Mikkilineni; S. Y. W. Su","Honeywell Corp. Syst. Dev., Golden Valley, MN, USA; NA","IEEE Transactions on Software Engineering","","1988","14","6","838","848","A query processing strategy which is based on pipelining and data-flow techniques is presented. Timing equations are developed for calculating the performance of four join algorithms: nested block, hash, sort-merge, and pipelined sort-merge. They are used to execute the join operation in a query in distributed fashion and in pipelined fashion. Based on these equations and similar sets of equations developed for other relational algebraic operations, the performance of query execution was evaluated using the different join algorithms. The effects of varying the values of processing time, I/O time, communication time, buffer size, and join selectively on the performance of the pipelined join algorithms are investigated. The results are compared to the results obtained by employing the same algorithms for executing queries using the distributed processing approach which does not exploit the vertical concurrency of the pipelining approach. These results establish the benefits of pipelining.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6162","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6162","","Query processing;Pipeline processing;Relational databases;Timing;Differential algebraic equations;Distributed processing;Concurrent computing;Performance analysis;Algorithm design and analysis;Data models","database theory;distributed databases;merging;performance evaluation;pipeline processing;relational databases;sorting","relational databases;distributed databases;timing equations;relational join algorithms;pipelined query processing environment;nested block;hash;sort-merge;pipelined sort-merge;query execution;distributed processing","","11","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing high-change modules and modules with the highest measurement values in two large-scale open-source products","A. G. Koru; J. Tian","Dept. of Inf. Syst., Maryland Univ., Baltimore, MD, USA; NA","IEEE Transactions on Software Engineering","","2005","31","8","625","642","Identifying change-prone modules can enable software developers to take focused preventive actions that can reduce maintenance costs and improve quality. Some researchers observed a correlation between change proneness and structural measures, such as size, coupling, cohesion, and inheritance measures. However, the modules with the highest measurement values were not found to be the most troublesome modules by some of our colleagues in industry, which was confirmed by our previous study of six large-scale industrial products. To obtain additional evidence, we identified and compared high-change modules and modules with the highest measurement values in two large-scale open-source products, Mozilla and OpenOffice, and we characterized the relationship between them. Contrary to common intuition, we found through formal hypothesis testing that the top modules in change-count rankings and the modules with the highest measurement values were different. In addition, we observed that high-change modules had fairly high places in measurement rankings, but not the highest places. The accumulated findings from these two open-source products, together with our previous similar findings for six closed-source products, should provide practitioners with additional guidance in identifying the change-prone modules.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498769","Index Terms- Product metrics;maintainability;maintenance planning;open-source software.","Large-scale systems;Open source software;Size measurement;Software measurement;Software maintenance;Costs;Industrial relations;Testing;Software quality;Inspection","public domain software;software maintenance;software metrics;software quality;software cost estimation","large-scale open-source products;software developers;software maintenance cost reduction;large-scale industrial products;Mozilla software product;OpenOffice software product;formal hypothesis testing;change-prone modules;product metrics;software maintenance planning;open-source software","","38","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Discipline Matters: Refactoring of Preprocessor Directives in the<monospace>#ifdef</monospace>Hell","F. Medeiros; M. Ribeiro; R. Gheyi; S. Apel; C. Kästner; B. Ferreira; L. Carvalho; B. Fonseca","Federal Institute of Alagoas, AL, Brazil; Computing Institute, Federal University of Alagoas, Maceió, AL, Brazil; Department of Computing and Systems, Federal University of Campina Grande, PB, Brazil; Department of Informatics and Mathematics, University of Passau, Passau, Germany; Institute for Software Research, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA; Computing Institute, Federal University of Alagoas, Maceió, AL, Brazil; Computing Institute, Federal University of Alagoas, Maceió, AL, Brazil; Computing Institute, Federal University of Alagoas, Maceió, AL, Brazil","IEEE Transactions on Software Engineering","","2018","44","5","453","469","The C preprocessor is used in many C projects to support variability and portability. However, researchers and practitioners criticize the C preprocessor because of its negative effect on code understanding and maintainability and its error proneness. More importantly, the use of the preprocessor hinders the development of tool support that is standard in other languages, such as automated refactoring. Developers aggravate these problems when using the preprocessor in undisciplined ways (e.g., conditional blocks that do not align with the syntactic structure of the code). In this article, we proposed a catalogue of refactorings and we evaluated the number of application possibilities of the refactorings in practice, the opinion of developers about the usefulness of the refactorings, and whether the refactorings preserve behavior. Overall, we found 5,670 application possibilities for the refactorings in 63 real-world C projects. In addition, we performed an online survey among 246 developers, and we submitted 28 patches to convert undisciplined directives into disciplined ones. According to our results, 63 percent of developers prefer to use the refactored (i.e., disciplined) version of the code instead of the original code with undisciplined preprocessor usage. To verify that the refactorings are indeed behavior preserving, we applied them to more than 36 thousand programs generated automatically using a model of a subset of the C language, running the same test cases in the original and refactored programs. Furthermore, we applied the refactorings to three real-world projects: BusyBox, OpenSSL, and SQLite. This way, we detected and fixed a few behavioral changes, 62 percent caused by unspecified behavior in the C programming language.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2688333","CNPq; FAPEAL PPGs; CAPES; DEVASSES; European Union's Seventh Framework Programme for research, technological development and demonstration; NSF; Science of Security Lablet; AFRL; DARPA; German Research Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7888579","Configurable systems;preprocessors;and refactoring","Syntactics;C languages;Guidelines;Linux;Kernel;Standards","C language;program compilers;program diagnostics;program processors;public domain software;software maintenance","refactored programs;preprocessor directives;C preprocessor;automated refactoring;preprocessor usage;BusyBox;OpenSSL;SQLite;C programming language","","1","","50","","","","","","IEEE","IEEE Journals & Magazines"
"A framework for expressing the relationships between multiple views in requirements specification","B. Nuseibeh; J. Kramer; A. Finkelstein","Dept. of Comput., Imperial Coll. of Sci., Technol. & Med., London, UK; Dept. of Comput., Imperial Coll. of Sci., Technol. & Med., London, UK; Dept. of Comput., Imperial Coll. of Sci., Technol. & Med., London, UK","IEEE Transactions on Software Engineering","","1994","20","10","760","773","Composite systems are generally comprised of heterogeneous components whose specifications are developed by many development participants. The requirements of such systems are invariably elicited from multiple perspectives that overlap, complement, and contradict each other. Furthermore, these requirements are generally developed and specified using multiple methods and notations, respectively. It is therefore necessary to express and check the relationships between the resultant specification fragments. We deploy multiple ViewPoints that hold partial requirements specifications, described and developed using different representation schemes and development strategies. We discuss the notion of inter-ViewPoint communication in the context of this ViewPoints framework, and propose a general model for ViewPoint interaction and integration. We elaborate on some of the requirements for expressing and enacting inter-ViewPoint relationships-the vehicles for consistency checking and inconsistency management. Finally, though we use simple fragments of the requirements specification method CORE to illustrate various components of our work, we also outline a number of larger case studies that we have used to validate our framework. Our computer-based ViewPoints support environment, The Viewer, is also briefly described.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.328995","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=328995","","Vehicles;Context modeling;Programming;Diffusion tensor imaging;Automotive engineering;Interconnected systems;Software engineering;Information analysis;Information processing","formal specification","multiple views;requirements specification;heterogeneous components;multiple ViewPoints;partial requirements specifications;inter-ViewPoint communication;ViewPoints framework;consistency checking;inconsistency management;requirements specification method;CORE;computer-based ViewPoints support;The Viewer","","205","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic inclusion of middleware performance attributes into architectural UML software models","T. Verdickt; B. Dhoedt; F. Gielen; P. Demeester","Dept. of Inf. Technol., Ghent Univ., Belgium; Dept. of Inf. Technol., Ghent Univ., Belgium; Dept. of Inf. Technol., Ghent Univ., Belgium; Dept. of Inf. Technol., Ghent Univ., Belgium","IEEE Transactions on Software Engineering","","2005","31","8","695","711","Distributed systems often use a form of communication middleware to cope with different forms of heterogeneity, including geographical spreading of the components, different programming languages and platform architectures, etc. The middleware, of course, impact the architecture and the performance of the system. This paper presents a model transformation framework to automatically include the architectural impact and the overhead incurred by using a middleware layer between several system components. Using this framework, architects can model the system in a middleware-independent fashion. Accurate, middleware-aware models can then be obtained automatically using a middleware model repository. The actual transformation algorithm is presented in more detail. The resulting models can be used to obtain performance models of the system. From those performance models, early indications of the system performance can be extracted.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498773","Index Terms- Distributed software engineering tools and techniques;performance of systems: modeling techniques.","Middleware;Unified modeling language;Software performance;System performance;Software engineering;Computer architecture;Design engineering;Communication system software;Computer languages;Hardware","middleware;Unified Modeling Language;software architecture;object-oriented programming;software performance evaluation;distributed object management","middleware performance attributes;architectural UML software model;distributed systems;communication middleware;geographical spreading components;programming languages;model transformation framework;middleware model repository;system performance;distributed software engineering tools;middleware-aware model","","20","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Watermarking, tamper-proofing, and obfuscation - tools for software protection","C. S. Collberg; C. Thomborson","Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA; NA","IEEE Transactions on Software Engineering","","2002","28","8","735","746","We identify three types of attack on the intellectual property contained in software and three corresponding technical defenses. A defense against reverse engineering is obfuscation, a process that renders software unintelligible but still functional. A defense against software piracy is watermarking, a process that makes it possible to determine the origin of software. A defense against tampering is tamper-proofing, so that unauthorized modifications to software (for example, to remove a watermark) will result in nonfunctional code. We briefly survey the available technology for each type of defense.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027797","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027797","","Watermarking;Software protection;Computer crime;Data security;Intellectual property;Computer security;Java;Manufacturing;Computer Society;Reverse engineering","industrial property;reverse engineering;computer crime;copy protection;cryptography","watermarking;tamper proofing;obfuscation tools;software protection;intellectual property;reverse engineering;software piracy","","273","","98","","","","","","IEEE","IEEE Journals & Magazines"
"The role of opportunism in the software design reuse process","A. Sen","Dept. of Bus. Anal. & Res., Texas A&M Univ., College Station, TX, USA","IEEE Transactions on Software Engineering","","1997","23","7","418","436","Software design involves translating a set of task requirements into a structured description of a computer program that will perform the task. A software designer can use design schema, collaborative design knowledge, or can reuse design artifacts. Very little has been done to include reuse of design artifacts in the software development life cycle, despite tremendous promises of reuse. As a result, this technique has not seen widespread use, possibly due to a lack of cognitive understanding of the reuse process. This research explores the role of a specific cognitive aspect, opportunism, in demand-side software reuse. We propose a cognitive model based on opportunism that describes the software design process with reuse. Protocol analysis verifies that the software design with reuse is indeed opportunistic and reveals that some software designers employ certain tasks of the reuse process frequently. Based on these findings, we propose a reuse support system that incorporates blackboard technology and existing reuse library management system.","0098-5589;1939-3520;2326-3881","","10.1109/32.605760","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=605760","","Software design;Software reusability;Software tools;Collaborative software;Programming;Software libraries;Investments;Humans;Computer aided software engineering;Process design","software reusability;software libraries;software tools;blackboard architecture","opportunism;software design reuse process;task requirements;structured description;design schema;collaborative design knowledge;software development life cycle;cognitive model;demand-side software reuse;protocol analysis;blackboard technology;reuse library management system","","27","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Gray Computing: A Framework for Computing with Background JavaScript Tasks","Y. Pan; J. White; Y. Sun; J. Gray","Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN; Department of Computer Science, California State Polytechnic University, Pomona, CA; Department of Computer Science, University of Alabama, Tuscaloosa, AL","IEEE Transactions on Software Engineering","","2019","45","2","171","193","Website visitors are performing increasingly complex computational work on the websites’ behalf, such as validating forms, rendering animations, and producing data visualizations. In this article, we explore the possibility of increasing the work offloaded to web visitors’ browsers. The idle computing cycles of web visitors can be turned into a large-scale distributed data processing engine, which we term gray computing. Past research has looked primarily at either volunteer computing with specialized clients or browser-based volunteer computing where the visitors keep their browsers open to a single web page for a long period of time. This article provides a comprehensive analysis of the architecture, performance, security, cost effectiveness, user experience, and other issues of gray computing distributed data processing engines with heterogeneous computing power, non-uniform page view times, and high computing pool volatility. Several real-world applications are examined and gray computing is shown to be cost effective for a number of complex tasks ranging from computer vision to bioinformatics to cryptology.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2772812","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8105894","Software economics;JavaScript;web browser;cloud computing","Distributed processing;Browsers;Data processing;Web pages;Cloud computing","","","","","","96","","","","","","IEEE","IEEE Journals & Magazines"
"Semiautomatic implementation of protocols using an Estelle-C compiler","S. T. Vuong; A. C. Lau; R. I. Chan","Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada; Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada; Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada","IEEE Transactions on Software Engineering","","1988","14","3","384","393","The basic ideas underlying an Estelle-C compiler, which accepts an Estelle protocol specification and produces a protocol implementation in C, are presented. The implementation of the ISO (International Organization for Standardization) class-2 transparent protocol, using the semiautomatic approach, is discussed. A manual implementation of the protocol is performed and compared to the semiautomatic implementation. The semiautomatic approach to protocol implementation offers several advantages over the conventional manual one, including correctness and modularity in protocol implementation code, conformance to the specification, and reduction in implementation time. Finally, ongoing development of a new Estelle-C compiler is presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4658","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4658","","Transport protocols;Testing;Trademarks;Research and development;Standards development;Code standards;Workstations;Sun;Ethernet networks","program compilers;protocols","protocols;Estelle-C compiler;specification;ISO;class-2 transparent protocol;modularity","","33","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Runtime analysis of atomicity for multithreaded programs","L. Wang; S. D. Stoller","Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA; Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA","IEEE Transactions on Software Engineering","","2006","32","2","93","110","Atomicity is a correctness condition for concurrent systems. Informally, atomicity is the property that every concurrent execution of a set of transactions is equivalent to some serial execution of the same transactions. In multithreaded programs, executions of procedures (or methods) can be regarded as transactions. Correctness in the presence of concurrency typically requires atomicity of these transactions. Tools that automatically detect atomicity violations can uncover subtle errors that are hard to find with traditional debugging and testing techniques. This paper describes two algorithms for runtime detection of atomicity violations and compares their cost and effectiveness. The reduction-based algorithm checks atomicity based on commutativity properties of events in a trace; the block-based algorithm efficiently represents the relevant information about a trace as a set of blocks (i.e., pairs of events plus associated synchronizations) and checks atomicity by comparing each block with other blocks. To improve the efficiency and accuracy of both algorithms, we incorporate a multilockset algorithm for checking data races, dynamic escape analysis, and happen-before analysis. Experiments show that both algorithms are effective in finding atomicity violations. The block-based algorithm is more accurate but more expensive than the reduction-based algorithm.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599419","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599419","Concurrent programming;testing and debugging;Java;data race;atomicity.","Runtime;System recovery;Concurrent computing;Debugging;Algorithm design and analysis;Automatic testing;Costs;Java;Operating systems","multi-threading;program diagnostics;program testing;multiprocessing systems","runtime analysis;multithreaded programs;concurrent systems;atomicity violations;reduction-based algorithm;block-based algorithm;multilockset algorithm;data race checking;dynamic escape analysis;happen-before analysis","","63","","32","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient implementation of static string pattern matching machines","J. -. Aoe","Dept. of Inf. Sci. & Syst. Eng., Tokushima Univ., Japan","IEEE Transactions on Software Engineering","","1989","15","8","1010","1016","A technique for implementing a static transition table of a string pattern matching machine which locates all occurrences of a finite number of keywords in a string is described. The approach is based on S.C. Johnson's (1975) storage and retrieval method of the transition table of a finite-state machine. By restricting the transition table of the finite-state machine to that of the string pattern-matching machine, triple arrays of Johnsons's data structure can be reduced to two arrays. The retrieval program of the reduced data structure can be speeded up by a finite straight program without loops.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31357","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31357","","Pattern matching;Scanning probe microscopy;Data structures;Automata;Information retrieval;Libraries;Filtering;Frequency;Information analysis;Pattern analysis","data structures;information retrieval;subroutines","implementation technique;keyword location;static string pattern matching machines;transition table;finite-state machine;triple arrays;Johnsons's data structure;retrieval program;reduced data structure;finite straight program","","16","","26","","","","","","IEEE","IEEE Journals & Magazines"
"The Mobius framework and its implementation","D. D. Deavours; G. Clark; T. Courtney; D. Daly; S. Derisavi; J. M. Doyle; W. H. Sanders; P. G. Webster","Inf. & Telecommun. Technol. Center, Kansas Univ., Lawrence, KS, USA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","10","956","969","The Mobius framework is an environment for supporting multiple modeling formalisms and solution techniques. Models expressed in formalisms that are compatible with the framework are translated into equivalent models using Mobius framework components. This translation preserves the structure of the models, allowing efficient solutions. The framework is implemented in the tool by a well-defined abstract functional interface. Models and solution techniques interact with one another through the use of the standard interface, allowing them to interact with Mobius framework components, not formalism components. This permits novel combinations of modeling techniques, and will be a catalyst for new research in modeling techniques. This paper describes our approach, focusing on the ""atomic model"". We describe the formal description of the Mobius components as well as their implementations in our software tool.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041052","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041052","","Software tools;Stochastic systems;Petri nets;Stochastic processes;Computer networks;Process design;Computer network reliability;Telecommunication network reliability;Availability;System performance","digital simulation;Petri nets;Markov processes;software tools;formal specification","stochastic Petri nets;Mobius framework;multiple modeling formalisms;equivalent models;abstract functional interface;formal description;software tool;PEPA;execution policy;modeling tools;Markov models","","143","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Verification of concurrent control flow in distributed computer systems","S. S. Yau; W. Hong","Dept. of Electr. Eng. & Comput. Sci., Northwestern Univ., Evanston, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Northwestern Univ., Evanston, IL, USA","IEEE Transactions on Software Engineering","","1988","14","4","405","417","An approach to verifying control flow in distributed computer systems (DCS) is presented. The approach is based on control flow checking among software components distributed over processors and cooperating among them. In this approach, control-flow behavior of DCS software is modeled and contained in special software components called verifiers. The verifiers are distributed over the processors and consulted to check the correctness of the control flow in DCS software during its execution. Algorithms for deriving the verifiers are presented. This technique can detect global errors including synchronization errors as well as local errors. It can be used for sequential or concurrent software at various levels of details. Experiments show that using this technique requires no significant overhead.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4662","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4662","","Distributed control;Control systems;Concurrent computing;Distributed computing;Computer errors;Software maintenance;Error correction;Fault tolerance;Software testing;Sequential analysis","distributed processing;program verification;programming theory","distributed software;program verification;program correctness;concurrent control flow;control flow checking;software components;global errors;synchronization errors;local errors;concurrent software","","3","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Exercises in Software Design","J. L. Bentley; J. A. Dallen","AT&amp;T Bell Laboratories; NA","IEEE Transactions on Software Engineering","","1987","SE-13","11","1164","1169","Typical software engineering courses teach principles in lectures and readings, then apply them in the development of a single program (requiring several months). We recently taught a software engineering class that incorporated many smaller exercises (requiring several hours). The class was successful: students were able to experiment with a broad set of ideas, and make interesting mistakes without jeopardizing the grades of their development team. This paper describes some tools and techniques we taught, and suggests how they might be incorporated into typical software engineering classes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232865","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702163","Awk language;engineering design;prototyping;software engineering education","Software design;Software engineering;Computer science;Military computing;Pipelines;Design engineering;Software prototyping;Computer science education;Documentation;Geography","","Awk language;engineering design;prototyping;software engineering education","","1","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Estimation and prediction metrics for adaptive maintenance effort of object-oriented systems","F. Fioravanti; P. Nesi","Dept. of Syst. & Inf., Florence Univ., Italy; NA","IEEE Transactions on Software Engineering","","2001","27","12","1062","1084","Many software systems built in recent years have been developed using object-oriented technology and, in some cases, they already need adaptive maintenance in order to satisfy market and customer needs. In most cases, the estimation and prediction of maintenance effort is performed with difficulty due to the lack of metrics and suitable models. In this paper, a model and metrics for estimation/prediction of adaptive maintenance effort are presented and compared with some other solutions taken from the literature. The model proposed can be used as a general approach for adopting well-known metrics (typically used for the estimation of development effort) for the estimation/prediction of adaptive maintenance effort. The model and metrics proposed have been validated against real data by using multilinear regression analysis. The validation has shown that several well-known metrics can be profitably employed for the estimation/prediction of maintenance effort.","0098-5589;1939-3520;2326-3881","","10.1109/32.988708","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988708","","Costs;Object oriented modeling;Predictive models;Control systems;Software performance;Performance evaluation;Software systems;Regression analysis;Software maintenance;Tree data structures","statistical analysis;software maintenance;software metrics;object-oriented methods","software systems;object-oriented systems;prediction metrics;estimation metrics;adaptive maintenance effort;multilinear regression analysis","","58","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Knowledge-based programming: A survey of program design and construction techniques","A. T. Goldberg","Kestrel Institute, Palo Alto, CA 94304; Department of Computer and Information Sciences University of California, Santa Cruz, CA 95064","IEEE Transactions on Software Engineering","","1986","SE-12","7","752","768","An application of artificial intelligence (AI) to the development of software is presented for the construction of efficient implementations of programs from formal high-level specifications. Central to this discussion is the notion of program development by means of program transformation. Using this methodology, a formal specification is compiled (either manually or automatically) into an efficient implementation by the repeated application of correctness-preserving, source-to-source transformations. The author considers techniques for data structure selection, the procedural representation of logic assertions, store-versus-compute, finite differencing, loop fusion, and algorithm design methods presented from the point of view of algorithm design and high-level program optimization.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312977","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312977","Knowledge-based software development;program optimization;program synthesis;program transformation;very-high-level languages","Knowledge based systems;Software;Programming;Data structures;Optimization;Semantics","artificial intelligence;programming;programming theory","program design;artificial intelligence;formal high-level specifications;program development;program transformation;correctness-preserving;source-to-source transformations;data structure selection;procedural representation;logic assertions;finite differencing;loop fusion;algorithm design","","11","","","","","","","","IEEE","IEEE Journals & Magazines"
"On Fault Representativeness of Software Fault Injection","R. Natella; D. Cotroneo; J. A. Duraes; H. S. Madeira","Federico II University of Naples, Naples; Federico II University of Naples, Naples; Rua Pedro Nunes-Quinta da Nora, Coimbra; University of Coimbra, Polo II-Pinhal de Marrocos, Coimbra","IEEE Transactions on Software Engineering","","2013","39","1","80","96","The injection of software faults in software components to assess the impact of these faults on other components or on the system as a whole, allowing the evaluation of fault tolerance, is relatively new compared to decades of research on hardware fault injection. This paper presents an extensive experimental study (more than 3.8 million individual experiments in three real systems) to evaluate the representativeness of faults injected by a state-of-the-art approach (G-SWFIT). Results show that a significant share (up to 72 percent) of injected faults cannot be considered representative of residual software faults as they are consistently detected by regression tests, and that the representativeness of injected faults is affected by the fault location within the system, resulting in different distributions of representative/nonrepresentative faults across files and functions. Therefore, we propose a new approach to refine the faultload by removing faults that are not representative of residual software faults. This filtering is essential to assure meaningful results and to reduce the cost (in terms of number of faults) of software fault injection campaigns in complex software. The proposed approach is based on classification algorithms, is fully automatic, and can be used for improving fault representativeness of existing software fault injection approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.124","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6122035","Software fault injection;experimental dependability evaluation;software reliability;fault-tolerant systems","Software;Testing;Fault tolerance;Fault tolerant systems;Hardware;Fault location;Emulation","software fault tolerance","fault representativeness;software components;software fault injection approaches;fault tolerance;hardware fault injection;G-SWFIT;regression tests;fault location;nonrepresentative faults;classification algorithms","","52","","62","","","","","","IEEE","IEEE Journals & Magazines"
"A retrospective on the VAX VMM security kernel","P. A. Karger; M. E. Zurko; D. W. Bonin; A. H. Mason; C. E. Kahn","Open Software Found., Cambridge, MA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1991","17","11","1147","1165","The development of a virtual-machine monitor (VMM) security kernel for the VAX architecture is described. The focus is on how the system's hardware, microcode, and software are aimed at meeting A1-level security requirements while maintaining the standard interfaces and applications of the VMS and ULTRIX-32 operating systems. The VAX security kernel supports multiple concurrent virtual machines on a single VAX system, providing isolation and controlled sharing of sensitive data. Rigorous engineering standards were applied during development to comply with the assurance requirements for verification and configuration management. The VAX security kernel has been developed with a heavy emphasis on performance and system management tools. The kernel performs sufficiently well that much of its development was carried out in virtual machines running on the kernel itself, rather than in a conventional time-sharing system.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.106971","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=106971","","Kernel;Data security;Virtual machining;Computer architecture;Hardware;Software maintenance;Software standards;Application software;Voice mail;Operating systems","DEC computers;security of data;supervisory programs;virtual machines","VAX VMM;security kernel;virtual-machine monitor;microcode;A1-level security requirements;standard interfaces;ULTRIX-32 operating systems;multiple concurrent virtual machines;isolation;controlled sharing;sensitive data;configuration management;system management tools","","56","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling and evaluating design alternatives for an on-line instrumentation system: a case study","A. Waheed; D. T. Rover; J. K. Hollingsworth","MRJ Technol. Solutions, NASA Ames Res. Center, Moffett Field, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","6","451","470","This paper demonstrates the use of a model-based evaluation approach for instrumentation systems (ISs). The overall objective of this study is to provide early feedback to tool developers regarding IS overhead and performance; such feedback helps developers make appropriate design decisions about alternative system configurations and task scheduling policies. We consider three types of system architectures: network of workstations (NOW), symmetric multiprocessors (SMP), and massively parallel processing (MPP) systems. We develop a Resource OCCupancy (ROCC) model for an on-line IS for an existing tool and parameterize it for an IBM SP-2 platform. This model is simulated to answer several ""what if"" questions regarding two policies to schedule instrumentation data forwarding: collect-and-forward (CF) and batch-and-forward (BF). In addition, this study investigates two alternatives for forwarding the instrumentation data: direct and binary tree forwarding for an MPP system. Simulation results indicate that the BF policy can significantly reduce the overhead and that the tree forwarding configuration exhibits desirable scalability characteristics for MPP systems. Initial measurement-based testing results indicate more than 60 percent reduction in the direct IS overhead when the BF policy was added to Paradyn parallel performance measurement tool.","0098-5589;1939-3520;2326-3881","","10.1109/32.689402","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=689402","","Instruments;Computer aided software engineering;Feedback;Application software;Monitoring;Real time systems;System testing;Space technology;Workstations;Parallel processing","parallel processing;multiprocessing programs;system monitoring;software tools;software metrics","design alternatives;on-line instrumentation system;model-based evaluation approach;tool developers;design decisions;alternative system configurations;task scheduling policies;system architectures;symmetric multiprocessors;massively parallel processing;IBM SP-2 platform;collect-and-forward;batch-and-forward;tree forwarding configuration;scalability characteristics;Paradyn parallel performance measurement tool","","2","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Scalable and Effective Test Generation for Role-Based Access Control Systems","A. Masood; R. Bhatti; A. Ghafoor; A. P. Mathur","Air University, Islamabad; Oracle, Redwood Shores; Purdue University, West Lafayette; Purdue University, West Lafayette","IEEE Transactions on Software Engineering","","2009","35","5","654","668","Conformance testing procedures for generating tests from the finite state model representation of Role-Based Access Control (RBAC) policies are proposed and evaluated. A test suite generated using one of these procedures has excellent fault detection ability but is astronomically large. Two approaches to reduce the size of the generated test suite were investigated. One is based on a set of six heuristics and the other directly generates a test suite from the finite state model using random selection of paths in the policy model. Empirical studies revealed that the second approach to test suite generation, combined with one or more heuristics, is most effective in the detection of both first-order mutation and malicious faults and generates a significantly smaller test suite than the one generated directly from the finite state models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4967616","Role-Based Access Control (RBAC);finite state models;fault model;first-order mutants;malicious faults.","System testing;Access control;Fault detection;Application software;Genetic mutations;Permission;Aerospace electronics;Computer Society;Authentication;Operating systems","authorisation;fault tolerance;finite state machines","for role-based access control system;conformance testing;finite state model;malicious fault detection;first-order mutant","","19","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Interprocedural def-use associations for C systems with single level pointers","H. D. Pande; W. A. Landi; B. G. Ryder","Siemens Corp. Res. Inc., Princeton, NJ, USA; Siemens Corp. Res. Inc., Princeton, NJ, USA; NA","IEEE Transactions on Software Engineering","","1994","20","5","385","403","Def-use analysis links possible value-setting statements for a variable (i.e. definitions) to potential value-fetches (i.e. uses) of that value. This paper describes the first algorithm that calculates accurate interprocedural def-use associations in C software systems. Our algorithm accounts for program-point-specific pointer-induced aliases, though it is currently limited to programs using a single level of indirection. We prove the NP-hardness of the interprocedural reaching definitions problem and describe the approximations made by our polynomial-time algorithm. Initial empirical results are also presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.286418","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=286418","","Performance analysis;Polynomials;Algorithm design and analysis;Arithmetic;Software tools;Debugging;System testing;Information analysis;Software systems;Computer science","computational complexity;C language;program compilers;program diagnostics;software engineering;data structures","interprocedural def-use associations;C software systems;single level pointers;value-setting statements;value-fetches;program-point-specific pointer-induced aliases;indirection;NP-hardness;interprocedural reaching definitions problem;polynomial-time algorithm;static analysis","","37","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Algorithms for constructing minimal deduction graphs","C. -. Yang; J. J. -. Chen; H. L. Chau","Dept. of Comput. Sci., Univ. of North Texas, Denton, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","6","760","770","Two algorithms for constructing minimal deduction graphs (MDG) for inferring rules and facts in an extended version of the Horn clause logic are described. A deduction graph (DG) is minimal if the number of arcs in the graph is minimized. Horn clauses (HC) are extended to Horn formulas (HF), such that the head or the body of an HF can be a conjunction of positive literals or a disjunction of the bodies of some rule instances, respectively. Each algorithm constructs an MDG from its source to its sink, whose arcs infer the HF 'if source then sink'. The construction of an MDG is based on a sound and complete set of inference rules of reflexivity, transitivity, and conjunction for HFs which proceeds by expanding a tree rooted at its sink until its source has a successful backtracking to the root. Then the MDG is extracted from the tree. The nodes being expanded in such a tree are classified into seven types, which are assigned by different priorities for their growing into subtrees or for their pruning to reduce the tree space.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24729","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24729","","Hafnium;Tree graphs;Expert systems;Relational databases;Logic;Engines;Chaos;Inference algorithms;Classification tree analysis;Knowledge based systems","expert systems;graphs;inference mechanisms;logic programming","MDG construction algorithms;Hern clauses;minimal deduction graphs;Horn clause logic;DG;arcs;Horn formulas;HF;positive literals;rule instances;sink;inference rules;reflexivity;transitivity;backtracking;subtrees;tree space","","8","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A State-of-the-Practice Survey of Risk Management in Development with Off-the-Shelf Software Components","J. Li; R. Conradi; O. P. Slyngstad; M. Torchiano; M. Morisio; C. Bunse","Norwegian University of Science and Technology; Norwegian University of Science and Technology; Norwegian University of Science and Technology; Politecnico di Torino; Politecnico di Torino, Italy; the International University in Germany","IEEE Transactions on Software Engineering","","2008","34","2","271","286","An international survey on risk management in software development with off-the-shelf (OTS) components is reported upon and discussed. The survey investigated actual risk-management activities and their correlations with the occurrences of typical risks in OTS component-based development. Data from 133 software projects in Norway, Italy, and Germany were collected using a stratified random sample of IT companies. The results show that OTS components normally do not contribute negatively to the quality of the software system as a whole, as is commonly expected. However, issues such as the underestimation of integration effort and inefficient debugging remain problematic and require further investigation. The results also illustrate several promising effective risk-reduction activities, e.g., putting more effort into learning relevant OTS components, integrating unfamiliar components first, thoroughly evaluating the quality of candidate OTS components, and regularly monitoring the support capability of OTS providers. Five hypotheses are proposed regarding these risk-reduction activities. The results also indicate that several other factors, such as project, cultural, and human-social factors, have to be investigated to thoroughly deal with the possible risks of OTS-based projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4459339","Software Engineering/Reusable Software;Software Engineering/Management;Software Engineering/Software Engineering Process;Software Engineering/Reusable Software;Software Engineering/Management;Software Engineering/Software Engineering Process","Risk management;Software engineering;Computer Society;Software systems;Programming;Computer industry;Investments;Open source software;Project management;Debugging","project management;risk management;software development management;software packages","risk management;off-the-shelf software component;software development;component-based development;software project;software system quality;debugging;risk-reduction activity","","38","","56","","","","","","IEEE","IEEE Journals & Magazines"
"NON-VON's performance on certain database benchmarks","B. K. Hillyer; D. E. Shaw; A. Nigam","Department of Computer Science, Columbia University, New York, NY 10027; Department of Computer Science, Columbia University, New York, NY 10027; IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598","IEEE Transactions on Software Engineering","","1986","SE-12","4","577","583","In a paper by P.B. Hawthorn and D.J. DeWitt (1982) the projected performance of several proposed database machines was examined for three relational database queries. The present authors investigate the performance of a massively parallel machine called NON-VON for the same queries under comparable assumptions. In the case of simple queries, a NON-VON machine of comparable size to those considered by Hawthorn and DeWitt is found to be somewhat faster than the fastest machines examined in their study; for a more complex database operation, NON-VON is shown to be five to ten times faster than the fastest of these machines.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312905","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312905","Associative processor;computer architecture;database machine;database management;parallel processor;performance analysis;SIMD machine;tree machine","Magnetic heads;Process control;Clocks;Computer architecture;Random access memory;Loading;Disk drives","database management systems;parallel processing;performance evaluation;relational databases","performance;database machines;relational database queries;parallel machine;NON-VON","","7","","","","","","","","IEEE","IEEE Journals & Magazines"
"Atomic Remote Procedure Call","Kwei-Jay Lin; J. D. Gannon","Department of Computer Science, University of Maryland; NA","IEEE Transactions on Software Engineering","","1985","SE-11","10","1126","1135","Remote procedure call (RPC) is a programming primitive that makes building distributed programs easier. Atomicity, whkh implies totality and serializability, has been recognized as an important property to assure consistency in spite of computing node crashes. We have implemented an atomk remote procedure call mechanism which provides users a simple and reliable language primitive. Concurrency is controlled by attaching a call graph path identifier to each message representing a procedure call. Procedures keep their last accepted calling message paths to compare against incoming message paths. Only calls that can be serialized are accepted. Associated states of static variables are saved in backup processors on procedure entry and restored to corresponding variables in case of procedure crash. Detailed concurrency control and recovery algorithms are given, and illustrated with examples.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231860","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701928","Atomic action;concurrency;distributed system;reliability;remote procedure call","Computer crashes;Communication channels;Concurrent computing;Concurrency control;Computer languages;Joining processes;Telecommunication network reliability;Intelligent networks;Distributed computing;Resource management","","Atomic action;concurrency;distributed system;reliability;remote procedure call","","6","","17","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical evaluation of weak mutation","A. J. Offutt; S. D. Lee","Dept. of Inf. & Software Syst. Eng., George Mason Univ., Fairfax, VA, USA; NA","IEEE Transactions on Software Engineering","","1994","20","5","337","344","Mutation testing is a fault-based technique for unit-level software testing. Weak mutation was proposed as a way to reduce the expense of mutation testing. Unfortunately, weak mutation is also expected to provide a weaker test of the software than mutation testing does. This paper presents results from an implementation of weak mutation, which we used to evaluate the effectiveness versus the efficiency of weak mutation. Additionally, we examined several options in an attempt to find the most appropriate way to implement weak mutation. Our results indicate that weak mutation can be applied in a manner that is almost as effective as mutation testing, and with significant computational savings.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.286422","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=286422","","Genetic mutations;Software testing;Programming profession;Software systems;Algorithms;Automatic testing;System testing;Computer science;Systems engineering and theory;Software engineering","program testing;software engineering","weak mutation;mutation testing;fault-based technique;unit-level software testing;effectiveness;efficiency;computational savings","","84","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Approximate throughput analysis of cyclic queueing networks with finite buffers","R. O. Onvural; H. G. Perros","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","IEEE Transactions on Software Engineering","","1989","15","6","800","808","An approximation method for obtaining the throughput of cyclic queueing networks with blocking as a function of the number of customers in it is presented. The approximation method was developed for two different blocking mechanisms. It was also extended to the case of the central server model with blocking. Validation tests show that the algorithm is fairly accurate.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24733","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24733","","Throughput;Queueing analysis;Network servers;Approximation methods;Computer science;Signal processing;Testing;System recovery;Computer networks;Bismuth","computer networks;performance evaluation;queueing theory","approximate throughput analysis;validation tests;finite buffers;approximation method;cyclic queueing networks;customers;blocking mechanisms;central server model","","31","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Elaborating Requirements Using Model Checking and Inductive Learning","D. Alrajeh; J. Kramer; A. Russo; S. Uchitel","Imperial College London, London; Imperial College London, London; Imperial College London, London; Imperial College London, London","IEEE Transactions on Software Engineering","","2013","39","3","361","383","The process of Requirements Engineering (RE) includes many activities, from goal elicitation to requirements specification. The aim is to develop an operational requirements specification that is guaranteed to satisfy the goals. In this paper, we propose a formal, systematic approach for generating a set of operational requirements that are complete with respect to given goals. We show how the integration of model checking and inductive learning can be effectively used to do this. The model checking formally verifies the satisfaction of the goals and produces counterexamples when incompleteness in the operational requirements is detected. The inductive learning process then computes operational requirements from the counterexamples and user-provided positive examples. These learned operational requirements are guaranteed to eliminate the counterexamples and be consistent with the goals. This process is performed iteratively until no goal violation is detected. The proposed framework is a rigorous, tool-supported requirements elaboration technique which is formally guided by the engineer's knowledge of the domain and the envisioned system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6216384","Requirements elaboration;goal operationalization;behavior model refinement;model checking;inductive learning","Wheels;Computational modeling;Software;Adaptation models;Calculus;Switches;Semantics","formal specification;formal verification;learning by example","model checking;requirement engineering;RE;goal elicitation;requirement specification;operational requirements specification;formal verification;inductive learning process;tool-supported requirements elaboration","","5","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Integrative Double Kaizen Loop (IDKL): Towards a Culture of Continuous Learning and Sustainable Improvements for Software Organizations","O. H. Al-Baik; J. Miller","Electrical and Computer Engineering, University of Alberta, Edmonton, Alberta Canada (e-mail: albaik@ualberta.ca); Electrical and Computer Eng, University of Alberta, Edmonton, Alberta Canada T6H 2M4 (e-mail: jimm@ualberta.ca)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","In the past decades, software organizations have been relying on implementing process improvement methods to advance quality, productivity, and predictability of their development and maintenance efforts. However, these methods have proven to be challenging to implement in many situations, and when implemented, their benefits are often not sustained. Commonly, the workforce requires guidance during the initial deployment, but what happens after the guidance stops? Why do not traditional improvement methods deliver the desired results? And, how do we maintain the improvements when they are realized? In response to these questions, we have combined social and organizational learning methods with Lean&#x0027;s continuous improvement philosophy, Kaizen, which has resulted in an IDKL model that has successfully promoted continuous learning and improvement. The IDKL has evolved through a real-life project with an industrial partner; the study employed ethnographic action research with 231 participants and had lasted for almost 3 years. The findings show that on average, Lead Time has dropped by 46%, Process Cycle Efficiency has increased by 137%, First-Pass Process Yield has increased by 27%, and Customer Satisfaction has increased by 25%.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2829722","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8345680","Kaizen;Lean;organization learning;double loop learning;case study;empirical research","Continuous improvement;Software;Organizations;Standards organizations;Learning systems;Tools;Software engineering","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A formal evaluation of data flow path selection criteria","L. A. Clarke; A. Podgurski; D. J. Richardson; S. J. Zeil","Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA; Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA; Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA; Dept. of Comput. & Inf. Sci., Massachusetts Univ., Amherst, MA, USA","IEEE Transactions on Software Engineering","","1989","15","11","1318","1332","The authors report on the results of their evaluation of path-selection criteria based on data-flow relationships. They show how these criteria relate to each other, thereby demonstrating some of their strengths and weaknesses. A subsumption hierarchy showing their relationship is presented. It is shown that one of the major weaknesses of all the criteria is that they are based solely on syntactic information and do not consider semantic issues such as infeasible paths. The authors discuss the infeasible-path problem as well as other issues that must be considered in order to evaluate these criteria more meaningfully and to formulate a more effective path-selection criterion.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41326","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41326","","Software testing;Information science;Computer science;Marine vehicles;Programming profession;Contracts;Terminology","flowcharting;software engineering","formal evaluation;data flow path selection criteria;data-flow relationships;subsumption hierarchy;syntactic information;infeasible-path problem","","76","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Correction of ""A Comparative Study to Benchmark Cross-project Defect Prediction Approaches""","S. Herbold; A. Trautsch; J. Grabowski","Institute for Computer Science, Georg-August-Universität Göttingen, Göttingen, Lower Saxony Germany 37077 (e-mail: herbold@cs.uni-goettingen.de); Institute for Computer Science, Georg-August-Universität Göttingen, Göttingen, Lower Saxony Germany (e-mail: alexander.trautsch@stud.uni-goettingen.de); Institute for Computer Science, Georg-August-Universität Göttingen, Göttingen, Lower Saxony Germany (e-mail: grabowski@cs.uni-goettingen.de)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Unfortunately, the article ""A Comparative Study to Benchmark Cross-project Defect Prediction Approaches"" has a problem in the statistical analysis which was pointed out almost immediately after the pre-print of the article appeared online. While the problem does not negate the contribution of the the article and all key findings remain the same, it does alter some rankings of approaches used in the study. Within this correction, we will explain the problem, how we resolved it, and present the updated results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2790413","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8248781","cross-project defect prediction;benchmark;comparison;replication;correction","Sociology;Measurement;Benchmark testing;Statistical analysis;Ranking (statistics);Terminology","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A programming methodology for dual-tier multicomputers","S. B. Baden; S. J. Fink","Dept. of Comput. Sci. & Eng., California Univ., San Diego, La Jolla, CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","3","212","226","Hierarchically organized ensembles of shared memory multiprocessors possess a richer and more complex model of locality than previous generation multicomputers with single processor nodes. These dual-tier computers introduce many new factors into the programmer's performance model. We present a methodology for implementing block-structured numerical applications on dual-tier computers and a run-time infrastructure, called KeLP2, that implements the methodology. KeLP2 supports two levels of locality and parallelism via hierarchical SPMD control flow, run-time geometric meta-data, and asynchronous collective communication. KeLP applications can effectively overlap communication with computation under conditions where nonblocking point-to-point message passing fails to do so. KeLP's abstractions hide considerable detail without sacrificing performance and dual-tier applications written in KeLP consistently outperform equivalent single-tier implementations written in MPI. We describe the KeLP2 model and show how it facilitates the implementation of five block-structured applications specially formulated to hide communication latency on dual-tiered architectures. We support our arguments with empirical data from applications running on various single- and dual-tier multicomputers. KeLP2 supports a migration path from single-tier to dual-tier platforms and we illustrate this capability with a detailed programming example.","0098-5589;1939-3520;2326-3881","","10.1109/32.842948","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842948","","Application software;Concurrent computing;Parallel processing;Programming profession;Computer architecture;Computer applications;Runtime;Communication system control;Message passing;Delay","message passing;shared memory systems;parallel processing;parallel programming","programming methodology;dual-tier multicomputers;shared memory multiprocessors;performance model;block-structured numerical applications;run-time infrastructure;KeLP2;hierarchical SPMD control flow;run-time geometric meta-data;asynchronous collective communication;point-to-point message passing;communication latency;dual-tiered architectures","","15","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Test Code Quality and Its Relation to Issue Handling Performance","D. Athanasiou; A. Nugroho; J. Visser; A. Zaidman","Software Improvement Group, Amstelplein 1, 1096HA Amsterdam, The Netherlands; Software Improvement Group, Amstelplein 1, 1096HA Amsterdam, The Netherlands; Software Improvement Group, Amstelplein 1, 1096HA Amsterdam, The Netherlands; Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Mekelweg 4, 2628CD Delft, The Netherlands","IEEE Transactions on Software Engineering","","2014","40","11","1100","1125","Automated testing is a basic principle of agile development. Its benefits include early defect detection, defect causelocalization and removal of fear to apply changes to the code. Therefore, maintaining high quality test code is essential. This study introduces a model that assesses test code quality by combining source code metrics that reflect three main aspects of test codequality: completeness, effectiveness and maintainability. The model is inspired by the Software Quality Model of the SoftwareImprovement Group which aggregates source code metrics into quality ratings based on benchmarking. To validate the model we assess the relation between test code quality, as measured by the model, and issue handling performance. An experiment isconducted in which the test code quality model is applied to<inline-formula><tex-math notation=""LaTeX"">$18$</tex-math><alternatives><inline-graphic xlink:href=""zaidman-ieq1-2342227.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>open source systems. The test quality ratings are tested for correlation with issue handling indicators, which are obtained by mining issue repositories. In particular, we study the (1) defect resolution speed, (2) throughput and (3) productivity issue handling metrics. The results reveal a significant positive correlation between test code quality and two out of the three issue handling metrics (throughput and productivity), indicating that good test code quality positively influences issue handling performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2342227","NWO TestRoots project; RAAK-PRO project EQuA; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6862882","Testing;defects;bugs;metrics;measurement","Measurement;Software;Productivity;Throughput;Benchmark testing;Correlation","","","","28","","90","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study of Distributed Application Performance","K. A. Lantz; W. I. Nowicki; A. M. Theimer","Department of Computer Science, Stanford University; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","10","1162","1174","A major reason for the rarity of distributed applications, despite the proliferation of networks, is the sensitivity of their performance to various aspects of the network environment. We demonstrate that distributed applications can run faster than local ones, using common hardware. We also show that the primary factors affecting performance are, in approximate order of importance: speed of the user's workstation, speed of the remote host (if any), and the high-level (above the transport level) protocols used. In particular, the use of batching, pipelining, and structure in high-level protocols reduces the degradation often experienced between different bandwidth networks. Less significant, but still noticeable improvements result from proper design and implementation of the underlying transport protocols. Ultimately, with proper application of these techniques, network bandwidth is rendered virtually insignificant.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231864","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701932","Concurrency;distributed programming;distributed systems;performance evaluation;protocol design;protocol implementation","Transport protocols;Application software;Computational efficiency;Distributed computing;Bandwidth;Contracts;Computer science;Graphics;Hardware;Workstations","","Concurrency;distributed programming;distributed systems;performance evaluation;protocol design;protocol implementation","","9","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Test case prioritization: a family of empirical studies","S. Elbaum; A. G. Malishevsky; G. Rothermel","Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","2","159","182","To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of such prioritization is to increase a test suite's rate of fault detection. Previous work reported results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: 1) Can prioritization techniques be effective when targeted at specific modified versions; 2) what trade-offs exist between fine granularity and coarse granularity prioritization techniques; 3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? To address these questions, we have performed several new studies in which we empirically compared prioritization techniques using both controlled experiments and case studies.","0098-5589;1939-3520;2326-3881","","10.1109/32.988497","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988497","","Computer aided software engineering;Fault detection;Software testing;System testing;Costs;Software measurement;Radio access networks;Feedback;Debugging;Instruments","program testing","test case prioritization;regression testing;software testing;fault detection rate;fine granularity prioritization techniques;coarse granularity prioritization techniques;fault proneness measures","","377","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying Renaming Opportunities by Expanding Conducted Rename Refactorings","H. Liu; Q. Liu; Y. Liu; Z. Wang","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Software Engineering","","2015","41","9","887","900","To facilitate software refactoring, a number of approaches and tools have been proposed to suggest where refactorings should be conducted. However, identification of such refactoring opportunities is usually difficult because it often involves difficult semantic analysis and it is often influenced by many factors besides source code. For example, whether a software entity should be renamed depends on the meaning of its original name (natural language understanding), the semantics of the entity (source code semantics), experience and preference of developers, and culture of companies. As a result, it is difficult to identify renaming opportunities. To this end, in this paper we propose an approach to identify renaming opportunities by expanding conducted renamings. Once a rename refactoring is conducted manually or with tool support, the proposed approach recommends to rename closely related software entities whose names are similar to that of the renamed entity. The rationale is that if an engineer makes a mistake in naming a software entity it is likely for her to make the same mistake in naming similar and closely related software entities. The main advantage of the proposed approach is that it does not involve difficult semantic analysis of source code or complex natural language understanding. Another advantage of this approach is that it is less influenced by subjective factors, e.g., experience and preference of software engineers. The proposed approach has been evaluated on four open-source applications. Our evaluation results show that the proposed approach is accurate in recommending entities to be renamed (average precision 82 percent) and in recommending new names for such entities (average precision 93 percent). Evaluation results also suggest that a substantial percentage (varying from 20 to 23 percent) of rename refactorings are expansible.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2427831","National Natural Science Foundation of China; Program for New Century Excellent Talents in University; Beijing Higher Education Young Elite Teacher Project; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7097720","Software Refactoring;Rename;Code Smells;Refactoring Opportunity;Identification;Software refactoring;rename;code smells;refactoring opportunity;identification","Semantics;Natural languages;Open source software;Engines;Context;IEEE Potentials","software maintenance","renaming opportunities identification;conducted rename refactorings;software refactoring;semantic analysis;software entity naming","","3","","36","","","","","","IEEE","IEEE Journals & Magazines"
"A set of inference rules for quantified formula handling and array handling in verification of programs over integers","D. Sarkar; S. C. De Sarkar","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Kharagpur, India; Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Kharagpur, India","IEEE Transactions on Software Engineering","","1989","15","11","1368","1381","Because of the undecidability problem of program verification, it becomes necessary for an automated verifier to seek human assistance for proving theorems which fall beyond its capability. In order that the user be able to interact smoothly with the machine, it is desired that the theorems be maintained and processed by the prover in a form as close as possible to the popular algebraic notation. Motivated by the need of such an automated verifier, which works in an environment congenial to human participation and at the same time uses the methodologies of resolution provers of first-order logic, some inference rules have previously been proposed by the authors (ibid., vol.15, no.1, p.1-9, Jan. 1989) for integer arithmetic, and their completeness issues have been discussed. In the present work, the authors examine how these rules can be applied to quantified formulas vis-a-vis verification of programs involving arrays. An interesting situation, referred to as bound-extension, has been found to occur frequently in proving the quantified verification conditions of the paths in a program. A novel rule, called bound-extension rule, has been devised to consolidate and depict the various issues involved in a bound-extension process. It has been proved that the rule set proposed previously by the authors is adequate for handling a more general phenomenon, called bound-modification, which covers bound-extension in all its entirety.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41330","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41330","","Humans;Programmable logic arrays;Arithmetic;Costs;Computer science;Formal languages;Calculus;Programming profession;Virtual colonoscopy","decidability;inference mechanisms;program verification;theorem proving","inference rules;quantified formula handling;array handling;undecidability problem;program verification;automated verifier;first-order logic;integer arithmetic;quantified formulas;bound-extension rule;bound-modification","","1","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Design, construction, and application of a generic visual language generation environment","K. Zhang; D. -. Zhang; J. Cao","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","4","289","307","The implementation of visual programming languages (VPLs) and their supporting environments is time-consuming and tedious. To ease the task, researchers have developed some high-level tools to reduce the development effort. None of these tools, however, can be easily used to create a complete visual language in a seamless way as the lex/yacc tools do for textual language constructions. This paper presents the design, construction and application of a generic visual language generation environment, called VisPro. The VisPro design model improves the conventional model-view-controller framework in that its functional modules are decoupled to allow independent development and integration. The VisPro environment consists of a set of visual programming tools. Using VisPro, the process of VPL construction can be divided into two steps: lexicon definition and grammar specification. The former step defines visual objects and a visual editor, and the latter step provides language grammars with graph rewriting rules. The compiler for the VPL is automatically created according to the grammar specification. A target VPL is generated as a programming environment which contains the compiler and the visual editor. The paper demonstrates how VisPro is used by building a simple visual language and a more complex visual modeling language for distributed programming.","0098-5589;1939-3520;2326-3881","","10.1109/32.917521","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917521","","Programming environments;Graphical user interfaces;Computer languages;Software systems;User interfaces;Program processors;Buildings;Productivity;Application software;Education","visual languages;programming environments;compiler generators;subroutines;graph grammars;rewriting systems;distributed programming","generic visual language generation environment;visual programming language construction;VisPro;design model;model-view-controller framework;decoupled functional modules;independent module development;module integration;visual programming tools;lexicon definition;grammar specification;visual objects;visual editor;language grammars;graph rewriting rules;compiler construction;programming environment;visual modeling language;distributed programming","","39","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Timed Automata Patterns","J. S. Dong; P. Hao; S. Qin; J. Sun; W. Yi","National University of Singapore, Singapore; National University of Singapore, Singapore; Durham University, Durham; National University of Singapore, Singapore; North Eastern University, China, and Uppsala University, Sweden","IEEE Transactions on Software Engineering","","2008","34","6","844","859","Timed automata have proven to be useful for specification and verification of real-time systems. System design using timed automata relies on explicit manipulation of clock variables. A number of automated analyzers for timed automata have been developed. However, timed automata lack composable patterns for high-level system design. Specification languages like Timed Communicating Sequential Process (CSP) and Timed Communicating Object-Z (TCOZ) are well suited for presenting compositional models of complex real-time systems. In this work, we define a set of composable Timed Automata patterns based on hierarchical constructs in time-enriched process algebras. The patterns facilitate the hierarchical design of complex systems using timed automata. They also allow a systematic translation from Timed CSP/TCOZ models to timed automata so that analyzers for timed automata can be used to reason about TCOZ models. A prototype has been developed to support system design using timed automata patterns or, if given a TCOZ specification, to automate the translation from TCOZ to timed automata.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4586397","Specification;Validation;Specification;Validation","Automata;Real time systems;Algebra;Clocks;Power system modeling;Sun;Specification languages;Prototypes;Formal specifications;Timing","automata theory;formal specification;formal verification;process algebra;specification languages;systems analysis","timed automata patterns;system design;specification languages;timed communicating sequential process;timed communicating object-Z;time-enriched process algebras;TCOZ specification;real-time systems","","28","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Handling timing errors in distributed programs","A. J. Gordon; R. A. Finkel","Dept. of Math. & Comput. Sci., Colorado Sch. of Mines, Golden, CO, USA; NA","IEEE Transactions on Software Engineering","","1988","14","10","1525","1535","The authors describe a tool called TAP, which is defined to aid the programmer in discovering the causes of timing errors in running programs. TAP is similar to a postmortem debugger, using the history of interprocess communication to construct a timing graph, a directed graph where an edge joins node x to node y if event x directly precedes event y in time. The programmer can then use TAP to look at the graph to find the events that occurred in an unacceptable order. Because of the nondeterministic nature of distributed programs, the authors feel a history-keeping mechanism but always be active so that bugs can be dealt with as they occur. The goal is to collect enough information at run time to construct the timing graph if needed. Since it is always active, this mechanism must be efficient. The authors also describe experiments run using TAP and report the impact that TAP's history-keeping mechanism has on the running time of various distributed programs.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6197","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6197","","Timing;Programming profession;Computer bugs;History;Debugging;Error correction;Degradation;Computer science;Computer errors;Operating systems","directed graphs;distributed processing;program testing;software tools","distributed programs;TAP;timing errors;postmortem debugger;interprocess communication;timing graph;directed graph;history-keeping mechanism","","2","","21","","","","","","IEEE","IEEE Journals & Magazines"
"An Information-Based Model for Failure-Handling in Distributed Database Systems","F. Y. Chin; K. V. S. Ramarao","Center for Computer Studies and Applications, University of Hong Kong; NA","IEEE Transactions on Software Engineering","","1987","SE-13","4","420","431","We consider the failure atomicity problem of distributed transactions in conjunction with the maximization of database availability. We propose a new information-based model for the distributed transaction-execution, which explicitly expresses the information at each stage during a protocol. In addition to rederiving certain existing results, we prove a fundamental relation among the site failures and the network partitioning. We propose a realistic model for site failures under which we show that the costs of commit and termination protocols can be greatly reduced. Finally, we explore the possible recovery strategies for a failed site and show how they are improved under our site failure model.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233179","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702234","Atomic transactions;database availability;network partitioning;nonblocking commit protocols;nontrivial termination;recovery strategies;site failures","Database systems;Transaction databases;Protocols;Distributed databases;Availability;Costs;Degradation;Councils;Computer applications;Application software","","Atomic transactions;database availability;network partitioning;nonblocking commit protocols;nontrivial termination;recovery strategies;site failures","","1","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Safety Practices in Requirements Engineering: The Uni-REPM Safety Module","J. F. F. Vilela; J. Castro; L. E. G. Martins; T. Gorschek","Campus Quixad&#x00E1;, Universidade Federal do Ceara, 28121 Quixad&#x00E1;, CE Brazil (e-mail: jessykavilela@ufc.br); Ci&#x00EA;ncia da Computa&#x00E7;&#x00E3;o, Universidade Federal de Pernambuco, Recife, PE Brazil (e-mail: jbc@cin.ufpe.br); Departamento de Ci&#x00EA;ncia e Tecnologia, Universidade Federal de Sao Paulo, 28105 S&#x00E3;o Jos&#x00E9; dos Campos, SP Brazil (e-mail: legmartins@unifesp.br); Department of Systems and Software Engineering, Blekinge Institute of Technology, Karlskrona, Blekinge Sweden SE-371 79 (e-mail: tony.gorschek@bth.se)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Context: Software is an important part in safety- critical system (SCS) development since it is becoming a major source of hazards. Requirements-related hazards have been as- sociated with many accidents and safety incidents. Requirements issues tend to be mitigated in companies with high processes maturity levels since they do their business in a systematic, consistent and proactive approach. However, requirements en- gineers need systematic guidance to consider safety concerns early in the development process. Goal: the paper investigates which safety practices are suitable to be used in the Requirements Engineering (RE) process for SCS and how to design a safety maturity model for this area. Method: we followed the design science methodology to propose Uni-REPM SCS, a safety module for Unified Requirements Engineering Process Maturity Model (Uni-REPM). We also conducted a static validation with two practitioners and nine academic experts to evaluate its coverage, correctness, usefulness and applicability. Results: The module has seven main processes, fourteen sub-processes and 148 practices that form the basis of safety processes maturity. Moreover, we describe its usage through a tool. Conclusions: The validation indicates a good coverage of practices and well receptivity by the experts. Finally, the module can help companies in evaluating their current practices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2846576","Knowledge Foundation; Fundacao de Amparo a Ciencia e Tecnologia do Estado de Pernambuco; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8382315","Safety-critical systems;Requirements Engineering;Maturity Models;Uni-REPM;Safety Engineering","Safety;Companies;Software;Capability maturity model;Requirements engineering;Systematics;Standards","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"A case history analysis of software error cause-effect relationships","T. Nakajo; H. Kume","Fac. of Eng., Tokyo Univ., Japan; Fac. of Eng., Tokyo Univ., Japan","IEEE Transactions on Software Engineering","","1991","17","8","830","838","Approximately 700 errors in four commercial measuring-control software products were analyzed, and the cause-effect relationships of errors occurring during software development were identified. The analysis method used defined appropriate observation points along the path leading from cause to effect of a software error and gathered the corresponding data by analyzing each error using fault tree analysis. Each observation point's data were categorized, and the relationships between two adjoining points were summarized using a cross-indexing table. Four major error-occurrence mechanisms were identified; two are related to hardware and software interface specification misunderstandings, while the other two are related to system and module function misunderstandings. The effects of structured analysis and structured design methods on software errors were evaluated.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.83917","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=83917","","Computer aided software engineering;History;Error analysis;Cause effect analysis;Humans;Programming;Data analysis;US Department of Transportation;Computer errors;Marine vehicles","computerised control;failure analysis;software reliability;structured programming;system recovery;systems analysis","case history analysis;commercial measuring-control software products;cause-effect relationships;software development;observation points;software error;fault tree analysis;cross-indexing table;error-occurrence mechanisms;software interface specification misunderstandings;module function misunderstandings;structured analysis;structured design methods","","18","","12","","","","","","IEEE","IEEE Journals & Magazines"
"A probabilistic model for predicting software development effort","P. C. Pendharkar; G. H. Subramanian; J. A. Rodger","Sch. of Bus. Adm., Pennsylvania State Univ., University Park, PA, USA; Sch. of Bus. Adm., Pennsylvania State Univ., University Park, PA, USA; NA","IEEE Transactions on Software Engineering","","2005","31","7","615","624","Recently, Bayesian probabilistic models have been used for predicting software development effort. One of the reasons for the interest in the use of Bayesian probabilistic models, when compared to traditional point forecast estimation models, is that Bayesian models provide tools for risk estimation and allow decision-makers to combine historical data with subjective expert estimates. In this paper, we use a Bayesian network model and illustrate how a belief updating procedure can be used to incorporate decision-making risks. We develop a causal model from the literature and, using a data set of 33 real-world software projects, we illustrate how decision-making risks can be incorporated in the Bayesian networks. We compare the predictive performance of the Bayesian model with popular nonparametric neural-network and regression tree forecasting models and show that the Bayesian model is a competitive model for forecasting software development effort.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.75","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492375","Index Terms- Bayesian belief networks;software effort estimation;probability theory.","Predictive models;Programming;Bayesian methods;Regression tree analysis;Neural networks;Costs;Probability distribution;Decision making;Linear regression;Risk management","software development management;software cost estimation;risk management;decision making;belief networks;neural nets;Bayes methods;probability;regression analysis","Bayesian probabilistic model;software development effort prediction;point forecast estimation model;risk estimation;decision-making risk;Bayesian network model;belief updating procedure;real-world software project;nonparametric neural-network;regression tree forecasting model;software development effort forecasting;Bayesian belief network;software effort estimation;probability theory","","108","","37","","","","","","IEEE","IEEE Journals & Magazines"
"The Risks of Coverage-Directed Test Case Generation","G. Gay; M. Staats; M. Whalen; M. P. E. Heimdahl","Department of Computer Science & Engineering, University of South Carolina; Google, Inc; Department of Computer Science and Engineering, University of Minnesota; Department of Computer Science and Engineering, University of Minnesota","IEEE Transactions on Software Engineering","","2015","41","8","803","819","A number of structural coverage criteria have been proposed to measure the adequacy of testing efforts. In the avionics and other critical systems domains, test suites satisfying structural coverage criteria are mandated by standards. With the advent of powerful automated test generation tools, it is tempting to simply generate test inputs to satisfy these structural coverage criteria. However, while techniques to produce coverage-providing tests are well established, the effectiveness of such approaches in terms of fault detection ability has not been adequately studied. In this work, we evaluate the effectiveness of test suites generated to satisfy four coverage criteria through counterexample-based test generation and a random generation approach-where tests are randomly generated until coverage is achieved-contrasted against purely random test suites of equal size. Our results yield three key conclusions. First, coverage criteria satisfaction alone can be a poor indication of fault finding effectiveness, with inconsistent results between the seven case examples (and random test suites of equal size often providing similar-or even higher-levels of fault finding). Second, the use of structural coverage as a supplement-rather than a target-for test generation can have a positive impact, with random test suites reduced to a coverage-providing subset detecting up to 13.5 percent more faults than test suites generated specifically to achieve coverage. Finally, Observable MC/DC, a criterion designed to account for program structure and the selection of the test oracle, can-in part-address the failings of traditional structural coverage criteria, allowing for the generation of test suites achieving higher levels of fault detection than random test suites of equal size. These observations point to risks inherent in the increase in test automation in critical systems, and the need for more research in how coverage criteria, test generation approaches, the test oracle used, and system structure jointly influence test effectiveness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2421011","NASA; NSF; Fonds National de la Recherche, Luxembourg; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7081779","Software Testing;System Testing;Software testing;system testing","Testing;Aerospace electronics;NASA;Standards;Fault detection;Measurement;Software packages","program testing;risk management;software fault tolerance","risks;coverage-directed test case generation;structural coverage criteria;automated test generation tools;fault detection;counterexample-based test generation;random generation approach;random test suites;coverage criteria satisfaction;fault finding effectiveness;observable MC/DC;program structure;test oracle selection;test automation;critical systems;system structure;software testing","","26","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Generating Event Sequence-Based Test Cases Using GUI Runtime State Feedback","X. Yuan; A. M. Memon","University of Maryland, College Park; University of Maryland, College Park","IEEE Transactions on Software Engineering","","2010","36","1","81","95","This paper presents a fully automatic model-driven technique to generate test cases for graphical user interfaces (GUIs)-based applications. The technique uses feedback from the execution of a ¿seed test suite,¿ which is generated automatically using an existing structural event interaction graph model of the GUI. During its execution, the runtime effect of each GUI event on all other events pinpoints event semantic interaction (ESI) relationships, which are used to automatically generate new test cases. Two studies on eight applications demonstrate that the feedback-based technique 1) is able to significantly improve existing techniques and helps identify serious problems in the software and 2) the ESI relationships captured via GUI state yield test suites that most often detect more faults than their code, event, and event-interaction-coverage equivalent counterparts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5306073","GUI testing;automated testing;model-based testing;GUITAR testing system.","Graphical user interfaces;Runtime;State feedback;Automatic testing;Software testing;System testing;Application software;Costs;Fault diagnosis;Event detection","graphical user interfaces;program testing;software quality","event sequence based test cases;graphical user interfaces;GUI runtime state feedback;automatic model driven technique;event interaction coverage equivalent counterparts;software quality;event semantic interaction relationships","","61","","49","","","","","","IEEE","IEEE Journals & Magazines"
"A formal model of the software test process","J. W. Cangussu; R. A. DeCarlo; A. P. Mathur","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","8","782","796","A novel approach to model the system test phase of the software life cycle is presented. This approach is based on concepts and techniques from control theory and is useful in computing the effort required to reduce the number of errors and the schedule slippage under a changing process environment. Results from these computations are used, and possibly revised, at specific checkpoints in a feedback-control structure to meet the schedule and quality objectives. Two case studies were conducted to study the behavior of the proposed model. One study reported here uses data from a commercial project. The outcome from these two studies suggests that the proposed model might well be the first significant milestone along the road to a formal and practical theory of software process control.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027800","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027800","","Software testing;Process control;Temperature control;Control theory;System testing;Processor scheduling;Control systems;Life testing;Error correction;Roads","program testing;software process improvement","formal model;software test process;system test phase;software life cycle;schedule slippage;checkpoints;feedback-control structure;software process control","","54","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Keeping the Development Environment Up to Date—A Study of the Situated Practices of Appropriating the Eclipse IDE","S. Draxler; G. Stevens; A. Boden","Department for Information Systems and New Media, University of Siegen, Siegen, Germany; Department for Information Systems and New Media, University of Siegen, Siegen, Germany; Usability and User Experience Design Competence Center, Fraunhofer Institute for Applied Information Technology FIT, Sankt Augustin, Germany","IEEE Transactions on Software Engineering","","2014","40","11","1061","1074","Software engineers and developers are surrounded by highly complex software systems. What does it take to cope with these? We introduce a field study that explores the maintenance of the Eclipse Integrated Development Environment by software developers as part of their daily work. The study focuses on appropriation of the Eclipse IDE. We present an empirical view on appropriation as a means to maintain the collective ability to work. We visited seven different organizations and observed and interviewed their members. Each organization was chosen to provide an overall picture of Eclipse use throughout the industry. The results decompose the appropriation of Eclipse by software developers in organizations into four categories: learning, tailoring and discovering, as well as the cross-cutting category: collaboration. The categories are grounded in situations that provoked a need to change as well as in policies adopted for coping with this need. By discussing these categories against the background of Eclipse and its ecosystem, we want to illustrate in what ways appropriation of component- or plugin- based software is nowadays a common and highly complex challenge for Eclipse users, and how the related appropriation practices can be supported by IT systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2354047","German Federal Ministry for Education and Research; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6898825","Programmer workbench;human factors in software design;deployment;usage experience","Software;Organizations;Computer aided software engineering;Interviews;Context;Employment;Ecosystems","object-oriented languages;object-oriented programming;software engineering","development environment;Eclipse IDE;software engineers;software developers;highly complex software systems;Eclipse Integrated Development Environment;component-based software;plugin-based software;IT systems","","2","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Measurement programs in software development: determinants of success","A. Gopal; M. S. Krishnan; T. Mukhopadhyay; D. R. Goldenson","PRTM, Waltham, MA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","9","863","875","Measurement programs in software organizations are an important source of control over quality and cost in software development. The findings of this research presented here are based on an industry-wide survey conducted to examine the factors that influence success in software metrics programs. Our approach is to go beyond the anecdotal information on metrics programs that exists in the literature and use the industry-wide survey data to rigorously test for the effects of various factors that affect metrics programs success. We measure success in metrics programs using two variables-use of metrics information in decision-making and improved organizational performance. The various determinants of metrics program success are divided into two sets-organizational variables and technical variables. The influence of these variables on metrics programs success is tested using regression analysis. Our results support some of the factors discussed in the anecdotal literature such as management support, goal alignment, and communication and feedback. Certain other factors such as metrics quality and the ease of data collection are not as strongly influential on success. We conclude the paper with a detailed discussion of our results and suggestions for future work.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033226","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033226","","Software measurement;Programming;Testing;Software quality;Costs;Computer industry;Software metrics;Decision making;Regression analysis;Feedback","software metrics;software development management;software quality;economics","measurement programs;software development;quality;cost;software metrics programs;decision making;improved organizational performance;technical variables;organizational variables;regression analysis;management support;goal alignment;communication;feedback;data collection;success factors","","65","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Model Checking Timed and Stochastic Properties with CSL^{TA}","S. Donatelli; S. Haddad; J. Sproston","Università di Torino, Torino; LSV, CBRS and École Normale Supérieure de Cachan, Cachan; Università di Torino, Torino","IEEE Transactions on Software Engineering","","2009","35","2","224","240","Markov chains are a well-known stochastic process that provide a balance between being able to adequately model the system's behavior and being able to afford the cost of the model solution. The definition of stochastic temporal logics like continuous stochastic logic (CSL) and its variant asCSL, and of their model-checking algorithms, allows a unified approach to the verification of systems, allowing the mix of performance evaluation and probabilistic verification. In this paper we present the stochastic logic CSLTA, which is more expressive than CSL and asCSL, and in which properties can be specified using automata (more precisely, timed automata with a single clock). The extension with respect to expressiveness allows the specification of properties referring to the probability of a finite sequence of timed events. A typical example is the responsiveness property ""with probability at least 0.75, a message sent at time 0 by a system A will be received before time 5 by system B and the acknowledgment will be back at A before time 7"", a property that cannot be expressed in either CSL or asCSL. We also present a model-checking algorithm for CSLTA.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.108","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4721440","Model checking;Markov processes;Temporal logic","Stochastic processes;Stochastic systems;Costs;Automata;Probabilistic logic;Quality of service;Unified modeling language;Telecommunication computing;Clocks;Delay","formal logic;formal verification;Markov processes;probability","timed properties;stochastic properties;Markov chains;stochastic process;stochastic temporal logics;continuous stochastic logic;model checking algorithm;systems verification;performance evaluation;probabilistic verification;finite sequence;timed events","","40","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Dealing with Traceability in the MDDof Model Transformations","J. M. Vara; V. A. Bollati; Á. Jiménez; E. Marcos","Computing Languages and Systems - II, University Rey Juan Carlos, Mostoles, Madrid, Spain; Computing Languages and Systems - II, University Rey Juan Carlos, Mostoles, Madrid, Spain; Computing Languages and Systems - II, University Rey Juan Carlos, Mostoles, Madrid, Spain; Computing Languages and Systems - II, University Rey Juan Carlos, Mostoles, Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","6","555","583","Traceability has always been acknowledged as a relevant topic in Software Engineering. However, keeping track of the relationships between the different assets involved in a development process is a complex and tedious task. The fact that the main assets handled in any model-driven engineering project are models and model transformations eases the task. In order to take advantage of this scenario, which has not been appropriately capitalized on by the most widely adopted model transformation languages before, this work presents MeTAGeM-Trace, a methodological and technical proposal with which to support the model-driven development of model transformations that include trace generation. The underlying idea is to start from a high-level specification of the transformation which is subsequently refined into lower-level transformation models in terms of a set of DSLs until the source code that implements the transformation can be generated. Running this transformation produces not only the corresponding target models, but also a trace model between the elements of the source and target models. As part of the proposal, an EMF-based toolkit has been developed to support the development of ATL and ETL model transformations. This toolkit has been empirically validated by conducting a set of case studies following a systematic research methodology.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2316132","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6784505","Model-driven engineering;model transformations;traceability","Proposals;Object oriented modeling;Software;DSL;Complexity theory;Data models;Software engineering","research and development;software engineering;source code (software)","MDD;traceability;software engineering;model-driven engineering project;model transformation languages;MeTAGeM-Trace;trace generation;lower-level transformation models;DSL;source code;EMF-based toolkit;ATL model transformations;ETL model transformations;systematic research methodology","","6","","79","","","","","","IEEE","IEEE Journals & Magazines"
"Testing Formal Specifications to Detect Design Errors","R. A. Kemmerer","Department of Computer Science, University of California","IEEE Transactions on Software Engineering","","1985","SE-11","1","32","43","Formal specification and verification techniques are now apused to increase the reliability of software systems. However, these proaches sometimes result in specifying systems that cannot be realized or that are not usable. This paper demonstrates why it is necessary to test specifications early in the software life cycle to guarantee a system that meets its critical requirements and that also provides the desired functionality. Definitions to provide the framework for classifying the validity of a functional requirement with respect to a formal specification tion are also introduced. Finally, the design of two tools for testing formal specifications is discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231535","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701896","Design and development;formal verification;reliable software;requirements;specification;testing","Formal specifications;Formal verification;Information security;Software testing;Software systems;Life testing;System testing;Computer science","","Design and development;formal verification;reliable software;requirements;specification;testing","","87","","7","","","","","","IEEE","IEEE Journals & Magazines"
"The Probabilistic Program Dependence Graph and Its Application to Fault Diagnosis","G. K. Baah; A. Podgurski; M. J. Harrold","Georgia Institute of Technology, Atlanta; Case Western Reserve University, Cleveland; Georgia Institute of Technology, Atlanta","IEEE Transactions on Software Engineering","","2010","36","4","528","545","This paper presents an innovative model of a program's internal behavior over a set of test inputs, called the probabilistic program dependence graph (PPDG), which facilitates probabilistic analysis and reasoning about uncertain program behavior, particularly that associated with faults. The PPDG construction augments the structural dependences represented by a program dependence graph with estimates of statistical dependences between node states, which are computed from the test set. The PPDG is based on the established framework of probabilistic graphical models, which are used widely in a variety of applications. This paper presents algorithms for constructing PPDGs and applying them to fault diagnosis. The paper also presents preliminary evidence indicating that a PPDG-based fault localization technique compares favorably with existing techniques. The paper also presents evidence indicating that PPDGs can be useful for fault comprehension.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374423","Probabilistic graphical models;machine learning;fault diagnosis;program analysis.","Fault diagnosis;Graphical models;Application software;Testing;Software engineering;Automatic control;Information analysis;Runtime;Probability distribution;Computer Society","fault diagnosis;graph theory;probability;program diagnostics;reasoning about programs;uncertainty handling","probabilistic program dependence graph;fault diagnosis;probabilistic analysis;reasoning;uncertain program behavior;fault localization technique;probabilistic graphical models","","39","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Proof rules for flush channels","T. Camp; P. Kearns; M. Ahuja","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA; NA","IEEE Transactions on Software Engineering","","1993","19","4","366","378","Flush channels generalize conventional asynchronous communication constructs such as virtual circuits and datagrams. They permit the programmer to specify receipt-order restrictions on a message-by-message basis, providing an opportunity for more concurrency in a distributed program. A Hoare-style partial correctness verification methodology for distributed systems which use flush channel communication is developed, and it is shown that it it possible to reason about such systems in a relatively natural way.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.223804","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=223804","","Asynchronous communication;Programming profession;Concurrent computing;Computer science;Data engineering;Joining processes;Protocols;Context;Switching circuits;Packet switching","distributed processing;program verification","flush channels;asynchronous communication constructs;virtual circuits;datagrams;receipt-order restrictions;message-by-message basis;concurrency;distributed program;Hoare-style partial correctness verification methodology","","6","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Imprecise Matching of Requirements Specifications for Software Services Using Fuzzy Logic","M. C. Platenius; A. Shaker; M. Becker; E. Hüllermeier; W. Schäfer","Software Engineering Group, Heinz Nixdorf Institute, Paderborn University, Germany; Intelligent Systems Group, Department of Computer Science, Paderborn University, Germany; Software Engineering Group, Fraunhofer IEM, Paderborn, Germany; Intelligent Systems Group, Department of Computer Science, Paderborn University, Germany; Software Engineering Group, Heinz Nixdorf Institute, Paderborn University, Germany","IEEE Transactions on Software Engineering","","2017","43","8","739","759","Today, software components are provided by global markets in the form of services. In order to optimally satisfy service requesters and service providers, adequate techniques for automatic service matching are needed. However, a requester's requirements may be vague and the information available about a provided service may be incomplete. As a consequence, fuzziness is induced into the matching procedure. The contribution of this paper is the development of a systematic matching procedure that leverages concepts and techniques from fuzzy logic and possibility theory based on our formal distinction between different sources and types of fuzziness in the context of service matching. In contrast to existing methods, our approach is able to deal with imprecision and incompleteness in service specifications and to inform users about the extent of induced fuzziness in order to improve the user's decision-making. We demonstrate our approach on the example of specifications for service reputation based on ratings given by previous users. Our evaluation based on real service ratings shows the utility and applicability of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2632115","German Research Foundation (DFG); Collaborative Research Center “On-The-Fly Computing”; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7755807","Service selection;service matching;requirements specifications;non-functional properties;fuzzy logic;uncertainty;decision making","Uncertainty;Fuzzy logic;Context;Security;Software;Software engineering;Decision making","decision making;fuzzy logic;fuzzy set theory;pattern matching;software engineering","software components;requirement specification imprecise matching;software services;fuzzy logic;automatic service matching;possibility theory;user decision-making","","","","93","","","","","","IEEE","IEEE Journals & Magazines"
"Test-execution-based reliability measurement and modeling for large commercial software","J. Tian; Peng Lu; J. Palma","IBM Software Solutions Toronto Lab., New York, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","1995","21","5","405","414","The paper studies practical reliability measurement and modeling for large commercial software systems based on test execution data collected during system testing. The application environment and the goals of reliability assessment were analyzed to identify appropriate measurement data. Various reliability growth models were used on failure data normalized by test case executions to track testing progress and provide reliability assessment. Practical problems in data collection, reliability measurement and modeling, and modeling result analysis were also examined. The results demonstrated the feasibility of reliability measurement in a large commercial software development environment and provided a practical comparison of various reliability measurements and models under such an environment.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.387470","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=387470","","Software testing;Software measurement;System testing;Software systems;Application software;Software reliability;Relational databases;Time measurement;Programming","software reliability;testing;system monitoring","large commercial software;modeling;test-execution-based reliability measurement;test execution data;system testing;application environment;measurement data;reliability growth models;normalised failure data;testing progress tracking;reliability assessment;data collection;large commercial software development environment","","27","","15","","","","","","IEEE","IEEE Journals & Magazines"
"A New Security Testing Method and Its Application to the Secure Xenix Kernel","V. D. Gligor; C. S. Chandersekaran; Wen-Der Jiang; A. Johri; G. L. Luckenbaugh; L. E. Reich","Department of Electrical Engineering, University of Maryland; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","2","169","183","A new security testing method is proposed that combines the advantages of both traditional ""black box"" (monolithic functional) testing and ""white box"" (functional-synthesis-based) testing. The new method allows significant coverage both for security model-based tests and for individual kernel-call tests. It eliminates redundant kernel test cases 1) by using a variant of control synthesis graphs, 2) by analyzing dependencies between descriptive kernel-call specifications, and 3) by exploiting access check separability. A higher degree of test assurance is achieved than that of other security testing methods because the new method helps eliminate cyclic dependencies among test programs for different kernel calls. The application of this method to the testing of the Secure Xenix™ kernel is illustrated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232890","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702198","Access check graphs;control synthesis graphs;cyclic dependencies;data flow coverage;functional testing;kernels;security model;security testing","Kernel;Software testing;Data security;Computer security;Control system synthesis;Flow graphs;System testing;Scattering;Thumb;Trademarks","","Access check graphs;control synthesis graphs;cyclic dependencies;data flow coverage;functional testing;kernels;security model;security testing","","5","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Debugging effort estimation using software metrics","N. Gorla; A. C. Benander; B. A. Benander","Dept. of Comput. Sci., Cleveland State Univ., OH, USA; Dept. of Comput. Sci., Cleveland State Univ., OH, USA; Dept. of Comput. Sci., Cleveland State Univ., OH, USA","IEEE Transactions on Software Engineering","","1990","16","2","223","231","Measurements of 23 style characteristics, and the program metrics LOC, V(g), VARS, and PARS were collected from student Cobol programs by a program analyzer. These measurements, together with debugging time (syntax and logic) data, were analyzed using several statistical procedures of SAS (statistical analysis system), including linear, quadratic, and multiple regressions. Some of the characteristics shown to correlate significantly with debug time are GOTO usage, structuring of the IF-ELSE construct, level 88 item usage, paragraph invocation pattern, and data name length. Among the observed characteristic measures which are associated with lowest debug times are: 17% blank lines in the data division, 12% blank lines in the procedure division, and 13-character-long data items. A debugging effort estimator, DEST, was developed to estimate debug times.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44385","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44385","","Debugging;Software metrics;Lab-on-a-chip;Reactive power;Logic;Synthetic aperture sonar;Regression analysis;Statistical analysis;Programming profession;Computational Intelligence Society","program debugging;program testing;statistical analysis","debugging effort estimation;quadratic regressions;linear regressions;software metrics;style characteristics;LOC;V(g);VARS;PARS;Cobol programs;program analyzer;statistical procedures;SAS;statistical analysis system;multiple regressions;GOTO usage;IF-ELSE construct;level 88 item usage;paragraph invocation pattern;data name length;debug times;DEST","","19","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Using sensitivity analysis to validate a state variable model of the software test process","J. W. Cangussu; R. A. DeCarlo; A. P. Mathur","Dept. of Comput. Sci., Texas Univ. at Dallas, Richardson, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","5","430","443","We report on the sensitivity analysis of a state variable model (Model S) proposed earlier. Model S captures the dominant behavior of the system test phase of the software test process. Sensitivity analysis is a mathematical methodology to compute changes in the system behavior due to changes in system parameters or variables. This is particularly important when parameters are calibrated using noisy or small data sets. Nevertheless, by mathematically quantifying the effects of parameter variations on the behavior of the model, and thereby the STP, one can easily and quickly evaluate the effect of such variations on the process performance without having to perform extensive simulations. In all cases studied, model S behaved according to empirical observations which serves to validate the model. It is also shown that sensitivity analysis can suggest structural improvements in a model when the model does not behave as expected.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199072","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199072","","Sensitivity analysis;Software testing;Mathematical model;System testing;Feedback control;Acoustic testing;Performance evaluation;Calibration;Software quality;Quality management","sensitivity analysis;program testing","sensitivity analysis;state variable model;software test process;dominant behavior;feedback control;state model","","24","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Common Trends in Software Fault and Failure Data","M. Hamill; K. Goseva-Popstojanova","West Virginia University, Morgantown; West Virginia University, Morgantown","IEEE Transactions on Software Engineering","","2009","35","4","484","496","The benefits of the analysis of software faults and failures have been widely recognized. However, detailed studies based on empirical data are rare. In this paper, we analyze the fault and failure data from two large, real-world case studies. Specifically, we explore: 1) the localization of faults that lead to individual software failures and 2) the distribution of different types of software faults. Our results show that individual failures are often caused by multiple faults spread throughout the system. This observation is important since it does not support several heuristics and assumptions used in the past. In addition, it clearly indicates that finding and fixing faults that lead to such software failures in large, complex systems are often difficult and challenging tasks despite the advances in software development. Our results also show that requirement faults, coding faults, and data problems are the three most common types of software faults. Furthermore, these results show that contrary to the popular belief, a significant percentage of failures are linked to late life cycle activities. Another important aspect of our work is that we conduct intra- and interproject comparisons, as well as comparisons with the findings from related studies. The consistency of several main trends across software systems in this paper and several related research efforts suggests that these trends are likely to be intrinsic characteristics of software faults and failures rather than project specific.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.3","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4760152","Software faults and failures;fault location;fault types;software fault distribution;software reliability;empirical studies.","Failure analysis;Software quality;Programming;Software systems;Fault location;Software reliability;Fault detection;Humans;Terminology;Computer bugs","software fault tolerance;system recovery;systems analysis","software fault analysis;software failure data;complex system;software development;requirement fault;coding fault;software life cycle activity;software system","","51","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Software Reliability and Testing Time Allocation: An Architecture-Based Approach","R. Pietrantuono; S. Russo; K. S. Trivedi","Federico II University of Naples, Naples; Federico II University of Naples, Naples; Duke University, Durham","IEEE Transactions on Software Engineering","","2010","36","3","323","337","With software systems increasingly being employed in critical contexts, assuring high reliability levels for large, complex systems can incur huge verification costs. Existing standards usually assign predefined risk levels to components in the design phase, to provide some guidelines for the verification. It is a rough-grained assignment that does not consider the costs and does not provide sufficient modeling basis to let engineers quantitatively optimize resources usage. Software reliability allocation models partially address such issues, but they usually make so many assumptions on the input parameters that their application is difficult in practice. In this paper, we try to reduce this gap, proposing a reliability and testing resources allocation model that is able to provide solutions at various levels of detail, depending upon the information the engineer has about the system. The model aims to quantitatively identify the most critical components of software architecture in order to best assign the testing resources to them. A tool for the solution of the model is also developed. The model is applied to an empirical case study, a program developed for the European Space Agency, to verify model's prediction abilities and evaluate the impact of the parameter estimation errors on the prediction accuracy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383374","Reliability;software architecture;software testing.","Software reliability;Software testing;Reliability engineering;Predictive models;Software systems;Guidelines;Cost function;Application software;System testing;Resource management","program testing;software architecture;software reliability","software reliability;testing time allocation;architecture-based approach;rough-grained assignment","","41","","40","","","","","","IEEE","IEEE Journals & Magazines"
"FSM-based incremental conformance testing methods","K. EI-Fakih; N. Yevtushenko; G. Bochmann","Dept. of Comput. Sci., American Univ. of Sharjah, United Arab Emirates; NA; NA","IEEE Transactions on Software Engineering","","2004","30","7","425","436","The development of appropriate test cases is an important issue for conformance testing of protocol implementations and other reactive software systems. A number of methods are known for the development of a test suite based on a specification given in the form of a finite state machine. In practice, the system requirements evolve throughout the lifetime of the system and the specifications are modified incrementally. We adapt four well-known test derivation methods, namely, the HIS, W, Wp, and UIOv methods, for generating tests that would test only the modified parts of an evolving specification. Some application examples and experimental results are provided. These results show significant gains when using incremental testing in comparison with complete testing, especially when the modified part represents less than 20 percent of the whole specification.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318604","Protocol conformance testing;finite state machines;test derivation;incremental testing.","System testing;Protocols;Automata;Software testing;Software systems;Unified modeling language;Hardware;Programming","formal verification;conformance testing;protocols;finite state machines;formal specification;program testing","protocol conformance testing;reactive software systems;system requirements;finite state machine based incremental conformance testing methods;test derivation methods","","29","","23","","","","","","IEEE","IEEE Journals & Magazines"
"An experiment measuring the effects of personal software process (PSP) training","L. Prechelt; B. Unger","abaXXX Technol. AG, Stuttgart, Germany; NA","IEEE Transactions on Software Engineering","","2001","27","5","465","472","The personal software process is a process improvement methodology aimed at individual software engineers. It claims to improve software quality (in particular defect content), effort estimation capability, and process adaptation and improvement capabilities. We have tested some of these claims in an experiment comparing the performance of participants who had just previously received a PSP course to a different group of participants who had received other technical training instead. Each participant of both groups performed the same task. We found the following positive effects: the PSP group estimated their productivity (though not their effort) more accurately, made fewer trivial mistakes, and their programs performed more careful error-checking; further, the performance variability was smaller in the PSP group in various respects. However, the improvements are smaller than the PSP proponents usually assume, possibly due to the low actual usage of PSP techniques in the PSP group. We conjecture that PSP training alone does not automatically realize the PSP's potential benefits (as seen in some industrial PSP success stories) when programmers are left alone with motivating themselves to actually use the PSP techniques.","0098-5589;1939-3520;2326-3881","","10.1109/32.922716","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=922716","","Software measurement;Software quality;Productivity;Testing;Industrial training;Coordinate measuring machines;Data analysis;Management training;Programming profession;Quality management","software process improvement;training;software quality","experiment;personal software process training;software process improvement methodology;software quality;software effort estimation;productivity","","28","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling and verification of time dependent systems using time Petri nets","B. Berthomieu; M. Diaz","CNRS, Toulouse, France; CNRS, Toulouse, France","IEEE Transactions on Software Engineering","","1991","17","3","259","273","A description and analysis of concurrent systems, such as communication systems, whose behavior is dependent on explicit values of time is presented. An enumerative method is proposed in order to exhaustively validate the behavior of P. Merlin's time Petri net model, (1974). This method allows formal verification of time-dependent systems. It is applied to the specification and verification of the alternating bit protocol as a simple illustrative example.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.75415","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=75415","","Petri nets;Fires;Protocols;Reachability analysis","formal specification;parallel programming;Petri nets;program verification;protocols","verification;time dependent systems;time Petri nets;concurrent systems;communication systems;explicit values;formal verification;time-dependent systems;specification;alternating bit protocol","","590","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Goal-Centric Traceability: Using Virtual Plumblines to Maintain Critical Systemic Qualities","J. Cleland-Huang; W. Marrero; B. Berenbach","DePaul University, Chicago; DePaul University, Chicago; Siemens Corporate Research, Inc., Princeton","IEEE Transactions on Software Engineering","","2008","34","5","685","699","Successful software development involves the elicitation, implementation, and management of critical systemic requirements related to qualities such as security, usability, and performance. Unfortunately, even when such qualities are carefully incorporated into the initial design and implemented code, there are no guarantees that they will be consistently maintained throughout the lifetime of the software system. Even though it is well known that system qualities tend to erode as functional and environmental changes are introduced, existing regression testing techniques are primarily designed to test the impact of change upon system functionality rather than to evaluate how it might affect more global qualities. The concept of using goal-centric traceability to establish relationships between a set of strategically placed assessment models and system goals is introduced. This paper describes the process, algorithms, and techniques for utilizing goal models to establish executable traces between goals and assessment models, detect change impact points through the use of automated traceability techniques, propagate impact events, and assess the impact of change upon systemic qualities. The approach is illustrated through two case studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4553719","Management;Maintenance management;Management;Maintenance management","Usability;System testing;Programming;Quality management;Software development management;Security;Software systems;Change detection algorithms;Event detection;Real time systems","formal specification;program testing;regression analysis;software maintenance;software quality","goal-centric traceability;virtual plumblines;critical systemic qualities;software development;critical systemic requirements;software system lifetime;regression testing techniques;system functionality;automated traceability techniques","","18","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Fragtypes: a basis for programming environments","N. H. Madhavji","Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada","IEEE Transactions on Software Engineering","","1988","14","1","85","97","The author introduces a novel basis for programming environments that encourages development of software in fragments of various types, called fragtypes. Fragtypes range from a simple expression type to a complete subsystem type. As a result, they are suited to the development of software in an enlarged scope that includes both programming in the small and programming in the large. The author shows how proposed operations on fragtypes can achieve unusual effects on the software development process. Fragtypes and their associated construction rules form the basis of the programming environment MUPE-2, which is currently under development at McGill University. The target and the implementation language of this environment is the programming language Modula-2.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4625","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4625","","Programming environments;Software engineering;Power engineering and energy;Software systems;Systems engineering and theory;Councils;Computer science;Large-scale systems;Synthesizers;Utility programs","programming environments","programming environments;fragments;fragtypes;programming in the small;programming in the large;software development process;construction rules;MUPE-2;implementation language;programming language;Modula-2","","7","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic Software Updating Using a Relaxed Consistency Model","H. Chen; J. Yu; C. Hang; B. Zang; P. Yew","Fudan University, Shanghai; University of Michigan, Ann Arbor; Microsoft (China) Ltd., Shanghai; Fudan University, Shanghai; University of Minnesota at Twin Cities, Minneapolis","IEEE Transactions on Software Engineering","","2011","37","5","679","694","Software is inevitably subject to changes. There are patches and upgrades that close vulnerabilities, fix bugs, and evolve software with new features. Unfortunately, most traditional dynamic software updating approaches suffer some level of limitations; few of them can update multithreaded applications when involving data structure changes, while some of them lose binary compatibility or incur nonnegligible performance overhead. This paper presents POLUS, a software maintenance tool capable of iteratively evolving running unmodified multithreaded software into newer versions, yet with very low performance overhead. The main idea in POLUS is a relaxed consistency model that permits the concurrent activity of the old and new code. POLUS borrows the idea of cache-coherence protocol in computer architecture and uses a ”bidirectional write-through” synchronization protocol to ensure system consistency. To demonstrate the applicability of POLUS, we report our experience in using POLUS to dynamically update three prevalent server applications: vsftpd, sshd, and Apache HTTP server. Performance measurements show that POLUS incurs negligible runtime overhead on the three applications-a less than 1 percent performance degradation (but 5 percent for one case). The time to apply an update is also minimal.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5551162","Maintainability;reliability;runtime environments.","Software;Synchronization;Protocols;Bidirectional control;Registers;Runtime","computer architecture;hypermedia;multi-threading;program testing;software maintenance;software tools;transport protocols","dynamic software update;relaxed consistency model;data structure;binary compatibility;nonnegligible performance;POLUS;software maintenance tool;iteratively evolving running unmodified multithreaded software;concurrent activity;cache-coherence protocol;computer architecture;bidirectional write-through synchronization protocol;prevalent server application;HTTP server","","14","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Encapsulation of parallelism and architecture-independence in extensible database query execution","G. Graefe; D. L. Davison","Dept. of Comput. Sci., Portland State Univ., OR, USA; NA","IEEE Transactions on Software Engineering","","1993","19","8","749","764","Emerging database application domains demand not only high functionality, but also high performance. To satisfy these two requirements, the Volcano query execution engine combines the efficient use of parallelism on a wide variety of computer architectures with an extensible set of query processing operators that can be nested into arbitrarily complex query evaluation plans. Volcano's novel exchange operator permits designing, developing, debugging, and tuning data manipulation operators in single-process environments but executing them in various forms of parallelism. The exchange operator shields the data manipulation operators from all parallelism issues. The design and implementation of the generalized exchange operator are examined. The authors justify their decision to support hierarchical architectures and argue that the exchange operator offers a significant advantage for development and maintenance of database query processing software. They discuss the integration of bit vector filtering into the exchange operator paradigm with only minor modifications.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.238579","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=238579","","Encapsulation;Parallel processing;Query processing;Databases;Volcanoes;Computer architecture;Application software;Engines;Debugging;Software maintenance","distributed databases;parallel programming;query processing","extensible database query execution;database application domains;high functionality;high performance;Volcano query execution engine;parallelism;computer architectures;query processing operators;arbitrarily complex query evaluation plans;debugging;data manipulation operators;exchange operator;generalized exchange operator;hierarchical architectures;database query processing software;bit vector filtering","","12","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Experience using Web-based shotgun measures for large-system characterization and improvement","S. McLellan; A. Roesler; Z. Fei; S. Chandran; C. Spinuzzi","Schlumberger Lab. for Comput. Sci., Austin, TX, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1998","24","4","268","277","This article discusses our experience in using a World Wide Web-based shotgun measurement approach for mining and characterizing large software systems. The approach recognizes that measurement information is essentially management information, that different levels and functions of the organizational hierarchy require different information to make decisions, and that a measurement program is typically a discovery process about an organization's current modes of operations. What we found was the usefulness of a measurement program that also allows managers to dynamically formulate new goals and get answers to questions not specifically related to original goals but raised nonetheless by metric data. We describe three specific cases of decisions that were made using this approach and data collected from one large system and accessed using the company's intranet over the past two years.","0098-5589;1939-3520;2326-3881","","10.1109/32.677184","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=677184","","Software measurement;Software libraries;Current measurement;Application software;Systems engineering and theory;Software development management;Software tools;Software systems;Information management;Multidimensional systems","very large databases;software metrics;configuration management","Web-based shotgun measures;large-system characterization;measurement program;intranet;software mining;characterization;improvement paradigm;software metrics;shotgun measures;software reuse;process improvement;World Wide Web","","2","","21","","","","","","IEEE","IEEE Journals & Magazines"
"A design methodology for data-parallel applications","L. S. Nyland; J. F. Prins; A. Goldberg; P. H. Mills","Dept. of Comput. Sci., North Carolina Univ., Chapel Hill, NC, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","4","293","314","A methodology for the design and development of data-parallel applications and components is presented. Data-parallelism is a well understood form of parallel computation, yet developing simple applications can involve substantial efforts to express the problem in low level notations. We describe a process of software development for data-parallel applications starting from high level specifications, generating repeated refinements of designs to match different architectural models and performance constraints, enabling a development activity with cost benefit analysis. Primary issues are algorithm choice, correctness, and efficiency, followed by data decomposition, load balancing, and message passing coordination. Development of a data-parallel multitarget tracking application is used as a case study, showing the progression from high to low level refinements. We conclude by describing tool support for the process.","0098-5589;1939-3520;2326-3881","","10.1109/32.844491","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=844491","","Design methodology;Concurrent computing;Application software;Parallel processing;Milling machines;Parallel programming;Cost benefit analysis;Load management;Process design;Algorithm design and analysis","parallel programming;parallel algorithms;cost-benefit analysis;message passing;resource allocation","data-parallel application design;design methodology;data-parallelism;parallel computation;low level notations;high level specifications;repeated refinements;architectural models;performance constraints;development activity;cost benefit analysis;algorithm choice;data decomposition;load balancing;message passing coordination;data-parallel multitarget tracking application;case study;tool support","","8","","49","","","","","","IEEE","IEEE Journals & Magazines"
"The Gradient Model Load Balancing Method","F. C. H. Lin; R. M. Keller","ESL Inc.; NA","IEEE Transactions on Software Engineering","","1987","SE-13","1","32","38","A dynamic load balancing method is proposed for a class of large-diameter multiprocessor systems. The method is based on the ""gradient model,"" which entails transferring backlogged tasks to nearby idle processors according to a pressure gradient indirectly established by requests from idle processors. The algorithm is fully distributed and asynchronous. Global balance is achieved by successive refinements of many localized balances. The gradient model is formulated so as to be independent of system topology.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232563","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702130","Applicative systems;computer architecture;data flow;distributed systems;load balancing;multiprocessor systems;reduction architecture","Load modeling;Load management;Multiprocessing systems;Computer architecture;Throughput;Time measurement;Delay;Protocols;Expert systems;Topology","","Applicative systems;computer architecture;data flow;distributed systems;load balancing;multiprocessor systems;reduction architecture","","101","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Text Filtering and Ranking for Security Bug Report Prediction","F. Peters; T. Tun; Y. Yu; B. Nuseibeh","University of Limerick, Lero- The Irish Software Research Centre, Limerick, Co. Limerick Ireland (e-mail: fayolapeters@gmail.com); Department of Computing and Communications, The Open University, Milton Keynes, England United Kingdom of Great Britain and Northern Ireland (e-mail: Thein.Tun@open.ac.uk); Department of Computing and Communications, The Open University, Milton Keynes, England United Kingdom of Great Britain and Northern Ireland (e-mail: yijun.yu@open.ac.uk); University of Limerick, Lero - The Irish Software Research Centre, Limerick, Co. Limerick Ireland (e-mail: bashar.nuseibeh@lero.ie)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Security bug reports can describe security critical vulnerabilities in software. Bug tracking systems may contain thousands of bug reports, where relatively few of them are security related. Therefore finding unlabelled security bugs among them can be challenging. To help security engineers identify these reports quickly and accurately, text-based prediction models have been proposed. These can often mislabel security bug reports due to a number of reasons such as class imbalance, where the ratio of non-security to security bug reports is very high. More critically, we have observed that the presence of security related keywords in both security and non-security bug reports can lead to the mislabelling of security bug reports. This paper proposes FARSEC, a framework for filtering and ranking bug reports for reducing the presence of security related keywords. Before building prediction models, our framework identifies and removes non-security bug reports with security related keywords. We demonstrate that FARSEC improves the performance of text-based prediction models for security bug reports in 90% of cases. Specifically, we evaluate it with 45,940 bug reports from Chromium and four Apache projects. With our framework, we mitigate the class imbalance issue and reduce the number mislabelled security bug reports by 38%.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2787653","H2020 European Research Council; Science Foundation Ireland; EPSRC UK; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8240740","security cross words;security related keywords;security bug reports;text filtering;ranking;prediction models;transfer learning","Security;Computer bugs;Predictive models;Software;Data models;Measurement;Buildings","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"Abstractions for software architecture and tools to support them","M. Shaw; R. DeLine; D. V. Klein; T. L. Ross; D. M. Young; G. Zelesnik","Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1995","21","4","314","335","Architectures for software use rich abstractions and idioms to describe system components, the nature of interactions among the components, and the patterns that guide the composition of components into systems. These abstractions are higher level than the elements usually supported by programming languages and tools. They capture packaging and interaction issues as well as computational functionality. Well-established (if informal) patterns guide the architectural design of systems. We sketch a model for defining architectures and present an implementation of the basic level of that model. Our purpose is to support the abstractions used in practice by software designers. The implementation provides a testbed for experiments with a variety of system construction mechanisms. It distinguishes among different types of components and different ways these components can interact. It supports abstract interactions such as data flow and scheduling on the same footing as simple procedure call. It can express and check appropriate compatibility restrictions and configuration constraints. It accepts existing code as components, incurring no runtime overhead after initialization. It allows easy incorporation of specifications and associated analysis tools developed elsewhere. The implementation provides a base for extending the notation and validating the model.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.385970","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=385970","","Software architecture;Computer languages;Computer architecture;Software engineering;Computer science;Software design;Software tools;Packaging;System testing;Scheduling","software engineering;specification languages;software tools;scheduling;data flow computing","software architecture;software tools;abstractions;idioms;system component descriptions;component interactions;component composition;packaging;computational functionality;informal patterns;notation;software design;system construction mechanisms;abstract interactions;data flow;scheduling;model validation;compatibility restrictions;configuration constraints;initialization;specifications;analysis tools","","328","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Constrained expressions: Adding analysis capabilities to design methods for concurrent software systems","G. S. Avrunin; L. K. Dillon; J. C. Wileden; W. E. Riddle","Department of Mathematics and Statistics, University of Massachusetts, Amherst, MA 01003; Department of Computer and Information Science, University of Massachusetts, Amherst, MA 01003; Department of Computer Science, University of California, Santa Barbara, CA 93106; Department of Computer and Information Science, University of Massachusetts, Amherst, MA 01003; Software design & analysis, inc., Boulder, CO 80303","IEEE Transactions on Software Engineering","","1986","SE-12","2","278","292","An approach to the design of concurrent software systems based on the constrained expression formalism is described. This formalism provides a rigorous conceptual model for the semantics of concurrent computations, thereby supporting analysis of important system properties as part of the design process. This approach allows designers to use standard specification and design languages, rather than forcing them to deal with the formal model explicitly or directly. As a result, the approach attains the benefits of formal rigor without the associated pain of unnatural concepts or notations for its users. The conceptual model of concurrency underlying the constrained expression formalism treats the collection of possible behaviors of a concurrent system as a set of sequences of events. The constrained expression formalism provides a useful closed-form description of these sequences. Algorithms were developed for translating designs expressed in a wide variety of notations into these constrained expression descriptions. A number of powerful analysis techniques that can be applied to these descriptions have also been developed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312944","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312944","Ada-based design notation;analysis techniques;concurrent software systems;constrained expressions;design method;event-based","Design methodology;Software systems;Semantics;Concurrent computing;Educational institutions;Filtering;Computer languages","parallel processing;software engineering;specification languages","software engineering;analysis capabilities;design methods;concurrent software systems;constrained expression formalism;semantics;system properties;design process;specification;design languages;closed-form description","","18","","","","","","","","IEEE","IEEE Journals & Magazines"
"A transaction-based approach to vertical partitioning for relational database systems","W. W. Chu; I. T. Ieong","Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA; Dept. of Comput. Sci., California Univ., Los Angeles, CA, USA","IEEE Transactions on Software Engineering","","1993","19","8","804","812","An approach to vertical partitioning in relational databases in which the attributes of a relation are partitioned according to a set of transactions is proposed. The objective of vertical partitioning is to minimize the number of disk accesses in the system. Since transactions have more semantic meanings than attributes, this approach allows the optimization of the partitioning based on a selected set of important transactions. An optimal binary partitioning (OBP) algorithm based on the branch and bound method is presented, with the worst case complexity of O(2/sup n/), where n is the number of transactions. To handle systems with a large number of transactions, an algorithm BPi with complexity varying from O(n) to O(2/sup n/) is also developed. The experimental results reveal that the performance of vertical partitioning is sensitive to the skewness of transaction accesses. Further, BPi converges rather rapidly to OBP. Both OBP and BPi yield results comparable with that of global optimum obtained from an exhaustive search.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.238583","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=238583","","Relational databases;Partitioning algorithms;Transaction databases;Clustering algorithms;Delay;Information retrieval;Costs;Tree graphs;Integer linear programming","computational complexity;relational databases;transaction processing","transaction-based approach;vertical partitioning;relational databases;disk accesses;semantic meanings;optimal binary partitioning;OBP;branch and bound method;BPi;complexity;transaction accesses;global optimum","","29","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Motivation and Satisfaction of Software Engineers","C. França; F. Q. B. da Silva; H. Sharp","Departamento de Computação, Universidade Federal Rural de Pernambuco, 67744 Recife, Pernambuco Brazil (e-mail: cesarfranca@gmail.com); Centro de Informática, Universidade Federal de Pernambuco, 28116 Recife, Pernambuco Brazil (e-mail: fabio@cin.ufpe.br); Computing, The Open University, UK, Milton Keynes, Buckinghamshire United Kingdom of Great Britain and Northern Ireland MK7 6AA (e-mail: h.c.sharp@open.ac.uk)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Context: The proper management of people can help software organisations to achieve higher levels of success. However, the limited attention paid to the appropriate use of theories to underpin the research in this area leaves it unclear how to deal with human aspects of software engineers, such as motivation and satisfaction. Objectives: This article aims to expose what drives the motivation and satisfaction of software engineers at work. Methods: A multiple case study was conducted at four software organisations in Brazil. For 11 months, data was collected using semi-structured interviews, diary studies, and document analyses. Results: The Theory of Motivation and Satisfaction of Software Engineers (TMS-SE), presented in this article, combines elements from well established theories with new findings, and translates them into the software engineering context. Conclusion: The TMS-SE advances the understanding of people management in the software engineering field and presents a strong conceptual framework for future investigations in this area.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2842201","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8370133","Work motivation;Job satisfaction;Human resource management;Software Engineering","Software;Software engineering;Productivity;Interviews;Text analysis;Cultural differences;Electronic mail","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Specification of fault-tolerant system issues by predicate/transition nets and regular expressions-approach and case study","F. Belli; K. -. Grosspietsch","Dept. of Electr. & Electron. Eng., Paderborn Univ., St. Augustin, Germany; Dept. of Electr. & Electron. Eng., Paderborn Univ., St. Augustin, Germany","IEEE Transactions on Software Engineering","","1991","17","6","513","526","A method to systematically integrate fault tolerance properties into the design of complex software systems is presented. The method exploits a formal specification of the system in which the amount of necessary redundancy can be determined. The system description is based on a combination of a predicate/transition net with regular expressions. The net model provides a formal overview of the system behavior in general, supporting the correct understanding of potential concurrency in the system processes. Regular expressions are used to model the sequential behavior of single-system components in detail. Both model layers provide well-defined levels of error detection; the regular expressions enable the system designer to also determine and introduce redundancy to achieve error correction. The techniques used to describe and analyze system behavior are explained using a case study that contains a stepwise-refined specification and analysis of a multistory shelving system model that has been implemented using the method presented. It is shown that the method applies to any software system which is to be protected against the considered errors.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.87278","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=87278","","Fault tolerant systems;Software systems;Fault detection;Redundancy;Error correction;Hardware;Concurrent computing;Protection;Petri nets;Testing","fault tolerant computing;formal specification;Petri nets;software reliability","fault tolerance properties;complex software systems;formal specification;redundancy;system description;predicate/transition net;regular expressions;formal overview;system behavior;sequential behavior;single-system components;model layers;error detection;stepwise-refined specification;multistory shelving system model","","20","","19","","","","","","IEEE","IEEE Journals & Magazines"
"On the identification of covert storage channels in secure systems","C. -. Tsai; V. D. Gligor; C. S. Chandersekaran","VDG Inc., Chevy Chase, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","1990","16","6","569","580","A practical method for the identification of covert storage channels is presented and its application to the source code of the Secure Xenix kernel is illustrated. The method is based on the identification of all visible/alterable kernel variables by using information-flow analysis of language code. The method also requires that, after the sharing relationships among the kernel primitives and the visible/alterable variables are determined, the nondiscretionary access rules implemented by each primitive be applied to identify the potential storage channels. The method can be generalized to other implementation languages, and has the following advantages: it helps discover all potential storage channels is kernel code, thereby helping determine whether the nondiscretionary access rules are implemented correctly; it helps avoid discovery of false flow violations and their unnecessary analysis; and it helps identify the kernel locations where audit code and time-delay variables need to be placed for covert-channel handling.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.55086","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=55086","","Secure storage;Kernel;Information security;Communication channels;Information analysis;Communication system security;Timing;Control systems;Voice mail;Operating systems","operating systems (computers);security of data;software engineering","identification;covert storage channels;secure systems;source code;Secure Xenix kernel;visible/alterable kernel variables;information-flow analysis;language code;sharing relationships;nondiscretionary access rules;implementation languages;false flow violations;kernel locations;audit code;time-delay variables;covert-channel handling","","26","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Toward High Confidence Software","J. P. Cavano","Rome Air Development Center","IEEE Transactions on Software Engineering","","1985","SE-11","12","1449","1455","Moving toward high confidence software that can meet ever increasing demands for critical DOD applications will require planning, specifying, selecting, and managing the necessary development and testing activities that will ensure the success of the software project. In order to trust the decisions being made, there must be evidence (i.e., an information base of data and facts) that techniques and tools being chosen for application on critical projects will perform as expected. Today, these expectations are mostly intuitive; there is little hard evidence available to guide acquisition managers and software developers in making necessary decisions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232181","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701967","DOD applications;high confidence software;software reliability measurement methodology","Software reliability;US Department of Defense;Software development management;Software measurement;Application software;Project management;Software testing;Engineering management;Software safety;Programming","","DOD applications;high confidence software;software reliability measurement methodology","","4","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Patterns of Knowledge in API Reference Documentation","W. Maalej; M. P. Robillard","University of Hamburg, Germany; McGill University, Montréal","IEEE Transactions on Software Engineering","","2013","39","9","1264","1282","Reading reference documentation is an important part of programming with application programming interfaces (APIs). Reference documentation complements the API by providing information not obvious from the API syntax. To improve the quality of reference documentation and the efficiency with which the relevant information it contains can be accessed, we must first understand its content. We report on a study of the nature and organization of knowledge contained in the reference documentation of the hundreds of APIs provided as a part of two major technology platforms: Java SDK 6 and .NET 4.0. Our study involved the development of a taxonomy of knowledge types based on grounded methods and independent empirical validation. Seventeen trained coders used the taxonomy to rate a total of 5,574 randomly sampled documentation units to assess the knowledge they contain. Our results provide a comprehensive perspective on the patterns of knowledge in API documentation: observations about the types of knowledge it contains and how this knowledge is distributed throughout the documentation. The taxonomy and patterns of knowledge we present in this paper can be used to help practitioners evaluate the content of their API documentation, better organize their documentation, and limit the amount of low-value content. They also provide a vocabulary that can help structure and facilitate discussions about the content of APIs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6473801","API documentation;software documentation;empirical study;content analysis;grounded method;data mining;pattern mining;Java;.NET","Documentation;Taxonomy;Encoding;Reliability;Java;Software;Sociology","application program interfaces;learning (artificial intelligence);pattern classification","knowledge taxonomy;.NET 4.0 API;Java SDK 6 API;knowledge organization;knowledge nature;reference documentation efficiency;reference documentation quality;application program interface;API reference documentation;knowledge pattern","","32","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Reverse Engineering Input Syntactic Structure from Program Execution and Its Applications","Z. Lin; X. Zhang; D. Xu","Purdue University, West Lafayette; Purdue University, West Lafayette; Purdue University, West Lafayette","IEEE Transactions on Software Engineering","","2010","36","5","688","703","Program input syntactic structure is essential for a wide range of applications such as test case generation, software debugging, and network security. However, such important information is often not available (e.g., most malware programs make use of secret protocols to communicate) or not directly usable by machines (e.g., many programs specify their inputs in plain text or other random formats). Furthermore, many programs claim they accept inputs with a published format, but their implementations actually support a subset or a variant. Based on the observations that input structure is manifested by the way input symbols are used during execution and most programs take input with top-down or bottom-up grammars, we devise two dynamic analyses, one for each grammar category. Our evaluation on a set of real-world programs shows that our technique is able to precisely reverse engineer input syntactic structure from execution. We apply our technique to hierarchical delta debugging (HDD) and network protocol reverse engineering. Our technique enables the complete automation of HDD, in which programmers were originally required to provide input grammars, and improves the runtime performance of HDD. Our client study on network protocol reverse engineering also shows that our technique supersedes existing techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5210120","Input syntactic structure;reverse engineering;control dependence;grammar inference;delta debugging;top-down grammar;bottom-up grammar.","Reverse engineering;Application software;Protocols;Computer science;Software debugging;Information security;Runtime;XML;Software testing;Automation","data structures;grammars;program debugging;protocols;reverse engineering","program input syntactic structure;test case generation;software debugging;network security;bottom-up grammars;top-down grammars;hierarchical delta debugging;network protocol reverse engineering;HDD automation","","7","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis and refinement of software test data adequacy properties","A. Parrish; S. H. Zweben","Dept. of Comput. Sci., Alabama Univ., Tuscaloosa, AL, USA; NA","IEEE Transactions on Software Engineering","","1991","17","6","565","581","Test data adequacy criteria are standards that can be applied to decide if enough testing has been performed. Previous research in software testing has suggested 11 fundamental properties which reasonable criteria should satisfy if the criteria make use of the structure of the program being tested. It is shown that there are several dependencies among the 11 properties making them questionable as a set of fundamental properties, and that the statements of the properties can be generalized so that they can be appropriately analyzed with respect to criteria that do not necessarily make use of the program's structure. An analysis that shows the relationships among the properties with respect to different classes of criteria which utilize the program structure and the specification in different ways is discussed. It is shown how the properties differ under the two models in order to maintain consistency that the dependencies are largely a result of five very weak existential properties, and that by modifying three of the properties, these weaknesses can be eliminated. The result is a reduced set of seven properties, each of which is strong from a mathematical perspective.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.87282","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=87282","","Software testing;Performance evaluation;Performance analysis;Software standards;Standards development;Computer science;Information science","data integrity;formal specification;program testing;standards","software test data adequacy properties;standards;software testing;program structure;specification;consistency;weak existential properties","","19","","7","","","","","","IEEE","IEEE Journals & Magazines"
"The Effect of Pairs in Program Design Tasks","K. M. Lui; K. C. C. Chan; J. Nosek","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","197","211","Pair programming involves-two developers simultaneously collaborating with each other on the same programming task to design and code a solution. Algorithm design and its implementation are normally interwoven in that implementation often provides feedback to enhance the design. Previous controlled pair programming experiments did not explore the efficacy of pairs versus individuals in program design-related tasks separately from coding. Variations in programmer skills in a particular language or an integrated development environment and the understanding of programming instructions can mask the skill of subjects in program design-related tasks. Programming aptitude tests (PATs) have been shown to correlate with programming performance. PATs do not require understanding of programming instructions and do not require a skill in any specific computer language. Two controlled experiments were conducted, with full-time professional programmers being the subjects who worked on increasingly complex programming aptitude tasks related to problem solving and algorithmic design. In both experiments, pairs significantly outperformed individuals, providing evidence of the value of pairs in program design-related tasks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70755","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4378344","Experimental design;Programming teams;Experimental design;Programming teams","Programming profession;Algorithm design and analysis;Dynamic programming;Testing;Collaborative software;Collaborative work;Switches;Productivity;Time measurement;Collaboration","programming;reverse engineering","pair programming;program design task;integrated development environment;programming aptitude test;program understanding","","24","","62","","","","","","IEEE","IEEE Journals & Magazines"
"On the Evolution of Services","V. Andrikopoulos; S. Benbernou; M. P. Papazoglou","IAAS, University of Stuttgart, Stuttgart; Universit&#x0E9; Paris Descartes, Paris; ERISS, Tilburg University, Tilburg","IEEE Transactions on Software Engineering","","2012","38","3","609","628","In an environment of constant change and variation driven by competition and innovation, a software service can rarely remain stable. Being able to manage and control the evolution of services is therefore an important goal for the Service-Oriented paradigm. This work extends existing and widely adopted theories from software engineering, programming languages, service-oriented computing, and other related fields to provide the fundamental ingredients required to guarantee that spurious results and inconsistencies that may occur due to uncontrolled service changes are avoided. The paper provides a unifying theoretical framework for controlling the evolution of services that deals with structural, behavioral, and QoS level-induced service changes in a type-safe manner, ensuring correct versioning transitions so that previous clients can use a versioned service in a consistent manner.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728828","Services engineering;service evolution;versioning;service compatibility.","XML;Guidelines;Protocols;Business;Availability;Quality of service;Software","service-oriented architecture;Web services","software service;service-oriented paradigm;software engineering;programming languages;service-oriented computing","","60","","55","","","","","","IEEE","IEEE Journals & Magazines"
"ABYSS: an architecture for software protection","S. R. White; L. Comerford","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","1990","16","6","619","629","ABYSS (a basic Yorktown security system) is an architecture for protecting the execution of application software. It supports a uniform security service across the range of computing systems. The use of ABYSS in solving the software protection problem, especially in the lower end of the market, is discussed. Both current and planned software distribution channels are supportable by the architecture, and the system is nearly transparent to legitimate users. A novel use-once authorization mechanism, called a token, is introduced as a solution to the problem of providing authorizations without direct communication. Software vendors may use the system to obtain technical enforcement of virtually any terms and conditions of the sale of their software, including such things as rental software. Software may be transferred between systems, and backed up to guard against loss in case of failure. The problem of protecting software on these systems is discussed, and guidelines to its solution are offered.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.55090","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=55090","","Computer architecture;Software protection;Application software;Authorization;Software systems;Data security;Public key cryptography;Marketing and sales;Guidelines;Licenses","security of data;software engineering","software protection architecture;execution protection;software transfer;software back-up;loss guarding;ABYSS;a basic Yorktown security system;application software;uniform security service;computing systems;software distribution channels;use-once authorization;token;technical enforcement;rental software","","11","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Partition Analysis: A Method Combining Testing and Verification","D. J. Richardson; L. A. Clarke","Department of Computer and Information Science, University of Massachusetts; NA","IEEE Transactions on Software Engineering","","1985","SE-11","12","1477","1490","The partition analysis method compares a procedure's implementation to its specification, both to verify consistency between the two and to derive test data. Unlike most verification methods, partition analysis is applicable to a number of different types of specification languages, including both procedural and nonprocedural languages. It is thus applicable to high-level descriptions as well as to low-level designs. Partition analysis also improves upon existing testing criteria. These criteria usually consider only the implementation, but partition analysis selects test data that exercise both a procedure's intended behavior (as described in the specifications) and the structure of its implementation. To accomplish these goals, partition analysis divides or partitions a procedure's domain into subdomains in which all elements of each subdomain are treated uniformly by the specification and processed uniformly by the implementation. This partition divides the procedure domain into more manageable units. Information related to each subdomain is used to guide in the selection of test data and to verify consistency between the specification and the implementation. Moreover, the testing and verification processes are designed to enhance each other. Initial experimentation has shown that through the integration of testing and verification, as well as through the use of information derived from both the implementation and the specification, the partition analysis method is effective for evaluating program reliability. This paper describes the partition analysis method and reports the results obtained from an evaluation of its effectiveness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231892","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701971","Software testing;software verification;symbolic evaluation","Information analysis;Specification languages;Data analysis;Process design;Software testing;Computer errors;Information science","","Software testing;software verification;symbolic evaluation","","61","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Replicating and Re-Evaluating the Theory of Relative Defect-Proneness","M. D. Syer; M. Nagappan; B. Adams; A. E. Hassan","School of Computing, Queen’s University, Kingston, ON, Canada; School of Computing, Queen’s University, Kingston, ON, Canada; Genie Informatique et Genie Logiciel, Ecole Polytechnique de Montreal, Campus de l’Universite de Montreal; School of Computing, Queen’s University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2015","41","2","176","197","A good understanding of the factors impacting defects in software systems is essential for software practitioners, because it helps them prioritize quality improvement efforts (e.g., testing and code reviews). Defect prediction models are typically built using classification or regression analysis on product and/or process metrics collected at a single point in time (e.g., a release date). However, current defect prediction models only predict if a defect will occur, but not when, which makes the prioritization of software quality improvements efforts difficult. To address this problem, Koru et al. applied survival analysis techniques to a large number of software systems to study how size (i.e., lines of code) influences the probability that a source code module (e.g., class or file) will experience a defect at any given time. Given that 1) the work of Koru et al. has been instrumental to our understanding of the size-defect relationship, 2) the use of survival analysis in the context of defect modelling has not been well studied and 3) replication studies are an important component of balanced scholarly debate, we present a replication study of the work by Koru et al. In particular, we present the details necessary to use survival analysis in the context of defect modelling (such details were missing from the original paper by Koru et al.). We also explore how differences between the traditional domains of survival analysis (i.e., medicine and epidemiology) and defect modelling impact our understanding of the size-defect relationship. Practitioners and researchers considering the use of survival analysis should be aware of the implications of our findings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2361131","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6914599","Survival Analysis;Cox Models;Defect Modelling;Survival analysis;Cox models;defect modelling","Analytical models;Hazards;Software;Measurement;Data models;Mathematical model;Predictive models","program diagnostics;software quality;software reliability","relative defect-proneness theory;survival analysis techniques;source code module;size-defect relationship;defect modelling;software system defects","","3","","47","","","","","","IEEE","IEEE Journals & Magazines"
"A Space-Efficient Optimization of Call-by-Need","F. W. Burton; D. Maurer; H. -. Oberhauser; R. Wilhelm","Department of Computer Science, University of Utah; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","6","636","642","Call-by-need is widely regarded as an optimal (to within a constant factor) parameter passing mechanism for functional programming languages. Except for certain special cases involving higher order functions, call-by-need is optimal with respect to time. However, call-by-need is far from optimal with respect to space. We examine some of the space problems which can arise with call-by-need and other parameter passing mechanisms. A simple optimizing technique, based on work by Mycroft [1], is proposed. If it can be determined both that an expression must be evaluated eventually and that the evaluation of the expression is likely to reduce the space required by the program, then the evaluation is performed as soon as possible. This optimization does not result in optimal space performance in all cases. However, in most of the common cases where call-by-need causes a problem the proposed optimization avoids the problem. Since our technique is not always optimal, it is likely to be of greatest advantage in situations where efficiency is important but not critical. For example, functional languages with call-by-name semantics are increasingly being used as specification languages. Since such a specification is runnable, it may be used as a prototype. This makes it possible to experiment with a program and refine the specification before the implementation in the target language is started.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233474","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702269","Call-by-need;functional programming;optimization;space;strictness","Functional programming;Calculus;Performance evaluation;Specification languages;Prototypes;Parallel processing;Computer science;Cities and towns;Program processors","","Call-by-need;functional programming;optimization;space;strictness","","1","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Automated software test data generation","B. Korel","Dept. of Comput. Sci., Wayne State Univ., Detroit, MI, USA","IEEE Transactions on Software Engineering","","1990","16","8","870","879","An alternative approach to test-data generation based on actual execution of the program under test, function-minimization methods and dynamic data-flow analysis is presented. Test data are developed for the program using actual values of input variables. When the program is executed, the program execution flow is monitored. If during program execution an undesirable execution flow is observed then function-minimization search algorithms are used to automatically locate the values of input variables for which the selected path is traversed. In addition, dynamic data-flow analysis is used to determine those input variables responsible for the undesirable program behavior, significantly increasing the speed of the search process. The approach to generating test data is then extended to programs with dynamic data structures and a search method based on dynamic data-flow analysis and backtracking is presented. In the approach described, values of array indexes and pointers are known at each step of program execution; this information is used to overcome difficulties of array and pointer handling.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.57624","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=57624","","Automatic testing;Software testing;Data analysis;Input variables;Minimization methods;Costs;Monitoring;Automatic control;Data structures;Search methods","automatic programming;data structures;minimisation;program testing;search problems","automated software test data generation;function-minimization methods;dynamic data-flow analysis;input variables;program execution flow;function-minimization search algorithms;input variables;program behavior;dynamic data structures;backtracking;array indexes;pointers","","412","","31","","","","","","IEEE","IEEE Journals & Magazines"
"The evolving philosophers problem: dynamic change management","J. Kramer; J. Magee","Imperial Coll. of Sci. Technol. & Med., London Univ., UK; Imperial Coll. of Sci. Technol. & Med., London Univ., UK","IEEE Transactions on Software Engineering","","1990","16","11","1293","1306","A model for dynamic change management which separates structural concerns from component application concerns is presented. This separation of concerns permits the formulation of general structural rules for change at the configuration level without the need to consider application state, and the specification of application component actions without prior knowledge of the actual structural changes which may be introduced. In addition, the changes can be applied in such a way so as to leave the modified system in a consistent state, and cause no disturbance to the unaffected part of the operational system. The model is applied to an example problem, 'evolving philosophers'. The principles of this model have been implemented and tested in the Conic environment for distributed systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60317","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60317","","Application software;Functional programming;System testing;Computer industry;Humans;Software systems","software engineering","concerns separation;evolving philosophers problem;dynamic change management;structural concerns;component application concerns;separation of concerns;structural rules;configuration level;consistent state;Conic environment;distributed systems","","373","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic analysis and test case derivation for a restricted class of LOTOS expressions with data parameters","T. Higashino; G. v. Bochmann","Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada; Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada","IEEE Transactions on Software Engineering","","1994","20","1","29","42","We propose an automatic analysis and test case derivation method for LOTOS expressions with data values. We introduce the class of P-LOTOS expressions where the data types are restricted to Presburger arithmetic. That is, only the integer and Boolean types are used, and the operators of the integers are restricted to addition, subtraction, and comparison. For this class, we give an algorithm for deriving a set of test cases (a test suite). The algorithm is carried out by using a decision procedure for integer linear programming problems. We also give solutions for the deadlock detection problem, the detection of nonexecutable branches, and the detection of nondeterministic behaviors. We have implemented a tool for the analysis and test selection based on our techniques. The derivation of a test suite for a simplified Session protocol is described as an example.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.263753","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=263753","","Automatic testing;Computer aided software engineering;Protocols;Automata;System recovery;System testing;Open systems;Context;Software testing;Arithmetic","specification languages;formal specification;linear programming;integer programming;concurrency control","test case derivation;LOTOS expressions;data parameters;automatic analysis method;data values;P-LOTOS expressions;data types;Presburger arithmetic;integer;Boolean types;addition;subtraction;comparison;decision procedure;integer linear programming problems;deadlock detection problem;nonexecutable branch detection;nondeterministic behavior detection;test selection;simplified Session protocol;specification language","","17","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A visual language compiler","S. -. Chang; M. J. Tauber; B. Yu; J. -. Yu","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","5","506","525","The SIL-ICON compiler is a software system for the specification, interpretation, prototyping, and generation of icon-oriented systems. The system design of the SIL-ICON compiler is presented. The icon system G, the icon dictionary ID, the operator dictionary OD, and the extended task action grammar ETAG are described. An application example to design a text editor using the Heidelberg icon set is also presented in detail.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24700","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24700","","Dictionaries;Software systems;Software prototyping;Computer science;Silicon compiler;Application software;Programming environments;Algebra;Fuzzy set theory;Fuzzy systems","computer graphics;program compilers;user interfaces","visual language compiler;SIL-ICON compiler;software system;specification;interpretation;prototyping;generation;icon-oriented systems;system design;icon system;G;icon dictionary;ID;operator dictionary;OD;extended task action grammar;ETAG;text editor;Heidelberg icon set","","50","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluation and Measurement of Software Process Improvement—A Systematic Literature Review","M. Unterkalmsteiner; T. Gorschek; A. K. M. M. Islam; C. K. Cheng; R. B. Permadi; R. Feldt","Blekinge Institute of Technology, Karlskrona; Blekinge Institute of Technology, Karlskrona; University of Kaiserslautern, Kaiserslautern; General Electrics Healthcare, Freiburg; Amadeus S.A.S, Sophia Antipolis; Blekinge Institute of Technology, Karlskrona","IEEE Transactions on Software Engineering","","2012","38","2","398","424","BACKGROUND-Software Process Improvement (SPI) is a systematic approach to increase the efficiency and effectiveness of a software development organization and to enhance software products. OBJECTIVE-This paper aims to identify and characterize evaluation strategies and measurements used to assess the impact of different SPI initiatives. METHOD-The systematic literature review includes 148 papers published between 1991 and 2008. The selected papers were classified according to SPI initiative, applied evaluation strategies, and measurement perspectives. Potential confounding factors interfering with the evaluation of the improvement effort were assessed. RESULTS-Seven distinct evaluation strategies were identified, wherein the most common one, “Pre-Post Comparison,” was applied in 49 percent of the inspected papers. Quality was the most measured attribute (62 percent), followed by Cost (41 percent), and Schedule (18 percent). Looking at measurement perspectives, “Project” represents the majority with 66 percent. CONCLUSION-The evaluation validity of SPI initiatives is challenged by the scarce consideration of potential confounding factors, particularly given that “Pre-Post Comparison” was identified as the most common evaluation strategy, and the inaccurate descriptions of the evaluation context. Measurements to assess the short and mid-term impact of SPI initiatives prevail, whereas long-term measurements in terms of customer satisfaction and return on investment tend to be less used.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728832","Process implementation and change;process measurement;metrics/measurement;systematic literature review.","Software;Software measurement;Systematics;Current measurement;Data mining;Organizations","software process improvement","software process improvement;SPI;software development organization;customer satisfaction;return on investment","","77","","266","","","","","","IEEE","IEEE Journals & Magazines"
"A scenario-matching approach to the description and model checking of real-time properties","V. Braberman; N. Kicillof; A. Olivero","Comput. Sci. Dept., Buenos Aires Univ., Argentina; Comput. Sci. Dept., Buenos Aires Univ., Argentina; NA","IEEE Transactions on Software Engineering","","2005","31","12","1028","1041","A major obstacle in the technology transfer agenda of behavioral analysis and design methods is the need for logics or automata to express properties for control-intensive systems. Interaction-modeling notations may offer a replacement or a complement, with a practitioner-appealing and lightweight flavor, due partly to the sub specification of intended behavior by means of scenarios. We propose a novel approach consisting of engineering a new formal notation of this sort based on a simple compact declarative semantics: VTS (visual timed event scenarios). Scenarios represent event patterns, graphically depicting conditions over traces. They predicate general system events and provide features to describe complex properties not expressible with MSC-like notations. The underlying formalism supports partial orders and real-time constraints. The problem of checking whether a timed-automaton model has a matching trace is proven decidable. On top of this kernel, we introduce a notation to state properties over all system traces: conditional scenarios, allowing engineers to describe uniquely rich connections between antecedent and consequent portions of the scenario. An undecidability result is presented for the general case of the model-checking problem over dense-time domains, to later identify a decidable-yet practically relevant-subclass, where verification is solvable by generating antiscenarios expressed in the VTS-kernel notation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.131","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566605","Index Terms- Requirements/specifications;model checking;formal methods;scenario-based verification.","Real time systems;Design methodology;Logic design;Automata;Automatic control;Control system synthesis;Kernel;Computer industry;Electrical equipment industry;Control systems","formal verification;formal specification;automata theory;real-time systems","scenario-matching approach;model checking;behavioral analysis;design methods;control-intensive system;interaction-modeling;visual timed event scenarios;event patterns;real-time constraints;timed-automaton model;formal verification","","14","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Where Do Configuration Constraints Stem From? An Extraction Approach and an Empirical Study","S. Nadi; T. Berger; C. Kästner; K. Czarnecki","Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Hessen, Germany; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON","IEEE Transactions on Software Engineering","","2015","41","8","820","841","Highly configurable systems allow users to tailor software to specific needs. Valid combinations of configuration options are often restricted by intricate constraints. Describing options and constraints in a variability model allows reasoning about the supported configurations. To automate creating and verifying such models, we need to identify the origin of such constraints. We propose a static analysis approach, based on two rules, to extract configuration constraints from code. We apply it on four highly configurable systems to evaluate the accuracy of our approach and to determine which constraints are recoverable from the code. We find that our approach is highly accurate (93% and 77% respectively) and that we can recover 28% of existing constraints. We complement our approach with a qualitative study to identify constraint sources, triangulating results from our automatic extraction, manual inspections, and interviews with 27 developers. We find that, apart from low-level implementation dependencies, configuration constraints enforce correct runtime behavior, improve users' configuration experience, and prevent corner cases. While the majority of constraints is extractable from code, our results indicate that creating a complete model requires further substantial domain knowledge and testing. Our results aim at supporting researchers and practitioners working on variability model engineering, evolution, and verification techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2415793","NSERC; ARTEMIS JU; NSF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7065312","Variability models;Reverse-engineering;qualitative studies;Variability models;reverse-engineering;qualitative studies;static analyses;configuration constraints","Feature extraction;Kernel;Accuracy;Linux;Manuals;Interviews","configuration management;program diagnostics","configuration constraints;extraction approach;configuration combination;variability model;static analysis approach;configuration constraints extraction;constraint sources identification;variability model engineering;variability model evolution;variability model verification techniques","","13","","80","","","","","","IEEE","IEEE Journals & Magazines"
"Some Theory Concerning Certification of Mathematical Subroutines by Black Box Testing","R. P. Roe; J. H. Rowland","Department of Mathematics, Fort Lewis College; NA","IEEE Transactions on Software Engineering","","1987","SE-13","6","677","682","Several inequalities are derived for use in certifying function subroutines by means of black box testing. It is assumed that a function is approximated by means of a polynomial of limited degree on a closed interval. These inequalities give upper bounds on the error of the approximation over the entire interval based on the error measured over a finite sample and known properties of the function.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233205","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702273","Certification;sampling;testing","Certification;Algorithms;Testing;Polynomials;Sampling methods;Mathematics;Upper bound;Computer languages;NIST;Differential equations","","Certification;sampling;testing","","2","","21","","","","","","IEEE","IEEE Journals & Magazines"
"A Recursive Solution Method to Analyze the Performance of Static Locking Systems","A. Thomasian; In Kyung Ryu","IBM Thomas J. Watson Researeh Center, P.O. Box 704, Yorktown Heights, NY 10598.; NA","IEEE Transactions on Software Engineering","","1989","15","10","1147","1156","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1989.559761","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=559761","","Performance analysis;Transaction databases;Concurrency control;Scheduling;System recovery;Indexes;Control systems;Database systems;Hardware","","Concurrency control;database systems;performance evaluation;queueing networks","","6","","22","","","","","","IEEE","IEEE Journals & Magazines"
"SEES—A Software testing Environment Support System","N. Roussopoulos; R. T. Yeh","Department of Computer Science, University of Maryland; NA","IEEE Transactions on Software Engineering","","1985","SE-11","4","355","366","SEES is a database system to support program testing. The program database is automatically created during the compilation of the program by a compiler built using the YACC compiler-compiler.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232225","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702018","","Software testing;Programming profession;Computer architecture;Relational databases;Error correction;Database systems;Program processors;Writing;Workstations;Software tools","","","","2","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Testing homogeneous spreadsheet grids with the ""what you see is what you test"" methodology","M. Burnett; A. Sheretov; Bing Ren; G. Rothermel","Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","6","576","594","Although there has been recent research into ways to design environments that enable end users to create their own programs, little attention has been given to helping these end users systematically test their programs. To help address this need in spreadsheet systems (the most widely used type of end-user programming language), we previously introduced a visual approach to systematically testing individual cells in spreadsheet systems. However, the previous approach did not scale well in the presence of largely homogeneous grids, which introduce problems somewhat analogous to the array-testing problems of imperative programs. We present two approaches to spreadsheet testing that explicitly support such grids. We present the algorithms, time complexities, and performance data comparing the two approaches. This is part of our continuing work to bring to end users at least some of the benefits of formalized notions of testing without requiring knowledge of testing beyond a naive level.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010060","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010060","","System testing;Computer Society;Computer languages;Data visualization;Graphical user interfaces;Standards development;Error analysis;Proposals;Programming;Spreadsheet programs","spreadsheet programs;program testing;visual programming","homogeneous spreadsheet grid testing;design environments;end users;spreadsheet systems;end-user programming language;visual approach;homogeneous grids;array-testing problems;imperative programs;spreadsheet testing;time complexities;performance data;formalized notions;software testing;visual programming","","24","","32","","","","","","IEEE","IEEE Journals & Magazines"
"HYDRA: Massively Compositional Model for Cross-Project Defect Prediction","X. Xia; D. Lo; S. J. Pan; N. Nagappan; X. Wang","College of Computer Science and Technology, Zhejiang University Hangzhou, Zhejiang, China; School of Information Systems, Singapore Management University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; Testing, Verification and Measurement Research, Microsoft Research, Redmond, WA; College of Computer Science and Technology, Zhejiang University Hangzhou, Zhejiang, China","IEEE Transactions on Software Engineering","","2016","42","10","977","998","Most software defect prediction approaches are trained and applied on data from the same project. However, often a new project does not have enough training data. Cross-project defect prediction, which uses data from other projects to predict defects in a particular project, provides a new perspective to defect prediction. In this work, we propose a HYbrid moDel Reconstruction Approach (HYDRA) for cross-project defect prediction, which includes two phases: genetic algorithm (GA) phase and ensemble learning (EL) phase. These two phases create a massive composition of classifiers. To examine the benefits of HYDRA, we perform experiments on 29 datasets from the PROMISE repository which contains a total of 11,196 instances (i.e., Java classes) labeled as defective or clean. We experiment with logistic regression as the underlying classification algorithm of HYDRA. We compare our approach with the most recently proposed cross-project defect prediction approaches: TCA+ by Nam et al., Peters filter by Peters et al., GP by Liu et al., MO by Canfora et al., and CODEP by Panichella et al. Our results show that HYDRA achieves an average F1-score of 0.544. On average, across the 29 datasets, these results correspond to an improvement in the F1-scores of 26.22 , 34.99, 47.43, 28.61, and 30.14 percent over TCA+, Peters filter, GP, MO, and CODEP, respectively. In addition, HYDRA on average can discover 33 percent of all bugs if developers inspect the top 20 percent lines of code, which improves the best baseline approach (TCA+) by 44.41 percent. We also find that HYDRA improves the F1-score of Zero-R which predict all the instances to be defective by 5.42 percent, but improves Zero-R by 58.65 percent when inspecting the top 20 percent lines of code. In practice, Zero-R can be hard to use since it simply predicts all of the instances to be defective, and thus developers have to inspect all of the instances to find the defective ones. Moreover, we notice the improvement of HYDRA over other baseline approaches in terms of F1-score and when inspecting the top 20 percent lines of code are substantial, and in most cases the improvements are significant and have large effect sizes across the 29 datasets.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2543218","National Basic Research Program of China; NSFC; National Key Technology R&D Program; Ministry of Science and Technology of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7435328","Cross-project defect prediction;transfer learning;genetic algorithm;ensemble learning","Genetic algorithms;Predictive models;Training;Buildings;Architecture;Data models;Measurement","genetic algorithms;learning (artificial intelligence);pattern classification;regression analysis;software fault tolerance","HYDRA model;massively compositional model;cross-project defect prediction;software defect prediction approach;hybrid model reconstruction approach;genetic algorithm phase;GA phase;phase and ensemble learning phase;EL phase;PROMISE repository;logistic regression;classification algorithm","","44","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Real-time specification using Lucid","D. B. Skillicorn; J. I. Glasgow","Dept. of Comput. & Inf. Sci., Queen's Univ., Kingston, Ont., Canada; Dept. of Comput. & Inf. Sci., Queen's Univ., Kingston, Ont., Canada","IEEE Transactions on Software Engineering","","1989","15","2","221","229","A methodology is presented for transforming a functional specification written in Lucid to an equivalent specification that captures its real-time properties. The enhanced specification consists of a set of equations that can be solved for several properties, including execution time and external requirements, or may simply be checked for the existence of a solution. Lucid has a set of meaning-preserving transformations, and a proof system corresponding to a behavioral semantics has been constructed. Both of these tools can be used to reason about properties of the specification. The specification is executable and can be used as a prototype for the system being specified. It is possible to express architectural constraints within the same formal framework. Thus this type of specification can be used to guide the development of new real-time systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21748","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21748","","Life testing;Lab-on-a-chip;Software testing;Random variables;Cost function;Software systems;System testing;Computer errors;Life estimation;Lifetime estimation","formal specification;real-time systems","Lucid;functional specification;real-time properties;execution time;external requirements;meaning-preserving transformations;proof system;behavioral semantics;architectural constraints;real-time systems","","12","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Communication and Synchronization Primitives for Distributed Programs","N. Natarajan","Department of Computer Science and Engineering, University of Texas at Arlington","IEEE Transactions on Software Engineering","","1985","SE-11","4","396","416","A distributed program is a collection of several processes which execute concurrently, possibly in different nodes of a distributed system, and which cooperate with each other to realize a common goal. In this paper, we present a design of communication and synchronization primitives for distributed programs. The primitives are designed such that they can be provided by a kernel of a distributed operating system. An important feature of the design is that the configuration of a process, i.e., identities of processes with which the process communicates, is specified separately from the computation performed by the process. This permits easy configuration and reconfiguration of processes. We identify different kinds of communication failures, and provide distinct mechanisms for handling them. The communication primitives are not atomic actions. To enable the construction of atomic actions, two new program components, atomic agent and manager are introduced. These are devoid of policy decisions regarding concurrency control and atomic commitment. We introduce the notion of conflicts relation using which a designer can construct either an optimistic or a pessimistic concurrency control scheme. The design also incorporates primitives for constructing nested atomic actions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232229","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702022","Atomic action;communication failure;computing agent;distributed operating system;distributed piogram;distributed system;kernel;port","Kernel;Operating systems;Concurrency control;Distributed computing;Design optimization;Programming;Computer science;Computer networks;Centralized control;Context","","Atomic action;communication failure;computing agent;distributed operating system;distributed piogram;distributed system;kernel;port","","4","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Advanced exception handling mechanisms","P. A. Buhr; W. Y. R. Mok","Dept. of Comput. Sci., Waterloo Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","2000","26","9","820","836","It is no longer possible to consider exception handling as a secondary issue in language design, or even worse, a mechanism added after the fact via a library approach. Exception handling is a primary feature in language design and must be integrated with other major features, including advanced control flow, objects, coroutines, concurrency, real-time, and polymorphism. Integration is crucial as there are both obvious and subtle interactions between exception handling and other language features. Unfortunately, many exception handling mechanisms work only with a subset of the features and in the sequential domain. A framework for a comprehensive, easy to use, and extensible exception handling mechanism is presented for a concurrent, object-oriented environment. The environment includes language constructs with separate execution stacks, e.g. coroutines and tasks, so the exception environment is significantly more complex than the normal single-stack situation. The pros and cons of various exception features are examined, along with feature interaction with other language mechanisms. Both exception termination and resumption models are examined in this environment, and previous criticisms of the resumption model, a feature commonly missing in modern languages, are addressed.","0098-5589;1939-3520;2326-3881","","10.1109/32.877844","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877844","","Testing;Object oriented modeling;Libraries;Concurrent computing;Robustness;Process control;Writing;Robust control;Programming profession","exception handling;object-oriented programming;high level languages;multiprocessing programs","advanced exception handling mechanisms;language design;advanced control flow;objects;coroutines;concurrency;real-time systems;polymorphism;concurrent object-oriented environment;language constructs;execution stacks;feature interaction;exception termination model;exception resumption model","","24","","","","","","","","IEEE","IEEE Journals & Magazines"
"Multilevel data structures: models and performance","A. Moitra; S. S. Iyengar; F. B. Bastani; I. L. Yen","Dept. of Comput. Sci., Cornell Univ., Ithaca, NY, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1988","14","6","858","867","A stepwise method of deriving the high-performance implementation of a set of operations is proposed. This method is based on the ability to organize the data into a multilevel data structure to provide an efficient implementation of all the operations. Typically, for such data organization the performance may deteriorate over a period of time and that can be corrected by reorganizing the data. This data reorganization is done by the introduction of maintenance processes. For a particular example, the multilevel data organization and the different models of maintenance processes possible are considered. The various models of maintenance process provide varying amounts of concurrency by varying the degree of atomicity in different operations. Performance behavior for the different models is derived and a correctness proof for the developed implementation is outlined.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6164","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6164","","Data structures;Concurrent computing;Computer science;Processor scheduling;Binary search trees;Distributed computing;Tree data structures;Information science;Delay;Tail","data structures;program verification;programming theory;software reliability","program verification;multilevel data structure;data organization;maintenance processes;concurrency;correctness proof","","3","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Design and Implementation of Secure Xenix","V. D. Gligor; C. S. Chandersekaran; R. S. Chapman; L. J. Dotterer; M. S. Hetch; Wen-Der Jiang; A. Johri; G. L. Luckenbaugh; N. Vasudevan","Department of Electrical Engineering, University of Maryland; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","2","208","221","Secure Xenix™ is an experimental system designed to run on IBM PC/AT workstations. Like Xenix, it is a Unix™ System V implementation on the PC/AT workstation; unlike Xenix, it eliminates the Unix security deficiencies and it enhances security policies. In this paper, we present the design features of Secure Xenix, their integration within Xenix, and some of the lessons learned from this experiment to date.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232893","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702201","Access control lists;auditor;model interpretation;operating system security;secure attention key;security levels;trusted path;Unix;Xenix","Workstations;Operating systems;Project management;Trademarks;US Government;Information security;Testing;Kernel","","Access control lists;auditor;model interpretation;operating system security;secure attention key;security levels;trusted path;Unix;Xenix","","16","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Search-Based Crash Reproduction and Its Impact on Debugging","M. Soltani; A. Panichella; A. Van Deursen","Software Technology, Technische Universiteit Delft, 2860 Delft, Delft Netherlands (e-mail: m.soltani@tudelft.nl); Centre Interdisciplinary for Security, Reliability and Trust, Universite du Luxembourg, 81872 Luxembourg, Luxembourg Luxembourg L-2721 (e-mail: anni.panico@gmail.com); EEMCS / ST / SE, Delft University of Technology, Delft, ZH Netherlands 2628 CD (e-mail: arie.vandeursen@tudelft.nl)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Software systems fail. These failures are often reported to issue tracking systems, where they are prioritized and assigned to responsible developers to be investigated. When developers debug software, they need to reproduce the reported failure in order to verify whether their fix actually prevents the failure from happening again. Since manually reproducing each failure could be a complex task, several automated techniques have been proposed to tackle this problem. Despite showing advancements in this area, the proposed techniques showed various types of limitations. In this paper, we present EvoCrash, a new approach to automated crash reproduction based on a novel evolutionary algorithm, called Guided Genetic Algorithm (GGA). We report on our empirical study on using EvoCrash to reproduce 54 real-world crashes, as well as the results of a controlled experiment, involving human participants, to assess the impact of EvoCrash tests in debugging. Based on our results, EvoCrash outperforms state-of-the-art techniques in crash reproduction and uncovers failures that are undetected by classical coverage-based unit test generation tools. In addition, we observed that using EvoCrash helps developers provide fixes more often and take less time when debugging, compared to developers debugging without using EvoCrash tests.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2877664","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8502801","Search-Based Software Testing;Genetic Algorithms;Automated Crash Reproduction;Empirical Software Engineering","Debugging;Tools;Software;Core dumps;Explosions;Computer bugs","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Software CAD: a revolutionary approach","R. J. A. Buhr; G. M. Karam; C. J. Hayes; C. M. Woodside","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottowa, Ont., Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottowa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","1989","15","3","235","249","A research project is described in which an experimental software CAD environment called the Carleton embedded system design environment (CAEDE), oriented toward embedded systems and Ada, was developed to provide a demonstration of the concept and to serve as a research testbed. The major contribution of CAEDE is a demonstration of a visual paradigm which combines semantic depth and syntactic shallowness, relative to Ada, in a manner that makes it possible for the embedded-system designer to work in terms of abstract machines while still thinking Ada. A secondary contribution is the identification of Prolog as a promising approach for supporting tool development in an environment which supports the visual paradigm. Also described are experimental tools for temporal analysis, performance analysis, and the generation of skeleton Ada code.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21752","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21752","","Design automation;Embedded system;Software tools;Embedded software;Performance analysis;Skeleton;Computer graphics;Design engineering;Sun;Testing","Ada;automatic programming;CAD;programming environments;software engineering;software tools","automatic programming;research project;software CAD environment;Carleton embedded system design environment;CAEDE;embedded systems;Ada;visual paradigm;semantic depth;syntactic shallowness;abstract machines;Prolog;tool development;temporal analysis;performance analysis;skeleton Ada code","","24","","58","","","","","","IEEE","IEEE Journals & Magazines"
"A formal framework for ASTRAL intralevel proof obligations","A. Coen-Porisini; R. A. Kemmerer; D. Mandrioli","Dipartimento di Elettronica e Inf., Politecnico di Milano, Italy; NA; NA","IEEE Transactions on Software Engineering","","1994","20","8","548","561","ASTRAL is a formal specification language for real-time systems. It is intended to support formal software development, and therefore has been formally defined. This paper focuses on how to formally prove the mathematical correctness of ASTRAL specifications. ASTRAL is provided with structuring mechanisms that allow one to build modularized specifications of complex systems with layering. In this paper, further details of the ASTRAL environment components and the critical requirements components, which were not fully developed in previous papers, are presented. Formal proofs in ASTRAL can be divided into two categories: interlevel proofs and intralevel proofs. The former deal with proving that the specification of level i+1 is consistent with the specification of level i, and the latter deal with proving that the specification of level i is consistent and satisfies the stated critical requirements. This paper concentrates on intralevel proofs.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310665","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310665","","Real time systems;Formal specifications;Programming;Timing;Automata;Logic functions;Laboratories;Software;Computer science","formal specification;real-time systems;finite state machines;program verification;specification languages","intralevel proof obligations;ASTRAL;formal specification language;real-time systems;formal software development;mathematical correctness;formal methods;formal specification;verification;timing requirements;formal proof;state machines;ASLAN;TRIO","","10","","13","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic detection and exploitation of branch constraints for timing analysis","C. A. Healy; D. B. Whalley","Comput. Sci. Dept., Furman Univ., Greenville, SC, USA; NA","IEEE Transactions on Software Engineering","","2002","28","8","763","781","Predicting the worst-case execution time (WCET) and best-case execution time (BCET) of a real-time program is a challenging task. Though much progress has been made in obtaining tighter timing predictions by using techniques that model the architectural features of a machine, significant overestimations of WCET and underestimations of GCET can still occur. Even with perfect architectural modeling, dependencies on data values can constrain the outcome of conditional branches and the corresponding set of paths that can be taken in a program. While branch constraint information has been used in the past by some timing analyzers, it has typically been specified manually, which is both tedious and error prone. This paper describes efficient techniques for automatically detecting branch constraints by a compiler and automatically exploiting these constraints within a timing analyzer. The result is significantly tighter timing analysis predictions without requiring additional interaction with a user.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027799","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027799","","Timing;Automatic control;Performance analysis;Information analysis;Real time systems;Computer Society;Predictive models;Time measurement;Programming profession;Pipelines","timing;real-time systems;program testing;program compilers","branch constraints;timing analysis;worst-case execution time;best-case execution time;real-time program;architectural features;branch constraint information;compiler;real-time systems","","19","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Automated aspect-oriented decomposition of process-control systems for ultra-high dependability assurance","D. Wang; F. B. Bastani; L. -. Yen","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA","IEEE Transactions on Software Engineering","","2005","31","9","713","732","This paper presents a method for decomposing process-control systems. This decomposition method is automated, meaning that a series of principles that can be evolved to support automated tools are given to help a designer decompose complex systems into a collection of simpler components. Each component resulting from the decomposition process can be designed and implemented independently of the other components. Also, these components can be tested or verified by the end-user independently of each other. Moreover, the system properties, such as safety, stability, and reliability, can be mathematically inferred from the properties of the individual components. These components are referred to as IDEAL (independently developable end-user assessable logical) components. This decomposition method is applied to a case study specified by the High-Integrity Systems group at Sandia National Labs, which involves the control of a future version of the Bay Area Rapid Transit (BART) system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.99","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514442","Index Terms- Software decomposition;dependability assurance;process-control systems;aspect-oriented modeling.","Software safety;Testing;State-space methods;Protocols;Medical control systems;Control systems;Power system modeling;Application software;Robust stability;Software quality","object-oriented programming;safety-critical software;formal specification;formal verification","aspect-oriented decomposition;process-control systems;ultra-high dependability assurance;formal specification;formal verification;independently developable end-user assessable logical component;software decomposition;aspect-oriented modeling","","","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional schedulability analysis of real-time systems using time Petri nets","Dianxiang Xu; Xudong He; Yi Deng","Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","10","984","996","This paper presents an approach to the schedulability analysis of real-time systems modeled in time Petri nets by separating timing properties from other behavioral properties. The analysis of behavioral properties is conducted based on the reachability graph of the underlying Petri net, whereas timing constraints are checked in terms of absolute and relative firing domains. If a specific task execution is schedulable, we calculate the time span of the task execution, and pinpoint nonschedulable transitions to help adjust timing constraints. A technique for compositional timing analysis is also proposed to deal with complex task sequences, which not only improves efficiency but also facilitates the discussion of the reachability issue with regard to schedulability. We identified a class of well-structured time Petri nets such that their reachability can be easily analyzed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041054","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041054","","Real time systems;Petri nets;Timing;Reachability analysis;Processor scheduling;Computer Society;Delay effects;Monitoring;Logic","real-time systems;Petri nets;reachability analysis;flexible manufacturing systems;computer aided production planning;graph theory;production control","real-time systems;time Petri nets;schedulability;reachability graph;time span;compositional timing analysis;production control;flexible manufacturing systems;timing constraints","","37","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Optimum control limits for employing statistical process control in software process","P. Jalote; A. Saxena","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Kanpur, India; NA","IEEE Transactions on Software Engineering","","2002","28","12","1126","1134","There is increasing interest in using control charts for monitoring and improving software processes, particularly quality control processes like reviews and testing. In a control chart, control limits are established for attributes and, if any point falls outside the limits, it is assumed to be due to special causes that need to be identified and eliminated. If the control limits are too tight, they may raise too many ""false alarms"" and, if they are too wide, they may miss special situations. Optimal control limits will try to minimize the cost of these errors. In this paper, we develop a cost model for employing control charts for software processes using optimum control limits which can be determined. Our applications of the model suggest that, for quality control processes like inspection, optimum control limits may be tighter than those commonly used in manufacturing. We have also implemented this model as a Web service that can be used for determining optimum control limits.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158286","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158286","","Process control;Control charts;Quality control;Cost function;Monitoring;Software quality;Software testing;Optimal control;Error correction;Application software","statistical process control;software metrics;software process improvement;software quality","optimum control limits;statistical process control;control charts;software process monitoring;software process improvement;quality control processes;reviews;testing;cost model;inspection process;Web service","","34","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Version support for engineering database systems","K. R. Dittrich; R. A. Lorie","Forschungszentrum Inf., Karlsruhe, West Germany; NA","IEEE Transactions on Software Engineering","","1988","14","4","429","437","In engineering applications, multiple copies of object descriptions have to coexist in a single database. A scheme is proposed that enables users to explicitly deal with these object versions. After introducing a basic version model, the problem of rerouting interobject references on the creation of new versions is solved by providing generic references and user-specific environments. Logical version clusters are introduced that allow for the meaningful grouping of versions. Some remarks on implementation and a comparison with other approaches are also included.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4664","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4664","","Data engineering;Systems engineering and theory;Database systems;Design engineering;Transaction databases;Aging;Technology management;Data models;Very large scale integration","CAD;data structures;database management systems;engineering computing","version support;CAD;data structures;engineering database;object descriptions;object versions;rerouting;user-specific environments;version clusters","","67","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Software Projects in an Academic Environment","D. B. Wortman","Department of Computer Science, University of Toronto","IEEE Transactions on Software Engineering","","1987","SE-13","11","1176","1181","The ""software hut"" is a course project that is used in conjunction with a graduate-level course in software engineering. The purpose of this project is to give the students some ""real world"" experience with the design and implementation of software. This paper describes the author's experience in using such a project and presents some suggestions on how a project should be organized.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232867","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702165","Course project;education;graduate course;software engineering;software engineering education;software hut;teaching experiences","Software engineering;Software tools;Inspection;Computer science education;Educational programs;Software testing;System testing;Software systems;Uncertainty;Councils","","Course project;education;graduate course;software engineering;software engineering education;software hut;teaching experiences","","3","","9","","","","","","IEEE","IEEE Journals & Magazines"
"MSeer-An Advanced Technique for Locating Multiple Bugs in Parallel","R. Gao; W. E. Wong","Computer Science, University of Texas at Dallas, Richardson, Texas United States 75080 (e-mail: gxr116020@utdallas.edu); Computer Science, University of Texas at Dallas, Richardson, Texas United States 75083 (e-mail: ewong@utdallas.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","In practice, a program may contain multiple bugs. The simultaneous presence of these bugs may deteriorate the effectiveness of existing fault-localization techniques to locate program bugs. While it is acceptable to use all failed and successful tests to identify suspicious code for programs with exactly one bug, it is not appropriate to use the same approach for programs with multiple bugs because the due-to relationship between failed tests and underlying bugs cannot be easily identified. One solution is to generate fault-focused clusters by grouping failed tests caused by the same bug into the same clusters. We propose MSeer- an advanced fault localization technique for locating multiple bugs in parallel. Our major contributions include the use of (1) a revised Kendall tau distance to measure the distance between two failed tests, (2) an innovative approach to simultaneously estimate the number of clusters and assign initial medoids to these clusters, and (3) an improved K-medoids clustering algorithm to better identify the due-to relationship between failed tests and their corresponding bugs. Case studies on 840 multiple-bug versions of seven programs suggest that MSeer performs better in terms of effectiveness and efficiency than two other techniques for locating multiple bugs in parallel.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2776912","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8119545","Software fault localization;parallel debugging;multiple bugs;clustering;distance metrics","","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"A relational calculus with set operators, its safety, and equivalent graphical languages","G. Ozsoyoglu; H. Wang","Dept. of Comput. Eng. & Sci., Case Western Reserve Univ., Cleveland, OH, USA; Dept. of Comput. Eng. & Sci., Case Western Reserve Univ., Cleveland, OH, USA","IEEE Transactions on Software Engineering","","1989","15","9","1038","1052","The authors propose a relational calculus (RC/S) which uses set comparison and set manipulation operators to replace universal quantifiers and negations. It is argued that compared to the Codd relational calculus (RC), RC/S queries are easier to construct and comprehend. It is proved that the expressive power of RC is equivalent to the expressive power of RC/S, and algorithms for translating an RC query into an RC/S query and vice versa are given. A safe RC/S query is defined as one that has finite output and can be evaluated in finite time. Then a subset of RC/S queries, called RC/S* is defined, and it is proved that RC/S* is safe. RC/S* is compared to the existing largest safe subsets of RC, i.e. the evaluable formulas and the allowed formulas. Algorithms are given to transform any evaluable formula into an RC/S* query, and some RC/S* formulas that are not evaluable are given. RC/S* queries can be directly implemented using a graphical language similar to Query-by-Example (QBE). Two different graphical languages are described that are equivalent to the RC/S* in expressive power, and these languages are compared to QBE.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31363","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31363","","Calculus;Safety;Database languages;Intelligent systems;Automation;Relational databases;Power measurement;Prototypes;Design engineering;Data engineering","computer graphics;database theory;query languages;relational databases","algorithms;relational calculus;set operators;safety;graphical languages;set comparison;set manipulation;Codd relational calculus;RC/S queries;RC query;evaluable formulas;allowed formulas;RC/S* query;Query-by-Example;QBE","","21","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Specification of modular systems","H. Weber; H. Ehrig","Fachbereich Informatik, Universit&#x00E4;t Dortmund, Postfach 500 500, 4600 Dortmund, West Germany; Fachbereich Informatik, Technische Universit&#x00E4;t Berlin, Strasse des 17, juni 135, 1000 Berlin 10, West Germany","IEEE Transactions on Software Engineering","","1986","SE-12","7","784","798","A modularity concept for structuring large software systems is presented. The concept enforces an extreme modularity discipline that goes considerably beyond the one found in modern programming languages such as MODULA-2 or Ada. The concept is meant to be used to tightly control side effects in the execution of systems that are constructed of independently developed modules. A family of specification languages is introduced whose members are all based on the modularity concept and thus support the uniform monolinguistic specification of software systems at all development stages. The languages have been defined to enable matching informal, semiformal, and formal specifications and thus to make formal specification of modular systems practicable. The construction of large software systems as interconnections of modules is shown to lead to manageable system structures and to new degrees of freedom in the structuring of the software development process. The suitability of the modularity concept has been evaluated in a large software project for the development of a database management system. The concept and specification languages are explained with the aid of sample specifications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312979","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312979","Data abstraction;informal;procedural;algebraic specifications;modularity;software development;software structuring","Formal specifications;Software systems;Specification languages;Database systems;Production","software engineering;specification languages","modular systems;modularity concept;structuring large software systems;independently developed modules;specification languages;monolinguistic specification;formal specification;database management system","","6","","","","","","","","IEEE","IEEE Journals & Magazines"
"Towards a framework for software measurement validation","B. Kitchenham; S. L. Pfleeger; N. Fenton","Nat. Comput. Centre, Manchester, UK; NA; NA","IEEE Transactions on Software Engineering","","1995","21","12","929","944","In this paper we propose a framework for validating software measurement. We start by defining a measurement structure model that identifies the elementary component of measures and the measurement process, and then consider five other models involved in measurement: unit definition models, instrumentation models, attribute relationship models, measurement protocols and entity population models. We consider a number of measures from the viewpoint of our measurement validation framework and identify a number of shortcomings; in particular we identify a number of problems with the construction of function points. We also compare our view of measurement validation with ideas presented by other researchers and identify a number of areas of disagreement. Finally, we suggest several rules that practitioners and researchers can use to avoid measurement problems, including the use of measurement vectors rather than artificially contrived scalars.","0098-5589;1939-3520;2326-3881","","10.1109/32.489070","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=489070","","Software measurement;Software metrics;Computer Society;Particle measurements;Object oriented modeling;Measurement units;Instruments;Protocols;Area measurement;Software engineering","software metrics;measurement theory","software measurement validation;framework;measurement structure model;measures;measurement process;measurement protocols;unit definition models;instrumentation models;attribute relationship models;entity population models;measurement vectors","","246","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs","J. Xuan; M. Martinez; F. DeMarco; M. Clément; S. L. Marcote; T. Durieux; D. Le Berre; M. Monperrus","State Key Lab of Software Engineering, School of Computer, Wuhan University, Wuhan, China; Faculty of Informatics, University of Lugano, Lugano, Switzerland; University of Buenos Aires, Buenos Aires, Argentina; Department of Computer Science, University of Lille, Lille, France; University of Buenos Aires, Buenos Aires, Argentina; Department of Computer Science, University of Lille, Lille, France; University of Artois & CNRS, Lens, France; University of Lille & INRIA, Lille, France","IEEE Transactions on Software Engineering","","2017","43","1","34","55","We propose Nopol, an approach to automatic repair of buggy conditional statements (i.e., if-then-else statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of Nopol consists of three major phases. First, Nopol employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, Nopol encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem; then a feasible solution to the SMT instance is translated back into a code patch. We evaluate Nopol on 22 real-world bugs (16 bugs with buggy if conditions and six bugs with missing preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with buggy if conditions and missing preconditions. We illustrate the capabilities and limitations of Nopol using case studies of real bug fixes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2560811","INRIA Internship program; INRIA postdoctoral research fellowship; CNRS delegation program; National Natural Science Foundation of China; Young Talent Development Program of the China Computer Federation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7463060","Automatic repair;patch generation;SMT;fault localization","Maintenance engineering;Computer bugs;Runtime;Java;Encoding;Open source software;Indexes","computability;Java;object-oriented programming;program debugging;public domain software;software fault tolerance;software maintenance","Nopol;automatic conditional statement bug repairing;Java programs;buggy program;conditional expression;angelic fix localization;test execution;runtime trace collection;objected-oriented features;patch generation;satisfiability modulo theory problem;SMT problem;code patch;buggy IF conditions;open-source projects;Apache Commons Math;Apache Commons Lang","","36","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Software Configuration Engineering in Practice: Interviews, Survey, and Systematic Literature Review","M. Sayagh; N. Kerzazi; B. Adams; F. Petrillo","Génie informatique et génie logiciel, Ecole Polytechnique, Montreal, Quebec Canada (e-mail: mohammed.sayagh@polymtl.ca); Software engineering, the Ecole Nationale Superieure d'Informatique et d'Analyse des Systemes (ENSIAS), Rabat, Rabat Morocco (e-mail: n.kerzazi@um5s.net.ma); Genie Informatique et Genie Logiciel, Ecole Polytechnique de Montreal, Montreal, Quebec Canada H3T 1J4 (e-mail: bram.adams@polymtl.ca); Software Engineering, Ecole Polytechnique de Montreal, 5596 Montreal, Quebec Canada H3T 1J4 (e-mail: fabio@petrillo.com)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Modern software applications are adapted to different situations (e.g., memory limits, enabling/disabling features) by only changing few configuration option values without any source code modifications. According to several studies, this flexibility is expensive. Indeed, configuration errors represent one of the largest percentage of software errors, they are hard to debug and resolve, while comprehension of the code also is hampered by sprinkling conditional checks of configuration options. Although researchers have proposed various approaches to help debug or prevent configuration errors, especially from the end users' perspective, this paper takes a step back to understand the activities required by practitioners to engineer the software configuration options in their source code, the challenges they experience as well as best practices that they have or could adopt. By interviewing 14 software engineering experts, followed by a large survey on 229 software engineers, we identified 9 major activities related to configuration engineering, 22 challenges faced by developers, and 25 expert recommendations to improve software configuration quality. We complemented this study by a systematic literature review to enrich the experts' recommendations, and to identify possible solutions for the developers' challenges discussed and evaluated by the research community.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2867847","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8451922","","Software systems;Interviews;Systematics;Facebook;Bibliographies;Software algorithms","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Empirical studies of a prediction model for regression test selection","M. J. Harrold; D. Rosenblum; G. Rothermel; E. Weyuker","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","248","263","Regression testing is an important activity that can account for a large proportion of the cost of software maintenance. One approach to reducing the cost of regression testing is to employ a selective regression testing technique that: chooses a subset of a test suite that was used to test the software before the modifications; then uses this subset to test the modified software. Selective regression testing techniques reduce the cost of regression testing if the cost of selecting the subset from the test suite together with the cost of running the selected subset of test cases is less than the cost of rerunning the entire test suite. Rosenblum and Weyuker (1997) proposed coverage-based predictors for use in predicting the effectiveness of regression test selection strategies. Using the regression testing cost model of Leung and White (1989; 1990), Rosenblum and Weyuker demonstrated the applicability of these predictors by performing a case study involving 31 versions of the KornShell. To further investigate the applicability of the Rosenblum-Weyuker (RW) predictor, additional empirical studies have been performed. The RW predictor was applied to a number of subjects, using two different selective regression testing tools, Deja vu and TestTube. These studies support two conclusions. First, they show that there is some variability in the success with which the predictors work and second, they suggest that these results can be improved by incorporating information about the distribution of modifications. It is shown how the RW prediction model can be improved to provide such an accounting.","0098-5589;1939-3520;2326-3881","","10.1109/32.910860","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910860","","Predictive models;Software testing;Costs;Performance evaluation;Computer Society;Software maintenance;Software performance;Software quality","program testing;software maintenance;statistical analysis","prediction model;regression test selection;software maintenance;regression testing;test suite;coverage-based predictors;KornShell;Deja vu;TestTube","","35","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Is it possible to decorate graphical software design and architecture models with qualitative Information?-An experiment","L. Bratthall; C. Wohlin","Corporate Res. Dept., ABB AS, Billingstad, Norway; NA","IEEE Transactions on Software Engineering","","2002","28","12","1181","1193","Software systems evolve over time and it is often difficult to maintain them. One reason for this is that often it is hard to understand the previous release. Further, even if architecture and design models are available and up to date, they primarily represent the functional behavior of the system. To evaluate whether it is possible to also represent some nonfunctional aspects, an experiment has been conducted. The objective of the experiment is to evaluate the cognitive suitability of some visual representations that can be used to represent a control relation, software component size and component external and internal complexity. Ten different representations are evaluated in a controlled environment using 35 subjects. The results from the experiment show that representations with low cognitive accessibility weight can be found. In an example, these representations are used to illustrate some qualities in an SDL block diagram. It is concluded that the incorporation of these representations in architecture and design descriptions is both easy and probably worthwhile. The incorporation of the representations should enhance the understanding of previous releases and, hence, help software developers in evolving and maintaining complex software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158290","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158290","","Software design;Computer architecture;Software maintenance;Software systems;Software quality;Software testing;Size control;Software architecture;Unified modeling language;Fault diagnosis","specification languages;software architecture;software quality;software maintenance","graphical software design;architecture models;qualitative information;software systems;software component size;SDL block diagram;software evolution;software maintenance;software quality representation","","13","","44","","","","","","IEEE","IEEE Journals & Magazines"
"A uniform presentation of confidentiality properties","J. Jacob","Comput. Lab., Oxford Univ., UK","IEEE Transactions on Software Engineering","","1991","17","11","1186","1194","Security (in the sense of confidentiality) properties are properties of shared systems. A suitable model of shared systems, in which one can formally define the term security property and then proceed to catalog several security properties, is presented. The purpose is to present various information-flow properties in a manner that exposes their differences and similarities. Abstraction is the main tool, and everything that is not central to the purpose is discarded. The presentation is generic in the model of computation. The abstraction lays bare a regular structure into which many interesting information-flow properties fall. A shared system is represented by a relation. How this model lets one reason about information flow is discussed and the term information flow property is formally defined. Various information-flow properties are described. Composability and probabilistic security properties are addressed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.106973","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=106973","","Information security;Computational modeling;Jacobian matrices;Fluid flow measurement;System software;Hardware;Mathematics;Information theory;Radar","security of data","composability;confidentiality properties;security property;information-flow properties;probabilistic security properties","","4","","21","","","","","","IEEE","IEEE Journals & Magazines"
"An evaluation of the MOOD set of object-oriented software metrics","R. Harrison; S. J. Counsell; R. V. Nithi","Dept. of Electron. & Comput. Sci., Southampton Univ., UK; NA; NA","IEEE Transactions on Software Engineering","","1998","24","6","491","496","This paper describes the results of an investigation into a set of metrics for object-oriented design, called the MOOD metrics. The merits of each of the six MOOD metrics is discussed from a measurement theory viewpoint, taking into account the recognized object-oriented features which they were intended to measure: encapsulation, inheritance, coupling, and polymorphism. Empirical data, collected from three different application domains, is then analyzed using the MOOD metrics, to support this theoretical validation. Results show that (with appropriate changes to remove existing problematic discontinuities) the metrics could be used to provide an overall assessment of a software system, which may be helpful to managers of software development projects. However, further empirical studies are needed before these results can be generalized.","0098-5589;1939-3520;2326-3881","","10.1109/32.689404","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=689404","","Mood;Software metrics;Encapsulation;Software measurement;Computer Society;Application software;Software quality;Emotion recognition;Software systems;Project management","software metrics;object-oriented programming;data encapsulation;inheritance","object-oriented software metrics;object-oriented design;MOOD metrics;measurement theory;object-oriented features;encapsulation;inheritance;coupling;polymorphism","","117","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Data Mining Techniques for Software Effort Estimation: A Comparative Study","K. Dejaeger; W. Verbeke; D. Martens; B. Baesens","Katholieke Universiteit Leuven, Leuven; Katholieke Universiteit Leuven, Leuven; University of Antwerp, Antwerp; Katholieke Universiteit Leuven, Leuven and University of Southampton, Highfield Southampton","IEEE Transactions on Software Engineering","","2012","38","2","375","397","A predictive model is required to be accurate and comprehensible in order to inspire confidence in a business setting. Both aspects have been assessed in a software effort estimation setting by previous studies. However, no univocal conclusion as to which technique is the most suited has been reached. This study addresses this issue by reporting on the results of a large scale benchmarking study. Different types of techniques are under consideration, including techniques inducing tree/rule-based models like M5 and CART, linear models such as various types of linear regression, nonlinear models (MARS, multilayered perceptron neural networks, radial basis function networks, and least squares support vector machines), and estimation techniques that do not explicitly induce a model (e.g., a case-based reasoning approach). Furthermore, the aspect of feature subset selection by using a generic backward input selection wrapper is investigated. The results are subjected to rigorous statistical testing and indicate that ordinary least squares regression in combination with a logarithmic transformation performs best. Another key finding is that by selecting a subset of highly predictive attributes such as project size, development, and environment related attributes, typically a significant increase in estimation accuracy can be obtained.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928350","Data mining;software effort estimation;regression.","Software;Estimation;Data models;Data mining;Cognition;Artificial neural networks;Regression tree analysis","data mining;program testing;regression analysis;software cost estimation","data mining techniques;software effort estimation;predictive model;rule-based models;CART;M5;linear regression;nonlinear models;estimation techniques;feature subset selection;generic backward input selection wrapper;rigorous statistical testing;ordinary least squares regression;logarithmic transformation","","81","","108","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic file structure for partial match retrieval based on overflow bucket sharing","T. Yuen; D. H. Du","Department of Computer Science, University of Minnesota, Minneapolis, MN 55455; Department of Computer Science, Pace University, New York, NY 10038; Department of Computer Science, University of Minnesota, Minneapolis, MN 55455","IEEE Transactions on Software Engineering","","1986","SE-12","8","801","810","A hashing-based dynamic file structure is introduced for partial match retrieval using overflow bucket sharing. The sharing of overflow buckets is dynamic in the sense that an overflow bucket is shared by a varying number of primary buckets according to the local conditions of the file. The use and sharing of overflow buckets defers splitting of the data buckets, thereby increasing the storage utilization. For the same reason, plus the fact that the sharing is dynamic, the growth of the directory is slowed down. Under the proposed organization, the records are stored more compactly in the data buckets, and for those partial match queries in which few attributes are specified, groups of neighboring directory entries have high probability of being referenced together, so that the retrieval costs for these types of partial match queries are reduced. This file organization is found to be space efficient and is also time efficient for queries in which the number of specified attributes is small.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312983","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312983","Database management;file organization;information storage and retrieval;physical design","Organizations;Vectors;Indexes;Computer science;Educational institutions;Manganese;Large Hadron Collider","data structures;file organisation","hashing;partial match retrieval;overflow bucket sharing;dynamic file structure;data buckets;storage utilization;neighboring directory entries;retrieval costs;file organization;space efficient","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Verification Templates for the Analysis of User Interface Software Design","M. D. Harrison; P. Masci; J. C. Campos","School of Computing Science, Newcastle University, Newcastle Upon Tyne, England United Kingdom of Great Britain and Northern Ireland NE1 7RU (e-mail: michael.harrison@ncl.ac.uk); Departamento de Informatica, Universidade do Minho, 56059 Braga, Braga Portugal (e-mail: paolo.masci@inesctec.pt); Departamento de Inform&#x00E1;tica, Universidade do Minho, Braga, N/A Portugal 4715-057 Braga (e-mail: jose.campos@di.uminho.pt)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","The paper describes templates for model-based analysis of usability and safety aspects of user interface software design. The templates crystallize general usability principles commonly addressed in user-centred safety requirements, such as the ability to undo user actions, the visibility of operational modes, and the predictability of user interface behavior. These requirements have standard forms across different application domains, and can be instantiated as properties of specific devices. The modeling and analysis process is carried out using the Prototype Verification System (PVS), and is further facilitated by structuring the specification of the device using a format that is designed to be generic across interactive systems. A concrete case study based on a commercial infusion pump is used to illustrate the approach. A detailed presentation of the automated verification process using PVS shows how failed proof attempts provide precise information about problematic user interface software features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2804939","Engineering and Physical Sciences Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8289349","Human-Computer Interaction;Model-based development;Formal specifications;Formal verification;Prototype Verification System (PVS)","User interfaces;Safety;ISO Standards;Usability","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Aspect-Oriented Race Detection in Java","E. Bodden; K. Havelund","Technical University Darmstadt, Darmstadt; California Institute of Technology, Pasadena","IEEE Transactions on Software Engineering","","2010","36","4","509","527","In the past, researchers have developed specialized programs to aid programmers in detecting concurrent programming errors such as deadlocks, livelocks, starvation, and data races. In this work, we propose a language extension to the aspect-oriented programming language AspectJ, in the form of three new pointcuts, lock(), unlock(), and maybeShared(). These pointcuts allow programmers to monitor program events where locks are granted or handed back, and where values are accessed that may be shared among multiple Java threads. We decide thread locality using a static thread-local-objects analysis developed by others. Using the three new primitive pointcuts, researchers can directly implement efficient monitoring algorithms to detect concurrent-programming errors online. As an example, we describe a new algorithm which we call RACER, an adaption of the well-known ERASER algorithm to the memory model of Java. We implemented the new pointcuts as an extension to the AspectBench Compiler, implemented the RACER algorithm using this language extension, and then applied the algorithm to the NASA K9 Rover Executive and two smaller programs. Our experiments demonstrate that our implementation is effective in finding subtle data races. In the Rover Executive, RACER finds 12 data races, with no false warnings. Only one of these races was previously known.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5406531","Race detection;runtime verification;aspect-oriented programming;semantic pointcuts;static analysis.","Java;Programming profession;Computer languages;System recovery;Monitoring;Runtime;Protection;Instruments;Libraries","aspect-oriented programming;concurrency control;Java;multi-threading;program compilers;program debugging","aspect-oriented race detection;concurrent programming error detection;aspect-oriented programming language;AspectJ;multiple Java threads;static thread-local-objects analysis;ERASER algorithm;primitive pointcuts;AspectBench compiler;RACER algorithm;NASA K9 Rover Executive","","13","","52","","","","","","IEEE","IEEE Journals & Magazines"
"To Be Optimal or Not in Test-Case Prioritization","D. Hao; L. Zhang; L. Zang; Y. Wang; X. Wu; T. Xie","Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Department of Computer Science, University of Illinois at Urbana-Champaign","IEEE Transactions on Software Engineering","","2016","42","5","490","505","Software testing aims to assure the quality of software under test. To improve the efficiency of software testing, especially regression testing, test-case prioritization is proposed to schedule the execution order of test cases in software testing. Among various test-case prioritization techniques, the simple additional coverage-based technique, which is a greedy strategy, achieves surprisingly competitive empirical results. To investigate how much difference there is between the order produced by the additional technique and the optimal order in terms of coverage, we conduct a study on various empirical properties of optimal coverage-based test-case prioritization. To enable us to achieve the optimal order in acceptable time for our object programs, we formulate optimal coverage-based test-case prioritization as an integer linear programming (ILP) problem. Then we conduct an empirical study for comparing the optimal technique with the simple additional coverage-based technique. From this empirical study, the optimal technique can only slightly outperform the additional coverage-based technique with no statistically significant difference in terms of coverage, and the latter significantly outperforms the former in terms of either fault detection or execution time. As the optimal technique schedules the execution order of test cases based on their structural coverage rather than detected faults, we further implement the ideal optimal test-case prioritization technique, which schedules the execution order of test cases based on their detected faults. Taking this ideal technique as the upper bound of test-case prioritization, we conduct another empirical study for comparing the optimal technique and the simple additional technique with this ideal technique. From this empirical study, both the optimal technique and the additional technique significantly outperform the ideal technique in terms of coverage, but the latter significantly outperforms the former two techniques in terms of fault detection. Our findings indicate that researchers may need take cautions in pursuing the optimal techniques in test-case prioritization with intermediate goals.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2496939","National 973 Program of China; National Natural Science Foundation of China; National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7314957","Test-Case Prioritization;Integer Linear Programming;Greedy Algorithm;Empirical Study;Test-case prioritization;integer linear programming;greedy algorithm;empirical study","Software;Measurement;Schedules;Fault detection;Integer linear programming;Software testing","integer programming;linear programming;program testing","test-case prioritization techniques;software testing;software quality;regression testing;simple additional coverage-based technique;optimal coverage-based test-case prioritization;integer linear programming;ILP problem","","14","","47","","","","","","IEEE","IEEE Journals & Magazines"
"EMERALDS: a small-memory real-time microkernel","K. M. Zuberi; K. G. Shin","Microsoft Corp., Redmond, WA, USA; NA","IEEE Transactions on Software Engineering","","2001","27","10","909","928","EMERALDS (Extensible Microkernel for Embedded, ReAL-time, Distributed Systems) is a real-time microkernel designed for small-memory embedded applications. These applications must run on slow (15-25 MHz) processors with just 32-128 kbytes of memory, either to keep production costs down in mass produced systems or to keep weight and power consumption low. To be feasible for such applications, the OS must not only be small in size (less than 20 kbytes), but also have low overhead kernel services. Unlike commercial embedded OSs which rely on carefully optimized code to achieve efficiency, EMERALDS takes the approach of redesigning the basic OS services of task scheduling, synchronization, communication, and system call mechanism by using characteristics found in small-memory embedded systems, such as small code size and a priori knowledge of task execution and communication patterns. With these new schemes, the overheads of various OS services are reduced 20-40 percent without compromising any OS functionality.","0098-5589;1939-3520;2326-3881","","10.1109/32.962561","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=962561","","Application software;Real time systems;Hardware;Costs;Energy consumption;Embedded system;Control systems;Automotive engineering;Engines;Production systems","real-time systems;operating system kernels;network operating systems;scheduling;synchronisation","EMERALDS;real time distributed systems;real-time microkernel;small-memory embedded applications;slow processors;power consumption;real-time operating systems;real-time scheduling;task synchronization;system call mechanism;small-memory embedded systems;32 to 128 kbyte","","11","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Designing an agent synthesis system for cross-RPC communication","Yen-Min Huang; C. V. Ravishankar","IBM Corp., Research Triangle Park, NC, USA; NA","IEEE Transactions on Software Engineering","","1994","20","3","188","198","Remote procedure call (RPC) is the most popular paradigm used today to build distributed systems and applications. As a consequence, the term ""RPC"" has grown to include a range of vastly different protocols above the transport layer. A resulting problem is that programs often use different RPC protocols, cannot be interconnected directly, and building a solution for each case in a large heterogeneous environment is prohibitively expensive. We describe the design of a system that can synthesize programs (RPC agents) to accommodate RPC heterogeneities. Because of its synthesis capability, the system also facilitates the design and implementation of new RPC protocols through rapid prototyping. We have built a prototype system to validate the design and to estimate the agent development costs and cross-RPC performance. The evaluation shows that the synthesis approach provides a more general solution than existing approaches do, and with lower software development and maintenance costs, while maintaining reasonable cross-RPC performance.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.268920","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=268920","","Transport protocols;Costs;Software maintenance;Software prototyping;Prototypes;Programming;Fault tolerance;Multicast protocols;Buildings;Runtime","remote procedure calls;protocols;software prototyping;parallel programming;telecommunications computing","agent synthesis system;cross-RPC communication;remote procedure call;distributed systems;transport layer;RPC protocols;large heterogeneous environment;RPC agents;RPC heterogeneities;rapid prototyping;agent development costs;cross-RPC performance;maintenance costs","","11","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Domain-Specific Service Selection for Composite Services","O. Moser; F. Rosenberg; S. Dustdar","Vienna University of Technology, Vienna; IBM T.J. Watson Research Center, Hawthorne; Vienna University of Technology, Vienna","IEEE Transactions on Software Engineering","","2012","38","4","828","843","We propose a domain-specific service selection mechanism and system implementation to address the issue of runtime adaptation of composite services that implement mission-critical business processes. To this end, we leverage quality of service (QoS) as a means to specify rigid dependability requirements. QoS does not include only common attributes such as availability or response time but also attributes specific to certain business domains and processes. Therefore, we combine both domain-agnostic and domain-specific QoS attributes in an adaptive QoS model. For specifying the service selection strategy, we propose a domain-specific language called VieDASSL to specify so-called selectors. This language can be used to specify selector implementations based on the available QoS attributes. Both the QoS model implementation and the selectors can be adapted at runtime to deal with changing business and QoS requirements. Our approach is implemented on top of an existing WS-BPEL engine. We demonstrate its feasibility by implementing a case study from the telecommunication domain.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6231591","Service composition;quality of service;monitoring;service selection;domain specific languages","Quality of service;Runtime;Business;Adaptation models;Time factors;Availability;Engines","business data processing;quality of service;reliability;specification languages;Web services","domain-specific service selection mechanism;composite services;runtime adaptation;mission-critical business processes;quality of service;domain-agnostic QoS attributes;domain-specific QoS attributes;adaptive QoS model;domain-specific language;VieDASSL;selectors;business requirements;QoS requirements;WS-BPEL engine;telecommunication;Web services","","18","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Trace analysis for conformance and arbitration testing","G. V. Bochmann; R. Dssouli; J. R. Zhao","Montreal Univ., Que., Canada; Montreal Univ., Que., Canada; Montreal Univ., Que., Canada","IEEE Transactions on Software Engineering","","1989","15","11","1347","1356","The authors explore a testing approach where the concern for selecting the appropriate test input provided to the implementation under test (IUT) is separated as much as possible from the analysis of the observed output. Particular emphasis is placed on the analysis of the observed interactions of the IUT in order to determine whether the observed input/output trace conforms to the IUT's specification. The authors consider this aspect of testing with particular attention to testing of communication protocol implementations. Various distributed test architectures are used for this purpose, where partial input/output traces are observable by local observers at different interfaces. The error-detection power of different test configurations is determined on the basis of the partial trace visible to each local observer and their global knowledge about the applied test case. The automated construction of trace analysis modules from the formal specification of the protocol is also discussed. Different transformations of the protocol specification may be necessary to obtain the reference specification, which can be used by a local or global observer for checking the observed trace. Experience with the construction of an arbiter for the OSI (open systems interconnection) transport protocol is described.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41328","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41328","","System testing;Formal specifications;Modular construction;Transport protocols;Councils;Decision support systems;Fault detection;Software testing","conformance testing;open systems;program testing;protocols","conformance testing;arbitration testing;implementation under test;IUT;communication protocol implementations;distributed test architectures;partial input/output traces;local observers;error-detection power;global knowledge;automated construction;trace analysis modules;formal specification;reference specification;OSI;open systems interconnection;transport protocol","","37","","31","","","","","","IEEE","IEEE Journals & Magazines"
"CCFinder: a multilinguistic token-based code clone detection system for large scale source code","T. Kamiya; S. Kusumoto; K. Inoue","Graduate Sch. of Eng. Sci., Osaka Univ., Japan; NA; NA","IEEE Transactions on Software Engineering","","2002","28","7","654","670","A code clone is a code portion in source files that is identical or similar to another. Since code clones are believed to reduce the maintainability of software, several code clone detection techniques and tools have been proposed. This paper proposes a new clone detection technique, which consists of the transformation of input source text and a token-by-token comparison. For its implementation with several useful optimization techniques, we have developed a tool, named CCFinder (Code Clone Finder), which extracts code clones in C, C++, Java, COBOL and other source files. In addition, metrics for the code clones have been developed. In order to evaluate the usefulness of CCFinder and metrics, we conducted several case studies where we applied the new tool to the source code of JDK, FreeBSD, NetBSD, Linux, and many other systems. As a result, CCFinder has effectively found clones and the metrics have been able to effectively identify the characteristics of the systems. In addition, we have compared the proposed technique with other clone detection techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019480","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019480","","Cloning;Large-scale systems;Software systems;Maintenance engineering;Software maintenance;Software tools;Java;Linux;Computer aided software engineering;Programming profession","high level languages;large-scale systems;software maintenance;software metrics;computer aided software engineering;software tools;optimising compilers","CCFinder;multi-linguistic token-based code clone detection system;large-scale source code;software maintainability;input source text transformation;token-by-token comparison;optimization techniques;C language;C++ language;Java;COBOL;software metrics;case studies;JDK;Java Development Kit;FreeBSD;NetBSD;Linux;system characteristics identification;duplicated code;CASE tool","","583","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Software Development in Startup Companies: The Greenfield Startup Model","C. Giardino; N. Paternoster; M. Unterkalmsteiner; T. Gorschek; P. Abrahamsson","Faculty of Computer Science, Free University of Bolzano/Bozen, Dominikanerplatz 3, Italy; Software Engineering Research Lab Sweden, Blekinge Institute of Technology, Campus Gräsvik, 371 79 Karlskrona, Sweden; Software Engineering Research Lab Sweden, Blekinge Institute of Technology, Campus Gräsvik, 371 79 Karlskrona, Sweden; Software Engineering Research Lab Sweden, Blekinge Institute of Technology, Campus Gräsvik, 371 79 Karlskrona, Sweden; Department of Computer and Information Science, Norwegian University of Science and Technology NTNU, Sem Saelandsvei 7-9, Trondheim, Norway","IEEE Transactions on Software Engineering","","2016","42","6","585","604","Software startups are newly created companies with no operating history and oriented towards producing cutting-edge products. However, despite the increasing importance of startups in the economy, few scientific studies attempt to address software engineering issues, especially for early-stage startups. If anything, startups need engineering practices of the same level or better than those of larger companies, as their time and resources are more scarce, and one failed project can put them out of business. In this study we aim to improve understanding of the software development strategies employed by startups. We performed this state-of-practice investigation using a grounded theory approach. We packaged the results in the Greenfield Startup Model (GSM), which explains the priority of startups to release the product as quickly as possible. This strategy allows startups to verify product and market fit, and to adjust the product trajectory according to early collected user feedback. The need to shorten time-to-market, by speeding up the development through low-precision engineering activities, is counterbalanced by the need to restructure the product before targeting further growth. The resulting implications of the GSM outline challenges and gaps, pointing out opportunities for future research to develop and validate engineering practices in the startup context.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2509970","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7360225","Software Development;Startups;Grounded Theory;Software development;startups;grounded theory","Software;Companies;GSM;Context;Software engineering;History","software development management","Greenfield startup model;software startups;software engineering issues;engineering practices;software development strategies;state-of-practice investigation;grounded theory approach;GSM;user feedback;time-to-market","","24","","119","","","","","","IEEE","IEEE Journals & Magazines"
"On a unified framework for the evaluation of distributed quorum attainment protocols","D. A. Menasce; Y. Yesha; K. Kalpakis","Dept. of Comput. Sci., George Mason Univ., Fairfax, VA, USA; NA; NA","IEEE Transactions on Software Engineering","","1994","20","11","868","884","Quorum attainment protocols are an important part of many mutual exclusion algorithms. Assessing the performance of such protocols in terms of number of messages, as is usually done, may be less significant than being able to compute the delay in attaining the quorum. Some protocols achieve higher reliability at the expense of increased message cost or delay. A unified analytical model which takes into account the network delay and its effect on the time needed to obtain a quorum is presented. A combined performability metric, which takes into account both availability and delay, is defined, and expressions to calculate its value are derived for two different reliable quorum attainment protocols: D. Agrawal and A. El Abbadi's (1991) and Majority Consensus algorithms (R.H. Thomas, 1979). Expressions for the primary site approach are also given as upper bound on performability and lower bound on delay. A parallel version of the Agrawal and El Abbadi protocol is introduced and evaluated. This new algorithm is shown to exhibit lower delay at the expense of a negligible increase in the number of messages exchanged. Numerical results derived from the model are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.368122","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=368122","","Availability;Delay effects;Performance analysis;Access protocols;Computer science;Time measurement;Costs;Analytical models;Upper bound;Fault tolerant systems","distributed algorithms;protocols;software performance evaluation;software fault tolerance","unified framework;distributed quorum attainment protocols;mutual exclusion algorithms;protocol performance;unified analytical model;network delay;performability metric;Majority Consensus algorithms;primary site approach;performability;parallel version;performance analysis;fault tolerance;distributed systems;delay analysis;tree-based mutual exclusion protocols","","4","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Reliability Optimization in the Design of Distributed Systems","C. S. Raghavendra; S. Hariri","Department of Electrical Engineering&#8212;Systems, University of Southern California; NA","IEEE Transactions on Software Engineering","","1985","SE-11","10","1184","1193","The reliability of a distributed system depends on the reliabilities of its communication links and computing elements, as well as on the distribution of its resources, such as programs and data files. A useful measure of reliability in distributed systems is the terminal reliability between a pair of nodes which is the probability that at least one communication path exists between these nodes. An interesting optimization problem is that of maximizing the terminal reliability between a pair of computing elements under a given budget constraint. Analytical techniques to solve this problem are applicable only to special forms of reliability expressions. In this paper, three iterative algorithms for terminal reliability maximization are presented. The first two algorithms require the computation of terminal reliability expressions, and are therefore efficient for only small networks. The third algorithm, which is developed for large distributed systems, does not require the computation of terminal reliability expressions; this algorithm maximizes approximate objective functions and gives accurate results. Several examples are presented to illustrate the approximate optimization algorithm and an estimation of the error involved is also given.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231866","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701934","Distributed systems;iterative algorithms;pattern search method;reliability optimization;terminal reliability","Design optimization;Power system reliability;Telecommunication network reliability;Iterative algorithms;Distributed computing;Computer networks;Computer network reliability;Optimization methods;Constraint optimization;Reliability engineering","","Distributed systems;iterative algorithms;pattern search method;reliability optimization;terminal reliability","","23","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Bayesian Approaches to Matching Architectural Diagrams","D. Kimelman; M. Kimelman; D. Mandelin; D. Yellin","IBM Thomas J. Watson Research Center, Yorktown Heights; Independent Consultant; Mozilla Corporation, Mountain View; IBM Israel Software Lab, Jerusalem","IEEE Transactions on Software Engineering","","2010","36","2","248","274","IT system architectures and many other kinds of structured artifacts are often described by formal models or informal diagrams. In practice, there are often a number of versions of a model or diagram, such as a series of revisions, divergent variants, or multiple views of a system. Understanding how versions correspond or differ is crucial, and thus, automated assistance for matching models and diagrams is essential. We have designed a framework for finding these correspondences automatically based on Bayesian methods. We represent models and diagrams as graphs whose nodes have attributes such as name, type, connections to other nodes, and containment relations, and we have developed probabilistic models for rating the quality of candidate correspondences based on various features of the nodes in the graphs. Given the probabilistic models, we can find high-quality correspondences using search algorithms. Preliminary experiments focusing on architectural models suggest that the technique is promising.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5232811","Bayesian techniques;IT system architecture;modeling tools;change control.","Bayesian methods;Context modeling;Large-scale systems;Network topology;Centralized control;Merging;Collaboration;Adaptation model;Application software;Security","Bayes methods;configuration management;graphs;probability;software architecture","Bayesian methods;formal models;informal diagrams;IT system architectures;divergent variants;graphs;probabilistic models;architectural diagram matching;search algorithms","","3","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Fragmenting relations horizontally using a knowledge-based approach","D. -. Shin; K. B. Irani","Dept. of Sci. & Eng., Connecticut Univ., Storrs, CT, USA; NA","IEEE Transactions on Software Engineering","","1991","17","9","872","883","In distributed DBMSs, one major issue in developing a horizontal fragmentation technique is what criteria to use to guide the fragmentation. The authors propose to use, in addition to typical user queries, particular knowledge about the data itself. Use of this knowledge allows revision of typical user queries into more precise forms. The revised query expressions produce better estimations of user reference clusters to the database than the original query expressions. The estimated user reference clusters form a basis to partition relations horizontally. In the proposed approach, an ordinary many-sorted language is extended to represent the queries and knowledge compatibly. This knowledge is identified in terms of five axiom schemata. An inference procedure is developed to apply the knowledge to the queries deductively.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92906","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92906","","Distributed databases;Knowledge based systems;Costs;Inference mechanisms;Logic design;Helium;Parallel processing;Query processing;Computer science","distributed databases;inference mechanisms;information retrieval systems;knowledge based systems;knowledge representation","knowledge-based approach;distributed DBMSs;horizontal fragmentation technique;typical user queries;revised query expressions;estimated user reference clusters;many-sorted language;inference procedure","","8","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Multiphase stabilization","M. G. Gouda","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA","IEEE Transactions on Software Engineering","","2002","28","2","201","208","We generalize the concept of stabilization of computing systems. According to this generalization, the actions of a system S are partitioned into n partitions, called phase 1 through phase n. In this case, system S is said to be n-stabilizing to a state predicate Q iff S has state predicates P.0, ..., P.n such that P.0=true, P.n=Q, and the following two conditions hold for every j, 1/spl les/j/spl les/n. First, if S starts at a state satisfying P.(j-1) and if the only actions of S that are allowed to be executed are those of phase j or less, then S will reach a state satisfying P.j. Second, the set of states satisfying P.j is closed under any execution of the actions of phase j or less. By choosing n=1, this generalization degenerates to the traditional definition of stabilization. We discuss three advantages of this generalization over the traditional definition. First, this generalization captures many stabilization properties of systems that are traditionally considered nonstabilizing. Second, verifying stabilization when n>1 is usually easier than when n=1. Third, this generalization suggests a new method of fault recovery, called multiphase recovery.","0098-5589;1939-3520;2326-3881","","10.1109/32.988499","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988499","","Convergence;Fault tolerant systems;Tail","system recovery;stability","multiphase stabilization;computing systems;state predicate;multiphase recovery","","8","","13","","","","","","IEEE","IEEE Journals & Magazines"
"A case study in structure specification: a grid description of Scribe","H. Ossher","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","1989","15","11","1397","1416","The author describes a case study in which the grid mechanism was used to describe the structure of Scribe, a document-processing system in widespread use. The structure description is presented and explained in some detail, and the effectiveness of the grid for specifying the important structural features of Scribe is discussed. It is shown that the grid succeeds in its objective of presenting complex structures clearly. A grid specification forms a suitable basis for a narrative explanation of system structure. It is further noted that some detailed improvements would further enhance the expressiveness of the grid, and that environment support is essential for serious use of the grid.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.41332","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=41332","","Computer aided software engineering;Programming profession;Software systems;Computer errors;Control systems","computer graphics;data structures;software engineering","structure specification;grid mechanism;Scribe;document-processing system;grid specification;environment support","","3","","35","","","","","","IEEE","IEEE Journals & Magazines"
"A vector-based approach to software size measurement and effort estimation","T. E. Hastings; A. S. M. Sajeev","Sch. of Comput. Sci. & Software Eng., Monash Univ., Caulfield East, Vic., Australia; NA","IEEE Transactions on Software Engineering","","2001","27","4","337","350","Software size is a fundamental product measure that can be used for assessment, prediction and improvement purposes. However, existing software size measures, such as function points, do not address the underlying problem complexity of software systems adequately. This can result in disproportional measures of software size for different types of systems. We propose a vector size measure (VSM) that incorporates both functionality and problem complexity in a balanced and orthogonal manner. The VSM is used as the input to a vector prediction model (VPM) which can be used to estimate development effort early in the software life-cycle. We theoretically validate the approach against a formal framework. We also empirically validate the approach with a pilot study. The results indicate that the approach provides a mechanism to measure the size of software systems, classify software systems, and estimate development effort early in the software life-cycle to within /spl plusmn/20% across a range of application types.","0098-5589;1939-3520;2326-3881","","10.1109/32.917523","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917523","","Size measurement;Software measurement;Software systems;Software quality;Predictive models;Life estimation;Application software;Testing;Software metrics;Programming","software cost estimation;software metrics;size measurement;algebraic specification;computational complexity;vectors","software size measurement;software development effort estimation;vector size measure;vector prediction model;problem complexity;functionality;software life-cycle;formal framework;pilot study;software systems classification;application types;algebraic specification;gradient;magnitude;semantic properties;software metrics;software specification;syntactic properties;validation","","45","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Rate-Based Queueing Simulation Model of Open Source Software Debugging Activities","C. Lin; Y. Li","Department of Computer Science and Information Engineering, National Chiayi University, Chiayi, Taiwan; Laboratory of Industrial Engineering, Ecole Centrale Paris, Paris, France","IEEE Transactions on Software Engineering","","2014","40","11","1075","1099","Open source software (OSS) approach has become increasingly prevalent for software development. As the widespread utilization of OSS, the reliability of OSS products becomes an important issue. By simulating the testing and debugging processes of software life cycle, the rate-based queueing simulation model has shown its feasibility for closed source software (CSS) reliability assessment. However, the debugging activities of OSS projects are different in many ways from those of CSS projects and thus the simulation approach needs to be calibrated for OSS projects. In this paper, we first characterize the debugging activities of OSS projects. Based on this, we propose a new rate-based queueing simulation framework for OSS reliability assessment including the model and the procedures. Then a decision model is developed to determine the optimal version-updating time with respect to two objectives: minimizing the time for version update, and maximizing OSS reliability. To illustrate the proposed framework, three real datasets from Apache and GNOME projects are used. The empirical results indicate that our framework is able to effectively approximate the real scenarios. Moreover, the influences of the core contributor staffing levels are analyzed and the optimal version-updating times are obtained.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2354032","National Science Council, Taiwan; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6891380","Queueing theory;rate-based simulation;open source software (OSS);bug reporting;report judgment;bug fixing;optimal version-updating time;non-homogeneous continuous time Markov chain (NHCTMC);multi-attribute utility theory (MAUT)","Software reliability;Debugging;Software;Stochastic processes;Analytical models;Cascading style sheets","configuration management;program debugging;program testing;project management;public domain software;queueing theory;software reliability","open source software;OSS approach;software development;OSS products reliability;testing processes;debugging processes;software life cycle;rate-based queueing simulation model;closed source software;CSS reliability assessment;debugging activities;OSS projects;decision model;optimal version-updating time","","8","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Random Testing: Theoretical Results and Practical Implications","A. Arcuri; M. Z. Iqbal; L. Briand","Simula, Oslo; Simula Research Laboratory, Lysaker; Simula Research Laboratory, Lysaker","IEEE Transactions on Software Engineering","","2012","38","2","258","277","A substantial amount of work has shed light on whether random testing is actually a useful testing technique. Despite its simplicity, several successful real-world applications have been reported in the literature. Although it is not going to solve all possible testing problems, random testing appears to be an essential tool in the hands of software testers. In this paper, we review and analyze the debate about random testing. Its benefits and drawbacks are discussed. Novel results addressing general questions about random testing are also presented, such as how long does random testing need, on average, to achieve testing targets (e.g., coverage), how does it scale, and how likely is it to yield similar results if we rerun it on the same testing problem (predictability). Due to its simplicity that makes the mathematical analysis of random testing tractable, we provide precise and rigorous answers to these questions. Results show that there are practical situations in which random testing is a viable option. Our theorems are backed up by simulations and we show how they can be applied to most types of software and testing criteria. In light of these results, we then assess the validity of empirical analyzes reported in the literature and derive guidelines for both practitioners and scientists.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.121","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6104067","Coupon collector;random testing;theory;Schur function;predictability;partition testing;adaptive random testing.","Testing;Software;Upper bound;Color;Random variables;Algorithm design and analysis;Generators","program testing;software tools","random testing;software testing;software tool;mathematical analysis;partition testing","","52","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Constraint-based automatic test data generation","R. A. DeMilli; A. J. Offutt","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; NA","IEEE Transactions on Software Engineering","","1991","17","9","900","910","A novel technique for automatically generating test data is presented. The technique is based on mutation analysis and creates test data that approximate relative adequacy. It is a fault-based technique that uses algebraic constraints to describe test cases designed to find particular types of faults. A set of tools (collectively called Godzilla) that automatically generates constraints and solves them to create test cases for unit and module testing has been implemented. Godzilla has been integrated with the Mothra testing system and has been used as an effective way to generate test data that kill program mutants. The authors present an initial list of constraints and discuss some of the problems that have been solved to develop the complete implementation of the technique.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92910","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92910","","Automatic testing;Software testing;System testing;Genetic mutations;Algorithms;Fault detection;Costs;Software systems;Software engineering;Computer science","computational complexity;program testing","constraint-based data generation;automatic test data generation;mutation analysis;relative adequacy;fault-based technique;algebraic constraints;Godzilla;module testing;Mothra testing system","","363","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Software Bases for the Flexible Composition of Application Systems","R. T. Mittermeir; M. Oppitz","Department of Informatik, Universit&#228;t f&#252;r Bildungswissenschaften Klagenfurt; NA","IEEE Transactions on Software Engineering","","1987","SE-13","4","440","460","In various application areas, classes of computer programs can be identified such that each program belonging to a class, can be considered as a special variant of a generic program.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233181","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702236","Software base;software engineering;software generalization;software modularization;software reusability","Application software;Software systems;Databases;Software reusability;Skeleton;Control systems;Buildings;User interfaces;Software performance;Software engineering","","Software base;software engineering;software generalization;software modularization;software reusability","","11","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Gambit: An Interactive Database Design Tool for Data Structures, Integrity Constraints, and Transactions","R. P. Braegger; A. M. Dudler; J. Rebsamen; C. A. Zehnder","Federal Institute of Technology (ETH); NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","7","574","583","The design of a database is a rather complex and dynamic process that requires comprehensive knowledge and experience. There exist many manual design tools and techniques, but the step from a schema to an implementation is still a delicate subject. The interactive database design tool Gambit supports the whole process in an optimal way. It is based on an extended relational-entity relationship model. The designer is assisted in outlining and describing data structures and consistency preserving update transactions. The constraints are formulated using the database programming language Modula/R which is based upon first-order predicate calculus. The update transactions are generated automatically as Modula/R programs and include all defined integrity constraints. They are collected in so-called data modules that represent the only interface to the database apart from read operations. The prototype facility of Gambit allows the designer to test the design of the database. The results can be used as feedback leading to an improvement of the conceptual schema and the transactions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232501","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702061","Data modules;database design;database programming language;entity relationship model;integrity constraints;propagation path concept","Transaction databases;Data structures;Relational databases;Database systems;Computer languages;User interfaces;Prototypes;Testing;Feedback;Satellite broadcasting","","Data modules;database design;database programming language;entity relationship model;integrity constraints;propagation path concept","","5","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Linear and Branching System Metrics","L. de Alfaro; M. Faella; M. Stoelinga","University of California, Santa Cruz, Santa Cruz; Università di Napoli, Napoli; University of Twente, Enschede","IEEE Transactions on Software Engineering","","2009","35","2","258","273","We extend the classical system relations of trace inclusion, trace equivalence, simulation, and bisimulation to a quantitative setting in which propositions are interpreted not as boolean values, but as elements of arbitrary metric spaces. Trace inclusion and equivalence give rise to asymmetrical and symmetrical linear distances, while simulation and bisimulation give rise to asymmetrical and symmetrical branching distances. We study the relationships among these distances and we provide a full logical characterization of the distances in terms of quantitative versions of LTL and mu-calculus. We show that, while trace inclusion (respectively, equivalence) coincides with simulation (respectively, bisimulation) for deterministic boolean transition systems, linear and branching distances do not coincide for deterministic metric transition systems. Finally, we provide algorithms for computing the distances over finite systems, together with a matching lower complexity bound.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.106","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4721438","Logics of programs;Specification techniques;Modal logic","Logic;Extraterrestrial measurements;Cost accounting;Computational modeling;Reasoning about programs;Formal languages;Software tools;Digital audio players;Clocks;Automata","Boolean functions;formal specification;process algebra;program diagnostics;program verification;software metrics;temporal logic","linear system metrics;branching system metric;software trace inclusion;software trace equivalence;software bisimulation;LTL;mu-calculus;deterministic Boolean transition system;software verification;linear temporal logic property;system specification","","40","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Cognitive heuristics in software engineering applying and extending anchoring and adjustment to artifact reuse","J. Parsons; C. Saunders","Fac. of Bus. Adm., Memorial Univ. of Newfoundland, St. John's, Nfld., Canada; NA","IEEE Transactions on Software Engineering","","2004","30","12","873","888","The extensive literature on reuse in software engineering has focused on technical and organizational factors, largely ignoring cognitive characteristics of individual developers. Despite anecdotal evidence that cognitive heuristics play a role in successful artifact reuse, few empirical studies have explored this relationship. This paper proposes how a cognitive heuristic, called anchoring, and the resulting adjustment bias can be adapted and extended to predict issues that might arise when developers reuse code and/or designs. The research proposes that anchoring and adjustment can be manifested in three ways: propagation of errors in reuse artifacts, failure to include requested functionality absent from reuse artifacts, and inclusion of unrequested functionality present in reuse artifacts. Results from two empirical studies are presented. The first study examines reuse of object classes in a programming task, using a combination of practicing programmers and students. The second study uses a database design task with student participants. Results from both studies indicate that anchoring occurs. Specifically, there is strong evidence that developers tend to use the extraneous functionality in the artifacts they are reusing and some evidence of anchoring to errors and omissions in reused artifacts. Implications of these findings for both practice and future research are explored.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.94","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377186","Index Terms- Software psychology;requirements/specifications;reusable software;reusable libraries;reuse models;human factors in software design.","Software engineering;Software libraries;Human factors;Application software;Computer Society;Programming profession;Databases;Psychology;Software reusability;Object oriented modeling","formal specification;software reusability;human factors;software libraries;object-oriented programming;psychology","cognitive heuristics;software engineering;programming task;database design task;reusable software;software psychology;human factor;requirement specification;reusable libraries;software design","","29","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Managing requirements inconsistency with development goal monitors","W. N. Robinson; S. D. Pawlowski","Dept. of Comput. Inf. Syst., Georgia State Univ., Atlanta, GA, USA; NA","IEEE Transactions on Software Engineering","","1999","25","6","816","835","Managing the development of software requirements can be a complex and difficult task. The environment is often chaotic. As analysts and customers leave the project, they are replaced by others who drive development in new directions. As a result, inconsistencies arise. Newer requirements introduce inconsistencies with older requirements. The introduction of such requirements inconsistencies may violate stated goals of development. In this article, techniques are presented that manage requirements document inconsistency by managing inconsistencies that arise between requirement development goals and requirements development enactment. A specialized development model, called a requirements dialog meta-model, is presented. This meta-model defines a conceptual framework for dialog goal definition, monitoring, and in the case of goal failure, dialog goal reestablishment. The requirements dialog meta-model is supported in an automated multiuser World Wide Web environment, called DEALSCRIBE. An exploratory case study of its use is reported. This research supports the conclusions that: an automated tool that supports the dialog meta-model can automate the monitoring and reestablishment of formal development goals; development goal monitoring can be used to determine statements of a development dialog that fail to satisfy development goals; and development goal monitoring can be used to manage inconsistencies in a developing requirements document. The application of DEALSCRIBE demonstrates that a dialog meta-model can enable a powerful environment for managing development and document inconsistencies.","0098-5589;1939-3520;2326-3881","","10.1109/32.824411","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=824411","","Computerized monitoring;Condition monitoring;Power system management;Design engineering;Power engineering and energy;Computer aided software engineering;Software development management;Student members;Chaos;Web sites","systems analysis;formal specification;software development management;information resources","requirements inconsistency management;development goal monitors;software requirements development management;requirement development goals;requirements development enactment;requirements dialog meta-model;dialog goal definition;World Wide Web;multiuser environment;DEALSCRIBE","","34","","75","","","","","","IEEE","IEEE Journals & Magazines"
"On object systems and behavioral inheritance","D. Harel; O. Kupferman","Dept. of Comput. Sci. & Appl. Math., Weizmann Inst. of Sci., Rehovot, Israel; NA","IEEE Transactions on Software Engineering","","2002","28","9","889","903","We consider state-based behavior in object-oriented analysis and design, as it arises, for example, in specifying behavior in the UML using statecharts. We first provide a rigorous and analyzable model of object systems and their reactivity. The definition is for basic one-thread systems, but can be extended in appropriate ways to more elaborate models. We then address the notion of inheritance and behavioral conformity and the resulting substitutability of classes, whereby inheriting should retain the system's original behaviors. Inheritance is a central issue of crucial importance to the modeling, design, and verification of object-oriented systems, and the many deep and unresolved questions around it cannot be addressed without a precise definition of the systems under consideration. We use our definition to give a clear and rigorous picture of what exactly is meant by behavioral conformity and how computationally complex it is to detect.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033228","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033228","","Object oriented modeling;Unified modeling language;Computational modeling;Analytical models;Runtime;Proposals;Object oriented programming;Writing;Computer languages","inheritance;object-oriented methods;formal verification;object-oriented programming;specification languages;formal specification","object-oriented analysis and design;state-based behavior;UML;statecharts;object systems;reactivity;one-thread systems;behavioral inheritance;behavioral conformity;class substitutability;modeling;verification","","26","","26","","","","","","IEEE","IEEE Journals & Magazines"
"An Application of Name Based Addressing to Low Level Distributed Algorithms","M. Ahamad; A. J. Bernstein","Department of Computer Science, State University of New York at Stony Brook; NA","IEEE Transactions on Software Engineering","","1985","SE-11","1","59","67","An interprocess communication structure for a distributed language is described which provides message level communication, multicast, and a generalized naming facility. The design is oriented to the needs of low level algorithms which, for example, might be used in a distributed operating system to support resource allocation or enhance reliability. The proposal is illustrated by programming several distributed algorithms from the literature. An implementation is described that takes advantage of physical multicast technology, and reduces to more conventional schemes for common communication paradigms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231843","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701898","Distributed algorithms;distributed languages;multicast","Distributed algorithms;Resource management;Bandwidth;Broadcasting;Multicast algorithms;Operating systems;Application software;Costs;Ethernet networks;Broadcast technology","","Distributed algorithms;distributed languages;multicast","","2","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Effect of Domain Knowledge on Elicitation Effectiveness: An Internally Replicated Controlled Experiment","A. M. Aranda; O. Dieste; N. Juristo","Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Campus de Montegancedo, Boadilla del Monte, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Campus de Montegancedo, Boadilla del Monte, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Campus de Montegancedo, Boadilla del Monte, Spain","IEEE Transactions on Software Engineering","","2016","42","5","427","451","Context. Requirements elicitation is a highly communicative activity in which human interactions play a critical role. A number of analyst characteristics or skills may influence elicitation process effectiveness. Aim. Study the influence of analyst problem domain knowledge on elicitation effectiveness. Method. We executed a controlled experiment with post-graduate students. The experimental task was to elicit requirements using open interview and consolidate the elicited information immediately afterwards. We used four different problem domains about which students had different levels of knowledge. Two tasks were used in the experiment, whereas the other two were used in an internal replication of the experiment; that is, we repeated the experiment with the same subjects but with different domains. Results. Analyst problem domain knowledge has a small but statistically significant effect on the effectiveness of the requirements elicitation activity. The interviewee has a big positive and significant influence, as does general training in requirements activities and interview experience. Conclusion. During early contacts with the customer, a key factor is the interviewee; however, training in tasks related to requirements elicitation and knowledge of the problem domain helps requirements analysts to be more effective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2494588","Spanish Ministry of Ministry of Economy and Competitiveness; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7307191","Controlled experiment;domain knowledge;requirements elicitation;internal replication;Controlled experiment;domain knowledge;requirements elicitation;internal replication","Interviews;Knowledge engineering;Computer science;Software engineering;Requirements engineering;Training","software engineering","requirements elicitation;elicitation effectiveness;internally replicated controlled experiment;problem domain knowledge","","4","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Reducing Masking Effects in CombinatorialInteraction Testing: A Feedback DrivenAdaptive Approach","C. Yilmaz; E. Dumlu; M. B. Cohen; A. Porter","Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Borsa Istanbul, Istanbul, Turkey; Department of Computer Science and Engineering, University of Nebraska-Lincoln, NE; Department of Computer Science , University of Maryland, College Park, MD","IEEE Transactions on Software Engineering","","2014","40","1","43","66","The configuration spaces of modern software systems are too large to test exhaustively. Combinatorial interaction testing (CIT) approaches, such as covering arrays, systematically sample the configuration space and test only the selected configurations. The basic justification for CIT approaches is that they can cost-effectively exercise all system behaviors caused by the settings of t or fewer options. We conjecture, however, that in practice some of these behaviors are not actually tested because of unanticipated masking effects - test case failures that perturb system execution so as to prevent some behaviors from being exercised. While prior research has identified this problem, most solutions require knowing the masking effects a priori. In practice this is impractical, if not impossible. In this work, we reduce the harmful consequences of masking effects. First we define a novel interaction testing criterion, which aims to ensure that each test case has a fair chance to test all valid t-way combinations of option settings. We then introduce a feedback driven adaptive combinatorial testing process (FDA-CIT) to materialize this criterion in practice. At each iteration of FDA-CIT, we detect potential masking effects, heuristically isolate their likely causes (i.e., fault characterization), and then generate new samples that allow previously masked combinations to be tested in configurations that avoid the likely failure causes. The iterations end when the new interaction testing criterion has been satisfied. This paper compares two different fault characterization approaches - an integral part of the proposed approach, and empirically assesses their effectiveness and efficiency in removing masking effects on two widely used open source software systems. It also compares FDA-CIT against error locating arrays, a state of the art approach for detecting and locating failures. Furthermore, the scalability of the proposed approach is evaluated by comparing it with perfect test scenarios, in which all masking effects are known a priori. Our results suggest that masking effects do exist in practice, and that our approach provides a promising and efficient way to work around them, without requiring that masking effects be known a priori.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6654147","Combinatorial testing;adaptive testing;covering arrays;software quality assurance","Testing;Adaptive arrays;Educational institutions;Scalability;Servers;Electronic mail;Software systems","program testing;public domain software;software fault tolerance","perfect test scenarios;fault location;fault detection;error locating arrays;open source software systems;fault characterization approaches;potential masking effects detection;t-way combinations;FDA-CIT process;interaction testing criterion;system execution;test case failures;covering arrays;software systems;configuration spaces;feedback driven adaptive approach;CIT approach;combinatorial interaction testing;masking effects reduction","","14","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Learning Communicating Automata from MSCs","B. Bollig; J. Katoen; C. Kern; M. Leucker","ENS Cachan and CNRS, Cachan; RWTH Aachen University, Aachen; RWTH Aachen University, Aachen; Technical University Munich, Munich","IEEE Transactions on Software Engineering","","2010","36","3","390","408","This paper is concerned with bridging the gap between requirements and distributed systems. Requirements are defined as basic message sequence charts (MSCs) specifying positive and negative scenarios. Communicating finite-state machines (CFMs), i.e., finite automata that communicate via FIFO buffers, act as system realizations. The key contribution is a generalization of Angluin's learning algorithm for synthesizing CFMs from MSCs. This approach is exact-the resulting CFM precisely accepts the set of positive scenarios and rejects all negative ones-and yields fully asynchronous implementations. The paper investigates for which classes of MSC languages CFMs can be learned, presents an optimization technique for learning partial orders, and provides substantial empirical evidence indicating the practical feasibility of the approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374425","Software engineering/requirements/specifications/elicitation methods;software engineering/design/design concepts;computing methodologies/artificial intelligence/learning/induction;theory of computation/computation by abstract devices/models of computation/automata.","Learning automata;Software engineering;Software design;Unified modeling language;Communication channels;System recovery;Computer Society;Design engineering;Design methodology;Artificial intelligence","distributed processing;finite state machines;learning (artificial intelligence)","communicating automata;MSC;message sequence charts;distributed systems;communicating finite-state machines;finite automata;FIFO buffers;Angluin learning algorithm;optimization technique","","6","","41","","","","","","IEEE","IEEE Journals & Magazines"
"A human factors experimental comparison of SQL and QBE","M. Y. -. Yen; R. W. Scamell","Dept. of Bus. Comput. Inf. Syst., Alaska Univ., Anchorage, AK, USA; NA","IEEE Transactions on Software Engineering","","1993","19","4","390","409","SQL and QBE are compared in the same operating environment, and the effects of query language type and other variables on user performance and satisfaction are studied. The experimental design combined a factorial design and a counterbalanced design in an effort to compare SQL and QBE. The results indicated that query language type affects user performance in paper and pencil testing, with QBE users having higher scores than SQL users. In contrast, in online testing, query language type had no effect on user performance. In addition, under certain conditions, query complexity had a significant effect on user performance and user satisfaction was influenced by query language type. Moreover, order of exposure impacted user performance on the basis of interaction with query language type, query complexity, and programming experience.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.223806","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=223806","","Human factors;Database languages;Testing;ANSI standards;Standards development;Natural languages;User interfaces;Design for experiments;Laboratories;Relational databases","human factors;query languages","human factors experimental comparison;SQL;QBE;operating environment;query language type;user performance;factorial design;counterbalanced design;online testing;query complexity","","23","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating emerging software development technologies: lessons learned from assessing aspect-oriented programming","G. C. Murphy; R. J. Walker; E. L. A. Banlassad","Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada; NA; NA","IEEE Transactions on Software Engineering","","1999","25","4","438","455","Determining whether a new software development technique is useful and usable is a challenging taste. Various flavors of empirical study may be used to help with this task, including surveys, case studies, and experiments. Little guidance is available within the software engineering community to help choose among these alternatives when assessing a new and evolving software development technique within some cost bounds. We faced this challenge when assessing a new programming technique called aspect-oriented programming. To assess the technique, we chose to apply both a case study approach and a series of four experiments because we wanted to understand and characterize the kinds of information that each approach might provide. We describe and critique the evaluation methods we employed, and discuss the lessons we have learned. These lessons are applicable to other researchers attempting to assess new programming techniques that are in an early stage of development.","0098-5589;1939-3520;2326-3881","","10.1109/32.799936","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=799936","","Programming;Application software;Software engineering;Costs;Computer Society;Psychology;Management training;Software design;Computer science","programming;software engineering","emerging software development technologies;aspect-oriented programming;case study approach;evaluation methods","","13","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Contract Insertion with CCBot","S. A. Carr; F. Logozzo; M. Payer","Purdue University, West Lafayette, IN; FaceBook, Seattle, WA; Purdue University, West Lafayette, IN","IEEE Transactions on Software Engineering","","2017","43","8","701","714","Existing static analysis tools require significant programmer effort. On large code bases, static analysis tools produce thousands of warnings. It is unrealistic to expect users to review such a massive list and to manually make changes for each warning. To address this issue we propose CCBot (short for CodeContracts Bot), a new tool that applies the results of static analysis to existing code through automatic code transformation. Specifically, CCBot instruments the code with method preconditions, postconditions, and object invariants which detect faults at runtime or statically using a static contract checker. The only configuration the programmer needs to perform is to give CCBot the file paths to code she wants instrumented. This allows the programmer to adopt contract-based static analysis with little effort. CCBot's instrumented version of the code is guaranteed to compile if the original code did. This guarantee means the programmer can deploy or test the instrumented code immediately without additional manual effort. The inserted contracts can detect common errors such as null pointer dereferences and out-of-bounds array accesses. CCBot is a robust large-scale tool with an open-source C# implementation. We have tested it on real world projects with tens of thousands of lines of code. We discuss several projects as case studies, highlighting undiscovered bugs found by CCBot, including 22 new contracts that were accepted by the project authors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2625248","NSF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7736073","Contract-based verification;automated patching;assertions;class invariants","Contracts;C# languages;Instruments;Computer bugs;Reactive power;Semantics;Runtime","C# language;program compilers;program diagnostics;program verification;software fault tolerance","automatic contract insertion;CCBot;static analysis tools;CodeContracts Bot;automatic code transformation;object invariants;fault detection;static contract checker;file paths;contract-based static analysis;null pointer dereferences;out-of-bounds array accesses;open-source C# implementation;contract-based verification","","1","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Understanding code mobility","A. Fuggetta; G. P. Picco; G. Vigna","Dipt. di Elettronica e Inf., Politecnico di Milano, Italy; NA; NA","IEEE Transactions on Software Engineering","","1998","24","5","342","361","The technologies, architectures, and methodologies traditionally used to develop distributed applications exhibit a variety of limitations and drawbacks when applied to large scale distributed settings (e.g., the Internet). In particular, they fail in providing the desired degree of configurability, scalability, and customizability. To address these issues, researchers are investigating a variety of innovative approaches. The most promising and intriguing ones are those based on the ability of moving code across the nodes of a network, exploiting the notion of mobile code. As an emerging research field, code mobility is generating a growing body of scientific literature and industrial developments. Nevertheless, the field is still characterized by the lack of a sound and comprehensive body of concepts and terms. As a consequence, it is rather difficult to understand, assess, and compare the existing approaches. In turn, this limits our ability to fully exploit them in practice, and to further promote the research work on mobile code. Indeed, a significant symptom of this situation is the lack of a commonly accepted and sound definition of the term mobile code itself. This paper presents a conceptual framework for understanding code mobility. The framework is centered around a classification that introduces three dimensions: technologies, design paradigms, and applications. The contribution of the paper is two-fold. First, it provides a set of terms and concepts to understand and compare the approaches based on the notion of mobile code. Second, it introduces criteria and guidelines that support the developer in the identification of the classes of applications that can leverage off of mobile code, in the design of these applications, and, finally, in the selection of the most appropriate implementation technologies. The presentation of the classification is intertwined with a review of state-of-the-art in the field. Finally, the use of the classification is exemplified in a case study.","0098-5589;1939-3520;2326-3881","","10.1109/32.685258","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=685258","","Computer networks;Pervasive computing;IP networks;Large-scale systems;Scalability;Guidelines;Appropriate technology;Mobile agents;Availability;Hardware","distributed processing;software portability;object-oriented programming","code mobility;software architecture;methodologies;distributed applications;configurability;scalability;customizability;computer network;research field;conceptual framework;design paradigms;mobile code;classification;case study;mobile agent;object oriented programming","","534","","73","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing expected time by scheduler-luck games","S. Dolev; A. Israeli; S. Moran","Dept. of Math. & Comput. Sci., Ben-Gurion Univ. of the Negev, Beer-Sheva, Israel; NA; NA","IEEE Transactions on Software Engineering","","1995","21","5","429","439","We introduce a novel technique, the scheduler luck game (in short sl-game) for analyzing the performance of randomized distributed protocols. We apply it in studying uniform self-stabilizing protocols for leader election under read/write atomicity. We present two protocols for the case where each processor in the system can communicate with all other processors and analyze their performance using the sl-game technique.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.387472","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=387472","","Protocols;Nominations and elections;Processor scheduling;Performance analysis;Tail;Computer science;Atomic measurements;Distributed algorithms;Time measurement","computational complexity;protocols;processor scheduling;scheduling;software performance evaluation;random processes;distributed processing;game theory","scheduler-luck games;expected time analysis;performance analysis;randomized distributed protocols;uniform self-stabilizing protocols;leader election;read/write atomicity;processor","","20","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis of faults in an N-version software experiment","S. S. Brilliant; J. C. Knight; N. G. Leveson","Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; NA","IEEE Transactions on Software Engineering","","1990","16","2","238","247","The authors have conducted a large-scale experiment in N-version programming. A total of 27 versions of a program were prepared independently from the same specification at two universities. The results of executing the versions revealed that the versions were individually extremely reliable but that the number of input cases in which more than one failed was substantially more than would be expected if they were statistically independent. After the versions had been executed, the failures of each version were examined and the associated faults located. It appears that minor differences in the software development environment would not have a major impact in reducing the incidence of faults that cause correlated failures.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44387","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44387","","Educational institutions;Application software;Computer science;Large-scale systems;Fault tolerance;Aircraft propulsion;Programming profession;Computer languages;Software reliability;Production","fault tolerant computing;program testing;software engineering","failure analysis;fault location;statistical correlation;N-version programming;software development environment","","78","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Formalization of the Whole-Part relationship in the Unified Modeling Language","F. Barbier; B. Henderson-Sellers; A. Le Parc-Lacayrelle; J. -. Bruel","Pau Univ., France; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","5","459","470","A formal definition for the semantics of the Whole-Part relationship in the Unified Modeling Language or UML is introduced. This provides a fully directly usable specification which can be incorporated into version 2.0 of UML. An improvement to the current metamodel fragment relating to relationships is proposed, supplemented by the introduction of axioms expressed in the Object Constraint Language or OCL. The overall formalization relates to a clear and concise emphasis on carefully enunciated (primary) characteristics that apply to all instances of a new Whole-Part metatype. Specific kinds of the Whole-Part relationship are defined in terms of secondary characteristics, which must be possessed by subtypes: In UML 1.4, these are Aggregation (a.k.a. white diamond) and Composition (a.k.a. black diamond). Primary and secondary characteristics may then be consistently combined with each other. Consequently, this allows the possible introduction of supplementary forms of Whole-Part. Such a revision is necessary since Aggregation and Composition in UML 1.4 do not cover the full spectrum of Whole-Part theory.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199074","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199074","","Unified modeling language;Object oriented modeling;Bibliographies;Design methodology;Assembly;Composite materials;Terminology","specification languages;object-oriented languages","formal definition;semantics;Whole-Part relationship;metamodel fragment;Object Constraint Language;OCL;object-oriented modeling;aggregation;composition;whole-part theory;Unified Modeling Language;UML;directly usable specification","","44","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Rendezvous facilities: Concurrent C and the Ada language","N. H. Gehani; W. D. Roome","AT&T Bell Lab., Murray Hill, NJ, USA; AT&T Bell Lab., Murray Hill, NJ, USA","IEEE Transactions on Software Engineering","","1988","14","11","1546","1553","The concurrent programming facilities in both Concurrent C and the Ada language are based on the rendezvous concept. Although these facilities are similar, there are substantial differences. Facilities in Concurrent C were designed keeping in perspective the concurrent programming facilities in the Ada language and their limitations. Concurrent C facilities have also been modified as a result of experience with its initial implementations. The authors compare the concurrent programming facilities in Concurrent C and Ada and show that it is easier to write a variety of concurrent programs in Concurrent C than in Ada.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9043","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9043","","Feedback;Terminology;Resumes","Ada;C language;parallel programming","parallel programming;Concurrent C;Ada;concurrent programming;rendezvous concept","","15","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Power-Laws in a Large Object-Oriented Software System","G. Concas; M. Marchesi; S. Pinna; N. Serra","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","10","687","708","We present a comprehensive study of an implementation of the Smalltalk object oriented system, one of the first and purest object-oriented programming environment, searching for scaling laws in its properties. We study ten system properties, including the distributions of variable and method names, inheritance hierarchies, class and method sizes, system architecture graph. We systematically found Pareto - or sometimes log-normal - distributions in these properties. This denotes that the programming activity, even when modeled from a statistical perspective, can in no way be simply modeled as a random addition of independent increments with finite variance, but exhibits strong organic dependencies on what has been already developed. We compare our results with similar ones obtained for large Java systems, reported in the literature or computed by ourselves for those properties never studied before, showing that the behavior found is similar in all studied object oriented systems. We show how the Yule process is able to stochastically model the generation of several of the power-laws found, identifying the process parameters and comparing theoretical and empirical tail indexes. Lastly, we discuss how the distributions found are related to existing object-oriented metrics, like Chidamber and Kemerer's, and how they could provide a starting point for measuring the quality of a whole system, versus that of single classes. In fact, the usual evaluation of systems based on mean and standard deviation of metrics can be misleading. It is more interesting to measure differences in the shape and coefficients of the data?s statistical distributions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1019","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302780","D.2.3.a Object-oriented programming;D.2.4.h Statistical methods;D.2.8.a Complexity measures;D.2.8.d Product metrics;D.2.8.e Software science;D.3.2.p Object-oriented languages;G.3.p Stochastic processes","Software systems;Object oriented modeling;Object oriented programming;Power system modeling;Shape measurement;Java;Power generation;Tail;Statistical distributions;Statistical analysis","","","","119","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Performance analysis of two-phase locking","A. Thomasian; I. K. Ryu","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA","IEEE Transactions on Software Engineering","","1991","17","5","386","402","A straightforward analytic solution method is developed which takes into account the variability of transaction size (the number of lock requests). The authors first obtain analytic expressions for the probability of lock conflict, probability of deadlock, and the waiting time per lock conflict. They then develop a family of noniterative analytic solutions to evaluate the overall system performance by considering the expansion in transaction response time due to transaction blocking. The accuracy of these solutions is verified by validation against simulation results. Also introduced is a new measure for the degree of lock contention, which is a product of the mean number of lock conflicts per transaction and the mean waiting time per lock conflict (when blocked by an active transaction). It is shown that the variability in transaction size results in an increase in both measures as compared to fixed-size transactions of comparable size. The authors also provide a solution method for the case when the processing times of transaction steps are different.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90443","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=90443","","Performance analysis;System recovery;Delay;Degradation;System performance;Costs;Concurrency control;Hardware;Throughput;Frequency estimation","concurrency control;distributed databases;system recovery;transaction processing","two-phase locking;transaction size;lock requests;probability;lock conflict;deadlock;system performance;transaction response time;transaction blocking;simulation;lock contention","","33","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Exception Handling for Repair in Service-Based Processes","G. Friedrich; M. G. Fugini; E. Mussi; B. Pernici; G. Tagni","Alpen-Adria Universit&#x0E4;t Klagenfurt, Kalgenfurt; Politecnico di Milano, Milano; Politecnico di Milano, Milano; Politecnico di Milano, Milano; Vrije Universiteit Amsterdam, Amsterdam","IEEE Transactions on Software Engineering","","2010","36","2","198","215","This paper proposes a self-healing approach to handle exceptions in service-based processes and to repair the faulty activities with a model-based approach. In particular, a set of repair actions is defined in the process model, and repairability of the process is assessed by analyzing the process structure and the available repair actions. During execution, when an exception arises, repair plans are generated by taking into account constraints posed by the process structure, dependencies among data, and available repair actions. The paper also describes the main features of the prototype developed to validate the proposed repair approach for composed Web services; the self-healing architecture for repair handling and the experimental results are illustrated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383376","Exception handling;failures;faults;repair;self-healing processes;Web services;process management.","Web services;Prototypes;Service oriented architecture;Logic design;Proposals","exception handling;program verification;software fault tolerance;software maintenance;software prototyping;Web services","exception handling;service based process repairing;Web services;self-healing architecture;process structure analysis;prototype development","","60","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Enforcing Exception Handling Policies with a Domain-Specific Language","E. A. Barbosa; A. Garcia; M. P. Robillard; B. Jakobus","OPUS Research Group, Informatics Department, Pontifical Catholic University of Rio de Janeiro, Rua Marquês de São Vicente, 255-Gávea, Rio de Janeiro, Brazil; OPUS Research Group, Informatics Department, Pontifical Catholic University of Rio de Janeiro, Rua Marquês de São Vicente, 255-Gávea, Rio de Janeiro, Brazil; School of Computer Science, McGill University, Montreal, Canada; OPUS Research Group, Informatics Department, Pontifical Catholic University of Rio de Janeiro, Rua Marquês de São Vicente, 255-Gávea, Rio de Janeiro, Brazil","IEEE Transactions on Software Engineering","","2016","42","6","559","584","Current software projects deal with exceptions in implementation and maintenance phases without a clear definition of exception handling policies. We call an exception handling policy the set of design decisions that govern the use of exceptions in a software project. Without an explicit exception handling policy, developers can remain unaware of the originally intended use of exceptions. In this paper, we present Exception Handling Policies Language (EPL), a domain-specific language to specify and verify exception handling policies. The evaluation of EPL was based on a user-centric observational study and case studies. The user-centric study was performed to observe how potential users of the language actually use it. With this study, we could better understand the trade-offs related to different language design decisions based on concrete and well-documented observations and experiences reported by participants. We identified some language characteristics that hindered its use and that motivated new language constructs. In addition, we performed case studies with one open-source project and two industry-strength systems to investigate how specifying and verifying exception handling policies may assist in detecting exception handling problems. The results show that violations of exception handling policies help to indicate potential faults in the exception handling code.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2506164","Fundação Carlos Chagas Filho de Amparo à Pesquisa do Estado do Rio de Janeiro (FAPERJ); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7348692","Exception handling;Exception handling policy;Policy specification;Domain-specific language;Exception handling;exception handling policy;policy specification;domain-specific language","Java;Software reliability;Robustness;Software systems","exception handling;formal specification;formal verification;programming languages;project management;public domain software;software maintenance","industry-strength systems;open-source project;user-centric study;exception handling policy verification;exception handling policy specification;EPL evaluation;language design decisions;maintenance phase;implementation phase;software projects;domain-specific language;exception handling policies language","","4","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Shortening matching time in OPS5 production systems","J. A. Kang; A. M. K. Cheng","Humax Co. Ltd., South Korea; NA","IEEE Transactions on Software Engineering","","2004","30","7","448","457","A rule-based system must satisfy stringent timing constraints when applied to a real-time environment. As the scale of rule-based expert systems increases, the efficiency of systems becomes a pressing concern. The most critical performance factor in the implementation of a production system is the condition-testing algorithm. We propose a new method based on the widely used RETE match algorithm. We show an approach designed to reduce the response time of rule-based expert systems by reducing the matching time. There are two steps in the method we propose: The first makes an index structure of the tokens to reduce the /spl alpha/-node-level join candidates. The second chooses the highest time tag for certain /spl beta/-nodes to reduce the amount of combinatorial match that is problematical in a real-time production system application. For this purpose, a simple compiler is implemented in C and the response time of test programs is measured.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318606","Matching;knowledge-based systems;expert systems;rule-based systems;OPS5;Rete;response time.","Production systems;Real time systems;Expert systems;Delay;Knowledge based systems;Timing;Pressing;Program processors;Testing;Time measurement","expert systems;real-time systems;program testing;optimising compilers","real-time environment;rule-based expert systems;condition-testing algorithm;RETE match algorithm;/spl alpha/-node-level join candidates;OPS5 production system;program compiler;test programs;knowledge-based systems","","13","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Source Code Summarization of Context for Java Methods","P. W. McBurney; C. McMillan","College of Computer Science and Engineering, University Notre Dame, Notre Dame, IN; Computer Science, University of Notre Dame, South Bend, VA","IEEE Transactions on Software Engineering","","2016","42","2","103","119","Source code summarization is the task of creating readable summaries that describe the functionality of software. Source code summarization is a critical component of documentation generation, for example as Javadocs formed from short paragraphs attached to each method in a Java program. At present, a majority of source code summarization is manual, in that the paragraphs are written by human experts. However, new automated technologies are becoming feasible. These automated techniques have been shown to be effective in select situations, though a key weakness is that they do not explain the source code's context. That is, they can describe the behavior of a Java method, but not why the method exists or what role it plays in the software. In this paper, we propose a source code summarization technique that writes English descriptions of Java methods by analyzing how those methods are invoked. We then performed two user studies to evaluate our approach. First, we compared our generated summaries to summaries written manually by experts. Then, we compared our summaries to summaries written by a state-of-the-art automatic summarization tool. We found that while our approach does not reach the quality of human-written summaries, we do improve over the state-of-the-art summarization tool in several dimensions by a statistically-significant margin.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2465386","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7181703","Source code summarization;automatic documentation;program comprehension;Source code summarization;automatic documentation;program comprehension","Context;Documentation;Java;Natural languages;Software;Generators;XML","Java;object-oriented methods","source code summarization technique;Java methods;software functionality;documentation generation;Javadocs;Java program;user studies;automatic summarization tool;human-written summaries","","14","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Models of software development environments","D. E. Perry; G. E. Kaiser","AT&T Bell Lab., Murray Hill, NJ, USA; NA","IEEE Transactions on Software Engineering","","1991","17","3","283","295","A general model of software development environments that consists of structures, mechanisms, and policies is presented. The advantage of this model is that it distinguishes intuitively those aspects of an environment that are useful in comparing and contrasting software development environments. Four classes of environments-the individual, the family, the city. and the state-are characterized by means of a sociological metaphor based on scale. The utility of the taxonomy is that it delineates the important classes of interactions among software developers and exposes the ways in which current software development environments inadequately support the development of large systems. The generality of the model is demonstrated by its application to a previously published taxonomy that categorizes environments according to how they relate to language-centered, structure-oriented, toolkit, and method-based environments.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.75417","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=75417","","Programming;Cities and towns;Taxonomy;Application software;Software tools;Software systems;Hardware;Power system modeling;Research and development","programming environments","language centered environments;structure-oriented environments;toolkit environments;software development environments;sociological metaphor;method-based environments","","31","","117","","","","","","IEEE","IEEE Journals & Magazines"
"Range Fixes: Interactive Error Resolution for Software Configuration","Y. Xiong; H. Zhang; A. Hubaux; S. She; J. Wang; K. Czarnecki","School of Electronics Engineering and Computer Science, Institute of Software, Peking University, Beijing, PR China; School of Electronics Engineering and Computer Science, Institute of Software, Peking University, Beijing, PR China; ASML, Eindhoven, the Netherlands; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; School of Electronics Engineering and Computer Science, Institute of Software, Peking University, Beijing, PR China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada","IEEE Transactions on Software Engineering","","2015","41","6","603","619","To prevent ill-formed configurations, highly configurable software often allows defining constraints over the available options. As these constraints can be complex, fixing a configuration that violates one or more constraints can be challenging. Although several fix-generation approaches exist, their applicability is limited because (1) they typically generate only one fix or a very long fix list, difficult for the user to identify the desirable fix; and (2) they do not fully support non-Boolean constraints, which contain arithmetic, inequality, and string operators. This paper proposes a novel concept, range fix, for software configuration. A range fix specifies the options to change and the ranges of values for these options. We also design an algorithm that automatically generates range fixes for a violated constraint. We have evaluated our approach with three different strategies for handling constraint interactions, on data from nine open source projects over two configuration platforms. The evaluation shows that our notion of range fix leads to mostly simple yet complete sets of fixes, and our algorithm is able to generate fixes within one second for configuration systems with a few thousands options and constraints.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2383381","National Basic Research Program of China; High-Tech Research and Development Program of China; National Natural Science Foundation of China; NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6991616","Consistency Management;Error Resolution;Range Fix;Software Configuration;Consistency management;error resolution;range fix;software configuration","Concrete;Linux;Biological system modeling;Reactive power;Kernel;Navigation","constraint handling;software engineering","range fixes;interactive error resolution;software configuration;fix-generation approaches;constraint interaction handling;open source projects","","8","","46","","","","","","IEEE","IEEE Journals & Magazines"
"VERTAF: an application framework for the design and verification of embedded real-time software","Pao-Ann Hsiung; Shang-Wei Lin; Chih-Hao Tseng; Trong-Yen Lee; Jin-Ming Fu; Win-Bin See","Dept. of Comput. Sci. & Inf. Eng., Nat. Chung Chen Univ., Ming-Hsiung, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Chung Chen Univ., Ming-Hsiung, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Chung Chen Univ., Ming-Hsiung, Taiwan; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","10","656","674","The growing complexity of embedded real-time software requirements calls for the design of reusable software components, the synthesis and generation of software code, and the automatic guarantee of nonfunctional properties such as performance, time constraints, reliability, and security. Available application frameworks targeted at the automatic design of embedded real-time software are poor in integrating functional and nonfunctional requirements. To bridge this gap, we reveal the design flow and the internal architecture of a newly proposed framework called verifiable embedded real-time application framework (VERTAF), which integrates software component-based reuse, formal synthesis, and formal verification. A formal UML-based embedded real-time object model is proposed for component reuse. Formal synthesis employs quasistatic and quasidynamic scheduling with automatic generation of multilayer portable efficient code. Formal verification integrates a model checker kernel from SGM, by adapting it for embedded software. The proposed architecture for VERTAF is component-based and allows plug-and-play for the scheduler and the verifier. Using VERTAF to develop application examples significantly reduced design effort and illustrated how high-level reuse of software components combined with automatic synthesis and verification can increase design productivity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339277","Index Terms- Application framework;code generation;embedded real-time software;formal synthesis;formal verification;scheduling;software components;UML modeling.","Application software;Embedded software;Software performance;Software reusability;Computer architecture;Formal verification;Software design;Time factors;Security;Bridges","embedded systems;object-oriented programming;software reusability;program verification;program compilers;software portability;distributed object management;processor scheduling;software architecture","embedded real-time software requirements;reusable software components;software code generation;formal verification;UML-based embedded real-time object model;formal synthesis;quasidynamic scheduling;multilayer portable efficient code;model checker kernel;design productivity;verifiable embedded real-time application framework","","24","","61","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting source code changes by mining change history","A. T. T. Ying; G. C. Murphy; R. Ng; M. C. Chu-Carroll","IBM Thomas J. Watson Res. Center, Hawthorne, NY, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","9","574","586","Software developers are often faced with modification tasks that involve source which is spread across a code base. Some dependencies between source code, such as those between source code written in different languages, are difficult to determine using existing static and dynamic analyses. To augment existing analyses and to help developers identify relevant source code during a modification task, we have developed an approach that applies data mining techniques to determine change patterns - sets of files that were changed together frequently in the past - from the change history of the code base. Our hypothesis is that the change patterns can be used to recommend potentially relevant source code to a developer performing a modification task. We show that this approach can reveal valuable dependencies by applying the approach to the Eclipse and Mozilla open source projects and by evaluating the predictability and interestingness of the recommendations produced for actual modification tasks on these systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324645","Index Terms- Enhancement;maintainability;clustering;classification;association rules;data mining.","History;Data mining;Association rules;Pattern analysis;Computer Society;Software systems;Computer languages;Frequency;Programming profession;Computer science","data mining;software tools;program verification;software maintenance;configuration management","software developers;modification task;code base;source code changes prediction;data mining technique;change history;Eclipse open source project;Mozilla open source project;software maintainability;pattern clustering;pattern classification;association rules","","241","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Fault Localization for Dynamic Web Applications","S. Artzi; J. Dolby; F. Tip; M. Pistoia","IBM Software Group, Littleton; IBM Thomas J. Watson Research Center, Yorktown Heights; IBM Thomas J. Watson Research Center, Yorktown Heights; IBM Thomas J. Watson Research Center, Yorktown Heights","IEEE Transactions on Software Engineering","","2012","38","2","314","335","In recent years, there has been significant interest in fault-localization techniques that are based on statistical analysis of program constructs executed by passing and failing executions. This paper shows how the Tarantula, Ochiai, and Jaccard fault-localization algorithms can be enhanced to localize faults effectively in web applications written in PHP by using an extended domain for conditional and function-call statements and by using a source mapping. We also propose several novel test-generation strategies that are geared toward producing test suites that have maximal fault-localization effectiveness. We implemented various fault-localization techniques and test-generation strategies in Apollo, and evaluated them on several open-source PHP applications. Our results indicate that a variant of the Ochiai algorithm that includes all our enhancements localizes 87.8 percent of all faults to within 1 percent of all executed statements, compared to only 37.4 percent for the unenhanced Ochiai algorithm. We also found that all the test-generation strategies that we considered are capable of generating test suites with maximal fault-localization effectiveness when given an infinite time budget for test generation. However, on average, a directed strategy based on path-constraint similarity achieves this maximal effectiveness after generating only 6.5 tests, compared to 46.8 tests for an undirected test-generation strategy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975173","Fault localization;statistical debugging;program analysis;web applications;PHP.","HTML;Databases;Servers;Open source software;Browsers;Algorithm design and analysis;Concrete","program testing;software fault tolerance;statistical analysis","fault localization;dynamic Web applications;statistical analysis;source mapping;fault localization effectiveness;test generation strategies;path constraint;Tarantula;Ochiai;Jaccard;Apollo;open-source PHP applications","","14","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Companson and Diagnosis of Large Replicated Files","W. K. Fuchs; Kun-Lung Wu; J. A. Abraham","Compiler Systems Group, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","1","15","22","This paper examines the problem of comparing large replicated files in a context in which communication dominates the cost of comparison. A low-cost checking matrix is proposed for comparison of these replicated files. The checking matrix is composed of check symbols generated by a divide-and-conquer encoding algorithm. The matrix allows for detection and diagnosis of disagreeing pages with very little communication overhead. In contrast to a previous O(N) proposal, the storage requirement for the checking matrix is O(log N), where N is the number of pages in the file. The matrix can be stored in main memory without the need for extra accesses to disk during normal updates of pages.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232561","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702128","Data transmission;distributed files;file comparison;replicated files","Jacobian matrices;Proposals;Costs;Software systems;Database systems;Context;Encoding;Software performance;Monitoring;Software algorithms","","Data transmission;distributed files;file comparison;replicated files","","2","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Formulating Criticality-Based Cost-Effective Fault Tolerance Strategies for Multi-Tenant Service-Based Systems","Y. Wang; Q. He; D. Ye; Y. Yang","State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Vic, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Vic, Australia","IEEE Transactions on Software Engineering","","2018","44","3","291","307","The proliferation of cloud computing has fueled the rapid growth of multi-tenant service-based systems (SBSs), which serve multiple tenants simultaneously by composing existing services in the form of business processes. In a distributed and volatile operating environment, runtime anomalies may occur to the component services of an SBS and cause end-to-end quality violations. Engineering multi-tenant SBSs that can quickly handle runtime anomalies cost effectively has become a significant challenge. Different approaches have been proposed to formulate fault tolerance strategies for engineering SBSs. However, none of the existing approaches has sufficiently considered the service criticality based on multi-tenancy where multiple tenants share the same SBS instance with different multi-dimensional quality preferences. In this paper, we propose Criticality-based Fault Tolerance for Multi-Tenant SBSs (CFT4MTS), a novel approach that formulates cost-effective fault tolerance strategies for multi-tenant SBSs by providing redundancy for the critical component services. First, the criticality of each component service is evaluated based on its multi-dimensional quality and multiple tenants sharing the component service with differentiated quality preferences. Then, the fault tolerance problem is modelled as an Integer Programming problem to identify the optimal fault tolerance strategy. The experimental results show that, compared with three existing representative approaches, CFT4MTS can alleviate degradation in the quality of multi-tenant SBSs in a much more effective and efficient way.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2681667","Australian Research Council Discovery; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7876832","Cloud computing;criticality;fault tolerance;multi-tenancy;redundancy;service-based system","Fault tolerant systems;Streaming media;Runtime;Redundancy;Cloud computing;Business","cloud computing;integer programming;service-oriented architecture;software fault tolerance","optimal fault tolerance;criticality-based cost-effective fault tolerance;multi-tenant service-based systems;multidimensional quality preferences;cloud computing;Criticality-based Fault Tolerance for Multi-Tenant SBSs;CFT4MTS;end-to-end quality violations;runtime anomalies cost;service criticality;multitenancy;critical component services;Integer Programming problem","","2","","46","","","","","","IEEE","IEEE Journals & Magazines"
"A Remote Procedure Call Facility for Interconnecting Heterogeneous Computer Systems","B. N. Bershad; D. T. Ching; E. D. Lazowska; J. Sanislo; M. Schwartz","Department of Computer Science, University of Washington; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","8","880","894","Heterogeneity in hardware and software is an inevitable consequence of experimental computer research. At the University of Washington, the Heterogeneous Computer Systems (HCS) project is a major research and development effort whose goal is to simplify the interconnection of heterogeneous computer systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233507","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702307","Distributed computer systems;heterogeneous computer systems;remote procedure call","Hardware;Transport protocols;Application software;Workstations;Research and development;Runtime;Buildings;Performance analysis;Testing;Computer science","","Distributed computer systems;heterogeneous computer systems;remote procedure call","","36","","29","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Comparison of Model Validation Techniques for Defect Prediction Models","C. Tantithamthavorn; S. McIntosh; A. E. Hassan; K. Matsumoto","Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan; Department of Electrical and Computer Engineering, Montreal, QC, McGill UniversityCanada; School of Computing, Queen’s University, Kingston, ON, Canada; Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan","IEEE Transactions on Software Engineering","","2017","43","1","1","18","Defect prediction models help software quality assurance teams to allocate their limited resources to the most defect-prone modules. Model validation techniques, such as<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""tantithamthavorn-ieq1-2584050.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>-fold cross-validation, use historical data to estimate how well a model will perform in the future. However, little is known about how accurate the estimates of model validation techniques tend to be. In this paper, we investigate the bias and variance of model validation techniques in the domain of defect prediction. Analysis of 101 public defect datasets suggests that 77 percent of them are highly susceptible to producing unstable results– - selecting an appropriate model validation technique is a critical experimental design choice. Based on an analysis of 256 studies in the defect prediction literature, we select the 12 most commonly adopted model validation techniques for evaluation. Through a case study of 18 systems, we find that single-repetition holdout validation tends to produce estimates with 46-229 percent more bias and 53-863 percent more variance than the top-ranked model validation techniques. On the other hand, out-of-sample bootstrap validation yields the best balance between the bias and variance of estimates in the context of our study. Therefore, we recommend that future defect prediction studies avoid single-repetition holdout validation, and instead, use out-of-sample bootstrap validation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2584050","JSPS; Advancing Strategic International Networks to Accelerate the Circulation of Talented Researchers; Interdisciplinary Global Networks for Accelerating Theory and Practice in Software Ecosystem; JSPS Fellows; Natural Sciences and Engineering Research Council of Canada (NSERC); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7497471","Defect prediction models;model validation techniques;bootstrap validation;cross validation;holdout validation","Predictive models;Data models;Analytical models;Context;Context modeling;Software;Logistics","","","","39","","122","","","","","","IEEE","IEEE Journals & Magazines"
"A domain-specific software architecture for adaptive intelligent systems","B. Hayes-Roth; K. Pfleger; P. Lalanda; P. Morignot; M. Balabanovic","Knowledge Syst. Lab., Stanford Univ., Palo Alto, CA, USA; Knowledge Syst. Lab., Stanford Univ., Palo Alto, CA, USA; Knowledge Syst. Lab., Stanford Univ., Palo Alto, CA, USA; Knowledge Syst. Lab., Stanford Univ., Palo Alto, CA, USA; Knowledge Syst. Lab., Stanford Univ., Palo Alto, CA, USA","IEEE Transactions on Software Engineering","","1995","21","4","288","301","A good software architecture facilitates application system development, promotes achievement of functional requirements, and supports system reconfiguration. We present a domain-specific software architecture (DSSA) that we have developed for a large application domain of adaptive intelligent systems (AISs). The DSSA provides: (a) an AIS reference architecture designed to meet the functional requirements shared by applications in this domain, (b) principles for decomposing expertise into highly reusable components, and (c) an application configuration method for selecting relevant components from a library and automatically configuring instances of those components in an instance of the architecture. The AIS reference architecture incorporates features of layered, pipe and filter, and blackboard style architectures. We describe three studies demonstrating the utility of our architecture in the subdomain of mobile office robots and identify software engineering principles embodied in the architecture.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.385968","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=385968","","Software architecture;Computer architecture;Application software;Decision support systems;Adaptive systems;Intelligent systems;Software libraries;Filters;Teleworking;Mobile robots","adaptive systems;software engineering;mobile robots;office automation;software agents;software reusability;blackboard architecture;reconfigurable architectures","domain-specific software architecture;adaptive intelligent systems;application system development;functional requirements;system reconfiguration;reference architecture;expertise decomposition;highly reusable components;application configuration method;library components selection;automatic configuration;layered architecture;pipe and filter architecture;blackboard architecture;mobile office robots;software engineering principles;software reuse;intelligent agents","","60","","39","","","","","","IEEE","IEEE Journals & Magazines"
"A controlled experiment to assess the benefits of procedure argument type checking","L. Prechelt; W. F. Tichy","Fakultat fur Inf., Karlsruhe Univ., Germany; NA","IEEE Transactions on Software Engineering","","1998","24","4","302","312","Type checking is considered an important mechanism for detecting programming errors, especially interface errors. This report describes an experiment to assess the defect-detection capabilities of static, intermodule type checking. The experiment uses ANSI C and Kernighan & Ritchie (K&R) C. The relevant difference is that the ANSI C compiler checks module interfaces (i.e., the parameter lists calls to external functions), whereas K&R C does not. The experiment employs a counterbalanced design in which each of the 40 subjects, most of them CS PhD students, writes two nontrivial programs that interface with a complex library (Motif). Each subject writes one program in ANSI C and one in K&R C. The input to each compiler run is saved and manually analyzed for defects. Results indicate that delivered ANSI C programs contain significantly fewer interface defects than delivered K&R C programs. Furthermore, after subjects have gained some familiarity with the interface they are using, ANSI C programmers remove defects faster and are more productive (measured in both delivery time and functionality implemented).","0098-5589;1939-3520;2326-3881","","10.1109/32.677186","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=677186","","Programming profession;Computer languages;Computer errors;Productivity;Computer Society;Libraries;Gain measurement;Time measurement;Program processors","program debugging;C language","programming errors;type checking;defect-detection;intermodule type checking;ANSI C;K&R C","","19","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Self-Management of Adaptable Component-Based Applications","L. Rosa; L. Rodrigues; A. Lopes; M. Hiltunen; R. Schlichting","INESC-ID and Universidade Técnica de Lisboa, Lisboa; INESC-ID and Universidade Técnica de Lisboa, Lisboa; University of Lisbon, Lisbon; AT&T Labs-Research, Florham Park; AT&T Labs Research, Florham Park","IEEE Transactions on Software Engineering","","2013","39","3","403","421","The problem of self-optimization and adaptation in the context of customizable systems is becoming increasingly important with the emergence of complex software systems and unpredictable execution environments. Here, a general framework for automatically deciding on when and how to adapt a system whenever it deviates from the desired behavior is presented. In this framework, the system's target behavior is described as a high-level policy that establishes goals for a set of performance indicators. The decision process is based on information provided independently for each component that describes the available adaptations, their impact on performance indicators, and any limitations or requirements. The technique consists of both offline and online phases. Offline, rules are generated specifying component adaptations that may help to achieve the established goals when a given change in the execution context occurs. Online, the corresponding rules are evaluated when a change occurs to choose which adaptations to perform. Experimental results using a prototype framework in the context of a web-based application demonstrate the effectiveness of this approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6197201","Adaptive systems;self-management;autonomic computing;goal policies","Runtime;Context;Software systems;Optimization;Catalogs","fault tolerant computing;optimisation","adaptable component based applications;customizable systems;complex software systems;unpredictable execution environments;desired behavior;target behavior;performance indicators;decision process;Web based application;autonomic computing","","14","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Caching Hints in Distributed Systems","D. B. Terry","Computer Science Laboratory, Xerox Palo Alto Research Center","IEEE Transactions on Software Engineering","","1987","SE-13","1","48","54","Caching reduces the average cost of retrieving data by amortizing the lookup cost over several references to the data. Problems with maintaining strong cache consistency in a distributed system can be avoided by treating cached information as hints. A new approach to managing caches of hints suggests maintaining a minimum level of cache accuracy, rather than maximizing the cache hit ratio, in order to guarantee performance improvements. The desired accuracy is based on the ratio of lookup costs to the costs of detecting and recovering from invalid cache entries. Cache entries are aged so that they get purged when their estimated accuracy falls below the desired level. The age thresholds are dictated solely by clients' accuracy requirements instead of being suggested by data storage servers or system administrators.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232834","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702132","Cache consistency;caching;computer-communication networks;data accuracy;database management;distributed data management;distributed systems;functional lifetimes;hints;resource location","Costs;Memory;File servers;Information retrieval;Network servers;Distributed databases;Computer network management;Resource management;Cache storage;Aging","","Cache consistency;caching;computer-communication networks;data accuracy;database management;distributed data management;distributed systems;functional lifetimes;hints;resource location","","26","","15","","","","","","IEEE","IEEE Journals & Magazines"
"A Taxonomy and Qualitative Comparison of Program Analysis Techniques for Security Assessment of Android Software","A. Sadeghi; H. Bagheri; J. Garcia; S. Malek","School of Information and Computer Sciences, University of California, Irvine, CA; Department of Computer Science and Engineering, University of Nebraska, Lincoln, NE; School of Information and Computer Sciences, University of California, Irvine, CA; School of Information and Computer Sciences, University of California, Irvine, CA","IEEE Transactions on Software Engineering","","2017","43","6","492","530","In parallel with the meteoric rise of mobile software, we are witnessing an alarming escalation in the number and sophistication of the security threats targeted at mobile platforms, particularly Android, as the dominant platform. While existing research has made significant progress towards detection and mitigation of Android security, gaps and challenges remain. This paper contributes a comprehensive taxonomy to classify and characterize the state-of-the-art research in this area. We have carefully followed the systematic literature review process, and analyzed the results of more than 300 research papers, resulting in the most comprehensive and elaborate investigation of the literature in this area of research. The systematic analysis of the research literature has revealed patterns, trends, and gaps in the existing literature, and underlined key challenges and opportunities that will shape the focus of future research efforts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2615307","National Science Foundation; Defense Advanced Research Projects Agency; Army Research Office; Department of Homeland Security; Air Force Office of Scientific Research; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7583740","Taxonomy and survey;security assessment;android platform;program analysis","Androids;Humanoid robots;Security;Taxonomy;Mobile communication;Malware;Systematics","Android (operating system);mobile computing;program diagnostics;security of data","taxonomy;program analysis;security assessment;Android software;mobile software;security threats;mobile platforms;dominant platform;Android security","","18","","517","","","","","","IEEE","IEEE Journals & Magazines"
"Metamorphic Testing for Software Quality Assessment: A Study of Search Engines","Z. Q. Zhou; S. Xiang; T. Y. Chen","School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; Department of Computer Science and Software Engineering, Swinburne University of Technology, Hawthorn, Victoria, Australia","IEEE Transactions on Software Engineering","","2016","42","3","264","284","Metamorphic testing is a testing technique that can be used to verify the functional correctness of software in the absence of an ideal oracle. This paper extends metamorphic testing into a user-oriented approach to software verification, validation, and quality assessment, and conducts large scale empirical studies with four major web search engines: Google, Bing, Chinese Bing, and Baidu. These search engines are very difficult to test and assess using conventional approaches owing to the lack of an objective and generally recognized oracle. The results are useful for both search engine developers and users, and demonstrate that our approach can effectively alleviate the oracle problem and challenges surrounding a lack of specifications when verifying, validating, and evaluating large and complex software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2478001","Australian Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7254235","Software quality;verification;validation;quality assessment;oracle problem;lack of system specification;metamorphic testing;user-oriented testing;search engine;Software quality;verification;validation;quality assessment;oracle problem;lack of system specification;metamorphic testing;user-oriented testing;search engine","Search engines;Testing;Web pages;Google;Software algorithms;Software quality","Internet;program testing;program verification;search engines;software quality;user interfaces","metamorphic testing;software quality assessment;user-oriented approach;software verification;software validation;Web search engines;Google;Chinese Bing;Baidu","","22","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling and Verification of Real-Time Protocols for Broadcast Networks","P. Jain; S. S. Lam","Department of Computer Sciences, University of Texas at Austin; NA","IEEE Transactions on Software Engineering","","1987","SE-13","8","924","937","A class of demand-assigned multiple-access (DAMA) protocols have been proposed for high-speed local area networks (LAN's) that offer integrated services for data, voice, video, and facsimile traffic. These protocols exploit the directionality of signal propagation and implement stringent real-time constraints to achieve collision-freedom. Correct implementation of DAMA protocols will require a very careful analysis of time-dependent interactions using a formal method. To date, most verification methods have been focused on asynchronous communication over point-to-point links.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233511","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702311","Broadcast channel;local area networks;multiple-access protocols;real-time constraints;verification","Broadcasting;Access protocols;Local area networks;Multimedia communication;Delay;Topology;Intserv networks;Facsimile;Telecommunication traffic;Traffic control","","Broadcast channel;local area networks;multiple-access protocols;real-time constraints;verification","","4","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Correct architecture refinement","M. Moriconi; X. Qian; R. A. Riemenschneider","Comput. Sci. Lab., SRI Int., Menlo Park, CA, USA; Comput. Sci. Lab., SRI Int., Menlo Park, CA, USA; Comput. Sci. Lab., SRI Int., Menlo Park, CA, USA","IEEE Transactions on Software Engineering","","1995","21","4","356","372","A method is presented for the stepwise refinement of an abstract architecture into a relatively correct lower-level architecture that is intended to implement it. A refinement step involves the application of a predefined refinement pattern that provides a routine solution to a standard architectural design problem. A pattern contains an abstract architecture schema and a more detailed schema intended to implement it. The two schemas usually contain very different architectural concepts (from different architectural styles). Once a refinement pattern is proven correct, instances of it can be used without proof in developing specific architectures. Individual refinements are compositional, permitting incremental development and local reasoning. A special correctness criterion is defined for the domain of software architecture, as well as an accompanying proof technique. A useful syntactic form of correct composition is defined. The main points are illustrated by means of familiar architectures for a compiler. A prototype implementation of the method has been used successfully in a real application.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.385972","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=385972","","Computer architecture;Software architecture;Concrete;Integrated circuit synthesis;Software prototyping;Prototypes;Application software;Software systems;Data engineering;Computer science","software engineering;program compilers;program verification","correct architecture refinement;stepwise refinement;abstract architecture schema;relatively correct lower-level architecture;predefined refinement pattern;standard architectural design problem;architectural styles;compositional refinements;incremental development;local reasoning;correctness criterion;software architecture;proof technique;syntactic form;correct composition;compiler;hierarchy;formal methods","","130","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating alternative software production functions","Qing Hu","Dept. of Decision & Inf. Syst., Florida Atlantic Univ., Boca Raton, FL, USA","IEEE Transactions on Software Engineering","","1997","23","6","379","387","Software development projects are notorious for cost overruns and schedule delays. While dozens of software cost models have been proposed, few of them seem to have any degree of consistent accuracy. One major factor contributing to this persistent and wide spread problem is an inadequate understanding of the real behavior of software development processes. We believe that software development could be studied as an economic production process and that established economic theories and methods could be used to develop and validate software production and cost models. We present the results of evaluating four alternative software production models using the P-test, a statistical procedure developed specifically for testing the truth of a hypothesis in the presence of alternatives in econometric studies. We found that the truth of the widely used Cobb-Douglas type of software production and cost models (e.g., COCOMO) cannot be maintained in the presence of quadratic or translog models. Overall, the quadratic software production function is shown to be the most plausible model for representing software production processes. Limitations of this study and future directions are also discussed.","0098-5589;1939-3520;2326-3881","","10.1109/32.601078","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=601078","","Programming;Cost function;Econometrics;Software maintenance;Delay;Production systems;Software systems;Software testing;Environmental economics;Size measurement","software cost estimation;economics;project management;statistical analysis","alternative software production function evaluation;software development projects;cost overruns;schedule delays;software cost models;consistent accuracy;software development processes;economic production process;economic theories;cost models;software production models;P-test;statistical procedure;hypothesis testing;econometric studies;translog models;quadratic software production function","","19","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Salient features of an executable specification language and its environment","P. Zave; W. Schell","AT&T Bell Laboratories, Murray Hill, NJ 07974; AT&T Bell Laboratories, Murray Hill, NJ 07974","IEEE Transactions on Software Engineering","","1986","SE-12","2","312","325","The executable specification language PAISLey and its environment are presented as a case study in the design of computer languages. It is shown that PAISLey is unusual (and for some features unique) in having the following desirable features: (1) there is both synchronous and asynchronous parallelism free of mutual-exclusion problems, (2) all computations are encapsulated, (3) specifications in the language can be executed no matter how incomplete they are, (4) timing constraints are executable, (5) specifications are organized so that bounded resource consumption can be guaranteed, (6) almost all forms of inconsistency can be detected by automated checking, and (7) a notable degree of simplicity is maintained. Conclusions are drawn concerning the differences between executable specification languages and programming languages, and potential uses for PAISLey are given.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312946","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312946","Distributed systems;executable specifications;functional programming;interpreters;language design;operational approach to software development;parallelism;performance simulation;programming environments;real-time systems;user interfaces","Parallel processing;Monitoring;Specification languages;Functional programming;Computational modeling","specification languages","executable specification language;PAISLey;computer languages;parallelism;mutual-exclusion;timing constraints;automated checking;simplicity","","28","","","","","","","","IEEE","IEEE Journals & Magazines"
"Exception handlers in functional programming languages","R. Govindarajan","Dept. of Electr. Eng., McGill Univ., Montreal, Que., Canada","IEEE Transactions on Software Engineering","","1993","19","8","826","834","Constructs for expressing exception handling can greatly help to avoid clutter in code by allowing the programmer to separate the code to handle unusual situations from the code for the normal case. The author proposes a new approach to embed exception handlers in functional languages. The proposed approach discards the conventional view of treating exceptions, as a means of effecting a control transfer; instead, exceptions are used to change the state of an object. The two types of exceptions, terminate and resume, are treated differently. A terminate exception, when raised, is viewed as shielding the input object. On the other hand, a resume exception designates the input object as curable and requires the immediate application of a handler function. This approach enables the clean semantics of functions raising exceptions without associating any implementation restriction and without loss of the referential transparency and the commutativity properties of functions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.238585","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=238585","","Functional programming;Resumes;Fault tolerance;Application software;Computer languages;Software standards;Programming profession;Parallel processing;Robustness;Software reliability","exception handling;functional programming;high level languages;programming theory","functional programming;exception handling;programmer;functional languages;terminate;resume;input object;implementation restriction;referential transparency;commutativity properties","","2","","11","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Review of the Application and Empirical Investigation of Search-Based Test Case Generation","S. Ali; L. C. Briand; H. Hemmati; R. K. Panesar-Walawege","Simula Research Laboratory, Lysaker and University of Oslo, Norway; Simula Research Laboratory, Lysaker and University of Oslo, Norway; Simula Research Laboratory, Lysaker and University of Oslo, Norway; Simula Research Laboratory, Lysaker and University of Oslo, Norway","IEEE Transactions on Software Engineering","","2010","36","6","742","762","Metaheuristic search techniques have been extensively used to automate the process of generating test cases, and thus providing solutions for a more cost-effective testing process. This approach to test automation, often coined “Search-based Software Testing” (SBST), has been used for a wide variety of test case generation purposes. Since SBST techniques are heuristic by nature, they must be empirically investigated in terms of how costly and effective they are at reaching their test objectives and whether they scale up to realistic development artifacts. However, approaches to empirically study SBST techniques have shown wide variation in the literature. This paper presents the results of a systematic, comprehensive review that aims at characterizing how empirical studies have been designed to investigate SBST cost-effectiveness and what empirical evidence is available in the literature regarding SBST cost-effectiveness and scalability. We also provide a framework that drives the data collection process of this systematic review and can be the starting point of guidelines on how SBST techniques can be empirically assessed. The intent is to aid future researchers doing empirical studies in SBST by providing an unbiased view of the body of empirical evidence and by guiding them in performing well-designed and executed empirical studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5210118","Evolutionary computing and genetic algorithms;frameworks;heuristics design;review and evaluation;test generation;testing strategies;validation.","System testing;Automatic testing;Software testing;Automation;Costs;Logic testing;Scalability;Guidelines;Genetic algorithms;Algorithm design and analysis","program testing;search problems","search based test case generation;metaheuristic search technique;cost effective testing process;test automation;search based software testing","","154","","55","","","","","","IEEE","IEEE Journals & Magazines"
"A distributed parallel programming framework","N. Stankovic; Kang Zhang","Nokia, Burlington, MA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","5","478","493","This paper presents Visper, a novel object-oriented framework that identifies and enhances common services and programming primitives, and implements a generic set of classes applicable to multiple programming models in a distributed environment. Groups of objects, which can be programmed in a uniform and transparent manner, and agent-based distributed system management, are also featured in Visper. A prototype system is designed and implemented in Java, with a number of visual utilities that facilitate program development and portability. As a use case, Visper integrates parallel programming in an MPI-like message-passing paradigm at a high level with services such as checkpointing and fault tolerance at a lower level. The paper reports a range of performance evaluation on the prototype and compares it to related works.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000451","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000451","","Parallel programming","parallel programming;application program interfaces;message passing;object-oriented programming;visual programming;software fault tolerance","distributed parallel programming framework;Visper;object-oriented framework;programming primitives;multiple programming models;distributed environment;agent-based distributed system management;visual utilities;MPI-like message passing paradigm;visual programming;fault tolerance","","15","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Name-Based Analysis of Equally Typed Method Arguments","M. Pradel; T. R. Gross","ETH Zurich, Zurich; ETH Zurich, Zurich","IEEE Transactions on Software Engineering","","2013","39","8","1127","1143","When calling a method that requires multiple arguments, programmers must pass the arguments in the expected order. For statically typed languages, the compiler helps programmers by checking that the type of each argument matches the type of the formal parameter. Unfortunately, types are futile for methods with multiple parameters of the same type. How can a programmer check that equally typed arguments are passed in the correct order? This paper presents two simple, yet effective, static program analyses that detect problems related to the order of equally typed arguments. The key idea is to leverage identifier names to infer the semantics of arguments and their intended positions. The analyses reveal problems that affect the correctness, understandability, and maintainability of a program, such as accidentally reversed arguments and misleading parameter names. Most parts of the analyses are language-agnostic. We evaluate the approach with 24 real-world programs written in Java and C. Our results show the analyses to be effective and efficient. One analysis reveals anomalies in the order of equally typed arguments; it finds 54 relevant problems with a precision of 82 percent. The other analysis warns about misleading parameter names and finds 31 naming bugs with a precision of 39 percent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6419711","Testing and debugging;maintenance;documentation;static program analysis;anomaly detection;method arguments","Java;Access control;Engines;Feature extraction;Context;Robustness;Program processors","C language;Java;program diagnostics","name-based analysis;equally typed method argument;statically typed language;formal parameter;static program analysis;program correctness;program understandability;program maintainability;Java language;C language","","3","","40","","","","","","IEEE","IEEE Journals & Magazines"
"General test result checking with log file analysis","J. H. Andrews; Yingjun Zhang","Dept. of Comput. Sci., Univ. of Western Ontario, London, Ont., Canada; Dept. of Comput. Sci., Univ. of Western Ontario, London, Ont., Canada","IEEE Transactions on Software Engineering","","2003","29","7","634","648","We describe and apply a lightweight formal method for checking test results. The method assumes that the software under test writes a text log file; this log file is then analyzed by a program to see if it reveals failures. We suggest a state-machine-based formalism for specifying the log file analyzer programs and describe a language and implementation based on that formalism. We report on empirical studies of the application of log file analysis to random testing of units. We describe the results of experiments done to compare the performance and effectiveness of random unit testing with coverage checking and log file analysis to other unit testing procedures. The experiments suggest that writing a formal log file analyzer and using random testing is competitive with other formal and informal methods for unit testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214327","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214327","","Software testing;Failure analysis;Software safety;Inspection;Automatic testing;Humans;System testing;Performance analysis;Writing;Real time systems","program testing;program debugging;program verification;formal specification;finite state machines","software testing;event-based debugging;log file;state-machine-based formalism;safety verification;lightweight formal methods;test oracles;unit testing","","36","","45","","","","","","IEEE","IEEE Journals & Magazines"
"The SeaView security model","T. F. Lunt; D. E. Denning; R. R. Schell; M. Heckman; W. R. Shockley","SRI Int., Menlo Park, CA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1990","16","6","593","607","A multilevel database is intended to provide the security needed for database systems that contain data at a variety of classifications and serve a set of users having different clearances. A formal security model for such a system is described. The model is formulated in two layers, one corresponding to a reference monitor that enforces mandatory security, and the second an extension of the standard relational model defining multilevel relations and formalizing policies for labeling new and derived data, data consistency, and discretionary security. The model also defines application-independent properties for entity integrity, referential integrity, and polyinstantiation integrity.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.55088","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=55088","","Data security;Database systems;Relational databases;Multilevel systems;Military computing;Transaction databases;Monitoring;Labeling;Protection;Authorization","relational databases;security of data;software engineering","policy formalization;new data;SeaView security model;multilevel database;classifications;users;clearances;formal security model;reference monitor;mandatory security;standard relational model;multilevel relations;labeling;derived data;data consistency;discretionary security;application-independent properties;entity integrity;referential integrity;polyinstantiation integrity","","82","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Test selection based on finite state models","S. Fujiwara; G. v. Bochmann; F. Khendek; M. Amalou; A. Ghedamsi","Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada; Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada; Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada; Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada; Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada","IEEE Transactions on Software Engineering","","1991","17","6","591","603","A method for the selection of appropriate test case, an important issue for conformance testing of protocol implementations as well as software engineering, is presented. Called the partial W-method, it is shown to have general applicability, full fault-detection power, and yields shorter test suites than the W-method. Various other issues that have an impact on the selection of a suitable test suite including the consideration of interaction parameters, various test architectures for protocol testing and the fact that many specifications do not satisfy the assumptions made by most test selection methods (such as complete definition, a correctly implemented reset function, a limited number of states in the implementation, and determinism), are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.87284","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=87284","","Protocols;Software testing;Automata;Hardware;System testing;Software engineering;Formal specifications;Councils","conformance testing;program testing;protocols","finite state models;conformance testing;protocol implementations;software engineering;partial W-method;full fault-detection power;test suites;interaction parameters;test architectures;protocol testing;reset function;determinism","","280","","24","","","","","","IEEE","IEEE Journals & Magazines"
"On the Need for Mixed Media in Distributed Requirements Negotiations","D. Damian; F. Lanubile; T. Mallardo","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","116","132","Achieving agreement with respect to software requirements is a collaborative process that traditionally relies on same-time, same-place interactions. As the trend toward geographically distributed software development continues, colocated meetings are becoming increasingly problematic. Our research investigates the impact of computer-mediated communication on the performance of distributed client/developer teams involved in the collaborative development of a requirements specification. Drawing on media-selection theories, we posit that a combination of lean and rich media is needed for an effective process of requirements negotiations when stakeholders are geographically dispersed. In this paper, we present an empirical study that investigates the performance of six educational global project teams involved in a negotiation process using both asynchronous text-based and synchronous videoconferencing-based communication modes. The findings indicate that requirement negotiations were more effective when the groups conducted asynchronous structured discussions of requirement issues prior to the synchronous negotiation meeting. Asynchronous discussions were useful in resolving issues related to uncertainty in requirements, thus allowing synchronous negotiations to focus more on removing ambiguities in the requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70758","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4378346","Requirements/Specifications;Distributed/Internet based software engineering tools and techniques;Experimental design;Asynchronous interaction;Synchronous interaction;Requirements/Specifications;Distributed/Internet based software engineering tools and techniques;Experimental design;Asynchronous interaction;Synchronous interaction","Collaboration;Programming;Collaborative software;Computer mediated communication;Grounding;Computer science;Dispersion;Uncertainty;Globalization;Computer industry","formal specification;multimedia communication","mixed media;distributed requirements negotiations;software requirements;geographically distributed software development;computer-mediated communication;distributed client-developer teams;requirements specification;media-selection theories;educational global project teams;asynchronous text-based communication;synchronous videoconferencing-based communication;synchronous negotiation meeting","","26","","50","","","","","","IEEE","IEEE Journals & Magazines"
"A transportable programming language (TPL) system. II. The bifunctional compiler system","S. Leong; S. Jodis; K. Sullivan; O. Jiang; P. A. D. de Maine","Dept. of Comput. Sci. & Eng., Auburn Univ., AL, USA; Dept. of Comput. Sci. & Eng., Auburn Univ., AL, USA; Dept. of Comput. Sci. & Eng., Auburn Univ., AL, USA; Dept. of Comput. Sci. & Eng., Auburn Univ., AL, USA; Dept. of Comput. Sci. & Eng., Auburn Univ., AL, USA","IEEE Transactions on Software Engineering","","1990","16","6","639","646","For pt.I see P.A.D. de Maine, S. Leong, and C.G. Dairs, Int. J. Comput. Inform. Sci., vol.14, p.161-82, 1985. The transportable programming language (TPL) method is a high-level-language approach that uses a bifunctional compiler to efficiently convert code among various dialects of a particular high-level language (HLL) via the hypothetical parent of the high-level language (HPHLL). The TPL compiler system that has been implemented has three parts: a rule modifier, a table generator, and a TPL compiler. A metalanguage, called the conversion rule description language (CRDL), is used to describe the conversion of a dialect to HPHLL and of the HPHLL to another dialect. The table generator translates those descriptions to tabular forms that drive the bifunctional compiler. The TPL compiler can then be used to translate programs coded in a local dialect into HPHLL and vice versa. The rule modifier alters the descriptions of a default-a synthetic 'most common'-dialect. It greatly simplifies the task of writing the conversion descriptions for a new environment or dialect. The TPL method is now being extended so that it can be used to retarget a dialect of any HLL to a standard environment such as Ada. Details of the TPL compiler system are given.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.55092","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=55092","","Computer languages;High level languages;Program processors;Computer science;Writing;Transportation;Performance evaluation;Software testing;Time to market","high level languages;program compilers","code conversion, program translation;transportable programming language;high-level-language;bifunctional compiler;hypothetical parent;rule modifier;table generator;metalanguage;conversion rule description language;tabular forms","","3","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Software Fault Tolerance: An Evaluation","T. Anderson; P. A. Barrett; D. N. Halliwell; M. R. Moulding","Centre for Software Reliability, University of Newcastle upon Tyne; NA; NA; NA","IEEE Transactions on Software Engineering","","1985","SE-11","12","1502","1510","In order to assess the effectiveness of software fault tolerance techniques for enhancing the reliability of practical systems, a major experimental project has been conducted at the University of Newcastle upon Tyne. Techniques were developed for, and applied to, a realistic implementation of a real-time system (a naval command and control system). Reliability data were collected by operating this system in a simulated tactical environment for a variety of action scenarios. This paper provides an overview of the project and presents the results of three phases of experimentation. An analysis of these results shows that use of the software fault tolerance approach yielded a substantial improvement in the reliability of the command and control system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231894","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701973","Real-time systems;software fault tolerance;software reliability","Fault tolerance;Software reliability;Fault tolerant systems;Reliability engineering;Command and control systems;Software systems;Programming;Software performance;Real time systems;Software maintenance","","Real-time systems;software fault tolerance;software reliability","","34","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Using origin analysis to detect merging and splitting of source code entities","M. W. Godfrey; L. Zou","Sch. of Comput. Sci., Waterloo Univ., Ont., Canada; Sch. of Comput. Sci., Waterloo Univ., Ont., Canada","IEEE Transactions on Software Engineering","","2005","31","2","166","181","Merging and splitting source code entities is a common activity during the lifespan of a software system; as developers rethink the essential structure of a system or plan for a new evolutionary direction, so must they be able to reorganize the design artifacts at various abstraction levels as seems appropriate. However, while the raw effects of such changes may be plainly evident in the new artifacts, the original context of the design changes is often lost. That is, it may be obvious which characters of which files have changed, but it may not be obvious where or why moving, renaming, merging, and/or splitting of design elements has occurred. In this paper, we discuss how we have extended origin analysis (Q. Tu et al., 2002), (M.W. Godfrey et al., 2002) to aid in the detection of merging and splitting of files and functions in procedural code; in particular, we show how reasoning about how call relationships have changed can aid a developer in locating where merges and splits have occurred, thereby helping to recover some information about the context of the design change. We also describe a case study of these techniques (as implemented in the Beagle tool) using the PostgreSQL database system as the subject.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401931","Index Terms- Software evolution;origin analysis;restructuring;reverse engineering;and reengineering.","Merging;Software systems;History;Software maintenance;Information analysis;Database systems;Reverse engineering;Environmental management;Software tools;Documentation","reverse engineering;systems re-engineering;software maintenance;software development management;merging;SQL;reasoning about programs","merging;source code entity;software system;origin analysis;PostgreSQL database system","","97","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A tool to help tune where computation is performed","Hyeonsang Eom; J. K. Hollingsworth","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA","IEEE Transactions on Software Engineering","","2001","27","7","618","629","We introduce a new performance metric, called load balancing factor (LBF), to assist programmers when evaluating different tuning alternatives. The LBF metric differs from traditional performance metrics since it is intended to measure the performance implications of a specific tuning alternative rather than quantifying where time is spent in the current version of the program. A second unique aspect of the metric is that it provides guidance about moving work within a distributed or parallel program rather than reducing it. A variation of the LBF metric can also be used to predict the performance impact of changing the underlying network. The LBF metric is computed incrementally and online during the execution of the program to be tuned. We also present a case study that shows that our metric can accurately predict the actual performance gains for a test suite of six programs.","0098-5589;1939-3520;2326-3881","","10.1109/32.935854","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=935854","","Programming profession;Computational modeling;Performance evaluation;Current measurement;Performance analysis;Load management;Time measurement;Performance gain;Testing;Distributed computing","software performance evaluation;software metrics;distributed programming;parallel programming","performance metric;load balancing factor;tuning;parallel program;distributed program","","","","26","","","","","","IEEE","IEEE Journals & Magazines"
"A noninterference monitoring and replay mechanism for real-time software testing and debugging","J. J. P. Tsai; K. -. Fang; H. -. Chen; Y. -. Bi","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA","IEEE Transactions on Software Engineering","","1990","16","8","897","916","A noninterference monitoring and replay mechanism using the recorded execution history of a program to control the replay of the program behavior and guarantee the reproduction of its errors is presented. Based on this approach, a noninterference monitoring architecture has been developed to collect the program execution data of a target real-time software system without affecting its execution. A replay mechanism designed to control the reproduction of the program behavior as well as the examination of the states of the target system and its behavior is presented. The monitoring system has been implemented using a Motorola 68000 computer in a Unix system environment. An example is used to illustrate how the mechanism detects timing errors of real-time software systems.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.57626","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=57626","","Monitoring;Software testing;Real time systems;Software systems;Software debugging;Timing;System testing;History;Error correction;Computer architecture","program debugging;program testing;real-time systems","real-time software testing;recorded execution history;program behavior;noninterference monitoring architecture;program execution data;target real-time software system;replay mechanism;monitoring system;Motorola 68000 computer;Unix system environment;timing errors;real-time software systems","","76","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Towards Prioritizing Documentation Effort","P. W. McBurney; S. Jiang; M. Kessentini; N. A. Kraft; A. Armaly; M. W. Mkaouer; C. McMillan","Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer and Information Science, University of Michigan-Dearborn, Dearborn, MI; ABB Corporate Research, Raleigh, NC; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer and Information Science, University of Michigan-Dearborn, Dearborn, MI; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN","IEEE Transactions on Software Engineering","","2018","44","9","897","913","Programmers need documentation to comprehend software, but they often lack the time to write it. Thus, programmers must prioritize their documentation effort to ensure that sections of code important to program comprehension are thoroughly explained. In this paper, we explore the possibility of automatically prioritizing documentation effort. We performed two user studies to evaluate the effectiveness of static source code attributes and textual analysis of source code towards prioritizing documentation effort. The first study used open-source API Libraries while the second study was conducted using closed-source industrial software from ABB. Our findings suggest that static source code attributes are poor predictors of documentation effort priority, whereas textual analysis of source code consistently performed well as a predictor of documentation effort priority.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2716950","National Science Foundation Graduate Research Fellowship Program; National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7953505","Code documentation;program comprehension;software maintenance","Documentation;Libraries;Java;Gold;Programming;Software;Neural networks","application program interfaces;program diagnostics;public domain software;software maintenance","prioritizing documentation effort;static source code attributes;open-source API Libraries;closed-source industrial software;documentation effort priority","","","","78","","","","","","IEEE","IEEE Journals & Magazines"
"Strategies for the prevention of communication deadlocks in distributed parallel programs","V. C. Barbosa","Programa de Engenharia de Sistemas e Computacao, Univ. Federal do Rio de Janeiro, Brazil","IEEE Transactions on Software Engineering","","1990","16","11","1311","1316","The occurrence of communication deadlocks caused by the unavailability of message buffers during the execution of distributed parallel programs is investigated. Such deadlocks can occur even if the program is designed for deadlock-freedom, since they are largely dependent on the system's ability to handle message buffering space. A class of deadlock prevention strategies which require that the programmer provide upper bounds on the buffer usage in the several communication channels involved is exploited, and it is argued that such bounds are relatively simple to obtain in many cases. The proposed strategies range from those which require a minimal amount of buffers to those which ensure a reasonable level of concurrency in process execution, although at the expense of more buffering space. It is shown that in general these strategies require the solution of NP-hard optimization problems, and an efficient heuristic to tackle the concurrency-optimal strategy is suggested. Randomly generated systems are then used to show that the heuristic tends to be very successful.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60319","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60319","","System recovery;Routing;Programming profession;Upper bound;Communication channels;Concurrent computing;Integrated circuit interconnections;Buffer storage;Parallel processing","computational complexity;parallel programming;programming theory","randomly generated systems;communication deadlocks;distributed parallel programs;unavailability;message buffers;message buffering space;deadlock prevention;upper bounds;buffer usage;communication channels;concurrency;process execution;NP-hard optimization problems;heuristic","","6","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A theory of interfaces and modules I/spl minus/composition theorem","S. S. Lam; A. U. Shankar","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA","IEEE Transactions on Software Engineering","","1994","20","1","55","71","We model a system as a directed acyclic graph where nodes represent modules and arcs represent interfaces. At the heart of our theory is a definition of what it means for a module to satisfy a set of interfaces as a service provider for some and as a service consumer for others. Our definition of interface satisfaction is designed to be separable; i.e., interfaces encode adequate information such that each module in a system can be designed and verified separately, and composable; i.e., we have proved a composition theorem for the system model in general.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.263755","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=263755","","Heart;Transport protocols;Communication networks;Routing protocols;Telecommunication network reliability;Australia;Computer science;Software engineering","directed graphs;systems analysis;user interfaces;formal specification","interface theory;modules;composition theorem;system modelling;directed acyclic graph;nodes;arcs;interface satisfaction;service provider;service consumer;module design;module verification;system model;system design;specification","","18","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing two functional programming systems","B. Hailpern; T. Huynh; G. Revesz","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","1989","15","5","532","542","A technique is presented for comparing the performance of functional languages with different evaluation strategies running on different machines. A set of small benchmarks is used, and th execution times of these programs running in the functional language and in the implementation language of the functional system are compared. The ratio of these execution times measured how well the functional system used the resources of the underlying hardware and implementation language. Also two functional programming systems are described. One system is a graph reduction interpreter for lambda calculus. The other is a DEL-style intermediate instruction set architecture for FP. The benchmarks in FP and the performances of the two systems on these benchmarks are presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24702","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24702","","Functional programming;Calculus;Workstations;Hardware;Application software;Statistical analysis;Complexity theory;Computer architecture;Software performance;Radio access networks","functional programming;high level languages;performance evaluation","performance comparison;functional programming systems;functional languages;evaluation strategies;benchmarks;execution times;implementation language;graph reduction interpreter;lambda calculus;DEL-style intermediate instruction set architecture;FP","","1","","36","","","","","","IEEE","IEEE Journals & Magazines"
"A methodology for architecture-level reliability risk analysis","S. M. Yacoub; H. H. Ammar","Hewlett-Packard Labs., Palo Alto, CA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","6","529","547","The paper presents a methodology for reliability risk assessment at the early stages of the development lifecycle, namely, the architecture level. We describe a heuristic risk assessment methodology that is based on dynamic metrics. The methodology uses dynamic complexity and dynamic coupling metrics to define complexity factors for the architecture elements (components and connectors). Severity analysis is performed using Failure Mode and Effect Analysis (FMEA) as applied to architecture models. We combine severity and complexity factors to develop heuristic risk factors for the architecture components and connectors. Based on analysis scenarios, we develop a risk assessment model that represents components, connectors, component risk factors, connector risk factors, and probabilities of component interactions. We also develop a risk analysis algorithm that aggregates risk factors of components and connectors to the architectural level. Using the risk aggregation and the risk analysis model, we show how to analyze the overall risk factor of the architecture as the function of the risk factors of its constituting components and connectors. A case study of a pacemaker architecture is used to illustrate the application of the methodology. The methodology is used to identify critical components and connectors and to investigate the sensitivity of the architecture risk factor to changes in the heuristic risk factors of the architecture elements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010058","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010058","","Risk analysis;Risk management;Connectors;Failure analysis;Performance analysis;Computer architecture;Computer Society;Frequency;Humans;Computer errors","software reliability;software metrics;risk management;software architecture","architecture-level reliability risk analysis methodology;software risk management plan;subjective judgement;domain experts;subjective risk assessment techniques;product attributes;product metrics;reliability risk assessment;development lifecycle;architecture level;heuristic risk assessment methodology;dynamic metrics;dynamic complexity;dynamic coupling metrics;complexity factors;architecture elements;severity analysis;Failure Mode and Effect Analysis;FMEA;architecture models;severity factors;heuristic risk factors;architecture components;analysis scenarios;risk assessment model;component risk factors;connector risk factors;component interactions;risk factors;architectural level;risk aggregation;pacemaker architecture;architecture risk factor;reliability risk analysis;software architecture","","88","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Invariant-Based Automatic Testing of Modern Web Applications","A. Mesbah; A. van Deursen; D. Roest","University of British Columbia, Vancouver; Delft University of Technology, Delft; Delft University of Technology, Delft","IEEE Transactions on Software Engineering","","2012","38","1","35","53","Ajax-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing Ajax applications automatically, based on a crawler to infer a state-flow graph for all (client-side) user interface states. We identify Ajax-specific faults that can occur in such states (related to, e.g., DOM validity, error messages, discoverability, back-button compatibility) as well as DOM-tree invariants that can serve as oracles to detect such faults. Our approach, called Atusa, is implemented in a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe three case studies, consisting of six subjects, evaluating the type of invariants that can be obtained for Ajax applications as well as the fault revealing capabilities, scalability, required manual effort, and level of automation of our testing approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728834","Automated testing;web applications;Ajax.","Web and internet services;Browsers;User interfaces;Robots;Servers","automatic testing;client-server systems;Internet;Java;program testing;trees (mathematics);user interfaces;XML","invariant-based automatic testing;AJAX-based Web 2.0 application;stateful asynchronous client-server communication;client-side runtime manipulation;state-flow graph;user interface;AJAX-specific fault identification;DOM-tree invariant;fault detection;generic invariant checking component;application-specific state validator;fault revealing capability","","53","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Generating annotated behavior models from end-user scenarios","C. Damas; B. Lambeau; P. Dupont; A. van Lamsweerde","Dept. of Compt. Sci. & Eng., Univ. Catholique de Louvain, Belgium; Dept. of Compt. Sci. & Eng., Univ. Catholique de Louvain, Belgium; Dept. of Compt. Sci. & Eng., Univ. Catholique de Louvain, Belgium; Dept. of Compt. Sci. & Eng., Univ. Catholique de Louvain, Belgium","IEEE Transactions on Software Engineering","","2005","31","12","1056","1073","Requirements-related scenarios capture typical examples of system behaviors through sequences of desired interactions between the software-to-be and its environment. Their concrete, narrative style of expression makes them very effective for eliciting software requirements and for validating behavior models. However, scenarios raise coverage problems as they only capture partial histories of interaction among-system component instances. Moreover, they often leave the actual requirements implicit. Numerous efforts have therefore been made recently to synthesize requirements or behavior models inductively from scenarios. Two problems arise from those efforts. On the one hand, the, scenarios must be complemented with additional input such as state assertions along episodes or flowcharts on such episodes. This makes such techniques difficult to use by the nonexpert end-users who provide the scenarios. On the other hand, the generated state machines may be hard to understand as their nodes generally convey no domain- specific properties. Their validation by analysts, complementary to model checking and animation by may therefore be quite difficult. This paper describes tool-supported techniques that overcome those two problems. Our tool generates a labeled transition system (LTS) for each system component from simple forms of message sequence charts (MSC) taken as examples or counterexamples of desired behavior. No additional input is required. A global LTS for the entire system is synthesized first. This LTS covers all scenario examples and excludes all counterexamples. It is inductively generated through an interactive procedure that extends known learning techniques for grammar induction. The procedure is incremental on training examples. It interactively produces additional scenarios that the end-user has to classify as examples or counterexamples of desired behavior. The LTS synthesis procedure may thus also be used independently for requirements elicitation through scenario questions generated by the tool. The synthesized system LTS is then projected on local LTS for each system component. For model validation by analysts, the tool generates state invariants that decorate the nodes of the local LTS.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.138","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566607","Index Terms- Scenario-based elicitation;synthesis of behavior models;scenario generation;invariant generation;labeled transition systems;message sequence charts;model validation;incremental learning;analysis tools.","Concrete;Flowcharts;Induction generators;Application software;History;Animation;Independent component analysis;Documentation;Production;Control systems","formal verification;learning (artificial intelligence);flowcharting;grammars;Petri nets","annotated behavior model;end-user scenarios;software requirement;flowcharts;state machines;domain- specific property;model checking;computer animation;labeled transition system;message sequence charts;learning technique;grammar induction;incremental training;system component;model validation by analysts","","49","","31","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study of Software Metrics","H. F. Li; W. K. Cheung","Department of Computer Science; NA","IEEE Transactions on Software Engineering","","1987","SE-13","6","697","708","Software metrics are computed for the purpose of evaluating certain characteristics of the software developed. A Fortran static source code analyzer, FORTRANAL, was developed to study 31 metrics, including a new hybrid metric introduced in this paper, and applied to a database of 255 programs, all of which were student assignments. Comparisons among these metrics are performed. Their cross-correlation confirms the internal consistency of some of these metrics which belong to the same class. To remedy the incompleteness of most of these metrics, the proposed metric incorporates context sensitivity to structural attributes extracted from a flow graph. It is also concluded that many volume metrics have similar performance while some control metrics surprisingly correlate well with typical volume metrics in the test samples used. A flexible class of hybrid metric can incorporate both volume and control attributes in assessing software complexity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233475","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702275","Control complexity;cross correlation;empirical study;Halstead's software science;hybrid metrics;software metric","Software metrics;Software measurement;Volume measurement;Size measurement;Software maintenance;Costs;Size control;Databases;Flow graphs;Testing","","Control complexity;cross correlation;empirical study;Halstead's software science;hybrid metrics;software metric","","30","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Static analysis and dynamic steering of time-dependent systems","E. Vicario","Dept. of Syst. & Inf., Florence Univ., Italy","IEEE Transactions on Software Engineering","","2001","27","8","728","748","An enumerative technique is presented which supports reachability and timeliness analysis of time-dependent models. The technique assumes a dense model of time and uses equivalence classes to enable discrete and compact enumeration of the state space. Properties of timed reachability among states are recovered through the analysis of timing constraints embedded within equivalence classes. In particular, algorithms are given to evaluate a tight profile for the set of feasible timings of any untimed run. Runtime refinement of static profiles supports a mixed static/dynamic strategy in the development of a failure avoidance mechanism for dynamic acceptance and a guarantee of hard real-time processes.","0098-5589;1939-3520;2326-3881","","10.1109/32.940727","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=940727","","Real time systems;Timing;Scheduling;Runtime;Testing;Polynomials;Time factors;Computer Society;State-space methods;Petri nets","program diagnostics;reachability analysis;equivalence classes;real-time systems;Petri nets","static analysis;dynamic steering;time-dependent systems;enumerative technique;reachability;timeliness analysis;time-dependent models;dense model;equivalence classes;compact enumeration;state space;timed reachability;timing constraints;tight profile;feasible timings;untimed run;runtime refinement;static profiles;mixed static/dynamic strategy;failure avoidance mechanism;dynamic acceptance;hard real-time processes","","67","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Distributed Checkpointing for Globally Consistent States of Databases","Sang Hyuk Son; A. K. Agrawala","Department of Computer Science, University of Virginia, Charlottesville, VA 22903.; NA","IEEE Transactions on Software Engineering","","1989","15","10","1157","1167","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1989.559763","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=559763","","Checkpointing;Distributed databases;Transaction databases;Database systems;Costs;Computer science;Algorithm design and analysis;Robustness;Interference","","Availability;checkpoint;consistency;distributed database;noninterference;recovery;transaction","","14","","24","","","","","","IEEE","IEEE Journals & Magazines"
"How Developers' Experience and Ability Influence Web Application Comprehension Tasks Supported by UML Stereotypes: A Series of Four Experiments","F. Ricca; M. Di Penta; M. Torchiano; P. Tonella; M. Ceccato","University of Genova, Italy; University of Sannio, Benevento; Politecnico di Torino, Torino; Fondazione Bruno Kessler, Trento; Fondazione Bruno Kessler, Trento","IEEE Transactions on Software Engineering","","2010","36","1","96","118","In recent years, several design notations have been proposed to model domain-specific applications or reference architectures. In particular, Conallen has proposed the UML Web Application Extension (WAE): a UML extension to model Web applications. The aim of our empirical investigation is to test whether the usage of the Conallen notation supports comprehension and maintenance activities with significant benefits, and whether such benefits depend on developers ability and experience. This paper reports and discusses the results of a series of four experiments performed in different locations and with subjects possessing different experience-namely, undergraduate students, graduate students, and research associates-and different ability levels. The experiments aim at comparing performances of subjects in comprehension tasks where they have the source code complemented either by standard UML diagrams or by diagrams stereotyped using the Conallen notation. Results indicate that, although, in general, it is not possible to observe any significant benefit associated with the usage of stereotyped diagrams, the availability of stereotypes reduces the gap between subjects with low skill or experience and highly skilled or experienced subjects. Results suggest that organizations employing developers with low experience can achieve a significant performance improvement by adopting stereotyped UML diagrams for Web applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5332231","Documentation;maintenance;and enhancement;software engineering;software/software engineering.","Unified modeling language;Application software;Object oriented modeling;Software maintenance;Web pages;Software engineering;Computer Society;Computer architecture;Service oriented architecture;Testing","Internet;Unified Modeling Language","Web application comprehension tasks;UML stereotypes;source code;stereotyped diagrams","","41","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Making Pointers Safe in System Programming Languages","D. B. Lomet","IBM Thomas J. Watson Research Center","IEEE Transactions on Software Engineering","","1985","SE-11","1","87","96","System programming languages usually provide pointers so as to permit efficient and understandable programs to be written. Some higher level languages either avoid pointers altogether or greatly circumscribe pointers to guarantee safety, i.e., so that programs cannot gain access to storage in an inappropriate way. By combining the ideas of 1) pointer scope front Algol 68, 2) tombstones for invalidating dangling references, and 3) freezing which permits freeable objects to have scoped pointers, we solVe the problem of providing convenient and efficient pointers while simultaneously guaranteeing safety.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231846","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701901","Efficiency;pointers;programming languages;safety","Computer languages;Safety;Data structures;Arithmetic;Protection;Aggregates;Programming profession;Delay effects","","Efficiency;pointers;programming languages;safety","","1","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental integration testing of concurrent programs","P. V. Koppol; R. H. Carver; Kuo-Chung Tai","High Speed Networks Res. Dept., Lucent Technol. Bell Labs, Holmdel, NJ, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","6","607","623","We present a method for selecting test sequences for concurrent programs from labeled transitions systems (LTS). A common approach to selecting test sequences from a set of LTSs is to derive a global LTS, called the reachability graph, and then force deterministic program executions according to paths selected from the graph. However, using a reachability graph for test path selection introduces a state explosion problem. To overcome this problem, a reduced graph can be generated using incremental reachability analysis, which consists of repeatedly generating a reachability graph for a subset of LTSs, reducing this graph, and using the reduced graph in place of the original LTSs. Unfortunately, existing incremental reachability analysis techniques generate reduced graphs with insufficient information for deterministic testing. We present an incremental approach to testing concurrent programs. Incremental testing consists of incremental reachability analysis for test path selection and deterministic testing for test execution. We define a new type of reachability graph for incremental analysis, called an annotated labeled transition system (ALTS). An ALTS is an LTS annotated with information necessary for deterministic testing. We propose practical coverage criteria for selecting tests paths from an ALTS and present an ALTS reduction algorithm. The results of several case studies are reported.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010062","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010062","","Reachability analysis;System testing;Concrete;Explosions;Specification languages;Performance evaluation;Carbon capture and storage","parallel programming;program testing;reachability analysis","incremental integration testing;concurrent program testing;test sequences;labeled transitions systems;global LTS;reachability graph;deterministic program executions;test path selection;state explosion problem;reduced graph;incremental reachability analysis techniques;deterministic testing;test execution;annotated labeled transition system;ALTS reduction algorithm;structural testing;incremental testing","","21","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Treatment-Outcome Constructs from Sequential Software Engineering Data","M. Nayebi; G. Ruhe; T. Zimmermann","Département de Génie Informatique et Génie Logiciel, Ecole Polytechnique de Montreal, 5596 Montreal, Quebec Canada (e-mail: mnayebi@polymtl.ca); Computer Science and Electrical Engineering, University of Calgary, Calgary, Alberta Canada T2N 1N4 (e-mail: ruhe@ucalgary.ca); Microsoft Research, Microsoft Corportation, Redmond, Washington United States 98052 (e-mail: tzimmer@microsoft.com)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Many investigations in empirical software engineering look at sequences of data resulting from development or management processes. In this paper, we propose an analytical approach called the Gandhi-Washington Method (GWM) to investigate the impact of recurring events in software projects. GWM takes an encoding of events and activities provided by a software analyst as input. It uses regular expressions to automatically condense and summarize information and infer treatments. Relating the treatments to the outcome through statistical tests, treatment-outcome constructs are automatically mined from the data. The output of this process is a set of treatment-outcome constructs. Each treatment in the set of mined constructs is significantly different from the other treatments considering the impact on the outcome and/or is structurally different from other treatments considering the sequence of events. We describe GWM and classes of problems to which GWM can be applied. We demonstrate the applicability of this method for empirical studies on sequences of file editing, code ownership, and release cycle time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2892956","Canadian Network for Research and Innovation in Machining Technology Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8620369","Pattern mining;Mining software repositories;Regular expressions;Analytics;Release cycle time patterns;Code ownership","Software;Computer bugs;Encoding;Data mining;Software engineering;Testing;Itemsets","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Trustrace: Mining Software Repositories to Improve the Accuracy of Requirement Traceability Links","N. Ali; Y. Guéhéneuc; G. Antoniol","École Polytechnique de Montréal, Montréal; École Polytechnique de Montréal, Montréal; École Polytechnique de Montréal, Montréal","IEEE Transactions on Software Engineering","","2013","39","5","725","741","Traceability is the only means to ensure that the source code of a system is consistent with its requirements and that all and only the specified requirements have been implemented by developers. During software maintenance and evolution, requirement traceability links become obsolete because developers do not/cannot devote effort to updating them. Yet, recovering these traceability links later is a daunting and costly task for developers. Consequently, the literature has proposed methods, techniques, and tools to recover these traceability links semi-automatically or automatically. Among the proposed techniques, the literature showed that information retrieval (IR) techniques can automatically recover traceability links between free-text requirements and source code. However, IR techniques lack accuracy (precision and recall). In this paper, we show that mining software repositories and combining mined results with IR techniques can improve the accuracy (precision and recall) of IR techniques and we propose Trustrace, a trust--based traceability recovery approach. We apply Trustrace on four medium-size open-source systems to compare the accuracy of its traceability links with those recovered using state-of-the-art IR techniques from the literature, based on the Vector Space Model and Jensen-Shannon model. The results of Trustrace are up to 22.7 percent more precise and have 7.66 percent better recall values than those of the other techniques, on average. We thus show that mining software repositories and combining the mined data with existing results from IR techniques improves the precision and recall of requirement traceability links.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.71","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6341764","Traceability;requirements;feature;source code;repositories;experts;trust-based model","Accuracy;Data mining;Software maintenance;Information retrieval;Open source software;Principal component analysis","data mining;data privacy;information retrieval;software maintenance","software repository mining;requirement traceability link;traceability method;software maintenance;software evolution;information retrieval technique;IR technique;precision accuracy;recall accuracy;Trustrace approach;trust-based traceability recovery approach;medium-size open-source system;vector space model;Jensen-Shannon model","","29","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic Configuration for Distributed Systems","J. Kramer; J. Magee","Department of Computing, Imperial College of Science and Technology; NA","IEEE Transactions on Software Engineering","","1985","SE-11","4","424","436","Dynamic system configuration is the ability to modify and extend a system while it is running. The facility is a requirement in large distributed systems where it may not be possible or economic to stop the entire system to allow modification to part of its hardware or software. It is also useful during production of the system to aid incremental integration of component parts, and during operation to aid system evolution. The paper introduces a model of the configuration process which permits dynamic incremental modification and extension. Using this model we determine the properties required by languages and their execution environments to support dynamic configuration. CONIC, the distributed system which has been developed at Imperial College with the specific objective of supporting dynamic configuration, is described to illustrate the feasibility of the model.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232231","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702024","Configuration;configuration process;configuration specification;distributed systems;flexibility;reusability;system evolution","Hardware;Environmental economics;Application software;Software systems;Economic forecasting;Production systems;Embedded computing;Humans;Software testing;System testing","","Configuration;configuration process;configuration specification;distributed systems;flexibility;reusability;system evolution","","179","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis and testing of programs with exception handling constructs","S. Sinha; M. J. Harrold","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","9","849","871","Analysis techniques, such as control flow, data flow, and control dependence, are used for a variety of software engineering tasks, including structural and regression testing, dynamic execution profiling, static and dynamic slicing, and program understanding. To be applicable to programs in languages such as Java and C++, these analysis techniques must account for the effects of exception occurrences and exception handling constructs; failure to do so can cause the analysis techniques to compute incorrect results and, thus, limit the usefulness of the applications that use them. This paper discusses the effects of exception handling constructs on several analysis techniques. The paper presents techniques to construct representations for programs with explicit exception occurrences-exceptions that are raised explicitly through throw statements-and exception handling constructs. The paper presents algorithms that use these representations to perform the desired analyses. The paper also discusses several software engineering applications that use these analyses. Finally, the paper describes empirical results pertaining to the occurrence of exception handling constructs in Java programs and their effect on some analysis tasks.","0098-5589;1939-3520;2326-3881","","10.1109/32.877846","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877846","","Java;Performance analysis;Information analysis;Software testing;Failure analysis;Software engineering;Cause effect analysis;Data analysis;Frequency;Computer Society","program testing;exception handling;software engineering;Java;C++ language;data flow analysis;program slicing;object-oriented programming","program testing;program analysis;exception handling constructs;Java;C++;control flow analysis;data flow analysis;control dependence analysis;throw statements;software engineering","","59","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Factors Affecting Distributed System Security","D. M. Nessett","Lawrence Livermore National Laboratory, University of California","IEEE Transactions on Software Engineering","","1987","SE-13","2","233","248","Recent work examining distributed system security requirements. is critiqued. A notion of trust based on distributed system topology and distributed system node evaluation levels proposed in that work is shown to be deficient. The notion fails to make allowances for the distributed system physical security environment, security factors related to the management of distributed systems by more than one jurisdictive authority, and the interactions that can occur between nodes supporting different mandatory and discretionary security mechanisms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233148","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702203","Computer communications security;computer security;distributed systems;distributed system security;heterogeneity","Communication system security;Computer security;Network topology;Proposals;National security;Application software;Computer architecture;Environmental management;Access control;Privacy","","Computer communications security;computer security;distributed systems;distributed system security;heterogeneity","","3","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Functional refinement and nested objects for object-oriented design","P. Jalote","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1989","15","3","264","270","An extended object-oriented design methodology is proposed which incorporates a top-down, stepwise refinement approach in a coherent fashion. The extended object-oriented design methodology also includes a phase of progressive object refinement to support the nesting of objects, which would allow entities in real life that are composed of subentities to be modeled. A design example is included, and experiences encountered using this methodology in a course are described.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21754","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21754","","Design methodology;Object oriented modeling;Software design;Software systems;Costs;Process design;Packaging;Computer science","Ada;object-oriented programming;software engineering","Ada;nested objects;object-oriented design;stepwise refinement;progressive object refinement;nesting","","26","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic test generation: a use case driven approach","C. Nebut; F. Fleurey; Y. Le Traon; J. -. Jezequel","LIRMM, Montpellier, France; LIRMM, Montpellier, France; LIRMM, Montpellier, France; LIRMM, Montpellier, France","IEEE Transactions on Software Engineering","","2006","32","3","140","155","Use cases are believed to be a good basis for system testing. Yet, to automate the test generation process, there is a large gap to bridge between high-level use cases and concrete test cases. We propose a new approach for automating the generation of system test scenarios in the context of object-oriented embedded software, taking into account traceability problems between high-level views and concrete test case execution. Starting from a formalization of the requirements based on use cases extended with contracts, we automatically build a transition system from which we synthesize test cases. Our objective is to cover the system in terms of statement coverage with those generated tests: an empirical evaluation of our approach is given based on this objective and several case studies. We briefly discuss the experimental deployment of our approach in the field at Thales Airborne Systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610607","Use case;test generation;scenarios;contracts;UML.","Automatic testing;Computer aided software engineering;System testing;Object oriented modeling;Contracts;Software testing;Unified modeling language;Embedded software;Concrete;Costs","program testing;automatic test pattern generation;object-oriented programming;program diagnostics;embedded systems;formal specification;Unified Modeling Language","automatic test generation;use case driven approach;system testing;object-oriented embedded software;traceability problem;high-level view;concrete test case execution;requirements formalization;Thales Airborne Systems;UML","","120","","46","","","","","","IEEE","IEEE Journals & Magazines"
"An automated approach to information systems decomposition","D. Paulson; Y. Wand","Fac. of Manage., Lethbridge Univ., Alta., Canada; NA","IEEE Transactions on Software Engineering","","1992","18","3","174","189","A method for automating the process of system decomposition is described. The method is based on a formal specification scheme, formal definition of good decomposition, heuristic rules governing the search for good candidate decompositions, and a measure of complexity that allows ranking of the candidate decompositions. The decomposition method has been implemented as a set of experimental computerized systems analysis tools and applied to a standard problem for which other designs already exist. The results are encouraging, in that decompositions generated using other methodologies map easily into those suggested by the computerized tools. Additionally, the use of the method indicates that when more than one 'good' decomposition is suggested by the system, the specifications might have been incomplete. That is, the computerized tools can identify areas where more information should be sought by analysis.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.126767","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=126767","","Information systems;System analysis and design;Information analysis;Electric breakdown;Formal specifications;Position measurement;Modeling;Power system management;Business","computational complexity;formal specification;software tools;systems analysis","automated approach;information systems decomposition;formal specification scheme;formal definition;heuristic rules;complexity;experimental computerized systems analysis tools;standard problem","","34","","45","","","","","","IEEE","IEEE Journals & Magazines"
"Improving the Usability of E-Commerce Applications using Business Processes","Y. Zou; Q. Zhang; X. Zhao","IEEE Computer Society; NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","837","855","E-commerce applications automate many daily business activities. Users interact with e-commerce applications through menu-driven User Interface (Ul) components such as toolbars, dialogs, and windows. However, the tremendous number of functionalities may overwhelm the users. Users struggle to locate the appropriate Ul components to accomplish the tasks required by business processes. In this paper, we enhance e-commerce applications by improving their usability using the knowledge embedded in business process definitions. Our improved application provides contextual information to fulfill each business task. The improved application guides users through the various tasks in a step-by-step fashion. Through a controlled experiment, we demonstrate that our improved application offers a better usability experience for novice users by giving them more guidance and reducing the time needed to locate the next Ul component in a complex Ul.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70709","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359465","Graphical User Interface;User Interface Reengineering;Business Process;Process Definition;and Usability;Graphical User Interface;User Interface Reengineering;Business Process;Process Definition;and Usability","Usability;Application software;Books;User interfaces;Business process re-engineering;Computer architecture;Navigation;Computer Society;Credit cards;Printing","electronic commerce;graphical user interfaces;user interface management systems","e-commerce application usability;business process task;menu-driven user interface component;contextual information","","10","","74","","","","","","IEEE","IEEE Journals & Magazines"
"An analysis of several software defect models","T. -. Yu; V. Y. Shen; H. E. Dunsmore","AT&T Bell Labs., Naperville, IL, USA; NA; NA","IEEE Transactions on Software Engineering","","1988","14","9","1261","1270","Results are presented of an analysis of several defect models using data collected from two large commercial projects. Traditional models typically use either program matrices (i.e. measurements from software products) or testing time or combinations of these as independent variables. The limitations of such models have been well-documented. The models considered use the number of defects detected in the earlier phases of the development process as the independent variable. This number can be used to predict the number of defects to be detected later, even in modified software products. A strong correlation between the number of earlier defects and that of later ones was found. Using this relationship, a mathematical model was derived which may be used to estimate the number of defects remaining in software. This defect model may also be used to guide software developers in evaluating the effectiveness of the software development and testing processes.<<ETX>></ETX>","0098-5589;1939-3520;2326-3881","","10.1109/32.6170","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6170","","Software testing;Mathematical model;Software measurement;Predictive models;Time measurement;Phase detection;Programming;Software metrics;Resource management;Microelectronics","programming theory;software reliability","software testing;software reliability;software defect models;mathematical model;software development","","37","","9","","","","","","IEEE","IEEE Journals & Magazines"
"Inconsistency handling in multiperspective specifications","A. C. W. Finkelstein; D. Gabbay; A. Hunter; J. Kramer; B. Nuseibeh","Dept. of Comput., London Univ., UK; Dept. of Comput., London Univ., UK; Dept. of Comput., London Univ., UK; Dept. of Comput., London Univ., UK; Dept. of Comput., London Univ., UK","IEEE Transactions on Software Engineering","","1994","20","8","569","578","The development of most large and complex systems necessarily involves many people-each with their own perspectives on the system defined by their knowledge, responsibilities, and commitments. To address this we have advocated distributed development of specifications from multiple perspectives. However, this leads to problems of identifying and handling inconsistencies between such perspectives. Maintaining absolute consistency is not always possible. Often this is not even desirable since this can unnecessarily constrain the development process, and can lead to the loss of important information. Indeed since the real-world forces us to work with inconsistencies, we should formalize some of the usually informal or extra-logical ways of responding to them. This is not necessarily done by eradicating inconsistencies but rather by supplying logical rules specifying how we should act on them. To achieve this, we combine two lines of existing research: the ViewPoints framework for perspective development, interaction and organization, and a logic-based approach to inconsistency handling. This paper presents our technique for inconsistency handling in the ViewPoints framework by using simple examples.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310667","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310667","","Data models;Logic;Diffusion tensor imaging;Databases","formal specification;data integrity;temporal logic;distributed processing","multiperspective specifications;inconsistency handling;complex systems;distributed development;specifications;development;logical rules;ViewPoints;multiple perspectives;specification;process modeling;first order predicate logic;temporal logic","","133","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Developer Testing in The IDE: Patterns, Beliefs, And Behavior","M. Beller; G. Georgios; A. Panichella; S. Proksch; S. Amann; A. Zaidman","Software Technology, Delft University of Technology, Delft, SH Netherlands (e-mail: m.m.beller@tudelft.nl); Faculty Electrical Engineering, Mathematics and Computer Science, TU Delft, Delft, ZH Netherlands (e-mail: g.gousios@tudelft.nl); Centre Interdisciplinary for Security, Reliability and Trust, Universite du Luxembourg, 81872 Luxembourg, Luxembourg Luxembourg L-2721 (e-mail: annibale.panichella@uni.lu); Computer Science, TU Darmstadt, Darmstadt, Hessen Germany 64289 (e-mail: research@mail.proks.ch); Computer Science, Technische Universitat Darmstadt, 26536 Darmstadt, Deutschland Germany 64289 (e-mail: amann@st.informatik.tu-darmstadt.de); Software Engineering Research Group, Delft University of Technology, Delft, Zuid Holland Netherlands 2628 CD (e-mail: a.e.zaidman@tudelft.nl)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Software testing is one of the key activities to software quality in practice. Despite its importance, however, we have a remarkable lack of knowledge on how developers test in real-world projects. In this paper, we report on the surprising results of a large-scale field study with 2,443 software engineers whose development activities we closely monitored over the course of 2.5 years in four Integrated Development Environments (IDEs). Our findings question several commonly shared assumptions and beliefs about developer testing: half of the developers in our study does not test; developers rarely run their tests in the IDE; only once they start testing, do they do it heftily; most programming sessions end without any test execution; only a quarter of test cases is responsible for three quarters of all test failures; 12% of tests show flaky behavior; Test-Driven Development (TDD) is not widely practiced; and software developers only spend a quarter of their time engineering tests, whereas they think they test half of their time. We observed only minor differences in the testing practices among developers in different IDEs, Java, and C#. We summarize these practices of loosely guiding ones development efforts with the help of testing as Test-Guided Development (TGD).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2776152","Bundesministerium fur Bildung und Forschung; Nederlandse Organisatie voor Wetenschappelijk Onderzoek; Deutsche Forschungsgemeinschaft; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8116886","Developer Testing;Unit Tests;Testing Effort;Field Study;Test-Driven Development (TDD);JUnit;TestRoots WatchDog;KaVE FeedBag++","Testing;Software;Visualization;Servers;Java;Androids;Humanoid robots","","","","2","","","","","","","","IEEE","IEEE Early Access Articles"
"A lingua franca for concurrent logic programming","H. Taylor","Dept. of Comput. Sci., Heriot-Watt Univ., Edinburgh, UK","IEEE Transactions on Software Engineering","","1992","18","3","225","236","Two of the more important concurrent logic programming languages with nonflat guards are GHC and Parlog. They balance the requirements of having clean semantics and providing good control facilities rather differently, and their respective merits are compared and contrasted. Since concurrent logic programming would benefit from both, but neither language is able to express all the programs expressible in the other language, a lingua franca of these languages is defined and justified. A method is given for translating GHC and Parlog to and from it. The method preserves the arities and execution conditions of each clause. It enables a lingua franca implementation to support both languages transparently, and to provide a simple concurrent logic programming language suitable for programming in its own right.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.126771","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=126771","","Logic programming;Parallel processing;Parallel programming;Bismuth;Computer science","language translation;logic programming;parallel languages;parallel programming","concurrent logic programming languages;nonflat guards;GHC;Parlog;clean semantics;control facilities;lingua franca;execution conditions","","","","21","","","","","","IEEE","IEEE Journals & Magazines"
"A comparison of software project overruns - flexible versus sequential development models","K. Molokken-Ostvold; M. Jorgensen","Simula Res. Lab., Lysaker, Norway; Simula Res. Lab., Lysaker, Norway","IEEE Transactions on Software Engineering","","2005","31","9","754","766","Flexible software development models, e.g., evolutionary and incremental models, have become increasingly popular. Advocates claim that among the benefits of using these models is reduced overruns, which is one of the main challenges of software project management. This paper describes an in-depth survey of software development projects. The results support the claim that projects which employ a flexible development model experience less effort overruns than do those which employ a sequential model. The reason for the difference is not obvious. We found, for example, no variation in project size, estimation process, or delivered proportion of planned functionality between projects applying different types of development model. When the managers were asked to provide reasons for software overruns and/or estimation accuracy, the largest difference was that more of flexible projects than sequential projects cited good requirement specifications-and good collaboration/communication with clients as contributing to accurate estimates.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.96","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514444","Index Terms- Cost estimation;management;project control and modeling;software development models.","Project management;Programming;Costs;Collaborative software;Software engineering;Software development management;Communication system control;Software performance;Contracts;Productivity","project management;formal specification;formal verification;software cost estimation;software development management;software process improvement","software project overruns;software development model;software project management;formal specifications;software cost estimation;project control","","42","","51","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of representation methods for reusable software components","W. B. Frakes; T. P. Pole","Dept. of Comput. Sci., Virginia Polytech. Inst. & State Univ., Falls Church, VA, USA; NA","IEEE Transactions on Software Engineering","","1994","20","8","617","630","An empirical study of methods for representing reusable software components is described. Thirty-five subjects searched for reusable components in a database of UNIX tools using four different representation methods: attribute-value, enumerated, faceted, and keyword. The study used Proteus, a reuse library system that supports multiple representation methods. Searching effectiveness was measured with recall, precision, and overlap. Search time for the four methods was also compared. Subjects rated the methods in terms of preference and helpfulness in understanding components. Some principles for constructing reuse libraries. Based on the results of this study, are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310671","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310671","","Software reusability;Software libraries;Databases;Application software;Software quality;Computer languages;Information retrieval;Indexing;Keyword search;Buildings","software reusability;knowledge representation;subroutines","representation methods;reusable software components;UNIX tools;attribute-value;keyword;Proteus;reuse library system;multiple representation;software reuse;experimentation;empirical methods;information storage and retrieval;reuse libraries;component indexing;keyword searching;faceted classification;enumerated classification;component understanding;database","","61","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Selecting Best Practices for Effort Estimation","T. Menzies; Z. Chen; J. Hihn; K. Lum","Department of Computer Science, West Virginia University, Morgantown, WV 26506-610; 941 W. 37th Place, SAL 337, Los Angeles, CA 90089; Jet Propulsion Laboratory, 4800 Oak Grove Drive, Pasadena, CA 91109-8099; Jet Propulsion Laboratory, 4800 Oak Grove Drive, Pasadena, CA 91109-8099","IEEE Transactions on Software Engineering","","2006","32","11","883","895","Effort estimation often requires generalizing from a small number of historical projects. Generalization from such limited experience is an inherently underconstrained problem. Hence, the learned effort models can exhibit large deviations that prevent standard statistical methods (e.g., t-tests) from distinguishing the performance of alternative effort-estimation methods. The COSEEKMO effort-modeling workbench applies a set of heuristic rejection rules to comparatively assess results from alternative models. Using these rules, and despite the presence of large deviations, COSEEKMO can rank alternative methods for generating effort models. Based on our experiments with COSEEKMO, we advise a new view on supposed ""best practices"" in model-based effort estimation: 1) Each such practice should be viewed as a candidate technique which may or may not be useful in a particular domain, and 2) tools like COSEEKMO should be used to help analysts explore and select the best method for a particular domain","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.114","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015511","Model-based effort estimation;COCOMO;deviation;data mining.","Best practices;Testing;Statistical analysis;Data mining;Predictive models;Humans;Guidelines;Software safety;Estimation error;Linear regression","data mining;project management;software cost estimation;statistical analysis","effort estimation method;standard statistical method;COSEEKMO toolkit;heuristic rejection rule","","103","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Proactive and Reactive Runtime Service Discovery: A Framework and Its Evaluation","A. Zisman; G. Spanoudakis; J. Dooley; I. Siveroni","City University London, London; City University London, London; University of Essex, Colchester; City University London, London","IEEE Transactions on Software Engineering","","2013","39","7","954","974","The identification of services during the execution of service-based applications to replace services in them that are no longer available and/or fail to satisfy certain requirements is an important issue. In this paper, we present a framework to support runtime service discovery. This framework can execute service discovery queries in pull and push mode. In pull mode, it executes queries when a need for finding a replacement service arises. In push mode, queries are subscribed to the framework to be executed proactively and, in parallel with the operation of the application, to identify adequate services that could be used if the need for replacing a service arises. Hence, the proactive (push) mode of query execution makes it more likely to avoid interruptions in the operation of service-based applications when a service in them needs to be replaced at runtime. In both modes of query execution, the identification of services relies on distance-based matching of structural, behavioral, quality, and contextual characteristics of services and applications. A prototype implementation of the framework has been developed and an evaluation was carried out to assess the performance of the framework. This evaluation has shown positive results, which are discussed in the paper.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.84","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6375710","Web-services discovery;composite web services;context-aware QoS model;application development in services","Runtime;Context;Servers;Educational institutions;Database languages;Unified modeling language;Informatics","quality of service;query processing;software quality;ubiquitous computing;Web services","proactive runtime service discovery;reactive runtime service discovery;service identification;service-based applications;service discovery queries execution;pull mode;push mode;replacement service;distance-based matching;structural characteristics;behavioral characteristics;quality characteristics;contextual characteristics;composite Web-services discovery;context-aware QoS model","","16","","62","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study on Views of Importance of Change Impact Analysis Issues","P. Rovegård; L. Angelis; C. Wohlin","Ericsson AB, Karlskrona; Aristotle University of Thessaloniki, Thessaloniki; Blekinge Institute of Technology, Ronneby","IEEE Transactions on Software Engineering","","2008","34","4","516","530","Change impact analysis is a change management activity that previously has been studied much from a technical perspective. For example, much work focuses on methods for determining the impact of a change. In this paper, we present results from a study on the role of impact analysis in the change management process. In the study, impact analysis issues were prioritised with respect to criticality by software professionals from an organisational perspective and a self-perspective. The software professionals belonged to three organisational levels: operative, tactical and strategic. Qualitative and statistical analyses with respect to differences between perspectives as well as levels are presented. The results show that important issues for a particular level are tightly related to how the level is defined. Similarly, issues important from an organisational perspective are more holistic than those important from a self-perspective. However, our data indicate that the self-perspective colours the organisational perspective, meaning that personal opinions and attitudes cannot easily be disregarded. In comparing the perspectives and the levels, we visualise the differences in a way that allow us to discuss two classes of issues: high-priority and medium-priority. The most important issues from this point of view concern fundamental aspects of impact analysis and its execution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527253","General;Configuration management process;Qualitative process analysis;General;Configuration management process;Qualitative process analysis","Software systems;Statistical analysis;Data visualization;Life estimation;Programming;Algorithm design and analysis;Automatic control;Process control;Telecommunication control;Engineering management","management of change;software development management","change impact analysis;change management;software professionals;organisational perspective","","45","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Whitening SOA Testing via Event Exposure","C. Ye; H. Jacobsen","University of Toronto, Toronto; University of Toronto, Toronto","IEEE Transactions on Software Engineering","","2013","39","10","1444","1465","Whitening the testing of service-oriented applications can provide service consumers confidence on how well an application has been tested. However, to protect business interests of service providers and to prevent information leakage, the implementation details of services are usually invisible to service consumers. This makes it challenging to determine the test coverage of a service composition as a whole and design test cases effectively. To address this problem, we propose an approach to whiten the testing of service compositions based on events exposed by services. By deriving event interfaces to explore only necessary test coverage information from service implementations, our approach allows service consumers to determine test coverage based on selected events exposed by services at runtime without releasing the service implementation details. We also develop an approach to design test cases effectively based on event interfaces concerning both effectiveness and information leakage. The experimental results show that our approach outperforms existing testing approaches for service compositions with up to 49 percent more test coverage and an up to 24 percent higher fault-detection rate. Moreover, our solution can trade off effectiveness, efficiency, and information leakage for test case generation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6495456","Web service composition;white-box testing;event interface;events","Testing;Service-oriented architecture;Books;Runtime;Catalogs;Jacobian matrices","program testing;service-oriented architecture;Web services","SOA testing whitening;service-oriented architecture;event exposure;service consumers;service providers;service composition;test coverage;test case design approach;fault-detection rate;test case generation;information leakage;Web services","","15","","84","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical analysis of c preprocessor use","M. D. Ernst; G. J. Badros; D. Notkin","Dept. of Electr. Eng. & Comput. Sci., MIT, Cambridge, MA, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","12","1146","1170","This is the first empirical study of the use of the C macro preprocessor, Cpp. To determine how the preprocessor is used in practice, this paper analyzes 26 packages comprising 1.4 million lines of publicly available C code. We determine the incidence of C preprocessor usage-whether in macro definitions, macro uses, or dependences upon macros-that is complex, potentially problematic, or inexpressible in terms of other C or C++ language features. We taxonomize these various aspects of preprocessor use and particularly note data that are material to the development of tools for C or C++, including translating from C to C++ to reduce preprocessor usage. Our results show that, while most Cpp usage follows fairly simple patterns, an effective program analysis tool must address the preprocessor. The intimate connection between the C programming language and Cpp, and Cpp's unstructured transformations of token streams often hinder both programmer understanding of C programs and tools built to engineer C programs, such as compilers, debuggers, call graph extractors, and translators. Most tools make no attempt to analyze macro usage, but simply preprocess their input, which results in a number of negative consequences; an analysis that takes Cpp into account is preferable, but building such tools requires an understanding of actual usage. Differences between the semantics of Cpp and those of C can lead to subtle bugs stemming from the use of the preprocessor, but there are no previous reports of the prevalence of such errors. Use of C++ can reduce some preprocessor usage, but such usage has not been previously measured. Our data and analyses shed light on these issues and others related to practical understanding or manipulation of real C programs. The results are of interest to language designers, tool writers, programmers, and software engineers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158288","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158288","","Programming profession;Packaging;Pattern analysis;Computer languages;Program processors;Data preprocessing;Computer bugs;Data analysis;Software tools;Design engineering","program processors;C language","empirical analysis;C preprocessor;Cpp;C code;C++;program analysis tool;tool writers;software engineers","","70","","40","","","","","","IEEE","IEEE Journals & Magazines"
"The Effect of Work Environments on Productivity and Satisfaction of Software Engineers","B. Johnson; T. Zimmermann; C. Bird","Department of Computer Science, North Carolina State University, 6798 Raleigh, North Carolina United States 27606 (e-mail: bijohnso@ncsu.edu); Research, Microsoft Corporation, Redmond, Washington United States 98052 (e-mail: tzimmer@microsoft.com); Microsoft Research, Microsoft Corportation, Redmond, Washington United States 98052 (e-mail: cbird@microsoft.com)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","The physical work environment of software engineers can have various effects on their satisfaction and the ability to get the work done. To better understand the factors of the environment that affect productivity and satisfaction of software engineers, we explored different work environments at Microsoft. We used a mixed-methods, multiple stage research design with a total of 1,159 participants: two surveys with 297 and 843 responses respectively and interviews with 19 employees. We found several factors that were considered as important for work environments: personalization, social norms and signals, room composition and atmosphere, work-related environment affordances, work area and furniture, and productivity strategies. We built statistical models for satisfaction with the work environment and perceived productivity of software engineers and compared them to models for employees in the Program Management, IT Operations, Marketing, and Business Program & Operations disciplines. In the satisfaction models, the ability to work privately with no interruptions and the ability to communicate with the team and leads were important factors among all disciplines. In the productivity models, the overall satisfaction with the work environment and the ability to work privately with no interruptions were important factors among all disciplines. For software engineers, another important factor for perceived productivity was the ability to communicate with the team and leads. We found that private offices were linked to higher perceived productivity across all disciplines.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2903053","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8658138","productivity;satisfaction;physical environments;work environments;software engineering;program management;IT operations;marketing;business program operations","Software;Productivity;Organizations;Software engineering;Interviews;Collaboration;Knowledge engineering","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Components of Typical Undergraduate Software Engineering Courses: Results from a Survey","L. M. Leventhal; B. T. Mynatt","Department of Computer Science, Bowling Green State University; NA","IEEE Transactions on Software Engineering","","1987","SE-13","11","1193","1198","A survey of undergraduate software engineering courses was conducted. The survey covered the issues of course level, course content, course organization, project characteristics, and department demographics. The descriptive statistics show that the typical course focuses on the software development life cycle and includes a project intended for actual use. The project is carried out by teams of students, with student leaders. A factor analysis disclosed that three different sorts of courses are currently being offered. The most predominant course is the Later-Life Cycle course, which focuses on the later stages of the software life cycle. Detailed design, coding, testing, and maintenance receive in-depth coverage in this style of course, and the student's grades are heavily dependent upon the project. The Early-Life-Cycle course emphasizes requirements analysis, specification, and system design. Written reports are an important component of this course, and the project is again a large portion of the students' grades. The third style of course is the Theoretical-Issues course. Software metrics, project management, and legal and ethical issues are covered. The students are upper level, and they use journal articles as a source of materials. The issues of suitable textbooks and sources of materials and training for teaching user-interface design surfaced as problem areas.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232869","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702167","Projects;software engineering;software life cycle;survey;undergraduate courses","Software engineering;Demography;Statistics;Programming;Testing;System analysis and design;Software metrics;Project management;Law;Legal factors","","Projects;software engineering;software life cycle;survey;undergraduate courses","","7","","17","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient digital search algorithm by using a double-array structure","J. -. Aoe","Dept. of Inf. Sci. & Syst. Eng., Tokusima Univ., Japan","IEEE Transactions on Software Engineering","","1989","15","9","1066","1077","An efficient digital search algorithm that is based on an internal array structure called a double array, which combines the fast access of a matrix form with the compactness of a list form, is presented. Each arc of a digital search tree, called a DS-tree, can be computed from the double array in 0(1) time; that is to say, the worst-case time complexity for retrieving a key becomes 0(k) for the length k of that key. The double array is modified to make the size compact while maintaining fast access, and algorithms for retrieval, insertion, and deletion are presented. If the size of the double array is n+cm, where n is the number of nodes of the DS-tree, m is the number of input symbols, and c is a constant particular to each double array, then it is theoretically proved that the worst-case times of deletion and insertion are proportional to cm and cm/sup 2/, respectively, and are independent of n. Experimental results of building the double array incrementally for various sets of keys show that c has an extremely small value, ranging from 0.17 to 1.13.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31365","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31365","","Information retrieval;Data structures;Dictionaries;Filters;Natural language processing;Vocabulary;Frequency;Information science;Systems engineering and theory;Binary trees","computational complexity;data structures;search problems;trees (mathematics)","digital search algorithm;double-array structure;internal array structure;matrix form;list form;arc;digital search tree;DS-tree;worst-case time complexity;key;retrieval;insertion;deletion;nodes;input symbols;constant","","76","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Program partition and logic program analysis","Jia Liang Han","Dept. of Math. & Comput., Southern Queensland Univ., Toowoomba, Qld., Australia","IEEE Transactions on Software Engineering","","1995","21","12","959","968","A program partition scheme for stratified programs introduced by Apt et al. (1988) is used to study efficient computation of logic programs. We consider three types of program partitions and their corresponding graph representations: 1) the natural partition, 2) stratified partitions, and 3) the reduced partition. The natural (program) partition consists of definitions of relations, each definition being a subprogram. Subprograms of a program partition may consist of several relations. A partition graph is introduced for a program partition, each node of which corresponds to a subprogram. The partition graph for a stratified partition is a directed acyclic graph (DAG). A stratified partition decomposes a program into modules. The stratified partition with the maximum number of modules is the reduced partition. The cost to achieve a reduced partition is linear in the program size, using well known graph algorithms. We introduce the modular interpretations, which are equivalent in semantics to the standard interpretation. The modular interpretations offer encapsulation and may reduce the computation cost for some modules significantly. The modular approach can play an important role in query optimization, efficient termination, programming design, and software engineering. We classify query types and answer types then discuss query optimization for some query types. Many efficient query processing strategies are applicable to restricted subclasses of programs. The program partition method allows us to select the most efficient strategy for each module. For example, if a module is a uniformly bounded recursion, then the module can be terminated efficiently. If a module defines the transitive closure, then efficient program transformations may be applied to this module.","0098-5589;1939-3520;2326-3881","","10.1109/32.489072","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=489072","","Logic programming;Query processing;Software maintenance;Costs;Partitioning algorithms;Software engineering;Computational efficiency;Writing;Relational databases;Automatic programming","logic programming;programming theory;directed graphs;query processing;relational databases;database theory;computational linguistics;program verification;divide and conquer methods","logic program analysis;program partition scheme;stratified programs;logic program computation;graph representations;natural partition;stratified partitions;reduced partition;relation definitions;subprogram;partition graph;directed acyclic graph;modules;graph algorithms;semantics;modular interpretations;encapsulation;computation cost;query optimization;efficient termination;programming design;software engineering","","3","","24","","","","","","IEEE","IEEE Journals & Magazines"
"An atomicity-generating protocol for anonymous currencies","L. J. Camp","Sch. of Gov., Harvard Univ., Cambridge, MA, USA","IEEE Transactions on Software Engineering","","2001","27","3","272","278","Atomicity is necessary for reliable electronic commerce transactions. Anonymity is also an issue of great importance not only to designers of commerce systems, but also to those concerned with the societal effects of information technologies, providing atomicity and anonymity is not trivial. Reliable systems, which provide highly atomic transactions, offer limited anonymity. Many anonymous systems (Rivest and Shamir, 1996) do not offer anonymous reliable transactions (Yee, 1994). Three basic approaches have been used: secure hardware for trusted record-keeping (Brands, 1993), storage of identity information with trustees for conditional anonymity (Low et al., 1993) or by providing dispute resolution only with the removal of anonymity (Chaum, 1988). In this work, the problem of anonymous atomic transactions for a generic token currency is solved using distributed trust and with the assumption that any single party may be corrupt. Defined is a transaction to include the provision of information goods or a contract to deliver specified goods, allowing for the highest degree of atomicity. The cryptographic strength of the atomicity guarantee can be made to the user's specification on a per transaction basis. The atomicity-generating protocol includes provision for dispute resolution and anonymous refunds. Also illustrated, is that any electronic token currency can be made reliable with the addition of this atomicity-generating protocol.","0098-5589;1939-3520;2326-3881","","10.1109/32.910862","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910862","","Electronic commerce;Privacy;Business;Cryptographic protocols;Protection;Information technology;Hardware;Secure storage;Contracts;Cryptography","electronic commerce;cryptography;data privacy;protocols;transaction processing","atomicity generating protocol;anonymous currencies;electronic commerce;social effects;atomic transactions;anonymous systems;trusted record-keeping;identity information;dispute resolution;anonymous atomic transactions;generic token currency;distributed trust;cryptography;anonymous refunds;electronic token currency","","2","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Semantics of EqL","B. Jayaraman","Dept. of Comput. Sci., North Carolina Univ., Chapel Hill, NC, USA","IEEE Transactions on Software Engineering","","1988","14","4","472","480","The formal semantics of a novel language, called EqL, are presented for first-order functional and Horn logic programming. An EqL program is a set of conditional pattern-directed rules, where the conditions are expressed as a conjunction of equations. The programming paradigm provided by this language may be called equational programming. The declarative semantics of equations is given in terms of their complete set of solutions, and the operational semantics for solving equations is an extension of reduction, called object refinement. The correctness of the operational semantics is established through the soundness and completeness theorems. Examples are given to illustrate the language and its semantics.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4670","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4670","","Equations;Logic programming;Functional programming;Acoustical engineering;Application software;Programming profession;Computer science;Modems","formal languages;formal logic;high level languages;logic programming","functional logic;Horn logic;formal semantics;EqL;logic programming;conditional pattern-directed rules;programming paradigm;equational programming;declarative semantics;object refinement","","2","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Automated analysis of concurrent systems with the constrained expression toolset","G. S. Avrunin; U. A. Buy; J. C. Corbett; L. K. Dillon; J. C. Wileden","Massachusetts, Univ., Amherst, MA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1991","17","11","1204","1222","The constrained expression approach to analysis of concurrent software systems can be used with a variety of design and programming languages and does not require a complete enumeration of the set of reachable states of the concurrent system. The construction of a toolset automating the main constrained expression analysis techniques and the results of experiments with that toolset are reported. The toolset is capable of carrying out completely automated analyses of a variety of concurrent systems, starting from source code in an Ada-like design language and producing system traces displaying the properties represented bv the analysts queries. The strengths and weaknesses of the toolset and the approach are assessed on both theoretical and empirical grounds.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.106975","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=106975","","Software systems;Lifting equipment;Handicapped aids;Computer languages;Performance analysis;Frequency;Navigation;Process control;Operating systems;Error correction codes","parallel programming;software tools","concurrent systems;constrained expression toolset;programming languages;reachable states;expression analysis techniques;source code;Ada-like design language;system traces","","43","","30","","","","","","IEEE","IEEE Journals & Magazines"
"A Semi-Automatic Approach for Extracting Software Product Lines","M. T. Valente; V. Borges; L. Passos","University of Minas Gerais, Belo Horizonte; COTEMIG, Brazil; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2012","38","4","737","754","The extraction of nontrivial software product lines (SPL) from a legacy application is a time-consuming task. First, developers must identify the components responsible for the implementation of each program feature. Next, they must locate the lines of code that reference the components discovered in the previous step. Finally, they must extract those lines to independent modules or annotate them in some way. To speed up product line extraction, this paper describes a semi-automatic approach to annotate the code of optional features in SPLs. The proposed approach is based on an existing tool for product line development, called CIDE, that enhances standard IDEs with the ability to associate background colors with the lines of code that implement a feature. We have evaluated and successfully applied our approach to the extraction of optional features from three nontrivial systems: Prevayler (an in-memory database system), JFreeChart (a chart library), and ArgoUML (a UML modeling tool).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928352","Software product lines;virtual separation of concerns;refactoring tools;annotations","Feature extraction;Color;Image color analysis;Multithreading;Semantics;Software;Context","feature extraction;product development;software maintenance;software reusability;Unified Modeling Language","semiautomatic approach;software product lines extraction;SPL;legacy application;program feature;code lines localization;optional feature code annotation;product line development;CIDE;background colors;optional feature extraction;Prevayler nontrivial systems;JFreeChart nontrivial systems;ArgoUML nontrivial systems","","11","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Mechanical verification and automatic implementation of communication protocols","T. P. Blumer; D. P. Sidhu","Research and Development Division, SDCA Burroughs Company, Paoli, PA 19301; Protocol Development Corporation, 1330 Beacon Street, Brookline, MA 02146; Research and Development Division, SDCA Burroughs Company, Paoli, PA 19301; Department of Computer Science, Iowa State University, Ames, IA 50011","IEEE Transactions on Software Engineering","","1986","SE-12","8","827","843","An automated technique for protocol development is discussed along with its application to the specification, verification, and semiautomatic implementation of an authentication protocol for computer networks. An overview is given of the specification language, implementation method, and software tools used with this technique. The authentication protocol is described, along with an example of its operation. The reachability analysis technique for the verification of some protocol properties is discussed, and protocol verification software that uses this technique is described. The results of mechanical verification of some properties of this protocol are presented with a partial implementation generated automatically from the protocol specification.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312985","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312985","Authentication;automated development tools;communication protocols;encryption;formal description technique;formal modeling;key distribution protocols;protocol implementation;protocol specification;protocol verification;state transition","Protocols;Automata;Encryption;Authentication;Formal specifications;Software tools;Reachability analysis","automatic programming;computer communications software;program verification;protocols;software tools;specification languages","automatic implementation;communication protocols;protocol development;authentication protocol;computer networks;specification language;software tools;reachability analysis;protocol verification;mechanical verification;protocol specification","","23","","","","","","","","IEEE","IEEE Journals & Magazines"
"Using version control data to evaluate the impact of software tools: a case study of the Version Editor","D. L. Atkins; T. Ball; T. L. Graves; A. Mockus","Oregon Univ., Eugene, OR, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","7","625","637","Software tools can improve the quality and maintainability of software, but are expensive to acquire, deploy, and maintain, especially in large organizations. We explore how to quantify the effects of a software tool once it has been deployed in a development environment. We present an effort-analysis method that derives tool usage statistics and developer actions from a project's change history (version control system) and uses a novel effort estimation algorithm to quantify the effort savings attributable to tool usage. We apply this method to assess the impact of a software tool called VE, a version-sensitive editor used in Bell Labs. VE aids software developers in coping with the rampant use of certain preprocessor directives (similar to #if/#endif in C source files). Our analysis found that developers were approximately 40 percent more productive when using VE than when using standard text editors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019478","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019478","","Software tools;Computer aided software engineering;Software maintenance;Control systems;History;Software quality;Statistics;Standards development;Control system analysis;Statistical analysis","configuration management;software tools;software metrics;text editing;software quality","version control data;software tool impact evaluation;Version Editor;software quality;software maintainability;large organizations;software effort analysis method;VE tool;preprocessor directives;C source files;text editors;tool usage statistics;change history","","34","","32","","","","","","IEEE","IEEE Journals & Magazines"
"A Developer Centered Bug Prediction Model","D. Di Nucci; F. Palomba; G. De Rosa; G. Bavota; R. Oliveto; A. De Lucia","University of Salerno, Fisciano, SA, Italy; University of Salerno, Fisciano, SA, Italy; University of Salerno, Fisciano, SA, Italy; Università della Svizzera Italiana (USI), Lugano, Switzerland; University of Molise, Pesche (IS), Campobasso, Italy; University of Salerno, Fisciano, SA, Italy","IEEE Transactions on Software Engineering","","2018","44","1","5","24","Several techniques have been proposed to accurately predict software defects. These techniques generally exploit characteristics of the code artefacts (e.g., size, complexity, etc.) and/or of the process adopted during their development and maintenance (e.g., the number of developers working on a component) to spot out components likely containing bugs. While these bug prediction models achieve good levels of accuracy, they mostly ignore the major role played by human-related factors in the introduction of bugs. Previous studies have demonstrated that focused developers are less prone to introduce defects than non-focused developers. According to this observation, software components changed by focused developers should also be less error prone than components changed by less focused developers. We capture this observation by measuring the scattering of changes performed by developers working on a component and use this information to build a bug prediction model. Such a model has been evaluated on 26 systems and compared with four competitive techniques. The achieved results show the superiority of our model, and its high complementarity with respect to predictors commonly used in the literature. Based on this result, we also show the results of a “hybrid” prediction model combining our predictors with the existing ones.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2659747","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7835258","Scattering metrics;bug prediction;empirical study;mining software repositories","Measurement;Computer bugs;Predictive models;Complexity theory;Scattering;Entropy;Software","object-oriented programming;program debugging;software maintenance;software reliability","software components;hybrid prediction model;code artefacts;software defects prediction;developer centered bug prediction model","","5","","54","","","","","","IEEE","IEEE Journals & Magazines"
"On comparisons of random, partition, and proportional partition testing","S. C. Ntafos","Comput. Sci. Prog., Texas Univ. at Dallas, Richardson, TX, USA","IEEE Transactions on Software Engineering","","2001","27","10","949","960","Early studies of random versus partition testing used the probability of detecting at least one failure as a measure of test effectiveness and indicated that partition testing is not significantly more effective than random testing. More recent studies have focused on proportional partition testing because a proportional allocation of the test cases (according to the probabilities of the subdomains) can guarantee that partition testing will perform at least as well as random testing. We show that this goal for partition testing is not a worthwhile one. Guaranteeing that partition testing has at least as high a probability of detecting a failure comes at the expense of decreasing its relative advantage over random testing. We then discuss other problems with previous studies and show that failure to include important factors (cost, relative effectiveness) can lead to misleading results.","0098-5589;1939-3520;2326-3881","","10.1109/32.962563","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=962563","","Software testing;Performance evaluation;Costs;Helium;Genetic mutations;Failure analysis;Information analysis;Fault detection;Sampling methods;Statistical analysis","program testing;software reliability","random testing;partition testing;proportional partition testing;probability;test effectiveness;test cases;software failure detection;program testing;software reliability","","32","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Specifying transaction-based information systems with regular expressions","F. Lustman","Dept. d'Inf. et de Recherche Oper., Montreal Univ., Que., Canada","IEEE Transactions on Software Engineering","","1994","20","3","207","217","The work is about the formal specification of transaction-based, interactive information systems. A transaction is a task that the user can execute independently, and the system can be defined as a partially ordered set of transactions. The general framework is the transformational paradigm, based on the classical Waterfall development model (W.W. Royce, 1970). The stages are systems analysis, software specification, design, and implementation. The systems analysis and software specification stages are covered. An informal, transaction-oriented method for systems analysis is proposed. The resulting system specification involves two parts: a high-level specification of each transaction and a formal specification of the system's control flow, i.e., the order of execution of the transactions. The system's control flow is expressed in a formal language describing concurrent regular expressions built on transaction names. At the software specification stage, some operational requirements, such as connect/disconnect transactions and the application of the all-or-nothing principle, are added to the system specification. Then a serial product automaton (SPA) is used to transform the concurrent expression into a single regular expression. This result is proven to be consistent with the system specification.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.268922","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=268922","","Information systems;Formal specifications;Automata;Software systems;Control systems;Very large scale integration;Protocols;Software design;Formal languages;Application software","formal specification;transaction processing;systems analysis;information systems","transaction-based information systems;regular expressions;formal specification;interactive information systems;transformational paradigm;classical Waterfall development model;systems analysis;software specification;transaction-oriented method;high-level specification;formal language;concurrent regular expressions;operational requirements;connect/disconnect transactions;all-or-nothing principle;serial product automaton;SPA;concurrent expression","","4","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Model-Based Self-Aware Performance and Resource Management Using the Descartes Modeling Language","N. Huber; F. Brosig; S. Spinner; S. Kounev; M. Bähr","Department of Computer Science, Chair of Software Engineering, University of Würzburg, Würzburg, Germany; Department of Computer Science, Chair of Software Engineering, University of Würzburg, Würzburg, Germany; Department of Computer Science, Chair of Software Engineering, University of Würzburg, Würzburg, Germany; Department of Computer Science, Chair of Software Engineering, University of Würzburg, Würzburg, Germany; Blue Yonder GmbH & Co. KG., Karlsruhe, Germany","IEEE Transactions on Software Engineering","","2017","43","5","432","452","Modern IT systems have increasingly distributed and dynamic architectures providing flexibility to adapt to changes in the environment and thus enabling higher resource efficiency. However, these benefits come at the cost of higher system complexity and dynamics. Thus, engineering systems that manage their end-to-end application performance and resource efficiency in an autonomic manner is a challenge. In this article, we present a holistic model-based approach for self-aware performance and resource management leveraging the Descartes Modeling Language (DML), an architecture-level modeling language for online performance and resource management. We propose a novel online performance prediction process that dynamically tailors the model solving depending on the requirements regarding accuracy and overhead. Using these prediction capabilities, we implement a generic model-based control loop for proactive system adaptation. We evaluate our model-based approach in the context of two representative case studies showing that with the proposed methods, significant resource efficiency gains can be achieved while maintaining performance requirements. These results represent the first end-to-end validation of our approach, demonstrating its potential for self-aware performance and resource management in the context of modern IT systems and infrastructures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2613863","Deutsche Forschungsgemeinschaft (DFG); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7577879","Autonomic;self-aware;adaptation;model-based;modeling language;performance;efficiency","Adaptation models;Resource management;Computer architecture;Predictive models;Unified modeling language;Software;Dynamic scheduling","software architecture;software performance evaluation","model-based self-aware performance;resource management;Descartes modeling language;IT systems;dynamic architectures;distributed architectures;system complexity;engineering systems;end-to-end application performance;resource efficiency;holistic model-based approach;DML;architecture-level modeling language;online performance prediction process;generic model-based control loop;proactive system adaptation;resource efficiency gains","","13","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Targeted Scrum: Applying Mission Command to Agile Software Development","D. P. Harvie; A. Agah","Department of Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS; Department of Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS","IEEE Transactions on Software Engineering","","2016","42","5","476","489","Software engineering and mission command are two separate but similar fields, as both are instances of complex problem solving in environments with ever changing requirements. Our research hypothesis is that modifications to agile software development based on inspirations from mission command can improve the software engineering process in terms of planning, prioritizing, and communication of software requirements and progress, as well as improving the overall software product. Targeted Scrum is a modification of Traditional Scrum based on three inspirations from Mission Command: End State, Line of Effort, and Targeting. These inspirations have led to the introduction of the Product Design Meeting and modifications of some current Scrum meetings and artifacts. We tested our research hypothesis using a semester-long undergraduate level software engineering class. Students developed two software projects, one using Traditional Scrum and the other using Targeted Scrum. We then assessed how well both methodologies assisted the software development teams in planning and developing the software architecture, prioritizing requirements, and communicating progress. We also evaluated the software product produced by both methodologies. We found that Targeted Scrum did better in assisting the software development teams in the planning and prioritization of the requirements. However, Targeted Scrum had a negligible effect on improving the software development teams external and internal communications. Finally, Targeted Scrum did not have an impact on the product quality by the top performing and worst performing teams. Targeted Scrum did assist the product quality of the teams in the middle of the performance spectrum.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2489654","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7296686","Scrum, Mission Command;Line of Effort;Product Design Meeting;Agile;Empirical Software Engineering;Scrum;mission command;line of effort;product design meeting;agile;empirical software engineering","Software;Planning;Scrum (Software development);Product design;Software engineering;Force","software architecture;software prototyping","targeted Scrum;mission command;agile software development;software engineering;software requirements;software product improvement;end state;line of effort;targeting;product design meeting;software projects;traditional Scrum;software architecture;product quality;performance spectrum","","4","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A practical approach to programming with assertions","D. S. Rosenblum","AT&T Bell Labs., Murray Hill, NJ, USA","IEEE Transactions on Software Engineering","","1995","21","1","19","31","Embedded assertions have been recognized as a potentially powerful tool for automatic runtime detection of software faults during debugging, testing, maintenance and even production versions of software systems. Yet despite the richness of the notations and the maturity of the techniques and tools that have been developed for programming with assertions, assertions are a development tool that has seen little widespread use in practice. The main reasons seem to be that (1) previous assertion processing tools did not integrate easily with existing programming environments, and (2) it is not well understood what kinds of assertions are most effective at detecting software faults. This paper describes experience using an assertion processing tool that was built to address the concerns of ease-of-use and effectiveness. The tool is called APP, an Annotation PreProcessor for C programs developed in UNIX-based development environments, APP has been used in the development of a variety of software systems over the past five years. Based-on this experience, the paper presents a classification of the assertions that were most effective at detecting faults. While the assertions that are described guard against many common kinds of faults and errors, the very commonness of such faults demonstrates the need for an explicit, high-level, automatically checkable specification of required behavior. It is hoped that the classification presented in this paper will prove to be a useful first step in developing a method of programming with assertions.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.341844","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=341844","","Fault detection;Software tools;Software systems;Runtime;Embedded software;Software debugging;Software maintenance;Automatic testing;Software testing;System testing","formal specification;software tools;software reliability;program debugging","embedded assertions;automatic runtime detection;software faults;debugging;testing;maintenance;production versions;notations;development tool;assertion processing tools;programming environments;assertion processing tool;APP;Annotation PreProcessor;C programs;UNIX-based development environments;automatically checkable specification","","119","","37","","","","","","IEEE","IEEE Journals & Magazines"
"A dynamic coordination policy for software system construction","V. S. Mookerjee; I. R. Chiang","Sch. of Manage., Texas Univ., Richardson, TX, USA; NA","IEEE Transactions on Software Engineering","","2002","28","7","684","694","In constructing a software system, extended periods of coding without adequate coordination (such as system integration and testing) can result in considerable fault correction effort. On the other hand, too much coordination can also prove counterproductive by disrupting the smooth flow of development work. The goal, therefore, is to find an optimal level of coordination so as to minimize system construction effort while adhering to functionality and schedule constraints. Previous research, however, has not considered dynamic project factors such as system growth, system stability and team learning when addressing the above coordination problem. Dynamic factors are important because they could lead to differences in the intensity (frequency) of coordination needed at different stages of system construction. Unlike existing studies, we propose a dynamic coordination policy that places coordination activities at optimal (and often nonuniform) intervals during the construction of a system. Our analysis shows that, if a system stabilizes slowly, more intense coordination should occur early in the project. Also, if the team's knowledge of the system improves with time (i.e. learning effects are present), more intense coordination should occur both near the beginning and near the end of the project. Our analysis also shows that, by encouraging more frequent coordination, superior development tools could facilitate team learning. Finally, the application of the coordination model to data from a NASA software project demonstrates that optimally coordinating a project could significantly reduce the system construction cost.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019482","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019482","","Software systems;System testing;Project management;Programming;Productivity;Software development management;Software testing;Scheduling;Stability;Frequency","software development management;project management","dynamic coordination policy;software system construction;extended coding periods;system integration;system testing;fault correction effort;development workflow;optimal coordination level;system construction effort minimization;functionality constraints;schedule constraints;dynamic project factors;system growth;system stability;team learning;system stabilization;software development projects;team system knowledge;team learning effects;coordination intensity;software development tools;NASA software project;system construction cost;software project management","","19","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Some critical remarks on a hierarchy of fault-detecting abilities of test methods [and reply]","R. A. DeMillo; A. P. Mathur; W. E. Wong; P. G. Frankl; E. J. Weyuker","Dipartimento di Elettronica e Inf., Padova Univ., Italy; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1995","21","10","858","863","In a recent article by P.G. Frankl and E.J. Weyuker (see ibid., vol.19, no.3, p.962-75, 1993), results are reported that appear to establish a hierarchy of software test methods based on their respective abilities to detect faults. The methods used by Frankl and Weyuker to obtain this hierarchy constitute a new and important addition to their arsenal of tools. These tools were developed specifically to establish simple, useful comparisons of test data generation methods. This is the latest step in an ambitious test method classification program undertaken by the Frankl and Weyuker and their collaborators. The article discusses the method and goes on to present a reply to the critique.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.469455","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=469455","","Genetic mutations;Software testing;Fault detection;Software tools;Marine vehicles","program testing;failure analysis","fault detecting abilities;fault-detecting abilities;test methods;software test methods;test data generation methods;test method classification program","","5","","26","","","","","","IEEE","IEEE Journals & Magazines"
"The Role of the Tester's Knowledge in Exploratory Software Testing","J. Itkonen; M. V. Mäntylä; C. Lassenius","Aalto University School of Science, Espoo; Aalto University School of Science, Espoo; Aalto University School of Science, Espoo","IEEE Transactions on Software Engineering","","2013","39","5","707","724","We present a field study on how testers use knowledge while performing exploratory software testing (ET) in industrial settings. We video recorded 12 testing sessions in four industrial organizations, having our subjects think aloud while performing their usual functional testing work. Using applied grounded theory, we analyzed how the subjects performed tests and what type of knowledge they utilized. We discuss how testers recognize failures based on their personal knowledge without detailed test case descriptions. The knowledge is classified under the categories of domain knowledge, system knowledge, and general software engineering knowledge. We found that testers applied their knowledge either as a test oracle to determine whether a result was correct or not, or for test design, to guide them in selecting objects for test and designing tests. Interestingly, a large number of failures, windfall failures, were found outside the actual focus areas of testing as a result of exploratory investigation. We conclude that the way exploratory testers apply their knowledge for test design and failure recognition differs clearly from the test-case-based paradigm and is one of the explanatory factors of the effectiveness of the exploratory testing approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6298893","Software testing;exploratory testing;validation;test execution;test design;human factors;methods for SQA;and V&V","Software testing;Context;Software;Knowledge engineering;Observers;Organizations","program testing;software engineering","tester knowledge;exploratory software testing;ET;functional testing;grounded theory;personal knowledge;domain knowledge;system knowledge;general software engineering knowledge;test oracle;windfall failures;test design;failure recognition;test-case-based paradigm","","26","","76","","","","","","IEEE","IEEE Journals & Magazines"
"A Model-Integrated Approach to Designing Self-Protecting Systems","S. Iannucci; S. Abdelwahed; A. Montemaggio; M. Hannis; L. Leonard; J. King; J. Hamilton","Computer Science and Engineering, Mississippi State University, Starkville, Mississippi United States (e-mail: stefano@dasi.msstate.edu); Electrical and Computer Engineering, Virginia Commonwealth University, Richmond, Virginia United States (e-mail: sabdelwahed@vcu.edu); Center for Cyber Innovation, Mississippi State University, Starkville, Mississippi United States (e-mail: am4691@cci.msstate.edu); Center for Cyber Innovation, Mississippi State University, Starkville, Mississippi United States (e-mail: mkh149@cci.msstate.edu); High Performance Modernization Program, U.S. Army Engineer Research and Development Center (ERDC), Vicksburg, Mississippi United States (e-mail: Leslie.C.Leonard@erdc.dren.mil); High Performance Modernization Program, U.S. Army Engineer Research and Development Center (ERDC), Vicksburg, Mississippi United States (e-mail: Jason.S.King@erdc.dren.mil); Center for Cyber Innovation, Mississippi State University, Starkville, Mississippi United States (e-mail: hamilton@cci.msstate.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","One of the major trends in research on Self-Protecting Systems is to use a model of the system to be protected to predict its evolution. However, very often, devising the model requires special knowledge of mathematical frameworks, that prevents the adoption of this technique outside of the academic environment. Furthermore, some of the proposed approaches suffer from the curse of dimensionality, as their complexity is exponential in the size of the protected system. In this paper, we introduce a model-integrated approach for the design of Self-Protecting Systems, which automatically generates and solves Markov Decision Processes (MDPs) to obtain optimal defense strategies for systems under attack. MDPs are created in such a way that the size of the state space does not depend on the size of the system, but on the scope of the attack, which allows us to apply it to systems of arbitrary size.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2880218","Engineer Research and Development Center; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8528892","Intrusion Response System;Autonomic Security Management","Computational modeling;Servers;Microwave integrated circuits;Predictive models;Security;Planning;Mathematical model","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Introducing object orientation into large and complex systems","H. -. Deubler; M. Koestler","Siemens Nixdorf Informationssyst. AG, Munich, Germany; Siemens Nixdorf Informationssyst. AG, Munich, Germany","IEEE Transactions on Software Engineering","","1994","20","11","840","848","The paper investigates the applicability of the object-oriented technique to large and complex systems as exemplified by the operating system BS2000 which has been under constant development for a number of years. The proposed system architecture ensures the harmonious coexistence of procedural and object-oriented parts of the system. New domains, which are implemented using the object-oriented paradigm, can be smoothly embedded in the existing system. The parallel usage of different implementation languages is rendered economically viable. In our framework some representative parts of the system were redesigned and implemented in a prototype. The extensibility of the design was checked by including further parts into this scheme. The results are encouraging, so that the object-oriented technique will be used in the further development process. The proposed technique can also be applied to systems with a different structure, even a monolithic one.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.368124","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=368124","","Operating systems;Software systems;Prototypes;Environmental economics;Computer architecture;Application software;Investments;Programming;Trademarks","object-oriented programming;operating systems (computers);structured programming","object orientation;complex systems;object-oriented technique;operating system BS2000;system architecture;object-oriented parts;object-oriented paradigm;parallel usage;implementation languages;monolithic structure;large software systems;systems architecture;OOP","","6","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Integrated Performance Models for Distributed Processing in Computer Communication Networks","A. Thomasian; P. F. Bay","IBM Thomas J. Watson Research Center; NA","IEEE Transactions on Software Engineering","","1985","SE-11","10","1203","1216","This paper deals with the analysis of large-scale closed queueing network (QN) models which are used for the performance analysis of computer communication networks (CCN's). The computer systems are interconnected by a wide-area network. Users accessing local/remote computers are affected by the contention (queueing delays) at the computer systems and the communication subnet. The computational cost of analyzing such models increases exponentially with the number of user classes (chains), even when the QN is tractable (product-form). In fact, the submodels of the integrated model are generally not product-form, e.g., due to blocking at computer systems (multiprogramming level constraints) and in the communication subnet (window flow control constraints). Two approximate solution methods are proposed in this paper to analyze the integrated QN model. Both methods use decomposition and iterative techniques to exploit the structure of the QN model such that computational cost is proportional to the number of chains. The accuracy of the solution methods is validated against each other and simulation. The model is used to study the effect that channel capacity assignments, window sizes for congestion control, and routing have on system performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231868","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701936","Approximate solution;decomposition and iteration;distributed processing;large scale queueing network model;memory constrained multiprogrammed computer system;window flow control","Distributed processing;Computer networks;Distributed computing;Communication networks;Performance analysis;Computational efficiency;Computational modeling;Large-scale systems;Queueing analysis;Delay","","Approximate solution;decomposition and iteration;distributed processing;large scale queueing network model;memory constrained multiprogrammed computer system;window flow control","","2","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Software reflexion models: bridging the gap between design and implementation","G. C. Murphy; D. Notkin; K. J. Sullivan","Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada; NA; NA","IEEE Transactions on Software Engineering","","2001","27","4","364","380","The artifacts constituting a software system often drift apart over time. We have developed the software reflexion model technique to help engineers perform various software engineering tasks by exploiting, rather than removing, the drift between design and implementation. More specifically, the technique helps an engineer compare artifacts by summarizing where one artifact (such as a design) is consistent with and inconsistent with another artifact (such as source). The technique can be applied to help a software engineer evolve a structural mental model of a system to the point that it is ""good enough"" to be used for reasoning about a task at hand. The software reflexion model technique has been applied to support a variety of tasks, including design conformance, change assessment, and an experimental reengineering of the million-lines-of-code Microsoft Excel product. We provide a formal characterization of the reflexion model technique, discuss practical aspects of the approach, relate experiences of applying the approach and tools, and place the technique into the context of related work.","0098-5589;1939-3520;2326-3881","","10.1109/32.917525","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917525","","Design engineering;Software performance;Reverse engineering;Software systems;Software engineering;Maintenance engineering;Computer Society;Cognitive science;Spreadsheet programs;Context modeling","reverse engineering;systems re-engineering;software maintenance;formal specification","software reflexion models;software system;software engineering tasks;structural mental model;design conformance;change assessment;experimental reengineering;Microsoft Excel product;formal characterization;reverse engineering;program understanding;software structure;program representation;model differencing","","184","","39","","","","","","IEEE","IEEE Journals & Magazines"
"The probability of load balancing success in a homogeneous network","C. G. Rommen","Dept. of Math. & Comput. Sci., Eastern Connecticut State Univ., Willimantic, CT, USA","IEEE Transactions on Software Engineering","","1991","17","9","922","933","The problem of load balancing in distributed systems composed of several homogeneous sites connected by a subnet is examined. The author determines a general formula for the probability that any one site in the system is underloaded while some other site in the system is overloaded. This probability can be used to define the likelihood of load balancing success in a distributed operating system. This probability gives insight into the utilization of the system and is an aid in determining a measure of effectiveness of the system. From this formula one can determine this probability when the workload is composed of processes typical to distributed systems. The influence of variants in the load balancing algorithm on this probability is demonstrated.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.92912","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=92912","","Load management;Intelligent networks;Local area networks;Distributed processing;Computer networks;Concurrent computing;Tail;Operating systems;Queueing analysis;Processor scheduling","computer networks;distributed processing;operating systems (computers);probability","homogeneous sites;subnet;general formula;distributed operating system;load balancing algorithm","","23","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Refinement Methodology for Ada","V. Rajlich","Department of Computer Science, Wayne State University","IEEE Transactions on Software Engineering","","1987","SE-13","4","472","478","This paper presents Refinement Methodology (RM) for the design of Ada® programs. The methodology combines stepwise refinement and the information hiding principle. The steps of the methodology are explained and illustrated by an example. A part of the methodology is a collection of rules by which procedures acquire parameters (called first and second rules for parameters).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233183","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702238","Ada;information hiding;life-cycle;methodologies;program design language;programming-in-the-large;programming language constructs;software design","Packaging;Computer languages;Design methodology;Software design;Testing;Programming profession;Data structures;Process design;Computer science;Trademarks","","Ada;information hiding;life-cycle;methodologies;program design language;programming-in-the-large;programming language constructs;software design","","2","","16","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Class Rebalancing Techniques on the Performance and Interpretation of Defect Prediction Models","C. Tantithamthavorn; A. E. Hassan; K. Matsumoto","Faculty of Information Technology, Monash University Faculty of Information Technology, 224480 Clayton, Victoria Australia (e-mail: chakkrit.tantithamthavorn@monash.edu); School of Computing, Queen's University, Kingston, Ontario Canada K7L 3N6 (e-mail: ahmed@cs.queensu.ca); Information Systems, Nara Institute of Science and Technology, Information Systems, Ikomashi, Naraken Japan (e-mail: matumoto@is.naist.jp)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Defect models that are trained on class imbalanced datasets (i.e., the proportion of defective and clean modules is not equally represented) are highly susceptible to produce inaccurate prediction models. Prior research compares the impact of class rebalancing techniques on the performance of defect models but arrives at contradictory conclusions due to the use of different choice of datasets, classification techniques, and performance measures. Such contradictory conclusions make it hard to derive practical guidelines for whether class rebalancing techniques should be applied in the context of defect models. In this paper, we investigate the impact of class rebalancing techniques on performance measures and the interpretation of defect models. We also investigate the experimental settings in which class rebalancing techniques are beneficial for defect models. Through a case study of 101 datasets that span across proprietary and open-source systems, we conclude that the impact of class rebalancing techniques on the performance of defect prediction models depends on the used performance measure and the used classification techniques. We observe that the optimized SMOTE technique and the under-sampling technique are beneficial when quality assurance teams wish to increase AUC and Recall, respectively, but they should be avoided when deriving knowledge and understandings from defect models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2876537","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8494821","Software quality assurance;software defect prediction;class rebalancing techniques;experimental design;empirical investigation","Predictive models;Training;Analytical models;Guidelines;Context modeling;Open source software","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"An Application of Statistical Databases in Manufacturing Testing","S. P. Ghosh","Department of Computer Science, IBM Re-search Laboratory","IEEE Transactions on Software Engineering","","1985","SE-11","7","591","598","This paper discusses some applications of statistical and database techniques for integrating tests in manufacturing products. It shows how statistical databases can be used for automatically controlling a manufacturing process in real time. Some new statistical methods of manufacturing testing, e. g., test-compression, testing-by-sampling, testing-by-factorial-design, are discussed. All these techniques are possible because of the availability of a statistical database.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.232503","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702063","Manufacturing process;manufacturing testing;statis-tical database","Databases;Manufacturing processes;System testing;Pulp manufacturing;Hardware;Automatic control;Process control;Statistical analysis;Computer aided manufacturing;Costs","","Manufacturing process;manufacturing testing;statis-tical database","","4","","12","","","","","","IEEE","IEEE Journals & Magazines"
"MobiGATE: a mobile computing middleware for the active deployment of transport services","Y. Zheng; A. T. S. Chan","Dept. of Comput. & Inf. Sci. & Eng., Florida Univ., Gainesville, FL, USA; NA","IEEE Transactions on Software Engineering","","2006","32","1","35","50","The use of gateway proxies is one important approach to facilitating adaptation across wireless and mobile environments. Importantly, augmented service entities deployed within the gateway proxy residing on the wired network can be composed and deployed to shield mobile clients from the effects of poor network characteristics. The usual approach to the static composition of service entities on the gateway proxy is to have these service entities interact with each other by explicitly invoking procedures on the named interface, but such a tight coupling of interfaces inhibits the flexible composition and adaptation of the service entities to the dynamic operating characteristics of wireless networks. In this paper, we present a mobile gateway for the active deployment of transport entities or, for short, MobiGATE (pronounced Mobi-Gate). MobiGATE is a mobile middleware framework that supports the robust and flexible composition of transport entities, known as streamlets. The flow of data traffic is subjected to processing by a chain of streamlets. Each streamlet encapsulates a service entity that adapts the flow of traffic across the wireless network. To facilitate the dynamic reconfiguration of the streamlets, we advocate applying the concept of coordination as the unifying approach to composing these transport service entities. Importantly, MobiGATE delineates a clear separation of interdependent parts from the service-specific computational codes of those service entities. It does this by using a separate coordination language, called MobiGATE coordination language (MCL), to describe the coordination among streamlet service entities. The complete design, implementation, and evaluation of the MobiGATE system are presented in this paper. Initial experimental results validate the flexibility of the coordination approach in promoting separation-of-concern in the reconfiguration of services, while achieving low computation and delay overheads.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583601","Mobile computing;coordination languages;adaptive middleware;dynamic reconfiguration;infrastructural proxies.","Mobile computing;Middleware;Wireless networks;Telecommunication traffic;Application software;Robustness;Delay;Wireless communication;Computer displays;Batteries","middleware;mobile computing;internetworking;telecommunication traffic","MobiGATE system;mobile computing middleware;gateway proxy;wireless environment;mobile environment;augmented service entity deployment;active transport service entity deployment;streamlet service entity;data traffic flow;MobiGATE coordination language;MCL;dynamic service reconfiguration","","5","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Program concept recognition and transformation","W. Kozaczynski; J. Ning; A. Engberts","Center for Stratetic Technol. Res., Chicago, IL, USA; Center for Stratetic Technol. Res., Chicago, IL, USA; Center for Stratetic Technol. Res., Chicago, IL, USA","IEEE Transactions on Software Engineering","","1992","18","12","1065","1075","The automated recognition of abstract high-level conceptual information or concepts, which can greatly aid the understanding of programs and therefore support many software maintenance and reengineering activities, is considered. An approach to automated concept recognition and its application to maintenance-related program transformations are described. A unique characteristic of this approach is that transformations of code can be expressed as transformations of abstract concepts. This significantly elevates the level of transformation specifications.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.184761","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=184761","","Software maintenance;Application software","programming theory;software maintenance","program understanding;software reengineering;software maintenance;automated concept recognition;program transformations","","76","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Systematic Elaboration of Scalability Requirements through Goal-Obstacle Analysis","L. Duboc; E. Letier; D. S. Rosenblum","State University of Rio de Janeiro, Rio de Janeiro; University College London, London; National University of Singapore, Singapore","IEEE Transactions on Software Engineering","","2013","39","1","119","140","Scalability is a critical concern for many software systems. Despite the recognized importance of considering scalability from the earliest stages of development, there is currently little support for reasoning about scalability at the requirements level. This paper presents a goal-oriented approach for eliciting, modeling, and reasoning about scalability requirements. The approach consists of systematically identifying scalability-related obstacles to the satisfaction of goals, assessing the likelihood and severity of these obstacles, and generating new goals to deal with them. The result is a consolidated set of requirements in which important scalability concerns are anticipated through the precise, quantified specification of scaling assumptions and scalability goals. The paper presents results from applying the approach to a complex, large-scale financial fraud detection system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6152130","Requirements/specifications;analysis;performance measures;quality analysis and evaluation;goal-oriented requirements engineering;KAOS;scalability","Scalability;Software;Batch production systems;Educational institutions;Analytical models;Natural languages","financial data processing;fraud;large-scale systems;reasoning about programs;systems analysis","systematic elaboration;goal-obstacle analysis;software systems;reasoning about scalability;scalability requirement elicitation;scalability requirement modeling;systematic scalability-related obstacle identification;goal satisfaction;complex large-scale financial fraud detection system;goal-oriented requirements engineering","","13","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Defect frequency and design patterns: an empirical study of industrial code","M. Vokac","Simula Res. Lab., Lysaker, Norway","IEEE Transactions on Software Engineering","","2004","30","12","904","917","Software ""design patterns"" seek to package proven solutions to design problems in a form that makes it possible to find, adapt, and reuse them. A common claim is that a design based on properly applied patterns will have fewer defects than more ad hoc solutions. This case study analyzes the weekly evolution and maintenance of a large commercial product (C++, 500,000 LOC) over three years, comparing defect rates for classes that participated in selected design patterns to the code at large. We found that there are significant differences in defect rates among the patterns, ranging from 63 percent to 154 percent of the average rate. We developed a new set of tools able to extract design pattern information at a rate of 3/spl times/10/sup 6/ lines of code per hour, with relatively high precision. Based on a qualitative analysis of the code and the nature of the patterns, we conclude that the Observer and Singleton patterns are correlated with larger code structures and, so, can serve as indicators of code that requires special attention. Conversely, code designed with the Factory pattern is more compact and possibly less closely coupled and, consequently, has lower defect numbers. The Template Method pattern was used in both simple and complex situations, leading to no clear tendency.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.99","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377188","Index Terms- Design patterns;defects;defect frequency;industrial code;case study;maintenance.","Frequency;Pattern analysis;Production facilities;Testing;Packaging;Lab-on-a-chip;Data mining;Computer architecture;User interfaces;Computer industry","object-oriented programming;software prototyping;software maintenance;software fault tolerance","software design patterns;industrial code;software evolution;software maintenance;lines of code;Observer patterns;Singleton patterns;Template Method pattern;defect frequency","","50","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Managing standards compliance","W. Emmerich; A. Finkelstein; C. Montangero; S. Antonelli; S. Armitage; R. Stevens","Dept. of Comput. Sci., Univ. Coll. London, UK; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1999","25","6","836","851","Software engineering standards determine practices that ""compliant"" software processes shall follow. Standards generally define practices in terms of constraints that must hold for documents. The document types identified by standards include typical development products, such as user requirements, and also process-oriented documents, such as progress reviews and management reports. The degree of standards compliance can be established by checking these documents against the constraints. It is neither practical nor desirable to enforce compliance at all points in the development process. Thus, compliance must be managed rather than imposed. We outline a model of standards and compliance and illustrate it with some examples. We give a brief account of the notations and method we have developed to support the use of the model and describe a support environment we have constructed. The principal contributions of our work are: the identification of the issue of standards compliance; the development of a model of standards and support for compliance management; the development of a formal model of product state with associated notation; a powerful policy scheme that triggers checks; and a flexible and scalable compliance management view.","0098-5589;1939-3520;2326-3881","","10.1109/32.824413","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=824413","","Standards development;Software standards;Energy management;Environmental management;Software development management;Engineering management;Computer Society;Software engineering;Systems engineering and theory;Certification","software standards;software development management","standards compliance management;software engineering standards;document types;user requirements;process-oriented documents;formal model","","36","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Vulnerability Discovery with Attack Injection","J. Antunes; N. Neves; M. Correia; P. Verissimo; R. Neves","University of Lisboa, Lisboa; University of Lisboa, Lisboa; University of Lisboa, Lisboa; University of Lisboa, Lisboa; Technical University of Lisbon, Lisboa","IEEE Transactions on Software Engineering","","2010","36","3","357","370","The increasing reliance put on networked computer systems demands higher levels of dependability. This is even more relevant as new threats and forms of attack are constantly being revealed, compromising the security of systems. This paper addresses this problem by presenting an attack injection methodology for the automatic discovery of vulnerabilities in software components. The proposed methodology, implemented in AJECT, follows an approach similar to hackers and security analysts to discover vulnerabilities in network-connected servers. AJECT uses a specification of the server's communication protocol and predefined test case generation algorithms to automatically create a large number of attacks. Then, while it injects these attacks through the network, it monitors the execution of the server in the target system and the responses returned to the clients. The observation of an unexpected behavior suggests the presence of a vulnerability that was triggered by some particular attack (or group of attacks). This attack can then be used to reproduce the anomaly and to assist the removal of the error. To assess the usefulness of this approach, several attack injection campaigns were performed with 16 publicly available POP and IMAP servers. The results show that AJECT could effectively be used to locate vulnerabilities, even on well-known servers tested throughout the years.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374427","Testing and debugging;software engineering;test design;testing tools;experimental evaluation;fault injection;attack injection.","Network servers;Software testing;Protocols;Debugging;Application software;Computer networks;Computer hacking;Communication system security;Automatic testing;Software engineering","computer crime;software engineering","vulnerability discovery;attack injection;networked computer systems;software components;security analysts;hackers analysts;AJECT;IMAP servers;POP servers","","24","","33","","","","","","IEEE","IEEE Journals & Magazines"
"On the use of clone detection for identifying crosscutting concern code","M. Bruntink; A. van Deursen; R. van Engelen; T. Tourwe","Dept. of Software Eng., CWI, Amsterdam, Netherlands; Dept. of Software Eng., CWI, Amsterdam, Netherlands; NA; NA","IEEE Transactions on Software Engineering","","2005","31","10","804","818","In systems developed without aspect-oriented programming, code implementing a crosscutting concern may be spread over many different parts of a system. Identifying such code automatically could be of great help during maintenance of the system. First of all, it allows a developer to more easily find the places in the code that must be changed when the concern changes and, thus, makes such changes less time consuming and less prone to errors. Second, it allows the code to be refactored to an aspect-oriented solution, thereby improving its modularity. In this paper, we evaluate the suitability of clone detection as a technique for the identification of crosscutting concerns. To that end, we manually identify five specific crosscutting concerns in an industrial C system and analyze to what extent clone detection is capable of finding them. We consider our results as a stepping stone toward an automated ""aspect miner"" based on clone detection.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.114","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542064","Index Terms- Clone detection;reverse engineering;aspect-oriented programming;crosscutting concerns;aspect mining.","Cloning;Scattering;Computer languages;Computer Society;Reverse engineering;Software systems;Programming;Automation;Application software","object-oriented programming;object-oriented methods;software maintenance;reverse engineering","clone detection;crosscutting concern code;system maintenance;aspect-oriented solution;industrial C system;aspect miner","","60","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Flexible software development for multiple computer systems","K. Schwan; A. K. Jones","Department of Computer and Information Science, Ohio State University, Columbus, OH 43210; Tartan Laboratories, Pittsburgh, PA 15213","IEEE Transactions on Software Engineering","","1986","SE-12","3","385","401","The authors develop a model of concurrent software and the associated programming tools that jointly permit flexible software development for experimental programming on the Cm* multiprocessor. The model's implementation in the TASK and Bliss-11 programming languages is described using a sample concurrent program.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312881","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312881","Flexible program construction;languages;program blueprints;programming environments","Force;Servers;Software;Computers;Programming;Hardware;Educational institutions","parallel processing;software engineering","software engineering;multiple computer systems;concurrent software;programming tools;flexible software development;experimental programming;Cm* multiprocessor;TASK;Bliss-11 programming languages","","4","","","","","","","","IEEE","IEEE Journals & Magazines"
"Object analysis patterns for embedded systems","S. Konrad; B. H. C. Cheng; L. A. Campbell","Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA; Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA; Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA","IEEE Transactions on Software Engineering","","2004","30","12","970","992","Some of the most challenging tasks in building a software system are capturing, refining, and analyzing requirements. How well these tasks are performed significantly impacts the quality of the developed software system. The difficulty of these tasks is greatly exacerbated for the software of embedded systems as these systems are commonly used for critical applications, have to operate reliably for long periods of time, and usually have a high degree of complexity. Current embedded systems software development practice, however, often deals with the (requirements) analysis phase in a superficial manner, instead emphasizing design and implementation. This research investigates how an approach similar to the well-known design patterns, termed object analysis patterns, can be applied in the analysis phase of embedded systems development, prior to design and coding. Specifically, our research explores how object-oriented modeling notations, such as the Unified Modeling Language (UML), can be used to represent structural and behavioral information as part of commonly occurring object analysis patterns. This work also investigates how UML-based conceptual models of embedded systems, based on the diagram templates in the object analysis patterns, can be automatically analyzed using the Spin model checker for adherence to properties specified in linear-time temporal logic (LTL) using a previously developed UML formalization framework. We have applied these patterns to several embedded systems applications obtained from the automotive industry. This paper describes one of our case studies and illustrates how our approach facilitates the construction of UML-based conceptual models of embedded systems and the analysis of these models for adherence to functional requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.102","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377192","Index Terms- Object-oriented modeling;embedded systems;requirements;patterns;conceptual modeling;object analysis;formal specification;model checking.","Pattern analysis;Embedded system;Object oriented modeling;Software systems;Unified modeling language;Embedded software;Application software;Programming;Information analysis;Automatic logic units","object-oriented methods;embedded systems;formal specification;formal verification;software quality;Unified Modeling Language;temporal logic;object-oriented programming","object analysis pattern;embedded system;software system;software development;design pattern;object-oriented modeling;Unified Modeling Language;structural information;behavioral information;Spin model checker;linear-time temporal logic;automotive industry","","24","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Using transformations in specification-based prototyping","V. Berzins; Luqi; A. Yehudai","Dept. of Comput. Sci., US Naval Postgraduate Sch., Monterey, CA, USA; Dept. of Comput. Sci., US Naval Postgraduate Sch., Monterey, CA, USA; Dept. of Comput. Sci., US Naval Postgraduate Sch., Monterey, CA, USA","IEEE Transactions on Software Engineering","","1993","19","5","436","452","The authors explore the use of software transformations for software evolution. Meaning-preserving program transformations have been widely used for program development from a fixed initial specification. They consider a wider class of transformations to support development in which the specification evolves, rather than being fixed in advance. A new and general classification of transformations based on their effect on system interfaces, externally observable behavior, and abstraction level of a system description is presented. This classification is used to rearrange chronological derivation sequences containing meaning-changing transformations into lattices containing only meaning-preserving transformations. A process model for software evolution that utilizes prototyping techniques is described. Ways in which this class of transformations can be used to support such a process are considered. A set of examples are presented to illustrate the ideas. Software tool support and directions for future research are discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.232011","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=232011","","Prototypes;Software prototyping;Software tools;Computer science;Lattices;Roads;Software libraries;System software;Software systems;Lead","formal specification;software prototyping","software tool support;specification-based prototyping;software transformations;software evolution;system interfaces;externally observable behavior;abstraction level;system description;chronological derivation sequences;process model","","19","","40","","","","","","IEEE","IEEE Journals & Magazines"
"The Design of a Multicore Extension of the SPIN Model Checker","G. J. Holzmann; D. Bosnacki","NA; NA","IEEE Transactions on Software Engineering","","2007","33","10","659","674","We describe an extension of the SPIN model checker for use on multicore shared-memory systems and report on its performance. We show how, with proper load balancing, the time requirements of a verification run can, in some cases, be reduced close to N-fold when N processing cores are used. We also analyze the types of verification problems for which multicore algorithms cannot provide relief. The extensions discussed here require only relatively small changes in the SPIN source code and are compatible with most existing verification modes such as partial order reduction, the verification of temporal logic formulas, bitstate hashing, and hash-compact compression.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70724","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302778","Software/Program Verification;Model Checking;Models of Computation;Logics and meanings of Programs;Distributed Programming","Multicore processing;Power system modeling;Central Processing Unit;Logic programming;Software systems;Problem-solving;Load management;Algorithm design and analysis;Computational modeling;Distributed computing","distributed algorithms;file organisation;program verification;resource allocation;shared memory systems;temporal logic","SPIN model checker;multicore shared-memory system;load balancing;partial order reduction;temporal logic formula;bitstate hashing;hash-compact compression;program verification;distributed algorithm","","65","","38","","","","","","IEEE","IEEE Journals & Magazines"
"A modified priority based probe algorithm for distributed deadlock detection and resolution","A. N. Choudhary; W. H. Kohler; J. A. Stankovic; D. Towsley","Dept. of Electr. & Comput. Eng., Massachusetts Univ., Amherst, MA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","1","10","17","A modified, priority-based probe algorithm for deadlock detection and resolution in distributed database system is presented. Various examples are used to show that the original priority-based algorithm, presented by M.K. Sinha and N. Natarajan (1985), either fails to detect deadlocks or reports deadlocks that do not exist in many situations. A modified algorithm that eliminates these problems is proposed. The algorithm has been tested through simulation and appears to be errorfree. The performance of the modified algorithm is briefly discussed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21721","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21721","","Probes;System recovery;Database systems;Testing;Distributed control;Distributed databases;Information science;Computational modeling;Performance evaluation;Access protocols","distributed databases;system recovery","deadlock resolution;modified priority based probe algorithm;distributed deadlock detection;distributed database system","","53","","9","","","","","","IEEE","IEEE Journals & Magazines"
"The effect of data abstraction on loop programming techniques","J. M. Bishop","Dept. of Electron. & Comput. Sci., Southampton Univ., UK","IEEE Transactions on Software Engineering","","1990","16","4","389","402","It is shown how loop algorithms can be encompassed in an iterator and then activated for any data type for which a generator can be defined. It takes the iterator-generator idea a step further than previous work in that it permits variations of the iterators to be defined dynamically through the use of selectors and actors, without loss of efficiency or clarity. It is further shown that selectors can be employed in the definition of a truly generic sorting routine. Guidelines for the decomposition of a system into generic data types, abstract data types, iterators, generators, and the programs that exercise them are given, and several complete programs show the implementation of the techniques in Ada.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.54291","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=54291","","Proposals;Packaging;Computer science","data structures;software engineering","data abstraction;loop programming techniques;loop algorithms;iterator;generic sorting routine;generic data types;abstract data types;generators;Ada","","12","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Software dependability in the Tandem GUARDIAN system","Inhwan Lee; R. K. Iyer","Center for Reliable & High Performance Comput., Illinois Univ., Urbana, IL, USA; NA","IEEE Transactions on Software Engineering","","1995","21","5","455","467","Based on extensive field failure data for Tandem's GUARDIAN operating system, the paper discusses evaluation of the dependability of operational software. Software faults considered are major defects that result in processor failures and invoke backup processes to take over. The paper categorizes the underlying causes of software failures and evaluates the effectiveness of the process pair technique in tolerating software faults. A model to describe the impact of software faults on the reliability of an overall system is proposed. The model is used to evaluate the significance of key factors that determine software dependability and to identify areas for improvement. An analysis of the data shows that about 77% of processor failures that are initially considered due to software are confirmed as software problems. The analysis shows that the use of process pairs to provide checkpointing and restart (originally intended for tolerating hardware faults) allows the system to tolerate about 75% of reported software faults that result in processor failures. The loose coupling between processors, which results in the backup execution (the processor state and the sequence of events) being different from the original execution, is a major reason for the measured software fault tolerance. Over two-thirds (72%) of measured software failures are recurrences of previously reported faults. Modeling, based on the data, shows that, in addition to reducing the number of software faults, software dependability can be enhanced by reducing the recurrence rate.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.387474","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=387474","","Software measurement;Operating systems;Fault tolerant systems;Hardware;Failure analysis;Fault tolerance;Data analysis;Checkpointing;Software reliability;Programming","software fault tolerance;operating systems (computers);system recovery;back-up procedures;software performance evaluation","Tandem GUARDIAN operating system;field failure data;software dependability;operational software;software faults;major defects;processor failures;backup processes;software failures;process pair technique;software fault tolerance;reliability;improvement;checkpointing;restart;loose processor coupling;previously reported fault recurrence;modeling","","84","","37","","","","","","IEEE","IEEE Journals & Magazines"
"A scenario-driven approach to trace dependency analysis","A. Egyed","Teknowledge Corp., Marina del Rey, CA, USA","IEEE Transactions on Software Engineering","","2003","29","2","116","132","Software development artifacts-such as model descriptions, diagrammatic languages, abstract (formal) specifications, and source code-are highly interrelated where changes in some of them affect others. Trace dependencies characterize such relationships abstractly. This paper presents an automated approach to generating and validating trace dependencies. It addresses the severe problem that the absence of trace information or the uncertainty of its correctness limits the usefulness of software models during software development. It also automates what is normally a time consuming and costly activity due to the quadratic explosion of potential trace dependencies between development artifacts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178051","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178051","","Unified modeling language;Programming;Software testing;Software systems;Explosions;Uncertainty;Costs;Design engineering;Software quality;Iterative methods","specification languages;formal specification;reverse engineering;software engineering","model descriptions;diagrammatic languages;abstract specifications;formal specifications;source code;trace dependencies;correctness limits;software models;software development;Unified Modeling Language;UML;traceability","","71","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Compiling real-time specifications into extended automata","X. Nicollin; J. Sifakis; S. Yovine","Lab. de Genie Inf., Inst. IMAG, Grenoble, France; Lab. de Genie Inf., Inst. IMAG, Grenoble, France; Lab. de Genie Inf., Inst. IMAG, Grenoble, France","IEEE Transactions on Software Engineering","","1992","18","9","794","804","A method for the implementation and analysis of real-time systems, based on the compilation of specification extended automata is proposed. The method is illustrated for a simple specification language that can be viewed as the extension of a language for the description of systems of communicating processes, by adding timeout and watchdog constructs. The main result is that such a language can be compiled into timed automata, which are extended automata with timers. Timers are special state variables that can be set to zero by transitions, and whose values measure the time elapsed since their last reset. Timed automata do not make any assumption about the nature of time and adopt an event-driven execution mode. Their complexity does not depend on the values of the parameters of timeouts and watchdogs used in specifications. These features allow the application on timed automata of efficient code generation and analysis techniques. In particular, it is shown how symbolic model-checking of real-time properties can be directly applied to this model.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.159837","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=159837","","Automata;Real time systems;Computer languages;Clocks;Synchronous generators;Timing;Specification languages;Time measurement;Formal verification;Logic","automata theory;communicating sequential processes;formal specification;program compilers;real-time systems;specification languages","real-time specifications;real-time systems;extended automata;simple specification language;communicating processes;timeout;watchdog constructs;timed automata;state variables;event-driven execution mode;complexity;efficient code generation;symbolic model-checking;real-time properties","","61","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Constructing submodule specifications and network protocols","D. P. Sidhu; J. Aristizabal","Dept. Comput. Sci., Maryland Univ., Baltimore, MD, USA; NA","IEEE Transactions on Software Engineering","","1988","14","11","1565","1577","Applications of an automated tool for module specification (ATMS) that finds the specification for a submodule of a system are presented. Given the specification of a system, together with the specification for n-1 submodules, the ATMS constructs the specification for the nth addition submodule such that the interaction among the n submodules is equivalent to the specification of the system. The implementation of the technique is based on an approach proposed by P. Merlin and G.B. Bochmann (1983). The specification of a system and its submodules consists of all possible execution sequences of their individual operations. The ATMS uses finite-state machine concepts to represent the specifications and interactions of the system and its submodules. The specification found by the ATMS for a missing module of a system is the most general one, if one exists. Application of the ATMS in the area of communication protocols is discussed. A manual process to find the specification for a missing module using the Merlin-Bochmann technique is time-consuming and prone to errors. The automated tool presented proves a reliable method for constructing such a module.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.9045","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=9045","","Protocols;System recovery;Modular construction;Manuals;Computer science;Process control;Data communication","finite automata;formal specification;protocols;software tools","formal specification;submodule specifications;network protocols;automated tool;module specification;execution sequences;finite-state machine;communication protocols","","8","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Implementing and Evaluating Candidate-Based Invariant Generation","A. Betts; N. Chong; P. Deligiannis; A. F. Donaldson; J. Ketema","Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","7","631","650","The discovery of inductive invariants lies at the heart of static program verification. Presently, many automatic solutions to inductive invariant generation are inflexible, only applicable to certain classes of programs, or unpredictable. An automatic technique that circumvents these deficiencies to some extent is candidate-based invariant generation, whereby a large number of candidate invariants are guessed and then proven to be inductive or rejected using a sound program analyzer. This paper describes our efforts to apply candidate-based invariant generation in GPUVerify, a static checker for programs that run on GPUs. We study a set of 383 GPU programs that contain loops, drawn from a number of open source suites and vendor SDKs. Among this set, 253 benchmarks require provision of loop invariants for verification to succeed. We describe the methodology we used to incrementally improve the invariant generation capabilities of GPUVerify to handle these benchmarks, through candidate-based invariant generation, using cheap static analysis to speculate potential program invariants. We also describe a set of experiments that we used to examine the effectiveness of our rules for candidate generation, assessing rules based on their generality (the extent to which they generate candidate invariants), hit rate (the extent to which the generated candidates hold), worth (the extent to which provable candidates actually help in allowing verification to succeed), and influence (the extent to which the success of one generation rule depends on candidates generated by another rule). We believe that our methodology may serve as a useful framework for other researchers interested in candidate-based invariant generation. The candidates produced by GPUVerify help to verify 231 of the 253 programs. This increase in precision, however, makes GPUVerify sluggish: the more candidates that are generated, the more time is spent determining which are inductive invariants. To speed up this process, we have investigated four under-approximating program analyses that aim to reject false candidates quickly and a framework whereby these analyses can run in sequence or in parallel. Across two platforms, running Windows and Linux, our results show that the best combination of these techniques running sequentially-speeds up invariant generation across our benchmarks by 1.17× (Windows) and 1.01× (Linux), with per-benchmark best speedups of 93.58× (Windows) and 48.34× (Linux), and worst slowdowns of 10.24× (Windows) and 43.31× (Linux). We find that parallelizing the strategies marginally improves overall invariant generation speedups to 1.27× (Windows) and 1.11× (Linux), maintains good best-case speedups of 91.18× (Windows) and 44.60× (Linux), and, importantly, dramatically reduces worst-case slowdowns to 3.15× (Windows) and 3.17× (Linux).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2718516","EU FP7 STREP project CARP; EPSRC PSL; Imperial College London’s EPSRC Impact Acceleration Account; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7955079","Formal verification;GPUs;invariant generation","Linux;Graphics processing units;Benchmark testing;Tools;Cognition;Acceleration","formal verification;graphics processing units;Linux;Microsoft Windows (operating systems);program verification","invariant generation speedups;inductive invariant generation;candidate invariants;invariant generation capabilities;potential program invariants;GPUVerify;SDK;candidate-based invariant generation evaluation;GPU programs;Windows;Linux;static program verification","","","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Copying and swapping: influences on the design of reusable software components","D. E. Harms; B. W. Weide","Dept. of Math. & Comput. Sci., Muskingum Coll., New Concord, OH, USA; NA","IEEE Transactions on Software Engineering","","1991","17","5","424","435","The authors argue that a simple alternative to copying as a data movement primitive-swapping (exchanging) the values of two variables-has potentially significant advantages in the context of the design of generic reusable software components. Specifically, the authors claim that generic module designs based on a swapping style are superior to designs based on copying, both in terms of execution-time efficiency and with respect to the likelihood of correctness of client programs and module implementations. Furthermore, designs based on swapping are more reusable than traditional designs. Specific arguments and examples to support these positions are presented.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.90445","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=90445","","Software design;Software reusability;Embedded software;Computer science;Computer languages;Formal specifications;Object oriented programming;Mathematics;Information science","data structures;software reusability","copying;data movement primitive;generic reusable software components;generic module designs;swapping style","","32","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Ginger2: an environment for computer-aided empirical software engineering","K. Torii; K. Matsumoto; K. Nakakoji; Y. Takada; S. Takada; K. Shima","Graduate Sch. of Inf. Sci., Nara Inst. of Sci. & Technol., Japan; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1999","25","4","474","492","Empirical software engineering can be viewed as a series of actions to obtain knowledge and a better understanding about some aspects of software development, given a set of problem statements in the form of issues, questions or hypotheses. Experience has made us aware of the criticality of integrating the various types of data that are collected and analyzed as well as the criticality of integrating the various types of activities that take place, such as experiment design and the experiment itself. This has led us to develop a Computer-Aided Empirical Software Engineering (CAESE) framework to support the empirical software engineering lifecycle. The paper first presents the CAESE framework that consists of three elements: (1) a process model for the ""lifecycle"" of empirical software engineering studies, including needs analysis, experiment design, actual experimentation, and analyzing and packaging results; (2) a model that helps empirical software engineers decide how to look at the ""world"" to be studied in a coherent manner; (3) an architecture, based on which CAESE environments can be built, consisting of tool sets for each phase of the process model, a process management mechanism, and the two types of integration mechanism that are vital for handling multiple types of data: data integration and control integration. Next, the paper describes the Ginger2 environment as an instantiation of our framework. It concludes with reports on case studies using Ginger2, which dealt with a variety of empirical data types including mouse and keystrokes, eye traces, 3D movement, skin resistance level, and videotaped data.","0098-5589;1939-3520;2326-3881","","10.1109/32.799942","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=799942","","Software engineering;Programming;Software packages;Packaging;Software tools;Data engineering;Design engineering;Computer architecture;Environmental management;Engineering management","computer aided software engineering;data structures;software architecture","Ginger2 environment;computer aided empirical software engineering;experiment design;empirical software engineering lifecycle;CAESE framework;process model;needs analysis;packaging results;CAESE environments;tool sets;process management mechanism;integration mechanism;multiple data types;data integration;control integration;case studies;empirical data types;keystrokes;eye traces;three dimensional movement;skin resistance level;videotaped data","","19","","32","","","","","","IEEE","IEEE Journals & Magazines"
"TURTLE: a real-time UML profile supported by a formal validation toolkit","L. Apvrille; J. -. Courtiat; C. Lohr; P. de Saqui-Sannes","Inst. Eurecom, Ecole Nat. Superieure des Telecommun., Sophia-Antipolis, France; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","7","473","487","We present a UML 1.5 profile named TURTLE (Timed UML and RT-LOTOS Environment) endowed with a formal semantics given in terms of RT-LOTOS. TURTLE relies on UML's extensibility mechanisms to enhance class and activity diagrams. Class diagrams are extended with specialized classes named Tclasses, which communicate and synchronize through gates. Also, associations between Tclasses are attributed by a composition operator (Parallel, Synchro, Invocation, Sequence, or Preemption) which provides them with a formal semantics. TURTLE extends UML activity diagrams with synchronization actions and temporal operators (deterministic delay, nondeterministic delay, time-limited offer, and time-capture). The real-time dimension of TURTLE has been further improved by the addition of two composition operators, periodic and suspend, as well as suspendable delay, latency, and time-limited offer operators at the activity diagram level. Core characteristics of TURLE are supported by TTool - the TURTLE toolkit - which includes a diagram editor, a RT-LOTOS code generator and a result analyzer. The toolkit reuses RTL, a RT-LOTOS validation tool offering debug-oriented simulation and exhaustive analysis. TTool hides RT-LOTOS to the end-user and allows him/her to directly check TURTLE modeling against logical errors and timing inconsistencies. Besides the foundations of the TURTLE profile, this paper also discusses its application in the context of space-based embedded software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318608","Real-time systems;UML;RT-LOTOS;formal validation.","Unified modeling language;Delay;Real time systems;Timing;Embedded software;Prototypes;Character generation;Analytical models;Computer errors;Application software","specification languages;formal verification;formal specification;real-time systems;program compilers","Timed UML and RT-LOTOS Environment;TURTLE real-time UML 1.5 profile;formal semantics;UML activity diagrams;class diagrams;Tclasses;TTool TURTLE toolkit;RT-LOTOS code generator;RT-LOTOS validation tool;debug-oriented simulation;space-based embedded software;formal validation toolkit","","40","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Feature Location Using Probabilistic Ranking of Methods Based on Execution Scenarios and Information Retrieval","D. Poshyvanyk; Y. Gueheneuc; A. Marcus; G. Antoniol; V. Rajlich","Department of Computer Science, Wayne State University, Detroit, MI; Departement d'informatique et recherche operationnelle, Universite de Montreal, Center-Ville, Monrreal Quebec, Canada; Department of Computer Science, Wayne State University, Detroit, MI; Departement d'informatique, Ecole Polytechnique de Montreal, Centre-Ville, Montreal, Quebec, Canada; Department of Computer Science, Wayne State University, Detroit, MI","IEEE Transactions on Software Engineering","","2007","33","6","420","432","This paper recasts the problem of feature location in source code as a decision-making problem in the presence of uncertainty. The solution to the problem is formulated as a combination of the opinions of different experts. The experts in this work are two existing techniques for feature location: a scenario-based probabilistic ranking of events and an information-retrieval-based technique that uses latent semantic indexing. The combination of these two experts is empirically evaluated through several case studies, which use the source code of the Mozilla Web browser and the Eclipse integrated development environment. The results show that the combination of experts significantly improves the effectiveness of feature location as compared to each of the experts used independently","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1016","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181710","Program understanding;feature identification;concept location;dynamic and static analyses;information retrieval;Latent Semantic Indexing;scenario-based probabilistic ranking;open source software.","Information retrieval;Performance analysis;Indexing;Programming profession;Computer Society;Decision making;Uncertainty;Information analysis;Open source software;Computer bugs","decision making;information retrieval;probability;program diagnostics","feature location;information retrieval;decision-making problem;scenario-based probabilistic event ranking;latent semantic indexing;Mozilla Web browser;Eclipse integrated development","","209","","49","","","","","","IEEE","IEEE Journals & Magazines"
"A case-study in timed refinement: a mine pump","B. P. Mahony; I. J. Hayes","Dept. of Comput. Sci., Queensland Univ., Qld., Australia; Dept. of Comput. Sci., Queensland Univ., Qld., Australia","IEEE Transactions on Software Engineering","","1992","18","9","817","826","A specification and top-level refinement of a simple mine pump control system, as well as a proof of correctness of the refinement, are presented as an example of the application of a formal method for the development of time-based systems. The overall approach makes use of a refinement calculus for timed systems, similar to the refinement calculi for sequential programs. The specification makes use of topologically continuous functions of time to describe both analog and discrete properties of both the system and its refinements. The basic building block of specifications is a specification statement that gives a clear separation between the specification of the assumptions that the system may make about the environment in which it is to be placed, and the effect the system is guaranteed to achieve if placed in such an environment. The top-level refinement of the system is developed by application of refinement laws that allow design decisions to be made, local state to be introduced, and the decomposition of systems into pipelined and/or parallel processes.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.159841","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=159841","","Calculus;Control systems;Computer science;Water;Terminology;Topology;Interconnected systems;Australia Council;Protection;Monitoring","computerised monitoring;formal specification;mining;pumps;theorem proving","top-level refinement;simple mine pump control system;proof of correctness;formal method;time-based systems;refinement calculus;timed systems;sequential programs;topologically continuous functions;discrete properties;basic building block;specification statement;refinement laws;design decisions;pipelined;parallel processes","","39","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying modules via concept analysis","M. Siff; T. Reps","Dept. of Math., Sarah Lawrence Coll., Bronxville, NY, USA; NA","IEEE Transactions on Software Engineering","","1999","25","6","749","768","Describes a general technique for identifying modules in legacy code. The method is based on concept analysis - a branch of lattice theory that can be used to identify similarities among a set of objects based on their attributes. We discuss how concept analysis can identify potential modules using both ""positive"" and ""negative"" information. We present an algorithmic framework to construct a lattice of concepts from a program, where each concept represents a potential module. We define the notion of a concept partition, present an algorithm for discovering all concept partitions of a given concept lattice, and prove the algorithm to be correct.","0098-5589;1939-3520;2326-3881","","10.1109/32.824377","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=824377","","Lattices;Partitioning algorithms;Information analysis;Reverse engineering;Software systems;Computer languages;Software tools;Prototypes;Computer science;Queueing analysis","subroutines;semantic networks;reverse engineering;software engineering;pattern matching","module identification;concept analysis;legacy code;lattice theory;similarity identification;object attributes;positive information;negative information;algorithmic framework;concept lattice;concept partitions;algorithm correctness;modularization;software migration;software restructuring;reverse engineering;design recovery","","63","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Categorization of common coupling and its application to the maintainability of the Linux kernel","L. Yu; S. R. Schach; K. Chen; J. Offutt","Dept. of Comput. Sci., Tennessee Technol. Univ., Cookeville, TN, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","10","694","706","Data coupling between modules, especially common coupling, has long been considered a source of concern in software design, but the issue is somewhat more complicated for products that are comprised of kernel modules together with optional nonkernel modules. This paper presents a refined categorization of common coupling based on definitions and uses between kernel and nonkernel modules and applies the categorization to a case study. Common coupling is usually avoided when possible because of the potential for introducing risky dependencies among software modules. The relative risk of these dependencies is strongly related to the specific definition-use relationships. In a previous paper, we presented results from a longitudinal analysis of multiple versions of the open-source operating system Linux. This paper applies the new common coupling categorization to version 2.4.20 of Linux, counting the number of instances of common coupling between each of the 26 kernel modules and all the other nonkernel modules. We also categorize each coupling in terms of the definition-use relationships. Results show that the Linux kernel contains a large number of common couplings of all types, raising a concern about the long-term maintainability of Linux.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.58","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339279","Index Terms- Modularity;dependencies;common coupling;definition-use analysis;kernel-based software;open-source software;Linux.","Linux;Kernel;Open source software;Operating systems;Application software;Computer Society;Computer architecture;Database systems;Software design;System software","Unix;operating system kernels;software reliability;public domain software;software maintenance;software metrics;software reusability","data coupling;software design;common coupling categorization;software modules;Linux kernel;definition-use analysis;open source software;kernel based software","","38","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating software reuse alternatives: a model and its application to an industrial case study","A. Tomer; L. Goldin; T. Kuflik; E. Kimchi; S. R. Schach","RAFAEL Ltd., Haifa, Israel; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","9","601","612","We propose a model that enables software developers to systematically evaluate and compare all possible alternative reuse scenarios. The model supports the clear identification of the basic operations involved and associates a cost component with each basic operation in a focused and precise way. The model is a practical tool that assists developers to weigh and evaluate different reuse scenarios, based on accumulated organizational data, and then to decide which option to select in a given situation. The model is currently being used at six different companies for cost-benefit analysis of alternative reuse scenarios; we give a case study that illustrates how it has been used in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324647","Index Terms- Reuse models;cost estimation;maintenance management;software libraries;process metrics;process measurement;planning.","Application software;Computer industry;Computer aided software engineering;Computer Society;Software systems;Cost benefit analysis;Software maintenance;Software libraries;Software measurement;Process planning","software cost estimation;software reusability;software maintenance;software management;software metrics;cost-benefit analysis;software libraries","cost-benefit analysis;software reuse model;software cost estimation;maintenance management;software libraries;process metrics;process measurement;industrial case study","","31","","13","","","","","","IEEE","IEEE Journals & Magazines"
"A Comparison of Tabular Expression-Based Testing Strategies","X. Feng; D. L. Parnas; T. H. Tse; T. O'Callaghan","University of Limerick, Limerick and United International College, Zhuhai, Guangdong; University of Limerick, Limerick; The University of Hong Kong, Hong Kong; University of Limerick, Limerick","IEEE Transactions on Software Engineering","","2011","37","5","616","634","Tabular expressions have been proposed as a notation to document mathematically precise but readable software specifications. One of the many roles of such documentation is to guide testers. This paper 1) explores the application of four testing strategies (the partition strategy, decision table-based testing, the basic meaningful impact strategy, and fault-based testing) to tabular expression-based specifications, and 2) compares the strategies on a mathematical basis through formal and precise definitions of the subsumption relationship. We also compare these strategies through experimental studies. These results will help researchers improve current methods and will enable testers to select appropriate testing strategies for tabular expression-based specifications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.78","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975175","Tabular expression;test case constraint;subsume;unconditionally subsume;conditionally subsume.","Testing;Redundancy;Documentation;Software quality;Software engineering;Electronic mail","formal specification;program testing;system documentation","tabular expression-based testing strategies;readable software specifications;partition strategy;decision table-based testing;meaningful impact strategy;fault-based testing","","4","","49","","","","","","IEEE","IEEE Journals & Magazines"
"On the Positive Effect of Reactive Programming on Software Comprehension: An Empirical Study","G. Salvaneschi; S. Proksch; S. Amann; S. Nadi; M. Mezini","Department of Computer Science, Reactive Systems Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany; Department of Computer Science, Software Technology Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany; Department of Computer Science, Software Technology Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany; Department of Computing Science, AB, University of AlbertaCanada; Department of Computer Science, Software Technology Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany","IEEE Transactions on Software Engineering","","2017","43","12","1125","1143","Starting from the first investigations with strictly functional languages, reactive programming has been proposed as the programming paradigm for reactive applications. Over the years, researchers have enriched reactive languages with more powerful abstractions, embedded these abstractions into mainstream languages-including object-oriented languages-and applied reactive programming to several domains, such as GUIs, animations, Web applications, robotics, and sensor networks. However, an important assumption behind this line of research is that, beside other claimed advantages, reactive programming makes a wide class of otherwise cumbersome applications more comprehensible. This claim has never been evaluated. In this paper, we present the first empirical study that evaluates the effect of reactive programming on comprehension. The study involves 127 subjects and compares reactive programming to the traditional object-oriented style with the Observer design pattern. Our findings show that program comprehension is significantly enhanced by the reactive-programming paradigm-a result that suggests to further develop research in this field.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2655524","European Research Council; German Federal Ministry of Education and Research; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7827078","Reactive programming;empirical study;controlled experiment;software comprehension","Programming;Runtime;Software development;Robot sensing systems","functional languages;object-oriented languages;object-oriented programming","object-oriented languages;software comprehension;functional languages;observer design pattern;program comprehension;reactive languages;reactive programming","","3","","72","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Generating Test Cases for Real-Time Systems Based on Symbolic Models","W. L. Andrade; P. D. L. Machado","Federal University of Campina Grande, Campina Grande; Federal University of Campina Grande, Campina Grande","IEEE Transactions on Software Engineering","","2013","39","9","1216","1229","The state space explosion problem is one of the challenges to be faced by test case generation techniques, particularly when data values need to be enumerated. This problem gets even worse for real-time systems (RTS) that also have time constraints. The usual solution in this context, based on finite state machines or time automata, consists of enumerating data values (restricted to finite domains) while treating time symbolically. In this paper, a symbolic model for conformance testing of real-time systems software named TIOSTS that addresses both data and time symbolically is presented. Moreover, a test case generation process is defined to select more general test cases with variables and parameters that can be instantiated at testing execution time. Generation is based on a combination of symbolic execution and constraint solving for the data part and symbolic analysis for timed aspects. Furthermore, the practical application of the process is investigated through a case study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6475130","Real-time systems and embedded systems;formal methods;symbolic execution;testing strategies","Testing;Clocks;Cost accounting;Real-time systems;Data models;Automata;Semantics","conformance testing;finite state machines;program testing;real-time systems","test case generation techniques;real-time systems;symbolic models;state space explosion problem;RTS;time constraints;finite state machines;time automata;conformance testing;TIOSTS;symbolic execution;constraint solving;data part;symbolic analysis;timed aspects","","6","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Predictive Mutation Testing","J. Zhang; L. Zhang; M. Harman; D. Hao; Y. Jia; L. Zhang","Computer Science, Peking University, 12465 Beijing, Beijing China 100871 (e-mail: zhangjie_marina@163.com); Computer Science, University of Texas at Dallas, Richardson, Texas United States 75080 (e-mail: lingming.zhang@utdallas.edu); CS, UCL, London, London United Kingdom of Great Britain and Northern Ireland WC1E 6BT (e-mail: mark.harman@ucl.ac.uk); EECS,Peking University, Institute of Software, Beijing, Beijing China 100871 (e-mail: haodan@pku.edu.cn); Computer Science, University College London, London, London United Kingdom of Great Britain and Northern Ireland WC1E 6B (e-mail: yue.jia@ucl.ac.uk); EECS,Peking University, Institute of Software, Beijing, Beijing China (e-mail: zhanglucs@pku.edu.cn)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Test suites play a key role in ensuring software quality. A good test suite may detect more faults than a poor-quality one. Mutation testing is a powerful methodology for evaluating the fault-detection ability of test suites. In mutation testing, a large number of mutants may be generated and need to be executed against the test suite under evaluation to check how many mutants the test suite is able to detect, as well as the kind of mutants that the current test suite fails to detect. Consequently, although highly effective, mutation testing is widely recognized to be also computationally expensive, inhibiting wider uptake. To alleviate this efficiency concern, we propose Predictive Mutation Testing (PMT): the first approach to predicting mutation testing results without executing mutants. In particular, PMT constructs a classification model, based on a series of features related to mutants and tests, and uses the model to predict whether a mutant would be killed or remain alive without executing it. PMT has been evaluated on 163 real-world projects under two application scenarios (cross-version and cross-project). The experimental results demonstrate that PMT improves the efficiency of mutation testing by up to 151.4X while incurring only a small accuracy loss. It achieves above 0.80 AUC values for the majority of projects, indicating a good tradeoff between the efficiency and effectiveness of predictive mutation testing. Also, PMT is shown to perform well on different tools and tests, be robust in the presence of imbalanced data, and have high predictability (over 60% confidence) when predicting the execution results of the majority of mutants.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2809496","EPSRC grant DAASE Dynamic Adaptive Automated Software Engineering; the National Key Research and Development Program; the National Natural Science Foundation of China; NSF Grant; UT Dallas faculty start-up fund; Google Faculty Research Award; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8304576","PMT;mutation testing;machine learning;binary classification","Predictive models;Tools;Software testing;Electronic mail;Guidelines;Software quality","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"Location Independent Remote Execution in NEST","R. Agrawal; A. K. Ezzat","AT&T Bell Laboratories; NA","IEEE Transactions on Software Engineering","","1987","SE-13","8","905","912","We consider a computing environment consisting of a network of autonomous, yet cooperating personal computer workstations and shared servers. Computing cycles in such an environment can be shared by creating a pool of compute servers in the network that may be used by the workstations to supplement their computing needs. Some processors may be permanently designated to be the compute servers. In addition, through an advertisement mechanism, any workstation may make itself temporarily available for a specific duration of time to be used as a compute server. In this paper, we present the design and implementation of a scheme for augmenting the UNIX® operating system with the location independent remote execution capability. This capability allows processes to be offloaded to the compute servers and preserves the execution environment of these processes as if they were still executing locally at the originating machine. Our model provides execution location independence of processes by preserving the process view of the file system, parent-child relationships, process groups, and process signaling across machine boundaries in a transparent way. We also present our scheme that allows processors to advertise themselves as available to some or all nodes in the network and withdraw as a compute server in a distributed manner. The scheme is robust in presence of node failures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233509","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702309","Computer network;distributed computing environment;distributed operating system;processor sharing;remote execution;UNIX system","Workstations;Network servers;Computer networks;Operating systems;File systems;Distributed computing;Intelligent networks;Microcomputers;Signal processing;Robustness","","Computer network;distributed computing environment;distributed operating system;processor sharing;remote execution;UNIX system","","13","","40","","","","","","IEEE","IEEE Journals & Magazines"
"A software size model","J. Verner; G. Tate","Sch. of Inf. Syst., New South Wales Univ., Kensington, NSW, Australia; NA","IEEE Transactions on Software Engineering","","1992","18","4","265","278","A bottom-up approach to software size estimation is described. It first identifies factors affecting software size, thus obtaining size explanation equations, and then seeks suitable predictors based on those explanation factors which can be used for size estimation. The approach, or model, is bottom-up in that it sizes individual software components or modules first, and then obtains subsystem and system sizes by summing component sizes. Since components may have different purposes and characteristics, the model allows for the partitioning of system components into several different types, each component type having different size explanation and estimation equations. The partitioning is not fixed, but depends on the particular software development technology. The model is applied to several different software systems, including both business applications and systems programs.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.129216","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=129216","","Costs;Programming;Business;Differential equations;Software systems;Application software;Software metrics;Project management;Software testing;Uncertainty","project engineering;software engineering;software metrics","software metrics;bottom-up approach;software size estimation;software development technology;business applications;systems programs","","56","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Resource Management for Complex, Dynamic Environments","M. S. Raunak; L. J. Osterweil","Loyola University MD, Baltimore; University of Massachusetts Amherst, Amherst","IEEE Transactions on Software Engineering","","2013","39","3","384","402","This paper describes an approach to the specification and management of the agents and resources that are required to support the execution of complex systems and processes. The paper suggests that a resource should be viewed as a provider of a set of capabilities that are needed by a system or process, where that set may vary dynamically over time and with circumstances. This view of resources is defined and then made the basis for the framework of an approach to specifying, managing, and allocating resources in the presence of real-world complexity and dynamism. The ROMEO prototype resource management system is presented as an example of how this framework can be instantiated. Some case studies of the use of ROMEO to support system execution are presented and used to evaluate the framework, the ROMEO prototype, and our view of the nature of resources.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6197203","Resources management;process modeling;discrete event simulation;healthcare processes","Resource management;Hospitals;Surgery;Software;Context;Databases","formal specification;software agents;software engineering","resource management;dynamic environments;complex environments;complex systems;complex processes;ROMEO prototype resource management system;software engineering","","18","","40","","","","","","IEEE","IEEE Journals & Magazines"
"A state-of-the-art survey on software merging","T. Mens","Programming Technol. Lab., Vrije Univ., Brussels, Belgium","IEEE Transactions on Software Engineering","","2002","28","5","449","462","Software merging is an essential aspect of the maintenance and evolution of large-scale software systems. This paper provides a comprehensive survey and analysis of available merge approaches. Over the years, a wide variety of different merge techniques has been proposed. While initial techniques were purely based on textual merging, more powerful approaches also take the syntax and semantics of the software into account. There is a tendency towards operation-based merging because of its increased expressiveness. Another tendency is to try to define merge techniques that are as general, accurate, scalable, and customizable as possible, so that they can be used in any phase in the software life-cycle and detect as many conflicts as possible. After comparing the possible merge techniques, we suggest a number of important open problems and future research directions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000449","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000449","","Merging","configuration management;software maintenance;merging","software merging;software maintenance;merge approaches;textual merging;software life-cycle;conflict detection;conflict resolution;large-scale software systems","","179","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Search Algorithms for Regression Test Case Prioritization","Z. Li; M. Harman; R. M. Hierons","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","4","225","237","Regression testing is an expensive, but important, process. Unfortunately, there may be insufficient resources to allow for the reexecution of all test cases during regression testing. In this situation, test case prioritization techniques aim to improve the effectiveness of regression testing by ordering the test cases so that the most beneficial are executed first. Previous work on regression test case prioritization has focused on greedy algorithms. However, it is known that these algorithms may produce suboptimal results because they may construct results that denote only local minima within the search space. By contrast, metaheuristic and evolutionary search algorithms aim to avoid such problems. This paper presents results from an empirical study of the application of several greedy, metaheuristic, and evolutionary search algorithms to six programs, ranging from 374 to 11,148 lines of code for three choices of fitness metric. The paper addresses the problems of choice of fitness metric, characterization of landscape modality, and determination of the most suitable search technique to apply. The empirical results replicate previous results concerning greedy algorithms. They shed light on the nature of the regression testing search space, indicating that it is multimodal. The results also show that genetic algorithms perform well, although greedy approaches are surprisingly effective, given the multimodal nature of the landscape","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123325","Search techniques;test case prioritization;regression testing.","Greedy algorithms;Cost function;Genetic algorithms;Software testing;Libraries;Fault detection","genetic algorithms;greedy algorithms;program testing;search problems","regression testing;test case prioritization technique;greedy algorithm;metaheuristics;evolutionary search algorithm;fitness metric;genetic algorithm","","269","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Graph directed locking","M. H. Eich","Dept. of Comput. Sci., Southern Methodist Univ., Dallas, TX, USA","IEEE Transactions on Software Engineering","","1988","14","2","133","140","A non-two-phase database concurrency control technique is introduced. The technique is deadlock-free, places no restrictions on the structure of the data, never requires data to be reread, never forces a transaction to be rolled back in order to achieve serializability, applies a type of lock conversion, and allows items to be released to subsequent transactions as soon as possible. The method introduced, database flow graph locking (FGL), uses a directed acyclic graph to direct the migration of locks between transactions. Unlike many previous non-two-phase methods, the database need not be structured in any specific fashion. The effect of these changes is that, with the same serializable schedule, FGL obtains a higher degree of concurrency than two-phase locking (2PL). Overhead requirements for database flow graph locking are comparable to those for two-phase locking, with 2PL being better in low conflict situations and FGL better in high conflict.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4633","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4633","","Protocols;Transaction databases;System recovery;Concurrency control;Concurrent computing;Tree graphs;Force control;Flow graphs;Control systems;Database systems","database theory;directed graphs;distributed databases;system recovery","distributed databases;database concurrency control;deadlock-free;lock conversion;database flow graph locking;directed acyclic graph;non-two-phase methods","","6","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Distrbution and Abstract Types in Emerald","A. Black; N. Hutchinson; E. Jul; H. Levy; L. Carter","Department of Computer Science, University of Washington; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","1","65","76","Emerald is an object-based language for programming distributed subsystems and applications. Its novel features include 1) a single object model that is used both for programming in the small and in the large, 2) support for abstract types, and 3) an explicit notion of object location and mobility. This paper outlines the goals of Em-erald, relates Emerald to previous work, and describes its type system and distribution support. We are currently constructing a prototype implementation of Emerald.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232836","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702134","Abstract data types;distributed operating system;distributed programming;object-oriented programming;process migration;type checking","Computer languages;Programming profession;Object oriented modeling;Prototypes;Operating systems;Object oriented programming;Local area networks;Workstations;Art;Packaging","","Abstract data types;distributed operating system;distributed programming;object-oriented programming;process migration;type checking","","130","","37","","","","","","IEEE","IEEE Journals & Magazines"
"The Crystal Multicomputer: Design and Implementation Experience","D. J. De Witt; R. Finkel; M. Solomon","Department of Computer Sciences, University of Wisconsin—Madison; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","8","953","966","This paper presents an overview of the hardware and software components of the Crystal multicomputer project. The goal of the Crystal project is to design and implement a vehicle that serves a variety of research projects involving distributed computation. Crystal can be used simultaneously by multiple research projects by partitioning the available processors according to the requirements of each project. Users can employ the Crystal multicomputer in several ways. Projects such as operating systems and database machines that need direct control of processor resources (clock, memory management, communication devices) can be implemented using a reliable communication service (the ""nugget"" that resides on each node processor. Projects that prefer a higher-level interface can be implemented using the Charlotte distributed operating system. Finally, users interested in Crystal principally as a cycle server can run UNIX® jobs on node machines using the ""remote"" unix service. Development, debugging, and execution of projects can take place remotely under the control of any of several UNIX hosts. Acquiring a partition of machines, resetting each machine, and then loading an application onto each machine is performed by invoking a UNIX-resident program (the ""nuggetmaster""). Communication with node machines in a partition is facilitated by a virtual terminal and window mechanism. Crystal is fully operational and has been used to support a variety of research projects. To illustrate the flexibility provided by the Crystal environment, four of these projects are described.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233513","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702313","Communication protocols;database machines;multicomputers;multiprocessors;operating systems;parallel computing","Operating systems;Communication system control;Hardware;Vehicles;Distributed computing;Database machines;Process control;Control systems;Clocks;Memory management","","Communication protocols;database machines;multicomputers;multiprocessors;operating systems;parallel computing","","7","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Asymptotic Perturbation Bounds for Probabilistic Model Checking with Empirically Determined Probability Parameters","G. Su; Y. Feng; T. Chen; D. S. Rosenblum","Department of Computer Science, School of Computing, National University of Singapore; Quantum Computation and Intelligent Systems, University of Technology Sydney; Department of Computer Science, Middlesex University London; Department of Computer Science, School of Computing, National University of Singapore","IEEE Transactions on Software Engineering","","2016","42","7","623","639","Probabilistic model checking is a verification technique that has been the focus of intensive research for over a decade. One important issue with probabilistic model checking, which is crucial for its practical significance but is overlooked by the state-of-the-art largely, is the potential discrepancy between a stochastic model and the real-world system it represents when the model is built from statistical data. In the worst case, a tiny but nontrivial change to some model quantities might lead to misleading or even invalid verification results. To address this issue, in this paper, we present a mathematical characterization of the consequences of model perturbations on the verification distance. The formal model that we adopt is a parametric variant of discrete-time Markov chains equipped with a vector norm to measure the perturbation. Our main technical contributions include a closed-form formulation of asymptotic perturbation bounds, and computational methods for two arguably most useful forms of those bounds, namely linear bounds and quadratic bounds. We focus on verification of reachability properties but also address automata-based verification of omega-regular properties. We present the results of a selection of case studies that demonstrate that asymptotic perturbation bounds can accurately estimate maximum variations of verification results induced by model perturbations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2508444","Singapore Ministry of Education; Australian Research Council; National Natural Science Foundation of China; CAS/SAFEA; State Key Laboratory of Novel Software Technology at Nanjing University; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7355393","Asymptotic perturbation bound;discrete-time Markov chain;numerical iteration;optimization;parametric Markov chain;perturbation analysis;probabilistic model checking;quadratic programming","Model checking;Markov processes;Probabilistic logic;Computational modeling;Mathematical model;Perturbation methods","automata theory;formal verification;linear programming;Markov processes;probability;quadratic programming;reachability analysis","probabilistic model checking;empirically determined probability parameters;verification technique;stochastic model;real-world system;statistical data;mathematical characterization;model perturbations;verification distance;formal model;parametric discrete-time Markov chains;vector norm;perturbation measure;closed-form formulation;asymptotic perturbation bounds;computational methods;linear bounds;quadratic bounds;reachability property verification;automata-based verification;omega-regular properties;maximum verification variation estimation","","3","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Process-translatable Petri nets for the rapid prototyping of process control systems","G. Bruno; G. Marchetto","Dipartimento di Automatica e Informatica, Politecnico di Torino, Corso Duca degli Abruzzi 24, 10129 Torino, Italy; Dipartimento di Automatica e Informatica, Politecnico di Torino, Corso Duca degli Abruzzi 24, 10129 Torino, Italy","IEEE Transactions on Software Engineering","","1986","SE-12","2","346","357","A methodology for the rapid prototyping of process control systems which is based on an original extension to classical Petri nets is presented. The proposed nets, called PROT nets, provide a suitable framework to support the following activities: building an operational specification model; evaluation, simulation, and validation of the model; and automatic translation into program structures. PROT nets are shown to be translatable into Ada program structures concerning concurrent processes and their synchronizations. The authors illustrate this translation in detail using, as a working example, the problem of tool handling in a flexible manufacturing system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1986.6312948","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6312948","Ada;Petri nets;process control systems;rapid prototyping;requirements specification;simulation;synchronizations","Petri nets;Prototypes;Process control;Fires;Software;Computational modeling;Firing","flexible manufacturing systems;graph theory;process computer control;software engineering","Petri nets;rapid prototyping;process control systems;PROT nets;specification model;evaluation;simulation;validation;program structures;Ada program structures;concurrent processes;tool handling;flexible manufacturing system","","66","","","","","","","","IEEE","IEEE Journals & Magazines"
"Estimating capacity for sharing in a privately owned workstation environment","M. W. Mutka","Dept. of Comput. Sci., Michigan State Univ., East Lansing, MI, USA","IEEE Transactions on Software Engineering","","1992","18","4","319","328","The author analyzes workstation patterns in order to understand opportunities for exploiting idle capacity. This study is based on traces of users workstation activity in a university environment. It identifies two areas where enhancements can be made. One area is the ability of a manager of the shared capacity of a workstation cluster to schedule jobs with deadline constraints. This opportunity is the result of the ability to make good predictions of the time-varying amount of capacity that is available for sharing. A prediction strategy is developed that is shown to have only a small amount of error. For the second area of enhancement, it is shown that it is feasible to allocate partitions of workstations for specific periods. This aids those users who on occasion need exclusive access to several machines. The author examines the profile of periods during which exclusive access to partitions can be given, the rate that owners preempt users of partitions, and the distribution of interpreemption intervals.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.129220","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=129220","","Workstations;Resource management;Quality of service;Environmental management;Pattern analysis;Operating systems;Power system modeling;Interference;Quality management","DP management;multi-access systems;resource allocation;scheduling","capacity management;partition allocation;job scheduling;workstation patterns;idle capacity;university environment;shared capacity;deadline constraints","","34","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Bayesian graphical models for software testing","D. A. Wooff; M. Goldstein; F. P. A. Coolen","Dept. of Math. Sci., Durham Univ., UK; Dept. of Math. Sci., Durham Univ., UK; Dept. of Math. Sci., Durham Univ., UK","IEEE Transactions on Software Engineering","","2002","28","5","510","525","This paper describes a new approach to the problem of software testing. The approach is based on Bayesian graphical models and presents formal mechanisms for the logical structuring of the software testing problem, the probabilistic and statistical treatment of the uncertainties to be addressed, the test design and analysis process, and the incorporation and implication of test results. Once constructed, the models produced are dynamic representations of the software testing problem. They may be used to drive test design, answer what-if questions, and provide decision support to managers and testers. The models capture the knowledge of the software tester for further use. Experiences of the approach in case studies are briefly discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000453","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000453","","Bayesian methods;Graphical models;Software testing","software reliability;program testing;belief networks","Bayesian graphical models;software testing;formal mechanisms;logical structuring;test design;software reliability","","37","","27","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical investigation of the influence of a type of side effects on program comprehension","J. J. Dolado; M. Harman; M. C. Otero; L. Hu","Dept. of Comput. Languages & Syst., Univ. of the Basque Country, San Sebastian, Spain; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","7","665","670","This paper reports the results of a study on the impact of a type of side effect (SE) upon program comprehension. We applied a crossover design on different tests involving fragments of C code that include increment and decrement operators. Each test had an SE version and a side-effect-free counterpart. The variables measured in the treatments were the number of correct answers and the time spent in answering. The results show that the side-effect operators considered significantly reduce performance in comprehension-related tasks, providing empirical justification for the belief that side effects are harmful.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214329","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214329","","Testing;Programming profession;Time measurement;Impedance;Computer languages;Humans;Algorithm design and analysis;Software maintenance","program testing;software engineering","side-effect-free programs;program comprehension;LinSERT algorithm;crossover design;program tests;side-effect operators","","17","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Coverage Estimation in Model Checking with Bitstate Hashing","S. Ikeda; M. Jibiki; Y. Kuno","NEC Corporation, Kawasaki; National Institute of Information and Communication Technology, Koganei; University of Tsukuba, Bunkyo","IEEE Transactions on Software Engineering","","2013","39","4","477","486","Explicit-state model checking which is conducted by state space search has difficulty in exploring satisfactory state space because of its memory requirements. Though bitstate hashing achieves memory efficiency, it cannot guarantee complete verification. Thus, it is desirable to provide a reliability indicator such as a coverage estimate. However, the existing approaches for coverage estimation are not very accurate when a verification run covers a small portion of state space. This mainly stems from the lack of information that reflects characteristics of models. Therefore, we propose coverage estimation methods using a growth curve that approximates an increase in reached states by enlarging a bloom filter. Our approaches improve estimation accuracy by leveraging the statistics from multiple verification runs. Coverage is estimated by fitting the growth curve to these statistics. Experimental results confirm the validity of the proposed growth curve and the applicability of our approaches to practical models. In fact, for practical models, our approaches outperformed the conventional ones when the actual coverage is relatively low.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6226428","Coverage estimation;model checking;bitstate hashing","Estimation;Reliability;Probabilistic logic;Accuracy;Mathematical model;Space exploration;Equations","curve fitting;file organisation;formal verification;statistics","bitstate hashing;explicit-state model checking;state space search;memory requirement;memory efficiency;formal verification;growth curve fitting;Bloom filter;statistics;coverage estimation;reliability indicator","","1","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient relational calculation for software analysis","D. Beyer; A. Noack; C. Lewerentz","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","2","137","149","Calculating with graphs and relations has many applications in the analysis of software systems, for example, the detection of design patterns or patterns of problematic design and the computation of design metrics. These applications require an expressive query language, in particular, for the detection of graph patterns, and an efficient evaluation of the queries even for large graphs. In this paper, we introduce RML, a simple language for querying and manipulating relations based on predicate calculus, and CrocoPat, an interpreter for RML programs. RML is general because it enables the manipulation not only of graphs (i.e., binary relations), but of relations of arbitrary arity. CrocoPat executes RML programs efficiently because it internally represents relations as binary decision diagrams, a data structure that is well-known as a compact representation of large relations in computer-aided verification. We evaluate RML by giving example programs for several software analyses and CrocoPat by comparing its performance with calculators for binary relations, a Prolog system, and a relational database management system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401929","Index Terms- Logic programming;graph algorithms;data structures;reverse engineering;reengineering.","Application software;Data structures;Pattern analysis;Software systems;Database languages;Calculus;Boolean functions;Software performance;Performance analysis;Calculators","query languages;binary decision diagrams;data structures;formal verification;relational databases;PROLOG;graph theory;reverse engineering;object-oriented programming;software metrics;systems re-engineering","software analysis;design patterns;design metrics;query language;graph pattern;relation manipulation language;RML;binary decision diagram;data structure;computer-aided verification;Prolog system;relational database management system","","49","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Using Timed Automata for Modeling Distributed Systems with Clocks: Challenges and Solutions","G. Rodriguez-Navas; J. Proenza","Universitat de les Illes Balears, Palma de Mallorca; Universitat de les Illes Balears, Palma de Mallorca","IEEE Transactions on Software Engineering","","2013","39","6","857","868","The application of model checking for the formal verification of distributed embedded systems requires the adoption of techniques for realistically modeling the temporal behavior of such systems. This paper discusses how to model with timed automata the different types of relationships that may be found among the computer clocks of a distributed system, namely, ideal clocks, drifting clocks, and synchronized clocks. For each kind of relationship, a suitable modeling pattern is thoroughly described and formally verified.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6374193","Embedded systems;real-time systems;clock synchronization;model checking;timed automata;hybrid automata","Real-time systems;Automata;Formal verification;Distributed processing;Embedded systems","automata theory;distributed processing;embedded systems;formal verification","timed automata;modeling distributed systems;model checking;formal verification;distributed embedded systems;temporal behavior;distributed system computer clocks;ideal clocks;drifting clocks;synchronized clocks","","8","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A Survey on Software Fault Localization","W. E. Wong; R. Gao; Y. Li; R. Abreu; F. Wotawa","State Key Laboratory of Software Engineering, Wuhan University, Department of Computer Science, University of Texas at Dallas, Richardson, TX; Department of Computer Science, University of Texas at Dallas, Richardson, TX; Department of Computer Science, University of Texas at Dallas, Richardson, TX; Department of Informatics Engineering, University of Porto, Palo Alto Research Center (PARC), Palo Alto, CA; Institute for Software Technology, Graz University of Technology, Austria","IEEE Transactions on Software Engineering","","2016","42","8","707","740","Software fault localization, the act of identifying the locations of faults in a program, is widely recognized to be one of the most tedious, time consuming, and expensive - yet equally critical - activities in program debugging. Due to the increasing scale and complexity of software today, manually locating faults when failures occur is rapidly becoming infeasible, and consequently, there is a strong demand for techniques that can guide software developers to the locations of faults in a program with minimal human intervention. This demand in turn has fueled the proposal and development of a broad spectrum of fault localization techniques, each of which aims to streamline the fault localization process and make it more effective by attacking the problem in a unique way. In this article, we catalog and provide a comprehensive overview of such techniques and discuss key issues and concerns that are pertinent to software fault localization as a whole.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2521368","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7390282","Software fault localization;program debugging;software testing;execution trace;suspicious code;survey","Debugging;Software engineering;Computer bugs;Software debugging;Fault diagnosis;Complexity theory","program debugging;software reliability","software fault localization;program debugging;software developers;program fault locations;human intervention","","85","","427","","","","","","IEEE","IEEE Journals & Magazines"
"A packaging system for heterogeneous execution environments","J. R. Callahan; J. M. Purtilo","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","1991","17","6","626","635","A packaging system that allows diverse software components to be easily interconnected within heterogeneous programming environments is described. Interface software and stubs are generated for programmers automatically once the programmers express their application's geometry in a few simple rules and module interconnection language attributes. By generating custom interface code for each application, based on analysis and extraction of interfacing requirements, the system is able to produce executables whose run-time performance is comparable to manually integrated applications. The system is implemented within the Unix environment.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.87286","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=87286","","Packaging;Application software;Computer architecture;Software packages;Software tools;LAN interconnection;Costs;Environmental economics;Prototypes;Programming profession","automatic programming;configuration management;programming environments;user interfaces","packaging system;heterogeneous execution environments;diverse software components;heterogeneous programming environments;geometry;module interconnection language attributes;custom interface code;interfacing requirements;Unix environment","","32","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Engineering Privacy","S. Spiekermann; L. F. Cranor","Humboldt University, Berin; Carnegie Mellon University, Pittsburgh","IEEE Transactions on Software Engineering","","2009","35","1","67","82","In this paper we integrate insights from diverse islands of research on electronic privacy to offer a holistic view of privacy engineering and a systematic structure for the discipline's topics. First we discuss privacy requirements grounded in both historic and contemporary perspectives on privacy. We use a three-layer model of user privacy concerns to relate them to system operations (data transfer, storage and processing) and examine their effects on user behavior. In the second part of the paper we develop guidelines for building privacy-friendly systems. We distinguish two approaches: ""privacy-by-policy"" and ""privacy-by-architecture."" The privacy-by-policy approach focuses on the implementation of the notice and choice principles of fair information practices (FIPs), while the privacy-by-architecture approach minimizes the collection of identifiable personal data and emphasizes anonymization and client-side data storage and processing. We discuss both approaches with a view to their technical overlaps and boundaries as well as to economic feasibility. The paper aims to introduce engineers and computer scientists to the privacy research domain and provide concrete guidance on how to design privacy-friendly systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4657365","Privacy;Legal Aspects of Computing;Security and Protection;Requirements/Specifications;Privacy;Legal Aspects of Computing;Security and Protection;Requirements/Specifications","Design engineering;Protection;Systems engineering and theory;Data privacy;Companies;Social network services;Law;Radiofrequency identification;Guidelines;Memory","data privacy;security of data","electronic privacy;privacy engineering;user privacy;privacy-friendly systems;privacy-by-policy;privacy-by-architecture","","123","","116","","","","","","IEEE","IEEE Journals & Magazines"
"Approaches to Co-Evolution of Metamodels and Models: A Survey","R. Hebig; D. E. Khelladi; R. Bendraou","Computer Science and Engineering Göteborg, Chalmers University of Technology, Göteborg, Sweden; Sorbonne Universités, UPMC Univ Paris 06, UMR 7606, LIP6, Paris, France; Sorbonne Universités, UPMC Univ Paris 06, UMR 7606, LIP6, Paris, France","IEEE Transactions on Software Engineering","","2017","43","5","396","414","Modeling languages, just as all software artifacts, evolve. This poses the risk that legacy models of a company get lost, when they become incompatible with the new language version. To address this risk, a multitude of approaches for metamodel-model co-evolution were proposed in the last 10 years. However, the high number of solutions makes it difficult for practitioners to choose an appropriate approach. In this paper, we present a survey on 31 approaches to support metamodel-model co-evolution. We introduce a taxonomy of solution techniques and classify the existing approaches. To support researchers, we discuss the state of the art, in order to better identify open issues. Furthermore, we use the results to provide a decision support for practitioners, who aim to adopt solutions from research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2610424","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7569018","Survey;software engineering;metamodels;models;design notations and documentation","Unified modeling language;Companies;Taxonomy;Biological system modeling;Atmospheric modeling;Libraries;Productivity","software engineering","coevolution approaches;metamodel-model coevolution;solution technique taxonomy;decision support","","8","","84","","","","","","IEEE","IEEE Journals & Magazines"
"Memory access dependencies in shared-memory multiprocessors","M. Dubois; C. Scheurich","Dept. of Electr. Eng.-Syst., Univ. of Southern California, Los Angeles, CA, USA; Dept. of Electr. Eng.-Syst., Univ. of Southern California, Los Angeles, CA, USA","IEEE Transactions on Software Engineering","","1990","16","6","660","673","The presence of high-performance mechanisms in shared-memory multiprocessors such as private caches, the extensive pipelining of memory access, and combining networks may render a logical concurrency model complex to implement or inefficient. The problem of implementing a given logical concurrency model in such a multiprocessor is addressed. Two concurrency models are considered, and simple rules are introduced to verify that a multiprocessor architecture adheres to the models. The rules are applied to several examples of multiprocessor architectures.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.55094","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=55094","","Concurrent computing;Hardware;Multiprocessing systems;Program processors;Pipeline processing;Coherence;Parallel algorithms;Computer architecture;Microprocessors;Supercomputers","multiprocessing systems;multiprogramming;storage allocation","memory access dependencies;shared-memory multiprocessors;private caches;pipelining;logical concurrency model;rules;multiprocessor architectures","","44","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic analysis of consistency between requirements and designs","M. Chechik; J. Gannon","Dept. of Comput. Sci., Toronto Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","2001","27","7","651","672","Writing requirements in a formal notation permits automatic assessment of such properties as ambiguity, consistency, and completeness. However, verifying that the properties expressed in requirements are preserved in other software life cycle artifacts remains difficult. The existing techniques either require substantial manual effort and skill or suffer from exponential explosion of the number of states in the generated state spaces. ""Light-weight"" formal methods is an approach to achieve scalability in fully automatic verification by checking an abstraction of the system for only certain properties. We describe light-weight techniques for automatic analysis of consistency between software requirements (expressed in SCR) and detailed designs in low-degree-polynomial time, achieved at the expense of using imprecise data-flow analysis techniques. A specification language SCR describes the systems as state machines with event-driven transitions. We define detailed designs to be consistent with their SCR requirements if they contain exactly the same transitions. We have developed a language for specifying detailed designs, an analysis technique to create a model of a design through data-flow analysis of the language constructs, and a method to automatically generate and check properties derived from requirements to ensure a design's consistency with them. These ideas are implemented in a tool named CORD, which we used to uncover errors in designs of some existing systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.935856","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=935856","","Thyristors;Data analysis;Software quality;Costs;Page description languages;Computer Society;Manuals;Explosions;State-space methods;Scalability","formal specification;formal verification;data flow analysis;specification languages;software tools","requirements analysis;software design;formal notation;requirements verification;software life cycle artifacts;formal methods;scalability;automatic verification;polynomial time;data-flow analysis;specification language;SCR language;state machines;event-driven transitions;CORD tool","","17","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Some properties of timed token medium access protocols","A. Valenzano; P. Montuschi; L. Ciminiera","Politecnico di Torino, Italy; Politecnico di Torino, Italy; NA","IEEE Transactions on Software Engineering","","1990","16","8","858","869","Timed-token protocols are used to handle, on the same local area network, both real-time and non-real-time traffic. The authors analyze this type of protocol, giving worst-case values for the throughput of non-real-time traffic and the average token rotation time. Results are obtained for synchronous traffic generated according to a generic periodic pattern under heavy conditions for non-real-time traffic and express not only theoretical lower bounds but values deriving from the analysis of some real networks. A model which addresses the asynchronous overrun problem is presented. The influence of introducing multiple priority classes for non-real-time traffic on the total throughput of this type of message is shown. It is also shown that the differences between the values obtained under worst-case assumptions are close to those obtained under best-case assumptions; the method may therefore be used to provide important guidelines in properly tuning timed-token protocol parameters for each specific network installation.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.57628","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=57628","","Access protocols;Telecommunication traffic;Traffic control;Throughput;Application software;FDDI;Bandwidth;Testing;Pattern analysis;Guidelines","electronic messaging;protocols;token networks","real-time traffic;timed token medium access protocols;local area network;non-real-time traffic;worst-case values;throughput;average token rotation time;synchronous traffic;generic periodic pattern;heavy conditions;theoretical lower bounds;real networks;asynchronous overrun problem;multiple priority classes;worst-case assumptions;best-case assumptions;timed-token protocol parameters;network installation","","20","","19","","","","","","IEEE","IEEE Journals & Magazines"
"A formal model for module interconnection languages","M. D. Rice; S. B. Seidman","Dept. of Math., Wesleyan Univ., Middletown, CT, USA; NA","IEEE Transactions on Software Engineering","","1994","20","1","88","101","A model is proposed that formalizes the design of hierarchical module structures. The model is specified by a collection of Z schema type definitions that is invariant across all applications. A particular application is described by specifying the values of generic parameters and adding application-specific declarations and constraints to the schema definitions. As applications, the definitions in the model are used to describe the Conic configuration language and the STILE graphical design and development environment.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.263757","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=263757","","LAN interconnection;Libraries;Specification languages;Application software;Software systems;Assembly systems;Software reusability;Modular construction;Formal verification;Mathematics","formal specification;specification languages;systems analysis;programming environments","module interconnection languages;formal model;hierarchical module structure design;Z schema type definitions;generic parameters;application-specific declarations;constraints;Conic;configuration language;STILE;graphical design environment;development environment;specification language","","19","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Mathematical principles for a first course in software engineering","H. D. Mills; V. R. Basili; J. D. Gannon; R. G. Hamlet","Dept. of Comput. & Inf. Sci., Florida Univ., Gainesville, FL, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","5","550","559","An introductory computer science course is developed, much as calculus is a basic course for mathematics and the physical sciences, concerned primarily with theoretical foundations and methodology rather than apprenticeship through applications. In this work, the principles taught in the course are described and an example illustrating them is given.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.24704","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=24704","","Software engineering;Calculus;Milling machines;Computer science;Mathematics;Engineering management;Application software;Computer languages;Art;Knowledge engineering","computer science education;educational courses;mathematics computing;software engineering","mathematical principles;first course;software engineering;introductory computer science course","","5","","14","","","","","","IEEE","IEEE Journals & Magazines"
"A Scheme to Enforce Data Dependence on Large Multiprocessor Systems","Chuan-Qi Zhu; Pen-Chung Yew","Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign; NA","IEEE Transactions on Software Engineering","","1987","SE-13","6","726","739","Enforcement of data dependence in parallel algorithms requires certain synchronization primitives. For simple data dependence, synchronization primitives like Full/Empty bit in HEP machine [5] can be very effective. However, if data dependence cannot be determined at compile time, or if very complicated, more efficient synchronization schemes and algorithms are needed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233477","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702277","Data dependence;multiprocessor;program vectorization;synchronization","Multiprocessing systems;Testing;Program processors;Parallel algorithms;Hardware;Research and development","","Data dependence;multiprocessor;program vectorization;synchronization","","55","","21","","","","","","IEEE","IEEE Journals & Magazines"
"A Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems","C. Andersson; P. Runeson","Department of Computer Science, Lund University, Box 118, SE-221 00 Lund, Sweden; Department of Computer Science, Lund University, Box 118, SE-221 00 Lund, Sweden","IEEE Transactions on Software Engineering","","2007","33","5","273","286","To contribute to the body of empirical research on fault distributions during development of complex software systems, a replication of a study of Fenton and Ohlsson is conducted. The hypotheses from the original study are investigated using data taken from an environment that differs in terms of system size, project duration, and programming language. We have investigated four sets of hypotheses on data from three successive telecommunications projects: 1) the Pareto principle, that is, a small number of modules contain a majority of the faults (in the replication, the Pareto principle is confirmed), 2) fault persistence between test phases (a high fault incidence in function testing is shown to imply the same in system testing, as well as prerelease versus postrelease fault incidence), 3) the relation between number of faults and lines of code (the size relation from the original study could be neither confirmed nor disproved in the replication), and 4) fault density similarities across test phases and projects (in the replication study, fault densities are confirmed to be similar across projects). Through this replication study, we have contributed to what is known on fault distributions, which seem to be stable across environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1005","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160967","Empirical research;replication;software fault distributions.","Software systems;System testing;Software engineering;Computer languages;Quality management;Telecommunication switching;Conducting materials","Pareto optimisation;software fault tolerance","Pareto principle;complex software system;software fault distribution","","72","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Software Dependencies, Work Dependencies, and Their Impact on Failures","M. Cataldo; A. Mockus; J. A. Roberts; J. D. Herbsleb","Research and Technology Center, Robert Bosch LLC, Pittsburgh; Avaya Labs Research, Basking Ridge; Duquesne University, Pittsburgh; Carnegie Mellon University, Pittsburgh","IEEE Transactions on Software Engineering","","2009","35","6","864","878","Prior research has shown that customer-reported software faults are often the result of violated dependencies that are not recognized by developers implementing software. Many types of dependencies and corresponding measures have been proposed to help address this problem. The objective of this research is to compare the relative performance of several of these dependency measures as they relate to customer-reported defects. Our analysis is based on data collected from two projects from two independent companies. Combined, our data set encompasses eight years of development activity involving 154 developers. The principal contribution of this study is the examination of the relative impact that syntactic, logical, and work dependencies have on the failure proneness of a software system. While all dependencies increase the fault proneness, the logical dependencies explained most of the variance in fault proneness, while workflow dependencies had more impact than syntactic dependencies. These results suggest that practices such as rearchitecting, guided by the network structure of logical dependencies, hold promise for reducing defects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5166450","Distribution/maintenance/enhancement;metrics/measurement;organizational management and coordination;quality analysis and evaluation.","Software systems;Predictive models;Quality management;Software engineering;Humans;Software development management;Programming","software fault tolerance;software maintenance;software metrics;software quality","software dependencies;work dependencies;customer-reported software faults;quality analysis","","108","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Numerical analysis of superposed GSPNs","P. Kemper","Fachbereich Inf., Dortmund Univ., Germany","IEEE Transactions on Software Engineering","","1996","22","9","615","628","The numerical analysis of various modeling formalisms profits from a structured representation for the generator matrix Q of the underlying continuous-time Markov chain, where Q is described by a sum of tensor (Kronecker) products of much smaller matrices. In this paper, we describe such a representation for the class of superposed generalized stochastic Petri nets (GSPNs), which is less restrictive than in previous work. Furthermore a new iterative analysis algorithm is proposed. It pays special attention to a memory-efficient representation of iteration vectors as well as to a memory-efficient structured representation of Q in consequence the new algorithm is able to solve models which have state spaces with several million states, where other exact numerical methods become impracticable on a common workstation.","0098-5589;1939-3520;2326-3881","","10.1109/32.541433","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=541433","","Numerical analysis;Stochastic processes;Petri nets;Tensile stress;Algorithm design and analysis;State-space methods;Sparse matrices;Iterative algorithms;Steady-state;Algebra","Petri nets;iterative methods;matrix algebra;vectors;tensors;stochastic systems;state-space methods;reachability analysis;Markov processes","numerical analysis;modeling formalisms;memory-efficient structured representation;generator matrix;continuous-time Markov chain;tensor products;Kronecker products;superposed generalized stochastic Petri nets;iterative analysis algorithm;state spaces;iteration vectors;steady-state analysis;decomposition;reachability analysis","","59","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Estimating the Speedup in Parallel Parsing","J. Cohen; S. Kolodner","Department of Computer Science, Brandeis University; NA","IEEE Transactions on Software Engineering","","1985","SE-11","1","114","124","A model for the operation of bottom-up parallel parsing using asynchronous processors is proposed. The model is based on an extension of shift-reduce parsers which are able to merge the information they keep on their stacks. The main objective of the paper is to provide estimates of the speedup attainable when using the proposed model. Three programs were written to measure the speedup. The first is a classical simulator which keeps track of the times spent performing the shift, reduce, and merge operations for each processor. The second is a program which generates ""typical"" strings in a language and simultaneously keeps track of the number of operations needed to parse the generated strings. The third is a program capable of deducing the num-ber of parsing operations by counting the number of selected terminals appearing in an input string. The results, applicable to the paralel parsing of programs written in a Pascal-like language, show how the speedup varies with the number of processors for different ratios of the times to shift, reduce, and merge. Although the speedup falls considerably below that predicted by theory, substantial gains are still attainable by using a fairly large number of parallel processors. With the decreasing costs of processors, parallel parsing and parallel compilation will become increasingly important and should allow considerable gains in speedup.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1985.231848","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1701903","Compilers;parallelism;parsing","Parallel processing;Velocity measurement;Computer science;Costs;Computational modeling;Computer simulation;Concurrent computing;Production;Writing","","Compilers;parallelism;parsing","","4","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting Vulnerable Software Components via Text Mining","R. Scandariato; J. Walden; A. Hovsepyan; W. Joosen","IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium; Department of Computer Science, Northern Kentucky University, Highland Heights, KY; IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium; IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium","IEEE Transactions on Software Engineering","","2014","40","10","993","1006","This paper presents an approach based on machine learning to predict which components of a software application contain security vulnerabilities. The approach is based on text mining the source code of the components. Namely, each component is characterized as a series of terms contained in its source code, with the associated frequencies. These features are used to forecast whether each component is likely to contain vulnerabilities. In an exploratory validation with 20 Android applications, we discovered that a dependable prediction model can be built. Such model could be useful to prioritize the validation activities, e.g., to identify the components needing special scrutiny.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2340398","EU FP7; Research Fund KU Leuven; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6860243","Vulnerabilities;prediction model;machine learning","Software;Predictive models;Measurement;Security;Androids;Humanoid robots;Text mining","data mining;learning (artificial intelligence);program verification;security of data","vulnerable software component;text mining;machine learning;security vulnerability;source code;Android application","","45","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Interactive, Evolutionary Search in Upstream Object-Oriented Class Design","C. L. Simons; I. C. Parmee; R. Gwynllyw","University of the West of England, Frenchay; University of the West of England, Frenchay; University of the West of England, Frenchay","IEEE Transactions on Software Engineering","","2010","36","6","798","816","Although much evidence exists to suggest that early life cycle software engineering design is a difficult task for software engineers to perform, current computational tool support for software engineers is limited. To address this limitation, interactive search-based approaches using evolutionary computation and software agents are investigated in experimental upstream design episodes for two example design domains. Results show that interactive evolutionary search, supported by software agents, appears highly promising. As an open system, search is steered jointly by designer preferences and software agents. Directly traceable to the design problem domain, a mass of useful and interesting class designs is arrived at which may be visualized by the designer with quantitative measures of structural integrity, such as design coupling and class cohesion. The class designs are found to be of equivalent or better coupling and cohesion when compared to a manual class design for the example design domains, and by exploiting concurrent execution, the runtime performance of the software agents is highly favorable.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5432223","Software design;evolutionary computation;interactive search.","Software agents;Software design;Software performance;Software tools;Design engineering;Software engineering;Evolutionary computation;Open systems;Visualization;Runtime","evolutionary computation;interactive systems;object-oriented methods;open systems;search problems;software agents;software engineering","upstream object oriented class design;life cycle software engineering design;software agent;interactive evolutionary search;open system;design problem domain;structural integrity;concurrent execution;runtime performance","","43","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Customizing the Representation Capabilities of Process Models: Understanding the Effects of Perceived Modeling Impediments","B. M. Samuel; L. A. Watkins III; A. Ehle; V. Khatri","Ivey Business School, Western University 1255 Western Road, London, ON, Canada; Kelley School of Business, Indiana University 1309 East 10th Street BU 570, Bloomington, IN; Kelley School of Business, Indiana University 1309 East 10th Street BU 570, Bloomington, IN; Kelley School of Business, Indiana University 1309 East 10th Street BU 570, Bloomington, IN","IEEE Transactions on Software Engineering","","2015","41","1","19","39","Process modeling is useful during the analysis and design of systems. Prior research acknowledges both impediments to process modeling that limits its use as well as customizations that can be employed to help improve the creation of process models. However, no research to date has provided a rich examination of the linkages between perceived process modeling impediments and process modeling customizations. In order to help address this gap, we first conceptualized perceived impediments to using process models as a “lack of fit” between process modeling and another factor: (1) the role the process model is intended for; and (2) the task at hand. We conducted a case study at two large health insurance carriers to understand why the lack of fit existed and then show different types of process modeling customizations used to address the lack of fit and found a variety of “physical” and “process” customizations employed to overcome the lack of fits. We generalize our findings into propositions for future research that examine the dynamic interaction between process models and their need to be understood by individuals during systems analysis and design.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2354043","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6898868","Software process models;requirements/specifications management;requirements/specifications process;requirements/specification stools;UML;use cases;activity diagrams","Unified modeling language;Analytical models;Organizations;Software;Interviews;Context","formal specification;software process improvement","process model representation capability customization;system design;system analysis;process model creation improvement;perceived process modeling impediments;process modeling customizations;lack-of-fit;health insurance carriers;physical customization;process customization;dynamic process model interaction","","5","","97","","","","","","IEEE","IEEE Journals & Magazines"
"The Method of Layers","J. A. Rolia; K. C. Sevcik","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; NA","IEEE Transactions on Software Engineering","","1995","21","8","689","700","Distributed applications are being developed that contain one or more layers of software servers. Software processes within such systems suffer contention delays both for shared hardware and at the software servers. The responsiveness of these systems is affected by the software design, the threading level and number of instances of software processes, and the allocation of processes to processors. The Method of Layers (MOL) is proposed to provide performance estimates for such systems. The MOL uses the mean value analysis (MVA) linearizer algorithm as a subprogram to assist in predicting model performance measures.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.403785","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=403785","","Predictive models;Computer architecture;Hardware;Application software;Delay;Software performance;Network servers;System performance;Throughput;Electronic switching systems","distributed processing;resource allocation;software performance evaluation;client-server systems;systems analysis","distributed applications;software servers;software processes;contention delays;shared hardware;software design;threading level;processor allocation;Method of Layers;performance estimates;value analysis linearizer algorithm;model performance measures","","186","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Eliminating exception handling errors with dependability cases: a comparative, empirical study","R. A. Maxion; R. T. Olszewski","Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","9","888","906","Programs fail mainly for two reasons: logic errors in the code and exception failures. Exception failures can account for up to two-thirds of system crashes, hence, are worthy of serious attention. Traditional approaches to reducing exception failures, such as code reviews, walkthroughs, and formal testing, while very useful, are limited in their ability to address a core problem: the programmer's inadequate coverage of exceptional conditions. The problem of coverage might be rooted in cognitive factors that impede the mental generation (or recollection) of exception cases that would pertain in a particular situation, resulting in insufficient software robustness. This paper describes controlled experiments for testing the hypothesis that robustness for exception failures can be improved through the use of various coverage-enhancing techniques: N-version programming, group collaboration, and dependability cases. N-version programming and collaboration are well known. Dependability cases, derived from safety cases, comprise a new methodology based on structured taxonomies and memory aids for helping software designers think about and improve exception handling coverage. All three methods showed improvements over control conditions in increasing robustness to exception failures but dependability cases proved most efficacious in terms of balancing cost and effectiveness.","0098-5589;1939-3520;2326-3881","","10.1109/32.877848","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877848","","Robust control;Collaboration;Software safety;Logic;Computer crashes;Vehicle crash testing;Impedance;Robustness;Taxonomy;Software design","exception handling;program debugging;software reliability","exception handling errors;dependability cases;logic errors;exception failures;system crashes;cognitive factors;software robustness;experiments;coverage-enhancing techniques;N-version programming;group collaboration;software designers;cost effectiveness;software reliability","","16","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Trace specifications: methodology and models","D. Hoffman; R. Snodgrass","Dept. of Comput. Sci., Victoria Univ., BC, Canada; NA","IEEE Transactions on Software Engineering","","1988","14","9","1243","1252","The authors summarize the trace specification language and present the trace specification methodology: a set of heuristics designed to make the reading and writing of complex specifications manageable. Also described is a technique for constructing formal, executable models from specifications written using the methodology. These models are useful as proof of specification consistency and as executable prototypes. Fully worked examples of the methodology and the model building techniques are included.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6168","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6168","","Formal specifications;Computer errors;Writing;Software prototyping;Costs;Process design;Computer science;Specification languages;Prototypes;Buildings","formal specification;specification languages","formal specification;trace specification language;trace specification methodology;specification consistency;executable prototypes","","27","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Model Transformation Modularization as a Many-Objective Optimization Problem","M. Fleck; J. Troya; M. Kessentini; M. Wimmer; B. Alkhazi","TU Wien, Wien, Austria; Universidad de Sevilla, Sevilla, Spain; University of Michigan, Ann Arbor, MI; TU Wien, Wien, Austria; University of Michigan, Ann Arbor, MI","IEEE Transactions on Software Engineering","","2017","43","11","1009","1032","Model transformation programs are iteratively refined, restructured, and evolved due to many reasons such as fixing bugs and adapting existing transformation rules to new metamodels version. Thus, modular design is a desirable property for model transformations as it can significantly improve their evolution, comprehensibility, maintainability, reusability, and thus, their overall quality. Although language support for modularization of model transformations is emerging, model transformations are created as monolithic artifacts containing a huge number of rules. To the best of our knowledge, the problem of automatically modularizing model transformation programs was not addressed before in the current literature. These programs written in transformation languages, such as ATL, are implemented as one main module including a huge number of rules. To tackle this problem and improve the quality and maintainability of model transformation programs, we propose an automated search-based approach to modularize model transformations based on higher-order transformations. Their application and execution is guided by our search framework which combines an in-place transformation engine and a search-based algorithm framework. We demonstrate the feasibility of our approach by using ATL as concrete transformation language and NSGA-III as search algorithm to find a trade-off between different well-known conflicting design metrics for the fitness functions to evaluate the generated modularized solutions. To validate our approach, we apply it to a comprehensive dataset of model transformations. As the study shows, ATL transformations can be modularized automatically, efficiently, and effectively by our approach. We found that, on average, the majority of recommended modules, for all the ATL programs, by NSGA-III are considered correct with more than 84 percent of precision and 86 percent of recall when compared to manual solutions provided by active developers. The statistical analysis of our experiments over several runs shows that NSGA-III performed significantly better than multi-objective algorithms and random search. We were not able to compare with existing model transformations modularization approaches since our study is the first to address this problem. The software developers considered in our experiments confirm the relevance of the recommended modularization solutions for several maintenance activities based on different scenarios and interviews.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2654255","Christian Doppler Forschungsgesellschaft; BMWFW; European Commission (FEDER); Spanish Government; CICYT project BELI; SEBASE; Andalusian Government project COPAS; Ford Motor Company; Ford Alliance Program; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7820199","Model transformation;modularization;ATL;NSGA-III;MDE;SBSE","Unified modeling language;Object oriented modeling;Adaptation models;Measurement;Algorithm design and analysis;Software engineering;Computer bugs","genetic algorithms;program debugging;search problems;software maintenance;software quality","model transformations modularization;model transformation modularization;model transformation programs;transformation languages;higher-order transformations;in-place transformation engine;concrete transformation language;ATL transformations;bug fixing;transformation rules;many-objective optimization problem;metamodels version;monolithic artifacts;automated search-based approach;NSGA-III;statistical analysis;maintenance activities","","1","","96","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Cycle Structure of the DES for Keys Having Palindromic (or Antipalindromic) Sequences of Round Keys","J. H. Moore; G. J. Simmons","Department of Applied Mathematics, Sandia National Laboratories; NA","IEEE Transactions on Software Engineering","","1987","SE-13","2","262","273","Certain DES keys have been called weak or semiweak based upon the number of distinct round keys which they produce. For the weak keys, all 16 round keys are identical and encryption is the same as decryption. For the semiweak keys, there are only two distinct round keys but no specific weakness of the DES with these keys has been demonstrated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.233150","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702205","Cryptography;cycle testing;data encryption standard (DES);fixed points and antifixed points;palindromic and antipalindromic sequences of round keys;weak and semiweak keys","Cryptography;Displays;Testing;Engines;Mathematics;Joining processes","","Cryptography;cycle testing;data encryption standard (DES);fixed points and antifixed points;palindromic and antipalindromic sequences of round keys;weak and semiweak keys","","13","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Translation and execution of distributed Ada programs: Is it still Ada?","R. A. Volz; T. N. Mudge; G. D. Buzzard; P. Krishnan","Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","1989","15","3","281","292","Some of the fundamental issues and tradeoffs involved in the translation and execution of programs written in the Ada language and intended for distributed execution are examined. The memory access architecture, binding time and degree of system homogeneity are the three basic characteristics in terms of which target systems can be described. Library subprograms and library packages are identified as natural distributable units of the language. The program-to-process/memory mapping and the unit of the language to be distributed are the key issues in the distribution of Ada. The implications of various alternatives for these are analyzed.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21756","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21756","","Packaging;Real time systems;Software engineering;Libraries;Computer architecture;Memory architecture;Parallel programming;Computer errors;NASA;Computer science","Ada;distributed processing;program interpreters;programming","distributed Ada programs;distributed execution;memory access architecture;binding time;system homogeneity;library packages","","16","","12","","","","","","IEEE","IEEE Journals & Magazines"
"CP-Miner: finding copy-paste and related bugs in large-scale software code","Z. Li; S. Lu; S. Myagmar; Y. Zhou","Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA","IEEE Transactions on Software Engineering","","2006","32","3","176","192","Recent studies have shown that large software suites contain significant amounts of replicated code. It is assumed that some of this replication is due to copy-and-paste activity and that a significant proportion of bugs in operating systems are due to copy-paste errors. Existing static code analyzers are either not scalable to large software suites or do not perform robustly where replicated code is modified with insertions and deletions. Furthermore, the existing tools do not detect copy-paste related bugs. In this paper, we propose a tool, CP-Miner, that uses data mining techniques to efficiently identify copy-pasted code in large software suites and detects copy-paste bugs. Specifically, it takes less than 20 minutes for CP-Miner to identify 190,000 copy-pasted segments in Linux and 150,000 in FreeBSD. Moreover, CP-Miner has detected many new bugs in popular operating systems, 49 in Linux and 31 in FreeBSD, most of which have since been confirmed by the corresponding developers and have been rectified in the following releases. In addition, we have found some interesting characteristics of copy-paste in operating system code. Specifically, we analyze the distribution of copy-pasted code by size (number lines of code), granularity (basic blocks and functions), and modification within copy-pasted code. We also analyze copy-paste across different modules and various software versions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610609","Software analysis;code reuse;code duplication;debugging aids;data mining.","Computer bugs;Large-scale systems;Linux;Operating systems;Data mining;Cloning;Programming profession;Kernel;Performance analysis;Software performance","program debugging;program diagnostics;operating systems (computers);data mining;software tools;software reusability;software maintenance","CP-Miner tool;debugging aids;large-scale software code;replicated code;copy-paste bugs;operating system code bugs;static code analyzer;data mining technique;Linux;FreeBSD;code reuse;code duplication","","187","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Structural testing of concurrent programs","R. N. Taylor; D. L. Levine; C. D. Kelly","California Univ., Irvine, CA, USA; California Univ., Irvine, CA, USA; California Univ., Irvine, CA, USA","IEEE Transactions on Software Engineering","","1992","18","3","206","215","Although structural testing techniques are among the weakest available with regard to developing confidence in sequential programs, they are not without merit. The authors extend the notion of structural testing criteria to concurrent programs and propose a hierarchy of supporting structural testing techniques. Coverage criteria described include concurrency state coverage, state transition coverage and synchronization coverage. Requisite support tools include a static concurrency analyzer and either a program transformation system or a powerful run-time monitor. Also helpful is a controllable run-time scheduler. The techniques proposed are suitable for Ada or CSP-like languages. Best results are obtained for programs having only static naming of tasking objects.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.126769","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=126769","","Sequential analysis;Software testing;Concurrent computing;Runtime;Monitoring;Buildings;Aerospace materials;Aircraft propulsion;Computer science;Laboratories","concurrency control;parallel programming;program testing;scheduling;software metrics","structural testing techniques;sequential programs;structural testing criteria;concurrent programs;concurrency state coverage;state transition coverage;synchronization coverage;support tools;static concurrency analyzer;program transformation system;powerful run-time monitor;controllable run-time scheduler;Ada;CSP-like languages;static naming;tasking objects","","110","","26","","","","","","IEEE","IEEE Journals & Magazines"
"An acyclic expansion algorithm for fast protocol validation","Y. Kakuda; Y. Wakahara; M. Norigoe","Kokusai Denshin Denwa Co. Ltd., Saitama, Japan; Kokusai Denshin Denwa Co. Ltd., Saitama, Japan; NA","IEEE Transactions on Software Engineering","","1988","14","8","1059","1070","For the development of communications software composed of many modules, protocol validation is considered essential to detect errors in the interactions among the modules. Protocol validation techniques previously proposed have required validation time that is too long for many actual protocols. The authors propose a novel fast protocol validation technique to overcome this drawback. The proposed technique is to construct the minimum acyclic form of state transitions in individual processes of the protocol, and to detect protocol errors such as system deadlocks and channel overflows fast. The authors also present a protocol validation system based on the technique to confirm its feasibility and show validation results for some actual protocols. As a result, the protocol validation system is expected to improve productivity in the development and maintenance of communications software.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.7616","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7616","","Protocols;Software maintenance;System recovery;Productivity;Communication system software;Automata;Programming;Software quality;Reachability analysis;Computer errors","computer communications software;finite automata;program verification;protocols;system recovery","finite state machines;acyclic expansion algorithm;protocol validation;communications software;state transitions;protocol errors;system deadlocks;channel overflows","","21","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Clone Management for Evolving Software","H. A. Nguyen; T. T. Nguyen; N. H. Pham; J. Al-Kofahi; T. N. Nguyen","Iowa State University, Ames; Iowa State University, Ames; Iowa State University, Ames; Iowa State University, Ames; Iowa State University, Ames","IEEE Transactions on Software Engineering","","2012","38","5","1008","1026","Recent research results suggest a need for code clone management. In this paper, we introduce JSync, a novel clone management tool. JSync provides two main functions to support developers in being aware of the clone relation among code fragments as software systems evolve and in making consistent changes as they create or modify cloned code. JSync represents source code and clones as (sub)trees in Abstract Syntax Trees, measures code similarity based on structural characteristic vectors, and describes code changes as tree editing scripts. The key techniques of JSync include the algorithms to compute tree editing scripts, to detect and update code clones and their groups, to analyze the changes of cloned code to validate their consistency, and to recommend relevant clone synchronization and merging. Our empirical study on several real-world systems shows that JSync is efficient and accurate in clone detection and updating, and provides the correct detection of the defects resulting from inconsistent changes to clones and the correct recommendations for change propagation across cloned code.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6007141","Clone management;clone consistency analysis;clone synchronization;clone merging","Cloning;Feature extraction;Software systems;Synchronization;Vegetation;Merging;Databases","Java;program compilers","evolving software;code clone management;JSync;clone management tool;code fragments;software systems;source code;abstract syntax trees;structural characteristic vectors;tree editing scripts;change propagation","","31","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Functional development of database applications","L. Orman","Johnson Graduate Sch. of Manage., Cornell Univ., Ithaca, NY, USA","IEEE Transactions on Software Engineering","","1988","14","9","1280","1292","A highly modular and uniformly functional development methodology for database applications is introduced. An event-oriented view of the database is adopted recording the observed events directly and treating the state of the environment as derived data. The relationship between the observed events and the derived state of the system is expressed using a purely functional language. The application systems in this environment are divided into their smallest possible components consisting of only functions and simple functional expressions. The multimode of small but highly independent components generated in this fashion are placed in the database along with the data, to utilize the database management system in maintaining the application system. The semantics of individual applications is captured within the data model serving those applications.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.6172","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6172","","Data models;Database systems;Transaction databases;Information retrieval;Decision support systems;Functional programming;Real time systems;Concurrent computing;Specification languages;Data structures","database management systems;functional programming","functional development methodology;event-oriented view;database;observed events;functional language;database management system;semantics","","11","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Distributed information systems: an advanced methodology","A. Aue; M. Breu","Eur. Methodology & Syst. Center, Siemens Nixdorf Informationsyst. AG, Munchen, Germany; Eur. Methodology & Syst. Center, Siemens Nixdorf Informationsyst. AG, Munchen, Germany","IEEE Transactions on Software Engineering","","1994","20","8","594","605","Information systems ranging over wide areas show properties that must be carefully analyzed and designed in order to meet the needs of the customers. Thus the development of such information systems is to be guided by software engineering methods that address problems like distribution of data and processes, communication aspects and fault tolerance. This paper shows the basic modeling concepts and the development process employed by the BOS Engineering Method to meet these requirements. The BOS Engineering Method applies the concept of business transactions to specify behavior in the early analysis phase. Appropriate abstraction levels are defined to reduce the complexity of specifying distribution issues. The development of complex distributed information systems needs a rigorous life cycle model. The BOS Engineering Method relaxes the waterfall life cycle model to allow controlled look ahead and feedback up and down the abstraction levels.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310669","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310669","","Distributed information systems;Information systems;Systems engineering and theory;Software engineering;Information analysis;Companies;Design engineering;Fault tolerant systems;Feedback;Hardware","software engineering;information systems;distributed processing","distributed information systems;software engineering;distribution of data;communication aspects;fault tolerance;BOS Engineering Method;business transactions;waterfall life cycle model;feedback;development process;requirements engineering;requirements analysis","","7","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Emulation of Software Faults: A Field Data Study and a Practical Approach","J. A. Duraes; H. S. Madeira","Instituto Superior de Engeharia de Coimbra, Rua Pedro Nunes—Quinta da Nora, 3030-199 Coimbra, Portugal; Departamento de Endenharia Informa´tica, University of Coimbra, Polo II—Pinhal de Marrocos, 3030-290 Coimbra, Portugal","IEEE Transactions on Software Engineering","","2006","32","11","849","867","The injection of faults has been widely used to evaluate fault tolerance mechanisms and to assess the impact of faults in computer systems. However, the injection of software faults is not as well understood as other classes of faults (e.g., hardware faults). In this paper, we analyze how software faults can be injected (emulated) in a source-code independent manner. We specifically address important emulation requirements such as fault representativeness and emulation accuracy. We start with the analysis of an extensive collection of real software faults. We observed that a large percentage of faults falls into well-defined classes and can be characterized in a very precise way, allowing accurate emulation of software faults through a small set of emulation operators. A new software fault injection technique (G-SWFIT) based on emulation operators derived from the field study is proposed. This technique consists of finding key programming structures at the machine code-level where high-level software faults can be emulated. The fault-emulation accuracy of this technique is shown. This work also includes a study on the key aspects that may impact the technique accuracy. The portability of the technique is also discussed and it is shown that a high degree of portability can be achieved","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.113","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015509","Fault injection;software faults;software reliability.","Emulation;Software reliability;Software measurement;Fault tolerant systems;Hardware;Programming;Computer industry;Software systems;Computer bugs;Software testing","program testing;software fault tolerance;software performance evaluation;software portability","software fault injection technique;field data study;fault tolerance mechanism;computer system;emulation accuracy;programming structure;machine code-level;G-SWFIT;fault representation;programming structures","","119","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Design tradeoffs for process scheduling in shared memory multiprocessor systems","L. M. Ni; C. -. E. Wu","Dept. of Comput. Sci., Michigan State Univ., East Lansing, MI, USA; Dept. of Comput. Sci., Michigan State Univ., East Lansing, MI, USA","IEEE Transactions on Software Engineering","","1989","15","3","327","334","A potential system software bottleneck is demonstrated in designing an efficient process scheduling method for multiprocessor systems with shared-memory communication mechanism. The process scheduling overhead is considered. The main contribution of this work is to find the design tradeoffs between monitor bottleneck due to scheduling overhead and low process utilization due to load imbalancing. Choosing an optimum number of scheduling monitors is the key to resolve the bottlenecks. Because of the excessive number of memory requests generated by the dynamic monitor selection method, the use of the fixed monitor selection method is recommended. An analytic estimation provides a lower bound in determining the optimum number of monitors. Hill-climbing simulation is then used to find the optimum number of monitors.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.21760","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=21760","","Process design;Multiprocessing systems;Monitoring;Processor scheduling;Message passing;Hardware;Computer science;Bandwidth;Load management;Concurrent computing","multiprocessing programs;multiprocessing systems;performance evaluation;scheduling;supervisory programs","hill climbing simulation;performance evaluation;process scheduling;shared memory multiprocessor systems;software bottleneck;monitor bottleneck;low process utilization;load imbalancing;memory requests;fixed monitor selection","","18","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Behavioral Testing of Refactoring Engines","G. Soares; R. Gheyi; T. Massoni","Federal University of Campina Grande, Campina Grande; Federal University of Campina Grande, Campina Grande; Federal University of Campina Grande, Campina Grande","IEEE Transactions on Software Engineering","","2013","39","2","147","162","Refactoring is a transformation that preserves the external behavior of a program and improves its internal quality. Usually, compilation errors and behavioral changes are avoided by preconditions determined for each refactoring transformation. However, to formally define these preconditions and transfer them to program checks is a rather complex task. In practice, refactoring engine developers commonly implement refactorings in an ad hoc manner since no guidelines are available for evaluating the correctness of refactoring implementations. As a result, even mainstream refactoring engines contain critical bugs. We present a technique to test Java refactoring engines. It automates test input generation by using a Java program generator that exhaustively generates programs for a given scope of Java declarations. The refactoring under test is applied to each generated program. The technique uses SafeRefactor, a tool for detecting behavioral changes, as an oracle to evaluate the correctness of these transformations. Finally, the technique classifies the failing transformations by the kind of behavioral change or compilation error introduced by them. We have evaluated this technique by testing 29 refactorings in Eclipse JDT, NetBeans, and the JastAdd Refactoring Tools. We analyzed 153,444 transformations, and identified 57 bugs related to compilation errors, and 63 bugs related to behavioral changes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6175911","Refactoring;automated testing;program generation","Java;Metals;Engines;Computer bugs;Testing;Automatic programming;Unified modeling language","automatic programming;Java;program testing","automated behavioral testing;compilation errors;refactoring transformation;refactoring engine developers;Java refactoring engines;Java program generator;SafeRefactor;JastAdd refactoring tools","","42","","49","","","","","","IEEE","IEEE Journals & Magazines"
"A control-flow normalization algorithm and its complexity","Z. Ammarguellat","Center for Supercomput. Res. & Dev., Illinois Univ., Urbana-Champaign, IL, USA","IEEE Transactions on Software Engineering","","1992","18","3","237","251","A single method for normalizing the control-flow of programs to facilitate program transformations, program analysis, and automatic parallelization is presented. While previous methods result in programs whose control flowgraphs are reducible, programs normalized by this technique satisfy a stronger condition than reducibility and are therefore simpler in their syntax and structure than with previous methods. In particular, all control-flow cycles are normalized into single-entry, single-exit while loops and all GOTOs are eliminated. Furthermore, the method avoids problems of code replication that are characteristic of node-splitting techniques. This restructuring obviates the control dependence graph, since afterwards control dependence relations are manifest in the syntax tree of the program. Transformations that effect this normalization are presented, and the complexity of the method is studied.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.126773","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=126773","","Automatic control;Data analysis;Tree graphs;Performance analysis;Algorithm design and analysis;Pathology;Program processors","computational complexity;graph theory;parallel algorithms;structured programming","control-flow normalization algorithm;complexity;automatic parallelization;control flowgraphs;control-flow cycles;GOTOs;node-splitting techniques;control dependence relations;syntax tree","","33","","48","","","","","","IEEE","IEEE Journals & Magazines"
"On The Detection of Test Smells: A Metrics-Based Approach for General Fixture and Eager Test","B. Van Rompaey; B. Du Bois; S. Demeyer; M. Rieger","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","800","817","As a fine-grained defect detection technique, unit testing introduces a strong dependency on the structure of the code. Accordingly, test coevolution forms an additional burden on the software developer which can be tempered by writing tests in a manner that makes them easier to change. Fortunately, we are able to concretely express what a good test is by exploiting the specific principles underlying unit testing. Analogous to the concept of code smells, violations of these principles are termed test smells. In this paper, we clarify the structural deficiencies encapsulated in test smells by formalizing core test concepts and their characteristics. To support the detection of two such test smells, General Fixture and Eager Test, we propose a set of metrics defined in terms of unit test concepts. We compare their detection effectiveness using manual inspection and through a comparison with human reviewing. Although the latter is the traditional means for test quality assurance, our results indicate it is not a reliable means for test smell detection. This work thus stresses the need for a more reliable detection mechanism and provides an initial contribution through the validation of test smell metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70745","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359471","Test design;Quality assurance;Maintainability;Test design;Quality assurance;Maintainability","Fixtures;Software testing;System testing;Automatic testing;Quality assurance;Costs;Guidelines;Computer Society;Writing;Inspection","program testing;software metrics","test smell detection;metrics-based approach;general fixture;eager test;fine-grained defect detection technique;code structure;software developer;manual inspection;human review;test quality assurance;reliable detection mechanism","","54","","42","","","","","","IEEE","IEEE Journals & Magazines"
"A system for generating language-oriented editors","T. Tenma; H. Tsubotani; M. Tanaka; T. Ichikawa","Dept. of Electr. Eng., Hiroshima Univ., Japan; Dept. of Electr. Eng., Hiroshima Univ., Japan; Dept. of Electr. Eng., Hiroshima Univ., Japan; Dept. of Electr. Eng., Hiroshima Univ., Japan","IEEE Transactions on Software Engineering","","1988","14","8","1098","1109","The authors seek to establish a simple and flexible framework for internal representation of language-dependent information, and the behavior of language-oriented tools for user's operations. They present a system for generating language-oriented editors based on object-oriented concepts. Features of the target language are represented as classes and their relations. A program is represented as an abstract syntax tree. Each node in the tree belongs to a node class. For generating more advanced editors, probes, internal-classes, and gates are incorporated into the system. The system generates a flexible and easily extendable language-oriented editor from a target language description in a highly modularized fashion by using the description language which the system provides.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.7620","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7620","","Probes;Productivity;Debugging;Programming environments;Costs","application generators;data structures;software tools;text editing","application generators;software tools;data structures;language dependent data representation;language-oriented editors;language-oriented tools;object-oriented;abstract syntax tree;target language description;description language","","2","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Achieving efficiency and portability in systems software: a case study on POSIX-compliant multithreaded programs","Y. Shinjo; C. Pu","Dept. of Comput. Sci., Tsukuba Univ., Ibaraki, Japan; NA","IEEE Transactions on Software Engineering","","2005","31","9","785","800","Portable (standards-compliant) systems software is usually associated with unavoidable overhead from the standards-prescribed interface. For example, consider the POSIX Threads standard facility for using thread-specific data (TSD) to implement multithreaded code. The first TSD reference must be preceded by pthread/spl I.bar/getspecific( ), typically implemented as a function or macro with 40-50 instructions. This paper proposes a method that uses the runtime specialization'facility of the Tempo program specializer to convert such unavoidable source code into simple memory references of one or two instructions for execution. Consequently, the source code remains standard compliant and the executed code's performance is similar to direct global variable access. Measurements show significant performance gains over a range of code sizes. A random number generator (10 lines of C) shows a speedup of 4.8 times on a SPARC and 2.2 times on a Pentium. A time converter (2,800 lines) was sped up by 14 and 22 percent, respectively, and a parallel genetic algorithm system (14,000 lines) was sped up by 13 and 5 percent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.98","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514446","Index Terms- Performance;portability;threads;software libraries;concurrent programming;runtime specialization;thread-specific data.","System software;Computer aided software engineering;Software libraries;Code standards;Runtime;Packaging;Assembly;Programming profession;Gain measurement","Unix;multi-threading;systems software;software portability;software metrics;software libraries","software portability;systems software;POSIX-compliant multithreaded programs;standards-prescribed interface;thread-specific data;Tempo program specializer;random number generator;SPARC;parallel genetic algorithm system","","3","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Measuring functional cohesion","J. M. Bieman; L. M. Ott","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; NA","IEEE Transactions on Software Engineering","","1994","20","8","644","657","We examine the functional cohesion of procedures using a data slice abstraction. Our analysis identifies the data tokens that lie on more than one slice as the ""glue"" that binds separate components together. Cohesion is measured in terms of the relative number of glue tokens, tokens that lie on more than one data slice, and super-glue tokens, tokens that lie on all data slices in a procedure, and the adhesiveness of the tokens. The intuition and measurement scale factors are demonstrated through a set of abstract transformations.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.310673","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=310673","","Software measurement;Data analysis;Software engineering;Software design;Measurement techniques;Flow graphs;Testing;Fluid flow measurement;Measurement standards;Computer languages","software metrics","functional cohesion;data slice abstraction;data tokens;glue tokens;super-glue tokens;data slices;software measurement scale;abstract transformations","","81","","43","","","","","","IEEE","IEEE Journals & Magazines"
"The SATIN Component System-A Metamodel for Engineering Adaptable Mobile Systems","S. Zachariadis; C. Mascolo; W. Emmerich","NA; NA; IEEE Computer Society","IEEE Transactions on Software Engineering","","2006","32","11","910","927","Mobile computing devices, such as personal digital assistants and mobile phones, are becoming increasingly popular, smaller, and more capable. We argue that mobile systems should be able to adapt to changing requirements and execution environments. Adaptation requires the ability-to reconfigure the deployed code base on a mobile device. Such reconfiguration is considerably simplified if mobile applications are component-oriented rather than monolithic blocks of code. We present the SATIN (system adaptation targeting integrated networks) component metamodel, a lightweight local component metamodel that offers the flexible use of logical mobility primitives to reconfigure the software system by dynamically transferring code. The metamodel is implemented in the SATIN middleware system, a component-based mobile computing middleware that uses the mobility primitives defined in the metamodel to reconfigure both itself and applications that it hosts. We demonstrate the suitability of SATIN in terms of lightweightedness, flexibility, and reusability for the creation of adaptable mobile systems by using it to implement, port, and evaluate a number of existing and new applications, including an active network platform developed for satellite communication at the European space agency. These applications exhibit different aspects of adaptation and demonstrate the flexibility of the approach and the advantages gained","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.115","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015513","Distributed objects;components;containers;mobile systems;middleware;pervasive computing;mobile code.","Systems engineering and theory;Application software;Mobile computing;Personal digital assistants;Middleware;Bandwidth;Computer networks;Mobile handsets;Software systems;Bluetooth","distributed object management;middleware;mobile computing;mobility management (mobile radio);object-oriented programming","SATIN component metamodel system;engineering adaptable mobile system;personal digital assistant;system adaptation targeting integrated network;software system;middleware system;European space agency","","19","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Grammar Recovery from Parse Trees and Metrics-Guided Grammar Refactoring","N. A. Kraft; E. B. Duffy; B. A. Malloy","University of Alabama, Tuscaloosa; Clemson University, Clemson; Clemson University, Clemson","IEEE Transactions on Software Engineering","","2009","35","6","780","794","Many software development tools that assist with tasks such as testing and maintenance are specific to a particular development language and require a parser for that language. Because a grammar is required to develop a parser, construction of these software development tools is dependent upon the availability of a grammar for the development language. However, a grammar is not always available for a language and, in these cases, acquiring a grammar is the most difficult, costly, and time-consuming phase of tool construction. In this paper, we describe a methodology for grammar recovery from a hard-coded parser. Our methodology is comprised of manual instrumentation of the parser, a technique for automatic grammar recovery from parse trees, and a semi-automatic metrics-guided approach to refactoring an iterative grammar to obtain a recursive grammar. We present the results of a case study in which we recover and refactor a grammar from version 4.0.0 of the GNU C++ parser and then refactor the recovered grammar using our metrics-guided approach. Additionally, we present an evaluation of the recovered and refactored grammar by comparing it to the ISO C++98 grammar.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.65","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5278661","Grammar;grammar recovery;grammar refactoring;grammar metrics;parse tree.","Programming;Software testing;Instruments;Software maintenance;Java;Computer Society;Manuals;Iterative methods;ISO;Debugging","C++ language;grammars;software maintenance;software metrics;system recovery","grammar recovery;parse trees;metrics-guided grammar refactoring;software development tools;hard-coded parser;iterative grammar;recursive grammar;GNU C++ parser;ISO C++98 grammar","","5","","49","","","","","","IEEE","IEEE Journals & Magazines"
"CBGA-ES+: A Cluster-Based Genetic Algorithm with Non-Dominated Elitist Selection for Supporting Multi-Objective Test Optimization","D. Pradhan; S. Wang; S. Ali; T. Yue; M. Liaaen","Software Engineering, Simula Research Laboratory, Oslo, Oslo Norway (e-mail: dipesh.pradhan@outlook.com); Software Engineering, Simula Research Laboratory, Lysaker, Lysaker Norway (e-mail: shuai@simula.no); Software Engineering, Simula Research Lab, Oslo, Oslo Norway 1325 (e-mail: shaukat@simula.no); Software Engineering, Simula Research Laboratory, Oslo, Oslo Norway 1325 Lysaker (e-mail: tao@simula.no); Software Testing, Cisco Systems, Oslo, Oslo Norway (e-mail: marliaae@cisco.com)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Many real-world test optimization problems (e.g., test case prioritization) are multi-objective intrinsically and can be tackled using various multi-objective search algorithms (e.g., Non-dominated Sorting Genetic Algorithm (NSGA-II)). However, existing multi-objective search algorithms have certain randomness when selecting parent solutions for producing offspring solutions. In a worse case, suboptimal parent solutions may result in offspring solutions with bad quality, and thus affect the overall quality of the solutions in the next generation. To address such a challenge, we propose CBGA-ES+, a novel cluster-based genetic algorithm with non-dominated elitist selection to reduce the randomness when selecting the parent solutions to support multi-objective test optimization. We empirically compared CBGA-ES+ with random search and greedy (as baselines), four commonly used multi-objective search algorithms (i.e., Multi-objective Cellular genetic algorithm (MOCell), NSGA-II, Pareto Archived Evolution Strategy (PAES), and Strength Pareto Evolutionary Algorithm (SPEA2)), and the predecessor of CBGA-ES+ (named CBGA-ES) using five multi-objective test optimization problems with eight subjects (two industrial, one real world, and five open source). The results showed that CBGA-ES+ managed to significantly outperform the selected search algorithms for a majority of the experiments. Moreover, for the solutions in the same search space, CBGA-ES+ managed to perform better than CBGA-ES, MOCell, NSGA-II, PAES, and SPEA2 for 2.2%, 13.6%, 14.5%, 17.4%, and 9.9%, respectively. Regarding the running time of the algorithm, CBGA-ES+ was faster than CBGA-ES for all the experiments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2882176","Certus SFI; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8540431","Elitist selection;Multi-objective genetic algorithm;Multi-objective test optimization;Search","Optimization;Genetic algorithms;Sociology;Statistics;Search problems;Clustering algorithms;Software algorithms","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Automating visual language generation","C. Crimi; A. Guercio; G. Pacini; G. Tortora; M. Tucci","CRIAI, Naples, Italy; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","1990","16","10","1122","1135","A system to generate and interpret customized visual languages in given application areas is presented. The generation is highly automated. The user presents a set of sample visual sentences to the generator. The generator uses inference grammar techniques to produce a grammar that generalizes the initial set of sample sentences, and exploits general semantic information about the application area to determine the meaning of the visual sentences in the inferred language. The interpreter is modeled on an attribute grammar. A knowledge base, constructed during the generation of the system, is then consulted to construct the meaning of the visual sentence. The architecture of the system and its use in the application environment of visual text editing (inspired by the Heidelberg icon set) enhanced with file management features are reported.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.60293","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=60293","","Cognitive science;User interfaces;Environmental management;Costs;Graphics;Microelectronics;Multimedia communication;Character generation","grammars;inference mechanisms;knowledge based systems;visual programming","automatic visual language generation;customized visual languages;inference grammar techniques;general semantic information;attribute grammar;knowledge base;application environment","","28","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Experiments in optimizing FP","B. G. Ryder; J. S. Pendergrast","Dept. of Comput. Sci., Rutgers Univ., New Brunswick, NJ, USA; NA","IEEE Transactions on Software Engineering","","1988","14","4","444","454","FPOPT, a globally optimizing compiler for FP, was built to study the efficiency of compiling a functional programming language by translating it into an intermediate language and then optimizing that intermediate language. The FPOPT system, the design of the intermediate language and the optimizations performed are described. The relative effectiveness of these optimizations, singly and in combinations, are compared using an instrumented version of FPOPT.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4668","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4668","","Optimizing compilers;Functional programming;Design optimization;Runtime;Instruments;Computer languages;Data structures;Proposals;Code standards;Aggregates","high level languages;optimisation;program compilers;program interpreters","FPOPT;globally optimizing compiler;FP;functional programming language;intermediate language","","","","16","","","","","","IEEE","IEEE Journals & Magazines"
"IAI Corporate Software Engineering Training and Education Program","J. Z. Lavi; M. I. B. Porat; A. Ben-David","Israel Aircraft Industries, Ben Gurion International Airport; NA; NA","IEEE Transactions on Software Engineering","","1987","SE-13","11","1207","1216","Israel Aircraft Industries has developed a comprehensive educational program in software engineering. Goals of the program include: the retraining of college graduates to become software engineers with specializations in one of three application areas (data processing, embedded computer systems, and CAD/CAM systems), and enhancement of the knowledge of currently practicing software engineers. The program is centered around three distinct full-time courses of study having an average duration of 7 months. The training program also includes a large number of short courses and seminars. The company is currently planning an M.Sc. program in embedded computer systems and software engineering in cooperation with one of the universities in Israel.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.1987.232871","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1702169","CAD/CAM;EDP;education;embedded computer systems;industry;laboratories;software engineering;training","Software engineering;Educational programs;CADCAM;Embedded software;Data engineering;Knowledge engineering;Embedded computing;Planning;Industrial training;Computer science education","","CAD/CAM;EDP;education;embedded computer systems;industry;laboratories;software engineering;training","","1","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Instance Migration Validity for Dynamic Evolution of Data-Aware Processes","W. Song; X. Ma; H. Jacobsen","Coomputer Science, Nanjing University of Science and Technology, Nanjing, Jiangsu China 210094 (e-mail: wsong@njust.edu.cn); State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu China 210046 (e-mail: xxm@nju.edu.cn); ECE, Department of Electrical and Computer Engineering, Toronto, Ontario Canada M5S3G4 (e-mail: jacobsen@eecg.toronto.edu)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Likely more than many other software artifacts, business processes constantly evolve to adapt to ever changing application requirements. To enable dynamic process evolution, where changes are applied to in-flight processes, running process instances have to be migrated. On the one hand, as many instances as possible should be migrated to the changed process. On the other hand, the validity to migrate an instance should be guaranteed to avoid introducing dynamic change bugs after migration. As our theoretical results show, when the state of variables is taken into account, migration validity of data-aware process instances is undecidable. Based on the trace of an instance, existing approaches leverage trace replaying to check migration validity. However, they err on the side of caution, not identifying many instances as potentially safe to migrate. We present a more relaxed migration validity checking approach based on the dependence graph of a trace. We evaluate effectiveness and efficiency of our approach experimentally showing that it allows for more instances to safely migrate than for existing approaches and that it scales in the number of instances checked.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2802925","Deutsche Forschungsgemeinschaft; National Natural Science Foundation of China; National Key RD Program of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8283529","Data-aware process;dynamic evolution;instance migration;migration validity;trace slicing","Process control;Business;Computer bugs;Electronic mail;Jacobian matrices;Software;Algorithm design and analysis","","","","1","","","","","","","","IEEE","IEEE Early Access Articles"
"Chameleon: a system for solving the data-translation problem","S. A. Mamrak; M. S. Kaelbling; C. K. Nicholas; M. Share","Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA; Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA","IEEE Transactions on Software Engineering","","1989","15","9","1090","1108","A comprehensive data translation system is described with the following characteristics: (1) it is derived from a formal model of the translation task; (2) it supports the building of translation tools; (3) it supports the use of translation tools; and (4) it is accessible to its targeted end users. A software architecture to achieve the translation capability is fully implemented. Translators have been generated using the architecture, both by the original software developers and by industrial associates who have installed the architecture at their own sites.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.31367","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=31367","","Information science;Books;Publishing;Production facilities;Laboratories;Computer architecture;Buildings;Software architecture;Computer industry;SGML","electronic data interchange","Chameleon;data-translation problem;data translation system;formal model;translation task;translation tools;software architecture","","9","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Model-Driven Development Claims with respect to Quality: A Family of Experiments","J. I. Panach Navarrete; O. Dieste; B. Marín; S. España; S. Vegas; O. Pastor; N. Juristo","Departamento de Informatica, Universitat de Valencia, Valencia, Valencia Spain 46100 (e-mail: joigpana@uv.es); Lenguajes y Sistemas Informaticos e Ingenieria de Software, Universidad Politecnica de Madrid, Boadilla del Monte, Madrid Spain 28660 (e-mail: odieste@fi.upm.es); Facultad de Ingeniería y Ciencias, Universidad Diego Portales, 11240 Santiago, Metropolitana Chile (e-mail: beatriz.marin@mail.udp.cl); Department of Information and Computing Sciences, Utrecht University, Utrecht, Utrecht Netherlands (e-mail: s.espana@uu.nl); Languajes, computer systems and software engineering, Universidad Politecnica de Madrid, Boadilla del Monte, Madrid Spain 28660 (e-mail: svegas@fi.upm.es); Sistemas Informáticos y Computación, Universidad Politécnica de Valencia, Valencia, Valencia Spain 46022 (e-mail: opastor@dsic.upv.es); Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politecnica de Madrid, Boadilla del Monte, Madrid Spain 28660 (e-mail: natalia@fi.upm.es)","IEEE Transactions on Software Engineering","","2018","PP","99","1","1","Context: There is a lack of empirical evidence on the differences between model-driven development (MDD), where code is automatically derived from conceptual models, and traditional software development method, where code is manually written. In our previous work, we compared both methods in a baseline experiment concluding that quality of the software developed following MDD was significantly better only for more complex problems (with more function points). Quality was measured through test cases run on a functional system. Objective: This paper reports six replications of the baseline to study the impact of problem complexity on software quality in the context of MDD. Method: We conducted replications of two types: strict replications and object replications. Strict replications were similar to the baseline, whereas we used more complex experimental objects (problems) in the object replications. Results: MDD yields better quality independently of problem complexity with a moderate effect size. This effect is bigger for problems that are more complex. Conclusions: Thanks to the bigger size of the sample after aggregating replications, we discovered an effect that the baseline had not revealed due to the small sample size. The baseline results hold, which suggests that MDD yields better quality for more complex problems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2884706","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8565892","D.1.2 Automatic Programming;D.2.1.e Methodologies;D.2.1.i Validation","Unified modeling language;Productivity;Complexity theory;Inspection;Model-driven development;Software quality","","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Creation of views for reuse of software with different data representations","G. S. Novak","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA","IEEE Transactions on Software Engineering","","1995","21","12","993","1005","Software reuse is inhibited by the many different ways in which equivalent data can be represented. We describe methods by which views can be constructed semi-automatically to describe how application data types correspond to the abstract types that are used in numerical generic algorithms. Given such views, specialized versions of the generic algorithms that operate directly on the application data can be produced by compilation. This enables reuse of the generic algorithms for an application with minimal effort. Graphical user interfaces allow views to be specified easily and rapidly. Algorithms are presented for deriving, by symbolic algebra, equations that relate the variables used in the application data to the variables needed for the generic algorithms. Arbitrary application data structures are allowed. Units of measurement are converted as needed. These techniques allow reuse of a single version of a generic algorithm for a variety of possible data representations and programming languages. These techniques can also be applied in data conversion and in object-oriented, functional, and transformational programming.","0098-5589;1939-3520;2326-3881","","10.1109/32.489074","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=489074","","Application software;Graphical user interfaces;Algebra;Equations;Data structures;Measurement units;Computer languages;Data conversion;Functional programming;Object oriented programming","graphical user interfaces;abstract data types;data structures;software reusability;symbol manipulation;object-oriented programming;functional programming;program compilers;visual programming","view creation;software reuse;data representations;equivalent data;application data types;abstract types;numerical generic algorithms;compilation;specialized generic algorithms;graphical user interfaces;symbolic algebra;variables;measurement unit conversion;programming languages;data conversion;object-oriented programming;transformational programming;functional programming","","13","","55","","","","","","IEEE","IEEE Journals & Magazines"
"A Framework for Evaluating the Results of the SZZ Approach for Identifying Bug-Introducing Changes","D. A. da Costa; S. McIntosh; W. Shang; U. Kulesza; R. Coelho; A. E. Hassan","Department of Informatics and Applied Mathematics (DIMAp), Federal University of Rio Grande do Norte, Natal-RN, Brazil; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Department of Informatics and Applied Mathematics (DIMAp), Federal University of Rio Grande do Norte, Natal-RN, Brazil; Department of Informatics and Applied Mathematics (DIMAp), Federal University of Rio Grande do Norte, Natal-RN, Brazil; Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen’s University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2017","43","7","641","657","The approach proposed by Silwerski, Zimmermann, and Zeller (SZZ) for identifying bug-introducing changes is at the foundation of several research areas within the software engineering discipline. Despite the foundational role of SZZ, little effort has been made to evaluate its results. Such an evaluation is a challenging task because the ground truth is not readily available. By acknowledging such challenges, we propose a framework to evaluate the results of alternative SZZ implementations. The framework evaluates the following criteria: (1) the earliest bug appearance, (2) the future impact of changes, and (3) the realism of bug introduction. We use the proposed framework to evaluate five SZZ implementations using data from ten open source projects. We find that previously proposed improvements to SZZ tend to inflate the number of incorrectly identified bug-introducing changes. We also find that a single bug-introducing change may be blamed for introducing hundreds of future bugs. Furthermore, we find that SZZ implementations report that at least 46 percent of the bugs are caused by bug-introducing changes that are years apart from one another. Such results suggest that current SZZ implementations still lack mechanisms to accurately identify bug-introducing changes. Our proposed framework provides a systematic mean for evaluating the data that is generated by a given SZZ implementation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2616306","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7588121","SZZ;evaluation framework;bug detection;software repository mining","Computer bugs;Software engineering;Electronic mail;Software;Manuals;History;Systematics","program debugging","SZZ approach;bug-introducing change identification;Silwerski-Zimmermann-Zeller approach;ground truth;open source projects;data evaluation","","4","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Response to ""Comments on factors that impact the implementation of a systems development methodology""","T. L. Roberts; M. L. Gibson; R. K. Rainer; K. T. Fields","University of Central Florida; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","282","286","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.910864","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910864","","Instruments;Telephony;Laboratories;Writing;Statistical analysis;Input variables;Performance analysis;Prototypes;Technological innovation","","","","3","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Parsing languages by pattern matching","T. Rus","Dept. of Comput. Sci., Iowa Univ., Iowa City, IA, USA","IEEE Transactions on Software Engineering","","1988","14","4","498","511","The language of universal algebras is used as an alternative approach for programming language specification. BNF (Backus-Naur form) rules are used for specifying the signature of the language algebras instead of the context-free syntax. The algorithm for program parsing is inductively defined by the following universal algebraic construction: any function defined on the generators of a free algebra taking values in the carrier of another similar algebra can be uniquely extended to a homomorphism between the two algebras; any conventional programming language can be specified by a finite set of BNF rules and its algebra of symbols is generated by a finite set of generator classes. Thus, any function defined on the finite set of generators offers an algebraic mechanism for a universal algorithm for source language program parsing. The right-hand side of the BNF rules are the patterns searched by the algorithm in the source text of the program. The essential feature of this algorithm is that it can be used as a driver for code generation and optimization in a translator.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.4672","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4672","","Pattern matching;Algebra;Programming profession;Program processors;Computer languages;Text recognition;Computer science;Cities and towns;Computer aided instruction","grammars;high level languages;program compilers;program interpreters","pattern matching;universal algebras;programming language specification;BNF;Backus-Naur form;signature;language algebras;program parsing;code generation;translator","","5","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Computational efficiency of parallel combinatorial OR-tree searches","Guo-Jie Li; B. W. Wah","Inst. of Comput. Technol., Acad. Sinica, Beijing, China; NA","IEEE Transactions on Software Engineering","","1990","16","1","13","31","The performance of parallel combinatorial OR-tree searches is analytically evaluated. This performance depends on the complexity of the problem to be solved, the error allowance function, the dominance relation, and the search strategies. The exact performance may be difficult to predict due to the nondeterminism and anomalies of parallelism. The authors derive the performance bounds of parallel OR-tree searches with respect to the best-first, depth-first, and breadth-first strategies, and verify these bounds by simulation. They show that a near-linear speedup can be achieved with respect to a large number of processors for parallel OR-tree searches. Using the bounds developed, the authors derive sufficient conditions for assuring that parallelism will not degrade performance and necessary conditions for allowing parallelism to have a speedup greater than the ratio of the numbers of processors. These bounds and conditions provide the theoretical foundation for determining the number of processors required to assure a near-linear speedup.<<ETX>>","0098-5589;1939-3520;2326-3881","","10.1109/32.44360","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=44360","","Computational efficiency;Parallel processing;Search problems;Polynomials;Testing;Tree graphs;Performance analysis;Degradation;Operations research;Expert systems","combinatorial mathematics;database management systems;decision theory;parallel processing;performance evaluation;theorem proving;trees (mathematics)","near linear speedup;parallel combinatorial OR-tree searches;performance;error allowance function;dominance relation;search strategies;simulation;sufficient conditions","","7","","35","","","","","","IEEE","IEEE Journals & Magazines"
"SMT-Based Bounded Model Checking for Embedded ANSI-C Software","L. Cordeiro; B. Fischer; J. Marques-Silva","Federal University of Amazonas, Brazil; University of Southampton, Southampton; University College Dublin, Dublin","IEEE Transactions on Software Engineering","","2012","38","4","957","974","Propositional bounded model checking has been applied successfully to verify embedded software, but remains limited by increasing propositional formula sizes and the loss of high-level information during the translation preventing potential optimizations to reduce the state space to be explored. These limitations can be overcome by encoding high-level information in theories richer than propositional logic and using SMT solvers for the generated verification conditions. Here, we propose the application of different background theories and SMT solvers to the verification of embedded software written in ANSI-C in order to improve scalability and precision in a completely automatic way. We have modified and extended the encodings from previous SMT-based bounded model checkers to provide more accurate support for variables of finite bit width, bit-vector operations, arrays, structures, unions, and pointers. We have integrated the CVC3, Boolector, and Z3 solvers with the CBMC front-end and evaluated them using both standard software model checking benchmarks and typical embedded software applications from telecommunications, control systems, and medical devices. The experiments show that our ESBMC model checker can analyze larger problems than existing tools and substantially reduce the verification time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928354","Software engineering;formal methods;verification;model checking","Encoding;Embedded software;Safety;Space exploration;Optimization;Electronic mail","embedded systems;formal verification","SMT based bounded model checking;embedded ANSI-C software;embedded software verification;SMT solvers;model checkers;software model checking benchmarks","","52","","64","","","","","","IEEE","IEEE Journals & Magazines"
