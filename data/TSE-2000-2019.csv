"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Measuring the Impact of Code Dependencies on Software Architecture Recovery Techniques","T. Lutellier; D. Chollak; J. Garcia; L. Tan; D. Rayside; N. Medvidović; R. Kroeger","University of Waterloo, Waterloo, ON, Canada; University of Waterloo, Waterloo, ON, Canada; University of California, Irvine, CA; University of Waterloo, Waterloo, ON, Canada; University of Waterloo, Waterloo, ON, Canada; University of Southern California, Los Angeles, CA; Google Inc., Mountain View, CA","IEEE Transactions on Software Engineering","","2018","44","2","159","181","Many techniques have been proposed to automatically recover software architectures from software implementations. A thorough comparison among the recovery techniques is needed to understand their effectiveness and applicability. This study improves on previous studies in two ways. First, we study the impact of leveraging accurate symbol dependencies on the accuracy of architecture recovery techniques. In addition, we evaluate other factors of the input dependencies such as the level of granularity and the dynamic-bindings graph construction. Second, we recovered the architecture of a large system, Chromium, that was not available previously. Obtaining the ground-truth architecture of Chromium involved two years of collaboration with its developers. As part of this work, we developed a new submodule-based technique to recover preliminary versions of ground-truth architectures. The results of our evaluation of nine architecture recovery techniques and their variants suggest that (1) using accurate symbol dependencies has a major influence on recovery quality, and (2) more accurate recovery techniques are needed. Our results show that some of the studied architecture recovery techniques scale to very large systems, whereas others do not.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2671865","Natural Sciences and Engineering Research Council of Canada; Google Faculty Research Award; Ontario Ministry of Research and Innovation; U.S. National Science Foundation; Infosys Technologies, Ltd.; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7859416","Software architecture;empirical software engineering;maintenance and evolution;program comprehension","Computer architecture;Software architecture;Software;Heuristic algorithms;Chromium;Software algorithms;Manuals","software architecture;software quality;system recovery","symbol dependencies;accurate recovery techniques;recovery quality;submodule-based technique;ground-truth architecture;input dependencies;software implementations;software architectures;software architecture recovery techniques;code dependencies","","3","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental design of a power transformer station controller using a controller synthesis methodology","H. Marchand; M. Samaan","IRISA/INRIA, Rennes, France; NA","IEEE Transactions on Software Engineering","","2000","26","8","729","741","The authors describe the incremental specification of a power transformer station controller using a controller synthesis methodology. They specify the main requirements as simple properties, named control objectives, that the controlled plant has to satisfy. Then, using algebraic techniques, the controller is automatically derived from this set of control objectives. In our case, the plant is specified at a high level, using the data-flow synchronous SIGNAL language, and then by its logical abstraction, called polynomial dynamical system. The control objectives are specified as invariance, reachability, ...properties, as well as partial order relations to be checked by the plant. The control objectives equations are synthesized using algebraic transformations.","0098-5589;1939-3520;2326-3881","","10.1109/32.879811","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879811","","Power transformers;Automatic control;Signal synthesis;Polynomials;Optimal control;Equations;Control system synthesis;Data structures;Boolean functions;Circuit faults","power transformers;power system control;parallel languages;control system synthesis","incremental design;power transformer station controller;controller synthesis methodology;incremental specification;simple properties;named control objectives;controlled plant;algebraic techniques;control objectives;high level specification;data-flow synchronous SIGNAL language;logical abstraction;polynomial dynamical system;invariance;reachability;partial order relations;control objectives equations;algebraic transformations","","20","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical data modeling in software engineering using radial basis functions","Miyoung Shin; A. L. Goel","Electron. & Telecommun. Res. Inst., Taejon, South Korea; NA","IEEE Transactions on Software Engineering","","2000","26","6","567","576","Many empirical studies in software engineering involve relationships between various process and product characteristics derived via linear regression analysis. We propose an alternative modeling approach using radial basis functions (RBFs) which provide a flexible way to generalize linear regression function. Further, RBF models possess strong mathematical properties of universal and best approximation. We present an objective modeling methodology for determining model parameters using our recent SG algorithm, followed by a model selection procedure based on generalization ability. Finally, we describe a detailed RBF modeling study for software effort estimation using a well-known NASA dataset.","0098-5589;1939-3520;2326-3881","","10.1109/32.852743","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=852743","","Software engineering;Linear regression;Data analysis;Mathematical model;NASA;Programming;Signal processing algorithms;Predictive models;Inspection;Computer aided software engineering","software development management;data models;radial basis function networks;statistical analysis","empirical data modeling;software engineering;radial basis functions;product characteristics;linear regression analysis;alternative modeling approach;linear regression function;RBF models;mathematical properties;best approximation;objective modeling methodology;model parameters;SG algorithm;model selection procedure;generalization ability;RBF modeling study;software effort estimation;NASA dataset","","66","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Software process representation and analysis for framework instantiation","H. C. Jiau; Chia Hung Kao; Kuo-Feng Ssu","Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","IEEE Transactions on Software Engineering","","2004","30","10","707","","When studying [T.C. Oliveira et al. (2004)], we found an error and some ambiguous parts. In this paper, we state those errors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339280","","Redundancy","software process improvement","software process representation;framework instantiation;instantiation layer commands","","1","","1","","","","","","IEEE","IEEE Journals & Magazines"
"A transformation approach to derive efficient parallel implementations","T. Rauber; G. Runger","Inst. fur Inf., Univ. Halle-Wittenberg, Germany; NA","IEEE Transactions on Software Engineering","","2000","26","4","315","339","The construction of efficient parallel programs usually requires expert knowledge in the application area and a deep insight into the architecture of a specific parallel machine. Often, the resulting performance is not portable, i.e., a program that is efficient on one machine is not necessarily efficient on another machine with a different architecture. Transformation systems provide a more flexible solution. They start with a specification of the application problem and allow the generation of efficient programs for different parallel machines. The programmer has to give an exact specification of the algorithm expressing the inherent degree of parallelism and is released from the low-level details of the architecture. We propose such a transformation system with an emphasis on the exploitation of the data parallelism combined with a hierarchically organized structure of task parallelism. Starting with a specification of the maximum degree of task and data parallelism, the transformations generate a specification of a parallel program for a specific parallel machine. The transformations are based on a cost model and are applied in a predefined order, fixing the most important design decisions like the scheduling of independent multitask activations, data distributions, pipelining of tasks, and assignment of processors to task activations. We demonstrate the usefulness of the approach with examples from scientific computing.","0098-5589;1939-3520;2326-3881","","10.1109/32.844492","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=844492","","Parallel processing;Parallel machines;Programming profession;Microprocessors;Scientific computing;Message passing;Computer architecture;Costs;Processor scheduling;Pipeline processing","bibliographies;parallel programming;formal specification;parallel machines;message passing","transformation approach;parallel implementations;parallel program design;expert knowledge;application area;parallel machine;transformation systems;parallel machines;exact specification;data parallelism;hierarchically organized structure;task parallelism;cost model;predefined order;design decisions;independent multitask activations;data distributions;task pipelining;processor assignment;task activations;scientific computing","","18","","58","","","","","","IEEE","IEEE Journals & Magazines"
"On the Value of Ensemble Effort Estimation","E. Kocaguneli; T. Menzies; J. W. Keung","West Virginia University, Morgantown; West Virginia University, Morgantown; The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Software Engineering","","2012","38","6","1403","1416","Background: Despite decades of research, there is no consensus on which software effort estimation methods produce the most accurate models. Aim: Prior work has reported that, given M estimation methods, no single method consistently outperforms all others. Perhaps rather than recommending one estimation method as best, it is wiser to generate estimates from ensembles of multiple estimation methods. Method: Nine learners were combined with 10 preprocessing options to generate 9 × 10 = 90 solo methods. These were applied to 20 datasets and evaluated using seven error measures. This identified the best n (in our case n = 13) solo methods that showed stable performance across multiple datasets and error measures. The top 2, 4, 8, and 13 solo methods were then combined to generate 12 multimethods, which were then compared to the solo methods. Results: 1) The top 10 (out of 12) multimethods significantly outperformed all 90 solo methods. 2) The error rates of the multimethods were significantly less than the solo methods. 3) The ranking of the best multimethod was remarkably stable. Conclusion: While there is no best single effort estimation method, there exist best combinations of such effort estimation methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.111","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6081882","Software cost estimation;ensemble;machine learning;regression trees;support vector machines;neural nets;analogy;k-NN","Costs;Software performance;Measurement uncertainty;Taxonomy;Machine learning;Regression tree analysis;Support vector machines;Neural networks","software development management","ensemble effort estimation;software effort estimation;single method;multiple estimation method;error measures","","79","","82","","","","","","IEEE","IEEE Journals & Magazines"
"The effectiveness of control structure diagrams in source code comprehension activities","D. Hendrix; J. H. Cross; S. Maghsoodloo","Dept. of Comput. Sci. & Software Eng., Auburn Univ., AL, USA; Dept. of Comput. Sci. & Software Eng., Auburn Univ., AL, USA; NA","IEEE Transactions on Software Engineering","","2002","28","5","463","477","Recently, the first two in a series of planned comprehension experiments were performed to measure the effect of the control structure diagram (CSD) on program comprehensibility. Upper- and lower-division computer science and software engineering students were asked to respond to questions regarding the structure and execution of one source code module of a public domain graphics library. The time taken for each response and the correctness of each response was recorded. Statistical analysis of the data collected from these two experiments revealed that the CSD was highly significant in enhancing the subjects' performance in this program comprehension task. The results of these initial experiments promise to shed light on fundamental questions regarding the effect of software visualizations on program comprehensibility.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000450","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000450","","Performance evaluation;Computer science;Software engineering;Computer graphics;Software libraries;Statistical analysis;Data visualization","diagrams;reverse engineering;program visualisation;program control structures","control structure diagrams;source code comprehension;experiments;program comprehensibility;computer science students;software engineering students;public domain graphics library;statistical analysis;data analysis;software visualizations","","19","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Inference of message sequence charts","R. Alur; K. Etessami; M. Yannakakis","Dept. of Comput. & Inf. Sci., Pennsylvania Univ., Philadelphia, PA, USA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","7","623","633","Software designers draw message sequence charts for early modeling of the individual behaviors they expect from the concurrent system under design. Can they be sure that precisely the behaviors they have described are realizable by some implementation of the components of the concurrent system? If so, can we automatically synthesize concurrent state machines realizing the given MSCs? If, on the other hand, other unspecified and possibly unwanted scenarios are ""implied"" by their MSCs, can the software designer be automatically warned and provided the implied MSCs? In this paper, we provide a framework in which all these questions are answered positively. We first describe the formal framework within which one can derive implied MSCs and then provide polynomial-time algorithms for implication, realizability, and synthesis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214326","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214326","","System recovery;Software design;Polynomials;Algorithm design and analysis;Automata;Formal verification;Unified modeling language;Pattern analysis;Timing;Pattern matching","formal verification;system recovery;program testing","message sequence charts;concurrent system;software designer;polynomial-time algorithms;deadlock freedom;concurrent state machines;formal verification;requirements analysis","","69","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Model-Based Adaptation of Behavioral Mismatching Components","C. Canal; P. Poizat; G. Salaün","Universidad de Málaga, Málaga; INRIA, Rocquencourt; Universidad de Málaga, Málaga","IEEE Transactions on Software Engineering","","2008","34","4","546","563","Component-Based Software Engineering focuses on the reuse of existing software components. In practice, most components cannot be integrated directly into an application-to-be, because they are incompatible. Software Adaptation aims at generating, as automatically as possible, adaptors to compensate mismatch between component interfaces, and is therefore a promising solution for the development of a real market of components promoting software reuse. In this article, we present our approach for software adaptation which relies on an abstract notation based on synchronous vectors and transition systems for governing adaptation rules. Our proposal is supported by dedicated algorithms that generate automatically adaptor protocols. These algorithms have been implemented in a tool, called Adaptor, that can be used through a user-friendly graphical interface.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527252","Software Engineering;Requirements/Specifications;Design Tools and Techniques;Software Architectures;Interoperability;Interface definition languages;Software Construction;Software Engineering;Requirements/Specifications;Design Tools and Techniques;Software Architectures;Interoperability;Interface definition languages;Software Construction","Adaptation model;Irrigation;Protocols;Middleware;Software engineering;Proposals;Assembly systems;Software tools;Contracts;Petri nets","application program interfaces;object-oriented programming;software reusability","model-based software adaptation;behavioral mismatching software component;component-based software engineering;component interface;software reusability;abstract notation;synchronous vector;transition system;automatic adaptor protocol generation","","73","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Diagnosing rediscovered software problems using symptoms","I. Lee; R. K. Iyer","Dept. of Electr. Eng., Hanyang Univ., Seoul, South Korea; NA","IEEE Transactions on Software Engineering","","2000","26","2","113","127","This paper presents an approach to automatically diagnosing rediscovered software failures using symptoms, in environments in which many users run the same procedural software system. The approach is based on the observation that the great majority of field software failures are rediscoveries of previously reported problems and that failures caused by the same defect often share common symptoms. Based on actual data, the paper develops a small software failure fingerprint, which consists of the procedure call trace, problem detection location, and the identification of the executing software. The paper demonstrates that over 60 percent of rediscoveries can be automatically diagnosed based on fingerprints; less than 10 percent of defects are misdiagnosed. The paper also discusses a pilot that implements the approach. Using the approach not only saves service resources by eliminating repeated data collection for and diagnosis of reoccurring problems, but it can also improve service response time for rediscoveries.","0098-5589;1939-3520;2326-3881","","10.1109/32.841113","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=841113","","Software systems;Fingerprint recognition;Databases;Computer Society;Delay;Software measurement;Operating systems;Computer crashes;Software testing;Costs","program diagnostics","symptoms;automatic rediscovered software failure diagnosis;procedural software system;software failure fingerprint;procedure call trace;problem detection location;executing software identification;service response time","","11","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Determining the Cause of a Design Model Inconsistency","A. Reder; A. Egyed","Johannes Kepler University Linz, Linz; Johannes Kepler University Linz, Linz","IEEE Transactions on Software Engineering","","2013","39","11","1531","1548","When a software engineer finds an inconsistency in a model, then the first question is why? What caused it? Obviously, there must be an error. But where could it be? Or is the design rule erroneous and if yes then which part? The cause of an inconsistency identifies the part of the model or design rule where the error must be. We believe that the visualization of an inconsistency ought to visualize the cause. Understanding the cause is of vital importance before a repair can even be formulated. Indeed, any automation (e.g., code generation, refactoring) has to be considered with caution if it involves model elements that cause inconsistencies. This paper analyzes the basic structure of inconsistent design rules as well as their behavior during validation and presents an algorithm for computing its cause. The approach is fully automated, tool supported, and was evaluated on 14,111 inconsistencies across 29 design models. We found that our approach computes correct causes for inconsistencies, these causes are nearly always a subset of the model elements investigated by the design rules' validation (a naive cause computation approximation), and the computation is very fast (99.8 percent of the causes are computable in <; 100 ms).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6560054","Design tools and techniques;programming environments/construction tools;validation","Unified modeling language;Computational modeling;Context;Maintenance engineering;Visualization;Context modeling;Light emitting diodes","software development management","design model inconsistency;software engineer;code generation;refactoring;model elements;inconsistent design rules","","10","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A formal security model for microprocessor hardware","V. Lotz; V. Kessler; G. H. Walter","Corp. Technol., Siemens AG, Munich, Germany; NA; NA","IEEE Transactions on Software Engineering","","2000","26","8","702","712","The paper introduces a formal security model for a microprocessor hardware system. The model has been developed as part of the evaluation process of the processor product according to ITSEC assurance level E4. Novel aspects of the model are the need for defining integrity and confidentiality objectives on the hardware level without the operating system or application specification and security policy being given, and the utilization of an abstract function and data space. The security model consists of a system model given as a state transition automaton on infinite structures and the formalization of security objectives by means of properties of automaton behaviors. Validity of the security properties is proved. The paper compares the model with published ones and summarizes the lessons learned throughout the modeling process.","0098-5589;1939-3520;2326-3881","","10.1109/32.879809","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879809","","Microprocessors;Hardware;Operating systems;Data security;Application software;Learning automata;Quality assurance;Formal specifications;Context modeling;Access control","microcomputers;security of data;data integrity;automata theory;theorem proving;formal verification;quality management","microprocessor hardware;formal security model;evaluation process;processor product;ITSEC assurance level;E4;integrity;confidentiality objectives;hardware level;operating system;application specification;security policy;abstract function;data space;system model;state transition automaton;infinite structures;security objectives;automaton behaviors;security properties;modeling process;validity proving","","2","","","","","","","","IEEE","IEEE Journals & Magazines"
"Applying Concept Analysis to User-Session-Based Testing of Web Applications","S. Sampath; S. Sprenkle; E. Gibson; L. Pollock; A. Souter Greenwald","IEEE Computer Society; NA; NA; IEEE Computer Society; IEEE Computer Society","IEEE Transactions on Software Engineering","","2007","33","10","643","658","The continuous use of the Web for daily operations by businesses, consumers, and the government has created a great demand for reliable Web applications. One promising approach to testing the functionality of Web applications leverages the user-session data collected by Web servers. User-session-based testing automatically generates test cases based on real user profiles. The key contribution of this paper is the application of concept analysis for clustering user sessions and a set of heuristics for test case selection. Existing incremental concept analysis algorithms are exploited to avoid collecting and maintaining large user-session data sets and to thus provide scalability. We have completely automated the process from user session collection and test suite reduction through test case replay. Our incremental test suite update algorithm, coupled with our experimental study, indicates that concept analysis provides a promising means for incrementally updating reduced test suites in response to newly captured user sessions with little loss in fault detection capability and program coverage.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70723","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302777","Software testing;Web applications;User-session-based testing;Test suite reduction;Concept analysis;Incremental test suite reduction","Application software;Automatic testing;Web server;Computer Society;Government;Algorithm design and analysis;Software testing;Clustering algorithms;Scalability;Fault detection","fault tolerant computing;Internet;program testing;user modelling","program coverage;fault detection;incremental test suite update algorithm;incremental test suite reduction;test case selection;user session clustering;user profile;test case generation;Web server;reliable Web application;user-session based testing;incremental concept analysis algorithm","","61","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Using patterns to design rules in workflows","F. Casati; S. Castano; M. Fugini; I. Mirbel; B. Pernici","Dipt. di Elettronica e Inf., Politecnico di Milano, Italy; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","8","760","785","In order to design workflows in changing and dynamic environments, a flexible, correct, and rapid realization of models of the activity flow is required. In particular, techniques are needed to design workflows capable of adapting themselves effectively when exceptional situations occur during process execution. The authors present an approach to flexible workflow design based on rules and patterns developed in the framework of the WIDE project. Rules allow a high degree of flexibility during workflow design by modeling exceptional aspects of the workflow separately from the main activity flow. Patterns model frequently occurring exceptional situations in a generalized way by providing the designer with skeletons of rules and suggestions about their instantiation, together with indications on relationships with other rules, with the activity flow, and with related information. Pattern based design relies on a pattern catalog containing patterns to be reused and on a formal basis for specializing and instantiating available patterns.","0098-5589;1939-3520;2326-3881","","10.1109/32.879813","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879813","","Computer Society;Proposals;Skeleton;Process control;Control systems;Business communication","bibliographies;workflow management software;object-oriented programming;software reusability;knowledge based systems","rule design;dynamic environments;activity flow;exceptional situations;process execution;flexible workflow design;WIDE project;exceptional aspects;main activity flow;frequently occurring exceptional situations;rule instantiation;pattern based design;pattern catalog;pattern reuse;formal basis","","34","","57","","","","","","IEEE","IEEE Journals & Magazines"
"A study of the effect of imperfect debugging on software development cost","Min Xie; Bo Yang","Dept. of Ind. & Syst. Eng., Nat. Univ. of Singapore, Singapore; NA","IEEE Transactions on Software Engineering","","2003","29","5","471","473","It is widely recognized that the debugging processes are usually imperfect. Software faults are not completely removed because of the difficulty in locating them or because new faults might be introduced. Hence, it is of great importance to investigate the effect of the imperfect debugging on software development cost, which, in turn, might affect the optimal software release time or operational budget. In this paper, a commonly used cost model is extended to the case of imperfect debugging. Based on this, the effect of imperfect debugging is studied. As the probability of perfect debugging, termed testing level here, is expensive to be increased, but manageable to a certain extent with additional resources, a model incorporating this situation is presented. Moreover, the problem of determining the optimal testing level is considered. This is useful when the decisions regarding the test team composition, testing strategy, etc., are to be made for more effective testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199075","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199075","","Programming;Cost function;Software debugging;Software testing;Resource management;Software reliability;Fault diagnosis;Software systems;Personnel","software engineering;program debugging;software reliability;software development management","debugging processes;imperfect debugging;testing optimization;software development cost;optimal software release time;software reliability;testing level","","54","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Automated tuning of parallel I/O systems: an approach to portable I/O performance for scientific applications","Ying Chen; M. Winslett","IBM Almaden Res. Center, San Jose, CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","4","362","383","Parallel I/O systems typically consist of individual processors, communication networks, and a large number of disks. Managing and utilizing these resources to meet performance, portability, and usability goals of high performance scientific applications has become a significant challenge. For scientists, the problem is exacerbated by the need to retune the I/O portion of their code for each supercomputer platform where they obtain access. We believe that a parallel I/O system that automatically selects efficient I/O plans for user applications is a solution to this problem. The authors present such an approach for scientific applications performing collective I/O requests on multidimensional arrays. Under our approach, an optimization engine in a parallel I/O system selects high quality I/O plans without human intervention, based on a description of the application I/O requests and the system configuration. To validate our hypothesis, we have built an optimizer that uses rule based and randomized search based algorithms to tune parameter settings in Panda, a parallel I/O library for multidimensional arrays. Our performance results obtained from an IBM SP using an out-of-core matrix multiplication application show that the Panda optimizer is able to select high quality I/O plans and deliver high performance under a variety of system configurations with a small total optimization overhead.","0098-5589;1939-3520;2326-3881","","10.1109/32.844494","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=844494","","Application software;Humans;Computer Society;Communication networks;Multidimensional systems;Analytical models;Cost function;Database systems;Resource management;Usability","bibliographies;parallel programming;software portability;natural sciences computing;program diagnostics;input-output programs;matrix multiplication;software libraries","automated tuning;parallel I/O systems;portable I/O performance;scientific applications;usability goals;supercomputer platform;parallel I/O system;user applications;collective I/O requests;multidimensional arrays;optimization engine;high quality I/O plans;human intervention;system configuration;randomized search based algorithms;parameter tuning;parameter settings;Panda;parallel I/O library;IBM SP;out-of-core matrix multiplication application;system configurations;total optimization overhead","","1","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Where Should We Fix This Bug? A Two-Phase Recommendation Model","D. Kim; Y. Tao; S. Kim; A. Zeller","The Hong Kong University of Science and Technology, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong; Saarland University, Saarland","IEEE Transactions on Software Engineering","","2013","39","11","1597","1610","To support developers in debugging and locating bugs, we propose a two-phase prediction model that uses bug reports' contents to suggest the files likely to be fixed. In the first phase, our model checks whether the given bug report contains sufficient information for prediction. If so, the model proceeds to predict files to be fixed, based on the content of the bug report. In other words, our two-phase model ""speaks up"" only if it is confident of making a suggestion for the given bug report; otherwise, it remains silent. In the evaluation on the Mozilla ""Firefox"" and ""Core"" packages, the two-phase model was able to make predictions for almost half of all bug reports; on average, 70 percent of these predictions pointed to the correct files. In addition, we compared the two-phase model with three other prediction models: the Usual Suspects, the one-phase model, and BugScout. The two-phase model manifests the best prediction performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6517844","Bug reports;machine learning;patch file prediction","Predictive models;Feature extraction;Computer bugs;Software;Computational modeling;Data mining;Noise","formal verification;program debugging","two-phase recommendation model;debugging;two-phase prediction model;bug report;two-phase model;speaks up;Mozilla packages;Firefox packages;Core packages;BugScout","","34","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Matching and Merging of Variant Feature Specifications","S. Nejati; M. Sabetzadeh; M. Chechik; S. Easterbrook; P. Zave","Simula Research Laboratory, Lysaker; Simula Research Laboratory, Lysaker; University of Toronto, Toronto; University of Toronto, Toronto; AT&T Laboratories-Research, Florham Park","IEEE Transactions on Software Engineering","","2012","38","6","1355","1375","Model Management addresses the problem of managing an evolving collection of models by capturing the relationships between models and providing well-defined operators to manipulate them. In this paper, we describe two such operators for manipulating feature specifications described using hierarchical state machine models: Match, for finding correspondences between models, and Merge, for combining models with respect to known or hypothesized correspondences between them. Our Match operator is heuristic, making use of both static and behavioral properties of the models to improve the accuracy of matching. Our Merge operator preserves the hierarchical structure of the input models, and handles differences in behavior through parameterization. This enables us to automatically construct merges that preserve the semantics of hierarchical state machines. We report on tool support for our Match and Merge operators, and illustrate and evaluate our work by applying these operators to a set of telecommunication features built by AT&T.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.112","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6086550","Model management;match;merge;hierarchical state machines;statecharts;behavior preservation;variability modeling;parameterization","Computational modeling;Semantics;Hierarchical systems;Pragmatics;Parameterization;Electronic mail;Voice mail","finite state machines;formal specification","variant feature specification;model management;hierarchical state machine model;match operator;static property;behavioral property;merge operator;hierarchical structure;tool support;telecommunication feature","","8","","83","","","","","","IEEE","IEEE Journals & Magazines"
"A practical method for specification and analysis of exception handling-a Java/JVM case study","E. Borger; W. Schulte","Dipartimento di Inf., Pisa Univ., Italy; NA","IEEE Transactions on Software Engineering","","2000","26","9","872","887","We provide a rigorous framework for language and platform independent design and analysis of exception handling mechanisms in modern programming languages and their implementations. To illustrate the practicality of the method we develop it for the exception handling mechanism of Java and show that its implementation on the Java Virtual Machine (JVM) Is correct. For this purpose we define precise abstract models for exception handling in Java and in the JVM and define a compilation scheme of Java to JVM code which allows us to prove that, in corresponding runs, Java and the JVM throw the same exceptions and with equivalent effect. Thus, the compilation scheme can, with reasonable confidence, be used as a standard reference for Java exception handling compilation.","0098-5589;1939-3520;2326-3881","","10.1109/32.877847","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877847","","Java;Computer aided software engineering;Object oriented modeling;Virtual machining;Programming profession;Computer languages;Runtime;Program processors;Concurrent computing;Mathematical model","exception handling;Java;program compilers;formal specification","exception handling specification;exception handling analysis;programming languages;Java;Java Virtual Machine;precise abstract models;compilation scheme","","4","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Dealing with Burstiness in Multi-Tier Applications: Models and Their Parameterization","G. Casale; N. Mi; L. Cherkasova; E. Smirni","Imperial College London, London; Northeastern University, Boston; Hewlett-Packard Laboratories, Palo Alto; College of William and Mary, Williamsburg","IEEE Transactions on Software Engineering","","2012","38","5","1040","1053","Workloads and resource usage patterns in enterprise applications often show burstiness resulting in large degradation of the perceived user performance. In this paper, we propose a methodology for detecting burstiness symptoms in multi-tier applications but, rather than identifying the root cause of burstiness, we incorporate this information into models for performance prediction. The modeling methodology is based on the index of dispersion of the service process at a server, which is inferred by observing the number of completions within the concatenated busy times of that server. The index of dispersion is used to derive a Markov-modulated process that captures burstiness and variability of the service process at each resource well and that allows us to define queueing network models for performance prediction. Experimental results and performance model predictions are in excellent agreement and argue for the effectiveness of the proposed methodology under both bursty and nonbursty workloads. Furthermore, we show that the methodology extends to modeling flash crowds that create burstiness in the stream of requests incoming to the application.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311395","Capacity planning;multi-tier applications;bursty workload;bottleneck switch;index of dispersion","Servers;Indexes;Dispersion;Switches;Predictive models;Estimation","client-server systems;Markov processes;queueing theory;resource allocation;software architecture","multitier applications;resource usage patterns;workload patterns;enterprise applications;user performance degradation;burstiness symptom detection;burstiness root cause identification;performance prediction model;server busy times;Markov-modulated process;service process variability;queueing network models;nonbursty workloads;flash crowd model","","22","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Factors that impact implementing a system development methodology"" [with reply]","S. B. Yadav; N. G. Shaw; L. Webb; C. Sutcu; T. L. Roberts; M. L. Gibson; R. K. Rainer; K. T. Fields","Dept. of ISQS, Texas Tech. Univ., Lubbock, TX, USA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","279","286","In this correspondence, we point out some of the major shortcomings that we have discovered in the article titled ""Factors that Impact Implementing a System Development Methodology"" by Roberts et al. (1998). The article was published in the August 1998 issue of IEEE Transactions of Software Engineering. The article contains multiple problems that, if not pointed out, have the potential to lead to a state of confusion among researchers and practitioners alike. In particular, the article has the following problems: (1) The authors claim that the lack of theoretical basis for their factors is due to the fact that SDM implementation has never been studied in the literature. In fact, there is a multitude of studies very similar to that of Roberts at al. (2) The study does not meet commonly accepted standards for factor analysis procedures such as a minimum sample size ratio of 5:1, a ratio mandated even by the authors' own citations. This and other factor analysis problems lead to results that are questionable. In order to make the published article more useful to researchers and practitioners, this paper corrects some of the inaccuracies in the Roberts et al.'s article by: providing a brief literature review of some articles that are similar to the Roberts et al., study; and noting some of the technical inaccuracies in the data analysis procedures used by the authors so that the results can be interpreted in the proper context.","0098-5589;1939-3520;2326-3881","","10.1109/32.910863","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910863","","Data analysis;Software engineering;Citation analysis;Design methodology","software engineering;data analysis","system development methodology;factor analysis procedures;minimum sample size ratio;literature review;data analysis","","5","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Timed Communicating Object Z","B. Mahony; Jin Song Dong","Div. of Inf. Technol., Defence Sci. & Technol. Organ., Salisbury, SA, Australia; NA","IEEE Transactions on Software Engineering","","2000","26","2","150","177","This paper describes a timed, multithreaded object modeling notation for specifying real-time, concurrent, and reactive systems. The notation Timed Communicating Object Z (TCOZ) builds on Object Z's strengths in modeling complex data and algorithms, and on Timed CSP's strengths in modeling process control and real-time interactions. TCOZ Is novel in that it includes timing primitives, properly separates process control and data/algorithm issues and supports the modeling of true multithreaded concurrency. TCOZ is particularly well-suited for specifying complex systems whose components have their own thread of control. The expressiveness of the notation is demonstrated by a case study in specifying a multilift system that operates in real-time.","0098-5589;1939-3520;2326-3881","","10.1109/32.841115","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=841115","","Object oriented modeling;Process control;Real time systems;Concurrent computing;Timing;Formal specifications;Control systems;Algorithm design and analysis;Carbon capture and storage","formal specification;real-time systems;object-oriented languages;object-oriented methods","timed multithreaded object modeling notation;real-time systems;concurrent systems;reactive systems;specification;Timed Communicating Object Z;Timed CSP;real-time interactions;process control;complex data;algorithms;timing primitives;multithreaded concurrency;multilift system","","74","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Reducing inspection interval in large-scale software development","D. E. Perry; A. Porter; M. W. Wade; L. G. Votta; J. Perpich","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","7","695","705","We have found that, when software is developed by multiple, geographically separated teams, the cost-benefit trade-offs of software inspection change. In particular, this situation can significantly lengthen the inspection interval (calendar time needed to complete an inspection). Our research goal was to find a way to reduce the inspection interval without reducing inspection effectiveness. We believed that Internet technology offered some potential solutions, but we were not sure which technology to use nor what effects it would have on effectiveness. To conduct this research, we drew on the results of several empirical studies we had previously performed. These results clarified the role that meetings and individuals play in inspection effectiveness and interval. We conducted further studies showing that manual inspections without meetings were just as effective as manual inspections with them. On the basis of these and other findings and our understanding of Internet technology, we built an economical and effective tool that reduced the interval without reducing effectiveness. This tool, Hypercode, supports meetingless software inspections with geographically distributed reviewers. HyperCode is a platform-independent tool, developed on top of an Internet browser, that integrates seamlessly into the current development process. By seamless, we mean the tool produces a paper flow that is almost identical to the current inspection process. HyperCode's acceptance by its user community has been excellent. Moreover, we estimate that using HyperCode has reduced the inspection interval by 20 to 25 percent. We believe that, had we focused solely on technology (without considering the information our studies had uncovered), we would have created a more complex, but not necessarily more effective tool. We probably would have supported group meetings, restricted each participant's access to review comments, and supported a wider variety of inspection methods. In other words, the principles derived from our empirical studies dramatically and successfully directed our search for a technological solution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019483","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019483","","Inspection;Large-scale systems;Programming;Internet;Calendars;Costs;Computer Society;Software tools;Quality assurance;Collaborative software","software tools;computer aided software engineering;software quality;inspection;groupware;Internet;online front-ends;cost-benefit analysis","software inspection interval;large-scale software development;software development teams;cost-benefit tradeoffs;software inspection effectiveness;Internet technology;group meetings;manual inspections;Hypercode;meetingless software inspections;geographically distributed reviewers;platform-independent tool;Internet browser;paper flow;user community acceptance;participant access;review comments;World Wide Web-based code inspections;asynchronous communication;automated support;information flow;workflow","","26","","21","","","","","","IEEE","IEEE Journals & Magazines"
"On the applicability of Weyuker Property 9 to object-oriented structural inheritance complexity metrics","Gursaran; G. Roy","Dept. of Math., Dayalbagh Educ. Inst., Agra, India; NA","IEEE Transactions on Software Engineering","","2001","27","4","381","384","In the metric suite for object oriented design put forward by S.R. Chidamber and C.F. Kemerer (1994), it is observed that E. Weyuker's (1988) Property 9 is not satisfied by any of the structural inheritance complexity metrics. The same is also observed for candidate structural inheritance complexity metrics proposed by A.F. Brito and R. Carapuca (1994). The authors formally show that particular classes of inheritance metrics (that include the above proposals) that are defined on a directed graph abstraction of the inheritance structure and that are contrived on the assumptions and definitions given by Chidamber and Kemerer, can never satisfy Property 9. Furthermore, it is also argued that the formalisation can be generalized to include other classes of structural metrics that are not necessarily inheritance metrics.","0098-5589;1939-3520;2326-3881","","10.1109/32.917526","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917526","","Proposals;Measurement standards;Memory management;Runtime","object-oriented programming;software metrics;inheritance;directed graphs","Weyuker Property 9;object oriented structural inheritance complexity metrics;metric suite;object oriented design;directed graph abstraction;inheritance metrics","","8","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Adaptive Service Composition in Flexible Processes","D. Ardagna; B. Pernici","NA; NA","IEEE Transactions on Software Engineering","","2007","33","6","369","384","In advanced service oriented systems, complex applications, described as abstract business processes, can be executed by invoking a number of available Web services. End users can specify different preferences and constraints and service selection can be performed dynamically identifying the best set of services available at runtime. In this paper, we introduce a new modeling approach to the Web service selection problem that is particularly effective for large processes and when QoS constraints are severe. In the model, the Web service selection problem is formalized as a mixed integer linear programming problem, loops peeling is adopted in the optimization, and constraints posed by stateful Web services are considered. Moreover, negotiation techniques are exploited to identify a feasible solution of the problem, if one does not exist. Experimental results compare our method with other solutions proposed in the literature and demonstrate the effectiveness of our approach toward the identification of an optimal solution to the QoS constrained Web service selection problem","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1011","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181707","Web services;quality of service;service composition;integer programming.","Web services;Quality of service;Constraint optimization;Context awareness;Mixed integer linear programming;Linear programming;Runtime environment;Context-aware services;Grid computing;Fluctuations","integer programming;linear programming;quality of service;Web services","adaptive service composition;service oriented system;business process;Web services;QoS constraints;integer linear programming problem;loop peeling;optimization;negotiation technique;quality of service","","509","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Inference graphs: a computational structure supporting generation of customizable and correct analysis components","L. K. Dillon; R. E. K. Stirewalt","Dept. of Comput. Sci. & Eng., Michigan State Univ., USA; Dept. of Comput. Sci. & Eng., Michigan State Univ., USA","IEEE Transactions on Software Engineering","","2003","29","2","133","150","Amalia is a generator framework for constructing analyzers for operationally defined formal notations. These generated analyzers are components that are designed for customization and integration into a larger environment. The customizability, and efficiency of Amalia analyzers owe to a computational structure called an inference graph. This paper describes this structure, how inference graphs enable Amalia to generate analyzers for operational specifications, and how we build in assurance. On another level, this paper illustrates how to balance the need for assurance, which typically implies a formal proof obligation, against other design concerns, whose solutions leverage design techniques that are not (yet) accompanied by mature proof methods. We require Amalia-generated designs to be transparent with respect to the formal semantic models upon which they are based. Inference graphs are complex structures that incorporate many design optimizations. While not formally verifiable, their fidelity with respect to a formal operational semantics can be discharged by inspection.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178052","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178052","","Design optimization;Design engineering;Computer Society;Object oriented modeling;Inspection;Assembly;Software design;Design methodology;Software engineering;Computer science","program verification;program diagnostics;graphs","operationally defined formal notations;Amalia analyzers;computational structure;inference graph;operational specifications;assurance;formal proof obligation;mature proof methods;formal semantic models;correctness","","6","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""More success and failure factors in software reuse""","M. Morisio; M. Ezran; C. Tully","Dipt. di Automatica e Informatica, Politecnico di Torino, Italy; NA; NA","IEEE Transactions on Software Engineering","","2003","29","5","478","","For original paper see ibid., p. 474. This is a clear example of how research in software engineering can progress when empirical methods are applied. Menzies and Di Stefano apply a number of data mining tools to the data set. While, inmost cases, their results are in agreement with ours, in some cases they are not. Our first and main observation is that our interpretation of the data set is based not only on the data set itself but also on the knowledge gathered during the interviews with project members. The main problem with the data set is its size: 23 data points. Although this data set is the largest one available about reuse projects, it is too limited to base analysis only on data mining techniques; data mining is usually applied to data sets with thousands if not millions of data points.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199077","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199077","","Data mining;Predictive models;Testing;Data analysis;Human factors;Computer Society;Software engineering;Production;Association rules","data mining;software reusability","software reuse;software engineering;data set;data mining","","2","","2","","","","","","IEEE","IEEE Journals & Magazines"
"On the Semantics of Associations and Association Ends in UML","D. Milicev","IEEE Computer Society","IEEE Transactions on Software Engineering","","2007","33","4","238","251","Association is one of the key concepts in UML that is intensively used in conceptual modeling. Unfortunately, in spite of the fact that this concept is very old and is inherited from other successful modeling techniques, a fully unambiguous understanding of it, especially in correlation with other newer concepts connected with association ends, such as uniqueness, still does not exist. This paper describes a problem with one widely assumed interpretation of the uniqueness of association ends, the restrictive interpretation, and proposes an alternative, the intentional interpretation. Instead of restricting the association from having duplicate links, uniqueness of an association end in the intentional interpretation modifies the way in which the association end maps an object of the opposite class to a collection of objects of the class at that association end. If the association end is unique, the collection is a set obtained by projecting the collection of all linked objects. In that sense, the uniqueness of an association end modifies the view to the objects at that end, but does not constrain the underlying object structure. This paper demonstrates how the intentional interpretation improves expressiveness of the modeling language and has some other interesting advantages. Finally, this paper gives a completely formal definition of the concepts of association and association ends, along with the related notions of uniqueness, ordering, and multiplicity. The semantics of the UML actions on associations are also defined formally","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123326","Object-oriented modeling;Unified Modeling Language (UML);association;association end;formal semantics;conceptual modeling;model-driven development.","Unified modeling language;Object oriented modeling;Application software;Switches;Erbium;Computer Society;Visualization;Software systems;Computer industry;Object oriented databases","entity-relationship modelling;formal specification;object-oriented programming;programming language semantics;Unified Modeling Language","UML;conceptual modeling;intentional interpretation;association end;object-oriented modeling;Unified Modeling Language;formal semantics;formal specification","","12","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""On the applicability of Weyuker property 9 to object-oriented structural inheritance complexity metrics""","Lu Zhang; Dan Xie","Dept. of Comput. Sci., Liverpool Univ., UK; NA","IEEE Transactions on Software Engineering","","2002","28","5","526","527","In this paper, we point out some discrepancies in a correspondence published in this journal recently by Gursaran and G. Roy (see ibid., vol. 27, no. 4, p. 381-4 (2001)). Due to the discrepancies, the central two ""theorems"" and two ""corollaries"" claimed in that correspondence may not be held true in some extreme circumstances and, therefore, its main conclusion cannot be drawn for all the possible cases.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000454","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000454","","Software metrics;Object oriented programming","software metrics;object-oriented programming","Weyuker property 9;object-oriented structural inheritance complexity metrics;software complexity metrics","","7","","6","","","","","","IEEE","IEEE Journals & Magazines"
"Using machine learning for estimating the defect content after an inspection","F. Padberg; T. Ragg; R. Schoknecht","Karlsruhe Univ., Germany; NA; NA","IEEE Transactions on Software Engineering","","2004","30","1","17","28","We view the problem of estimating the defect content of a document after an inspection as a machine learning problem: The goal is to learn from empirical data the relationship between certain observable features of an inspection (such as the total number of different defects detected) and the number of defects actually contained in the document. We show that some features can carry significant nonlinear information about the defect content. Therefore, we use a nonlinear regression technique, neural networks, to solve the learning problem. To select the best among all neural networks trained on a given data set, one usually reserves part of the data set for later cross-validation; in contrast, we use a technique which leaves the full data set for training. This is an advantage when the data set is small. We validate our approach on a known empirical inspection data set. For that benchmark, our novel approach clearly outperforms both linear regression and the current standard methods in software engineering for estimating the defect content, such as capture-recapture. The validation also shows that our machine learning approach can be successful even when the empirical inspection data set is small.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265733","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265733","","Machine learning;Inspection;Neural networks;Software engineering;Curve fitting;Linear regression;Software standards;Software testing;Quality assurance;Estimation error","learning (artificial intelligence);regression analysis;neural nets;program verification;program testing","defect content estimation;machine learning;nonlinear regression technique;neural network;empirical methods;software engineering;software inspection;program validation","","11","","25","","","","","","IEEE","IEEE Journals & Magazines"
"The Use of Multilegged Arguments to Increase Confidence in Safety Claims for Software-Based Systems: A Study Based on a BBN Analysis of an Idealized Example","B. Littlewood; D. Wright","NA; NA","IEEE Transactions on Software Engineering","","2007","33","5","347","365","The work described here concerns the use of so-called multilegged arguments to support dependability claims about software-based systems. The informal justification for the use of multilegged arguments is similar to that used to support the use of multiversion software in pursuit of high reliability or safety. Just as a diverse 1-out-of-2 system might be expected to be more reliable than each of its two component versions, so might a two-legged argument be expected to give greater confidence in the correctness of a dependability claim (for example, a safety claim) than would either of the argument legs alone. Our intention here is to treat these argument structures formally, in particular, by presenting a formal probabilistic treatment of ""confidence,"" which will be used as a measure of efficacy. This will enable claims for the efficacy of the multilegged approach to be made quantitatively, answering questions such as, ""How much extra confidence about a system's safety will I have if I add a verification argument leg to an argument leg based upon statistical testing?"" For this initial study, we concentrate on a simplified and idealized example of a safety system in which interest centers upon a claim about the probability of failure on demand. Our approach is to build a ""Bayesian belief network"" (BBN) model of a two-legged argument and manipulate this analytically via parameters that define its node probability tables. The aim here is to obtain greater insight than what is afforded by the more usual BBN treatment, which involves merely numerical manipulation. We show that the addition of a diverse second argument leg can indeed increase confidence in a dependability claim; in a reasonably plausible example, the doubt in the claim is reduced to one-third of the doubt present in the original single leg. However, we also show that there can be some unexpected and counterintuitive subtleties here; for example, an entirely supportive second leg can sometimes undermine an original argument, resulting, overall, in less confidence than what came from this original argument. Our results are neutral on the issue of whether such difficulties will arise in real life $that is, when real experts judge real systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1002","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160972","Safety claims;safety arguments;software safety;software reliability;Bayesian belief networks.","Leg;Software safety;Uncertainty;Battery powered vehicles;Particle measurements;System testing;Probability;Software reliability;Bayesian methods;Control systems","Bayes methods;belief networks;probability;software reliability","multilegged argument;software-based system;multiversion software;probabilistic treatment;Bayesian belief network;software safety;software reliability","","40","","26","","","","","","IEEE","IEEE Journals & Magazines"
"A decision-analytic stopping rule for validation of commercial software systems","T. Chavez","Rapt Technol. Corp., San Francisco, CA, USA","IEEE Transactions on Software Engineering","","2000","26","9","907","918","The decision of when to release a software product commercially is not a question of when the software has attained some objectively justifiable degree of correctness. It is, rather, a question of whether the software achieves a reasonable balance among engineering objectives, market demand, customer requirements, and marketing directives of the software organization. We present a rigorous framework for addressing this important decision. Conjugate distributions from statistical decision theory provide an attractive means of modeling the cost and rate of bugs given information acquired during software testing, as well as prior information provided by software engineers about the fidelity of the software before testing begins. In contrast to other methods, the stopping analysis yields a computationally simple rule for deciding when to release a commercial software product based on information revealed to engineers during software testing-complicated numerical procedures are not needed. Our method has the added benefits that it is sequential: it measures explicitly the costs of customer dissatisfaction associated with bugs as well as the costs of declining market position while the testing process continues; and it incorporates a practical framework for cost-criticality assessment that makes sense to professional software developers.","0098-5589;1939-3520;2326-3881","","10.1109/32.877849","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877849","","Software systems;Software testing;Programming;Computer bugs;Costs;Business;Decision theory;Software tools;Design engineering;Engineering management","program testing;software development management;program debugging;formal verification","decision-analytic stopping rule;commercial software system validation;engineering objectives;market demand;customer requirements;marketing directives;software organization;statistical decision theory;conjugate distributions;software testing;bugs;customer dissatisfaction;declining market position;cost-criticality assessment;professional software developers","","12","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Featured Transition Systems: Foundations for Verifying Variability-Intensive Systems and Their Application to LTL Model Checking","A. Classen; M. Cordy; P. Schobbens; P. Heymans; A. Legay; J. Raskin","University of Namur (FUNDP), Namur; University of Namur (FUNDP), Namur; University of Namur (FUNDP), Namur; University of Namur (FUNDP), Namur and INRIA Lille-Nord Europe, France; IRISA/INRIA Rennes, France Université de Liège, Rennes Liège; Université Libre de Bruxelles (ULB), Brussels","IEEE Transactions on Software Engineering","","2013","39","8","1069","1089","The premise of variability-intensive systems, specifically in software product line engineering, is the ability to produce a large family of different systems efficiently. Many such systems are critical. Thorough quality assurance techniques are thus required. Unfortunately, most quality assurance techniques were not designed with variability in mind. They work for single systems, and are too costly to apply to the whole system family. In this paper, we propose an efficient automata-based approach to linear time logic (LTL) model checking of variability-intensive systems. We build on earlier work in which we proposed featured transitions systems (FTSs), a compact mathematical model for representing the behaviors of a variability-intensive system. The FTS model checking algorithms verify all products of a family at once and pinpoint those that are faulty. This paper complements our earlier work, covering important theoretical aspects such as expressiveness and parallel composition as well as more practical things like vacuity detection and our logic feature LTL. Furthermore, we provide an in-depth treatment of the FTS model checking algorithm. Finally, we present SNIP, a new model checker for variability-intensive systems. The benchmarks conducted with SNIP confirm the speedups reported previously.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6389685","Formal methods;model checking;verification;variability;features;software product lines","Unified modeling language;Semantics;Software;Labeling;Automata;Quality assurance","automata theory;formal logic;formal verification;software quality","featured transition systems;variability-intensive system verification;LTL model checking;software product line engineering;quality assurance techniques;automata-based approach;linear time logic model checking;mathematical model;FTS model checking algorithm;SNIP;model checker","","62","","74","","","","","","IEEE","IEEE Journals & Magazines"
"BDD-based safety-analysis of concurrent software with pointer data structures using graph automorphism symmetry reduction","Farn Wang; K. Schmidt; Fang Yu; Geng-Dian Huang; Bow-Yaw Wang","Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","6","403","417","Dynamic data-structures with pointer links, which are heavily used in real-world software, cause extremely difficult verification problems. Currently, there is no practical framework for the efficient verification of such software systems. We investigated symmetry reduction techniques for the verification of software systems with C-like indirect reference chains like x/spl rarr/y/spl rarr/z/spl rarr/w. We formally defined the model of software with pointer data structures and developed symbolic algorithms to manipulate conditions and assignments with indirect reference chains using BDD technology. We relied on two techniques, inactive variable elimination and process-symmetry reduction in the data-structure configuration, to reduce time and memory complexity. We used binary permutation for efficiency, but we also identified the possibility of an anomaly of false image reachability. We implemented the techniques in tool Red 5.0 and compared performance with Mur/spl phi/ and SMC against several benchmarks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321062","Symbolic model checking;pointers;data structure;address manipulation;symmetry reduction;experiments.","Data structures;Boolean functions;Software safety;Software systems;Hardware;Vehicle dynamics;Formal verification;Computer Society;Software algorithms;Binary decision diagrams","binary decision diagrams;data structures;symbol manipulation;program verification;distributed programming;computational complexity;graph theory","BDD-based safety-analysis;concurrent software;pointer data structure;graph automorphism symmetry reduction;software system verification;indirect reference chain;symbolic algorithm;inactive variable elimination;process-symmetry reduction;false image reachability;Red 5.0;symbolic model checking;address manipulation","","2","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Small Errors in ""Toward Formalizing Domain Modeling Semantics in Language Syntax'","R. J. Botting","IEEE Computer Society","IEEE Transactions on Software Engineering","","2005","31","10","911","911","A recent paper on domain modeling had State Charts with semantic errors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.116","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542071","Index Terms- UML;semantics;state charts.","Computer errors;Rails;Computer science","","Index Terms- UML;semantics;state charts.","","","","2","","","","","","IEEE","IEEE Journals & Magazines"
"A validation of the component-based method for software size estimation","J. J. Dolado","Fac. de Inf., Pais Vasco Univ., San Sebastian, Spain","IEEE Transactions on Software Engineering","","2000","26","10","1006","1021","Estimation of software size is a crucial activity among the tasks of software management. Work planning and subsequent estimations of the effort required are made based on the estimate of the size of the software product. Software size can be measured in several ways: lines of code (LOC) is a common measure and is usually one of the independent variables in equations for estimating several methods for estimating the final LOC count of a software system in the early stages. We report the results of the validation of the component-based method (initially proposed by Verner and Tate, 1988) for software sizing. This was done through the analysis of 46 projects involving more than 100,000 LOC of a fourth-generation language. We present several conclusions concerning the predictive capabilities of the method. We observed that the component-based method behaves reasonably, although not as well as expected for ""global"" methods such as Mark II function points for software size prediction. The main factor observed that affects the performance is the type of component.","0098-5589;1939-3520;2326-3881","","10.1109/32.879821","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879821","","Lab-on-a-chip;Size measurement;Software measurement;Equations;Software systems;Project management;Life estimation;Linear regression;Neural networks;Genetic programming","software reusability;software development management;high level languages;software metrics","software component-based method;software size estimation;software management;work planning;lines of code;fourth-generation language;Mark II function points;software size prediction;neural networks;genetic programming","","73","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Quality-Aware Service Selection for Service-Based Systems Based on Iterative Multi-Attribute Combinatorial Auction","Q. He; J. Yan; H. Jin; Y. Yang","Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Information Systems and Technology, University of Wollongong, Wollongong, Australia; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Anhui University, Hefei, China","IEEE Transactions on Software Engineering","","2014","40","2","192","215","The service-oriented paradigm offers support for engineering service-based systems (SBSs) based on service composition where existing services are composed to create new services. The selection of services with the aim to fulfil the quality constraints becomes critical and challenging to the success of SBSs, especially when the quality constraints are stringent. However, none of the existing approaches for quality-aware service composition has sufficiently considered the following two critical issues to increase the success rate of finding a solution: 1) the complementarities between services; and 2) the competition among service providers. This paper proposes a novel approach called combinatorial auction for service selection (CASS) to support effective and efficient service selection for SBSs based on combinatorial auction. In CASS, service providers can bid for combinations of services and apply discounts or premiums to their offers for the multi-dimensional quality of the services. Based on received bids, CASS attempts to find a solution that achieves the SBS owner's optimisation goal while fulfilling all quality constraints for the SBS. When a solution cannot be found based on current bids, the auction iterates so that service providers can improve their bids to increase their chances of winning. This paper systematically describes the auction process and the supporting mechanisms. Experimental results show that by exploiting the complementarities between services and the competition among service providers, CASS significantly outperforms existing quality-aware service selection approaches in finding optimal solutions and guaranteeing system optimality. Meanwhile, the duration and coordination overhead of CASS are kept at satisfactory levels in scenarios on different scales.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.2297911","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6702520","Service-based system;combinatorial auction;quality of service;service composition;service selection;web services;integer programming","Scattering;Quality of service;Optimization;Contracts;Abstracts;Time factors","combinatorial mathematics;iterative methods;quality of service;service-oriented architecture;software quality;Web services","system optimality;optimal solutions;auction mechanisms;auction process;service providers;service quality;CASS approach;quality-aware service composition;quality constraints;service creation;SBS;service-oriented paradigm;iterative multiattribute combinatorial auction;service-based systems;quality-aware service selection","","36","","61","","","","","","IEEE","IEEE Journals & Magazines"
"A Learning-Based Framework for Engineering Feature-Oriented Self-Adaptive Software Systems","N. Esfahani; A. Elkhodary; S. Malek","George Mason University, Fairfax; George Mason University, Fairfax; George Mason University, Fairfax","IEEE Transactions on Software Engineering","","2013","39","11","1467","1493","Self-adaptive software systems are capable of adjusting their behavior at runtime to achieve certain functional or quality-of-service goals. Often a representation that reflects the internal structure of the managed system is used to reason about its characteristics and make the appropriate adaptation decisions. However, runtime conditions can radically change the internal structure in ways that were not accounted for during their design. As a result, unanticipated changes at runtime that violate the assumptions made about the internal structure of the system could degrade the accuracy of the adaptation decisions. We present an approach for engineering self-adaptive software systems that brings about two innovations: 1) a feature-oriented approach for representing engineers' knowledge of adaptation choices that are deemed practical, and 2) an online learning-based approach for assessing and reasoning about adaptation decisions that does not require an explicit representation of the internal structure of the managed software system. Engineers' knowledge, represented in feature-models, adds structure to learning, which in turn makes online learning feasible. We present an empirical evaluation of the framework using a real-world self-adaptive software system. Results demonstrate the framework's ability to accurately learn the changing dynamics of the system while achieving efficient analysis and adaptation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6574860","Self-adaptive software;autonomic computing;feature-orientation;machine learning","Software systems;Runtime;Adaptation models;Quality of service;Authentication;Measurement","inference mechanisms;learning (artificial intelligence);quality of service;software engineering","feature-models;adaptation decision reasoning;adaptation decision assessment;online learning-based approach;runtime conditions;quality-of-service goals;engineering feature-oriented self-adaptive software systems;learning-based framework","","35","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Well-defined generalized stochastic Petri nets: a net-level method to specify priorities","E. Teruel; G. Franceschinis; M. De Pierro","Departamento de Informatica e Ingenieria de Sistemas, Zaragoza Univ., Spain; NA; NA","IEEE Transactions on Software Engineering","","2003","29","11","962","973","Generalized stochastic Petri nets (GSPN), with immediate transitions, are extensively used to model concurrent systems in a wide range of application domains, particularly including software and hardware aspects of computer systems, and their interactions. These models are typically used for system specification, logical and performance analysis, or automatic code generation. In order to keep modeling separate from the analysis and to gain in efficiency and robustness of the modeling process, the complete specification of the stochastic process underlying a model should be guaranteed at the net level, without requiring the generation and exploration of the state space. In this paper, we propose a net-level method that guides the modeler in the task of defining the priorities (and weights) of immediate transitions in a GSPN model, to deal with confusion and conflict problems. The application of this method ensures well-definition without reducing modeling flexibility or expressiveness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245298","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245298","","Stochastic processes;Petri nets;State-space methods;Application software;Robustness;Software performance;Software systems;Software tools;Computer aided manufacturing;Computer networks","Petri nets;formal specification;stochastic processes;performance evaluation;concurrency theory","well-defined generalized stochastic Petri nets;net-level method;priorities specification;immediate transitions;concurrent systems;software aspects;hardware aspects;computer systems;stochastic process;specification;logical analysis;performance analysis;automatic code generation","","9","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Handling of irregularities in human centered systems: a unified framework for data and processes","T. Murata; A. Borgida","Dept. of Comput. Sci., Rutgers Univ., Piscataway, NJ, USA; NA","IEEE Transactions on Software Engineering","","2000","26","10","959","977","Practical process-support and workflow systems should be built to describe the simple, normal flow of events and then deal easily with irregularities, including tolerating deviations. Similarly, these systems should describe the normal format and constraints concerning the large amounts of data that are usually stored, but then deal with abnormalities and possibly accommodate exceptional values. We offer a framework for treating both kinds of irregularities uniformly by using the notion of exception handling (with human agents as potential online exception handlers) and applying it to processes that have been reified as objects in classes with steps as attributes. As a result, only a small number of new constructs, which can be applied orthogonally, need to be introduced. Special runtime checks are used to deal with the consequences of permitting deviations from the norm to persist as violations of constraints. A logical semantics of process coordination and deviations is presented as a specification for implementations.","0098-5589;1939-3520;2326-3881","","10.1109/32.879819","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879819","","Humans;Runtime;Safety;Engines;Information management;Information retrieval;Databases;Control systems;Error correction;Data models","exception handling;workflow management software;software engineering;human factors","human centered systems;process-support systems;workflow systems;exception handling;runtime checks;logical semantics;process coordination","","6","","50","","","","","","IEEE","IEEE Journals & Magazines"
"On the Distribution of Software Faults","H. Zhang","NA","IEEE Transactions on Software Engineering","","2008","34","2","301","302","The Pareto principle is often used to describe how faults in large software systems are distributed over modules. A recent paper by Andersson and Runeson again confirmed the Pareto principle of fault distribution. In this paper, we show that the distribution of software faults can be more precisely described as the Weibull distribution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70771","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4407730","Product metrics;Measurement applied to SQA and V&V;Product metrics;Measurement applied to SQA and V&V","Weibull distribution;Software systems;Packaging;Probability distribution;Pareto analysis;Distribution functions;Reliability engineering;Java;Application software;Databases","Pareto analysis;Weibull distribution","software faults distribution;Pareto principle;large software systems;Weibull distribution","","30","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing partition and random testing via majorization and Schur functions","P. J. Boland; H. Singh; B. Cukic","Dept. of Stat., Nat. Univ. of Ireland, Dublin, Ireland; NA; NA","IEEE Transactions on Software Engineering","","2003","29","1","88","94","The comparison of partition and random sampling methods for software testing has received considerable attention in the literature. A standard criterion for comparisons between random and partition testing based on their expected efficacy in program debugging is the probability of detecting at least one failure causing input in the program's domain. We investigate the relative effectiveness of partition testing versus random testing through the powerful mathematical technique of majorization, which was introduced by Hardy et al. (1952). The tools of majorization and the concepts of Schur (convex and concave) functions (1923) enable us to derive general conditions under which partition testing is superior to random testing and, consequently, to give further insights into the value of partition testing strategies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166591","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166591","","Software testing;Sampling methods;Software debugging;Arithmetic;Software reliability;System testing;Software design;Software measurement","program debugging;program testing","partition methods;random sampling methods;software testing;program debugging;majorization;Schur functions;partition testing;random testing","","16","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Toward constraint-object-oriented development","T. Bolognesi","Istituto di Elaborazione dell'Inf., CNR, Pisa, Italy","IEEE Transactions on Software Engineering","","2000","26","7","594","616","In this paper, we propose to conservatively extend object-oriented decomposition by letting it affect also operations (methods). Different objects may support different parts of the same operation. The responsibility of defining an operation, in terms of enabling conditions and effects on the state, is distributed over several interacting objects, which act as constraints and express different, partial views about the system behavior. Constraint-oriented reasoning has already been explored and applied in the context of formal specification languages for concurrent and reactive systems, and is sufficiently different from object-oriented reasoning to be considered as a paradigm in itself, with its own specific advantages. Nevertheless, the paper shows that the two approaches are sufficiently compatible to be profitably integrated. We introduce a constraint-oriented style for an object-oriented programming language (JAVA).","0098-5589;1939-3520;2326-3881","","10.1109/32.859530","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859530","","Formal specifications;Java;Programming profession;Object oriented programming;Design methodology;Computer languages;Software engineering;Engineering management;Isolation technology;Data encapsulation","formal specification;object-oriented programming;constraint handling;Java","object-oriented decomposition;constraint-object-oriented development;constraints;partial views;interacting objects;object-oriented reasoning;constraint-oriented reasoning;constraint-oriented specification;LOTOS;multi-object operation;JAVA programming","","4","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluation of several nonparametric bootstrap methods to estimate confidence intervals for software metrics","Skylar Lei; M. R. Smith","Gen. Dynamics Canada, Calgary, Alta., Canada; NA","IEEE Transactions on Software Engineering","","2003","29","11","996","1004","Sample statistics and model parameters can be used to infer the properties, or characteristics, of the underlying population in typical data-analytic situations. Confidence intervals can provide an estimate of the range within which the true value of the statistic lies. A narrow confidence interval implies low variability of the statistic, justifying a strong conclusion made from the analysis. Many statistics used in software metrics analysis do not come with theoretical formulas to allow such accuracy assessment. The Efron bootstrap statistical analysis appears to address this weakness. In this paper, we present an empirical analysis of the reliability of several Efron nonparametric bootstrap methods in assessing the accuracy of sample statistics in the context of software metrics. A brief review on the basic concept of various methods available for the estimation of statistical errors is provided, with the stated advantages of the Efron bootstrap discussed. Validations of several different bootstrap algorithms are performed across basic software metrics in both simulated and industrial software engineering contexts. It was found that the 90 percent confidence intervals for mean, median, and Spearman correlation coefficients were accurately predicted. The 90 percent confidence intervals for the variance and Pearson correlation coefficients were typically underestimated (60-70 percent confidence interval), and those for skewness and kurtosis overestimated (98-100 percent confidence interval). It was found that the Bias-corrected and accelerated bootstrap approach gave the most consistent confidence intervals, but its accuracy depended on the metric examined. A method for correcting the under-/ overestimation of bootstrap confidence intervals for small data sets is suggested, but the success of the approach was found to be inconsistent across the tested metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245301","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245301","","Software metrics;Statistical analysis;Statistics;Estimation error;State estimation;Software algorithms;Context modeling;Computer industry;Footwear industry;Software engineering","software metrics;software reliability","nonparametric bootstrap methods;software metrics;sample statistics;model parameters;Efron bootstrap statistical analysis;industrial software engineering;Spearman correlation coefficients;confidence intervals estimation","","27","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Bristlecone: Language Support for Robust Software Applications","B. Demsky; S. Sundaramurthy","University of California, Irvine, Irvine; University of California, Irvine, Irvine","IEEE Transactions on Software Engineering","","2011","37","1","4","23","We present Bristlecone, a programming language for robust software systems. Bristlecone applications have two components: a high-level organization specification that describes how the application's conceptual operations interact and a low-level operational specification that describes the sequence of instructions that comprise an individual conceptual operation. Bristlecone uses the high-level organization specification to recover the software system from an error to a consistent state and to reason how to safely continue the software system's execution after the error. We have implemented a compiler and runtime for Bristlecone. We have evaluated this implementation on three benchmark applications: a Web crawler, a Web server, and a multiroom chat server. We developed both a Bristlecone version and a Java version of each benchmark application. We used injected failures to evaluate the robustness of each version of the application. We found that the Bristlecone versions of the benchmark applications more successfully survived the injected failures. The Bristlecone compiler contains a static analysis that operates on the organization specification to generate a set of diagrams that graphically present the task interactions in the application. We have used the analysis to help understand the high-level structure of three Bristlecone applications: a game server, a Web server, and a chat server.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5416725","Software robustness.","Robustness;Application software;Software systems;Switches;Runtime;Computer languages;Costs;Web server;Java;Software tools","Java;program compilers;program diagnostics;programming languages;software fault tolerance;specification languages","language support;robust software applications;programming language;robust software systems;high-level organization specification;low-level operational specification;runtime;benchmark applications;Web crawler;Web server;multiroom chat server;Java version;injected failures;Bristlecone compiler;static analysis;task interactions;high-level structure;game server","","2","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Model-checking algorithms for continuous-time Markov chains","C. Baier; B. Haverkort; H. Hermanns; J. -. Katoen","Inst. fur Informatik I, Bonn Univ., Germany; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","6","524","541","Continuous-time Markov chains (CTMCs) have been widely used to determine system performance and dependability characteristics. Their analysis most often concerns the computation of steady-state and transient-state probabilities. This paper introduces a branching temporal logic for expressing real-time probabilistic properties on CTMCs and presents approximate model checking algorithms for this logic. The logic, an extension of the continuous stochastic logic CSL of Aziz et al. (1995, 2000), contains a time-bounded until operator to express probabilistic timing properties over paths as well as an operator to express steady-state probabilities. We show that the model checking problem for this logic reduces to a system of linear equations (for unbounded until and the steady-state operator) and a Volterra integral equation system (for time-bounded until). We then show that the problem of model-checking time-bounded until properties can be reduced to the problem of computing transient state probabilities for CTMCs. This allows the verification of probabilistic timing properties by efficient techniques for transient analysis for CTMCs such as uniformization. Finally, we show that a variant of lumping equivalence (bisimulation), a well-known notion for aggregating CTMCs, preserves the validity of all formulas in the logic.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205180","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205180","","Steady-state;Stochastic processes;Probabilistic logic;Transient analysis;System performance;Timing;Integral equations;Performance analysis;Throughput;Production systems","Markov processes;formal verification;probability;temporal logic;bisimulation equivalence;timing;Volterra equations","continuous-time Markov chains;model-checking algorithms;system performance;system dependability;transient-state probabilities;steady-state probabilities;branching temporal logic;real-time probabilistic properties;continuous stochastic logic;time-bounded until operator;probabilistic timing properties;linear equations;Volterra integral equation system;uniformization;lumping equivalence;bisimulation","","340","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Symbolic parametric safety analysis of linear hybrid systems with BDD-like data-structures","Farn Wang","Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan","IEEE Transactions on Software Engineering","","2005","31","1","38","51","We introduce a new BDD-like data structure called hybrid-restriction diagrams (HRDs) for the representation and manipulation of linear hybrid automata (LHA) state-spaces and present algorithms for weakest precondition calculations. This permits us to reason about the valuations of parameters that make safety properties satisfied. Advantages of our approach include the ability to represent discrete state information and concave polyhedra in a unified scheme, as well as to save both memory consumptions and manipulation times when processing the same substructures in state-space representations. Our experimental results document its efficiency in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392719","Index Terms- Data-structures;BDD;hybrid automata;verification;model-checking.","Safety;Data structures;Boolean functions;Binary decision diagrams;Automata;Cost accounting;Computer Society;Systems engineering and theory;Character generation;Protocols","binary decision diagrams;formal verification;automata theory;data structures;formal specification;directed graphs;symbol manipulation","BDD-like data structure;hybrid-restriction diagrams;linear hybrid automata;symbolic parametric safety analysis;formal verification;model-checking","","22","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Interface Grammars for Modular Software Model Checking","G. Hughes; T. Bultan","University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara","IEEE Transactions on Software Engineering","","2008","34","5","614","632","Verification techniques relying on state enumeration (e.g., model checking) face two important challenges: the state-space explosion, an exponential increase in the state space as the number of components increases; and environment generation, modeling components that are either not available for analysis, or that cannot be handled by the verification tool in use. We propose a semi-automated approach for attacking these problems. In our approach, interfaces for the components not under analysis are specified using a specification language based on grammars. Specifically, an interface grammar for a component specifies the sequences of method invocations that are allowed by that component. We have built an compiler that takes the interface grammar for a component as input and generates a stub for that component. The stub thus generated can be used to replace that component during state space exploration, either to moderate state space explosion, or to provide an executable environment for the component under verification. We conducted a case study by writing an interface grammar for the Enterprise JavaBeans (EJB) persistence interface, and using the resulting stub to check EJB clients using the JPF model checker. Our results show that EJB clients can be verified efficiently with JPF using our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.72","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4609388","Model checking;Languages;interface grammars;modular verification;Model checking;Languages;interface grammars;modular verification","State-space methods;Java;Explosions;Software tools;Specification languages;Space exploration;Automata;Production;Writing;Hardware","application program interfaces;finite state machines;formal specification;grammars;program compilers;program verification;specification languages","interface grammar;modular software model checking;state enumeration;state-space explosion;environment generation;semiautomated approach;specification language;compiler;program verification;Enterprise JavaBeans;finite state machine","","8","","39","","","","","","IEEE","IEEE Journals & Magazines"
"e-Transactions: end-to-end reliability for three-tier architectures","S. Frolund; R. Guerraoui","Hewlett-Packard Labs., Palo Alto, CA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","4","378","395","A three-tier application is organized as three layers: human users interact with front-end clients (e.g., browsers), middle-tier application servers (e.g., Web servers) contain the business logic of the application, and perform transactions against back-end databases. Although three-tier applications are becoming mainstream, they usually fail to provide sufficient reliability guarantees to the users. Usually, replication and transaction-processing techniques are applied to specific parts of the application, but their combination does not provide end-to-end reliability. The aim of this paper is to provide a precise specification of a desirable, yet realistic, end-to-end reliability contract in three-tier applications. The paper presents the specification in the form of the Exactly-Once Transaction (e-Transaction) abstraction: an abstraction that encompasses both safety and liveness properties in three-tier environments. It gives an example implementation of that abstraction and points out alternative implementations and tradeoffs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995430","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995430","","Humans;Web server;Logic;Transaction databases;Contracts;Safety","software reliability;client-server systems;transaction processing;software architecture;formal specification;database management systems","end-to-end reliability;three-tier architectures;front-end clients;middle-tier application servers;business logic;back-end databases;replication;transaction-processing;specification;Exactly-Once Transaction;e-Transaction;safety;liveness;software fault-tolerance;client server systems","","32","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional Dependability Evaluation for STATEMATE","E. Bode; M. Herbstritt; H. Hermanns; S. Johr; T. Peikenkamp; R. Pulungan; J. Rakow; R. Wimmer; B. Becker","OFFIS Institute for Information Technology, Oldenburg; Albert-Ludwigs-University, Freiburg im Breisgau; Saarland University, Saarbrücken; Saarland University, Saarbrücken; OFFIS Institute for Information Technology, Oldenburg; Saarland University, Saarbrücken; Carl von Ossietzky University, Oldenburg; Albert-Ludwigs-University, Freiburg im Breisgau; Albert-Ludwigs-University, Freiburg im Breisgau","IEEE Transactions on Software Engineering","","2009","35","2","274","292","Software and system dependability is getting ever more important in embedded system design. Current industrial practice of model-based analysis is supported by state-transition diagrammatic notations such as Statecharts. State-of-the-art modelling tools like STATEMATE support safety and failure-effect analysis at design time, but restricted to qualitative properties. This paper reports on a (plug-in) extension of STATEMATE enabling the evaluation of quantitative dependability properties at design time. The extension is compositional in the way the model is augmented with probabilistic timing information. This fact is exploited in the construction of the underlying mathematical model, a uniform continuous-time Markov decision process, on which we are able to check requirements of the form: ""The probability to hit a safety-critical system configuration within a mission time of 3 hours is at most 0.01."" We give a detailed explanation of the construction and evaluation steps making this possible, and report on a nontrivial case study of a high-speed train signalling system where the tool has been applied successfully.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.102","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4711060","Real-time and embedded systems;Fault tolerance;Modeling techniques;Reliability;availability;and serviceability;Model checking;Reliability;Design notations and documentation;State diagrams;Real-time and embedded systems;Fault tolerance;Modeling techniques;Reliability;availability;and serviceability;Model checking;Reliability;Design notations and documentation;State diagrams","Embedded system;Stochastic processes;Safety;Failure analysis;Timing;Mathematical model;Communication system signaling;Fault tolerant systems;Availability;Documentation","decision theory;embedded systems;failure analysis;fault tolerance;formal specification;Markov processes;probability;safety-critical software;software performance evaluation;system monitoring","compositional dependability evaluation;embedded system design;model-based analysis;state-transition diagrammatic notation;failure-effect analysis;probabilistic timing information;uniform continuous-time Markov decision process;safety-critical system configuration;model checking;fault tolerance;statemate;functional specification","","17","","43","","","","","","IEEE","IEEE Journals & Magazines"
"A modeling framework to implement preemption policies in non-Markovian SPNs","A. Bobbio; A. Puliafito; M. Tekel","Fac. di Sci., Univ. del Piemonte, Alessandrai, Italy; NA; NA","IEEE Transactions on Software Engineering","","2000","26","1","36","54","Petri nets represent a useful tool for performance, dependability, and performability analysis of complex systems. Their modeling power can be increased even more if nonexponentially distributed events are considered. However, the inclusion of nonexponential distributions destroys the memoryless property and requires to specify how the marking process is conditioned upon its past history. We consider, in particular, the class of stochastic Petri nets whose marking process can be mapped into a Markov regenerative process. An adequate mathematical framework is developed to deal with the considered class of Markov Regenerative Stochastic Petri Nets (MRSPN). A unified approach for the solution of MRSPNs where different preemption policies can be defined in the same model is presented. The solution is provided both in steady-state and in transient condition. An example concludes the paper.","0098-5589;1939-3520;2326-3881","","10.1109/32.825765","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=825765","","Power system modeling;Stochastic processes;Petri nets;Steady-state;Transient analysis;Computer Society;Performance analysis;History;Specification languages;Stochastic systems","Petri nets;Markov processes;software performance evaluation;specification languages;formal specification","modeling framework;preemption policies;performance analysis;dependability analysis;nonexponentially distributed events;mathematical framework;Markov Regenerative Stochastic Petri Nets;steady-state condition;transient condition;specification language","","34","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling the effects of combining diverse software fault detection techniques","B. Littlewood; P. T. Popov; L. Strigini; N. Shryane","Centre for Software Reliability, City Univ., London, UK; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","12","1157","1167","Considers what happens when several different fault-finding techniques are used together. The effectiveness of such multi-technique approaches depends upon a quite subtle interplay between their individual efficacies. The modeling tool we use to study this problem is closely related to earlier work on software design diversity which showed that it would be unreasonable even to expect software versions that were developed truly independently to fail independently of one another. The key idea was a ""difficulty function"" over the input space. Later work extended these ideas to introduce a notion of ""forced"" diversity. In this paper, we show that many of these results for design diversity have counterparts in diverse fault detection in a single software version. We define measures of fault-finding effectiveness and diversity, and show how these might be used to give guidance for the optimal application of different fault-finding procedures to a particular program. The effects on reliability of repeated applications of a particular fault-finding procedure are not statistically independent; such an incorrect assumption of independence will always give results that are too optimistic. For diverse fault-finding procedures, it is possible for effectiveness to be even greater than it would be under an assumption of statistical independence. Diversity of fault-finding procedures is a good thing and should be applied as widely as possible. The model is illustrated using some data from an experimental investigation into diverse fault-finding on a railway signalling application.","0098-5589;1939-3520;2326-3881","","10.1109/32.888629","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888629","","Diversity reception;Fault detection;Software design;Application software;Redundancy;Hardware;Aerospace control;Battery powered vehicles;Software engineering;Particle measurements","system recovery;program diagnostics;software reliability;signalling;railways","software fault detection techniques;multi-technique approach;modeling tool;software design diversity;independently developed software versions;system failure;difficulty function;forced diversity;fault-finding effectiveness;repeated application reliability;diverse fault-finding procedures;statistical independence;railway signalling application;fault removal;software testing;software reliability growth","","22","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Data Mining Static Code Attributes to Learn Defect Predictors""","H. Zhang; X. Zhang","NA; NA","IEEE Transactions on Software Engineering","","2007","33","9","635","637","In this correspondence, we point out a discrepancy in a recent paper, ""data mining static code attributes to learn defect predictors,"" that was published in this journal. Because of the small percentage of defective modules, using probability of detection (pd) and probability of false alarm (pf) as accuracy measures may lead to impractical prediction models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70706","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288196","defect prediction;accuracy measures;static code attributes;empirical","Data mining;Predictive models;Accuracy;Area measurement;Q measurement;Machine learning;Training data;Information retrieval;Resource management","data mining;learning (artificial intelligence)","data mining static code attributes;defect predictors;detection probability;false alarm probability","","54","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Correction to ""Technology for testing nondeterministic client/server database applications""","Gwan-Hwan Hwang; Sheng-Jen Chang; Huey-Der Chu","National Taiwan Normal University; NA; NA","IEEE Transactions on Software Engineering","","2004","30","4","278","278","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274046","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274046","","Testing;Transaction databases;Telecommunication computing;Computer science education;Laboratories;Management information systems","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"STRPN: a Petri-net approach for modeling spatial-temporal relations between moving multimedia objects","Ping-Yu Hsu; Yuan-Bin Chang; Yen-Liang Chen","Dept. of Bus. Adm., Nat. Central Univ., Chung-li, Taiwan; Dept. of Bus. Adm., Nat. Central Univ., Chung-li, Taiwan; Dept. of Bus. Adm., Nat. Central Univ., Chung-li, Taiwan","IEEE Transactions on Software Engineering","","2003","29","1","63","76","A multimedia presentation model provides designers a tool to formally specify the temporal and spatial relationships of objects. The formality helps designers to communicate with others, to check the integrity of designs, and provides a chance to simulate the designs. Although much research has been devoted to this subject, to the best of our knowledge, no multimedia models are able to describe the spatial-temporal relations of moving objects that may refer to each other for computing displaying addresses. The addresses may be recomputed several times during the objects' lifetimes to reflect their movements. Without a formal model, designers are forced to specify the relationships in an ad hoc manner that causes misunderstanding and hampers integrity check. The check includes if an object gets its addresses in time from another object, if an object is displayed in the right places, etc. The difficulty of designing such a formal model lies in integrating temporal constraints of objects with a real-time address transferring mechanism. In this paper, we present an extended Petri-net model, which models concurrent relationships of objects with new places, transitions, and firing rules to transfer and transform addresses in real time. Its descriptive power and correctness is demonstrated by five patterns of multimedia presentations and a sample play scripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166589","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166589","","Computational modeling;Multimedia computing;Computer displays;Power engineering and energy;Video sharing;Natural languages;Humans;Petri nets","multimedia computing;Petri nets;temporal logic;synchronisation","STRPN;Petri-net approach;spatial-temporal relations;moving multimedia objects;multimedia presentation model;temporal relationships;spatial relationships;integrity check;extended Petri-Net model;concurrent relationships;descriptive power;sample play scripts","","12","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Selected papers from the second IFIP Int'l conference on formal methods for open object based distributed systems, 1997","H. Bowman; J. Derrick; E. Brinksma","The University of Kent at Canterbury; NA; NA","IEEE Transactions on Software Engineering","","2000","26","7","577","578","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.859528","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859528","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Variability Mining: Consistent Semi-automatic Detection of Product-Line Features","C. Kästner; A. Dreiling; K. Ostermann","School of Computer Science, Carnegie Mellon University; University of Magdeburg and Deutsche Bank AG, Germany; Department of Mathematics and ComputerScience at Philipps University Marburg, Germany","IEEE Transactions on Software Engineering","","2014","40","1","67","82","Software product line engineering is an efficient means to generate a set of tailored software products from a common implementation. However, adopting a product-line approach poses a major challenge and significant risks, since typically legacy code must be migrated toward a product line. Our aim is to lower the adoption barrier by providing semi-automatic tool support-called variability mining -to support developers in locating, documenting, and extracting implementations of product-line features from legacy code. Variability mining combines prior work on concern location, reverse engineering, and variability-aware type systems, but is tailored specifically for the use in product lines. Our work pursues three technical goals: (1) we provide a consistency indicator based on a variability-aware type system, (2) we mine features at a fine level of granularity, and (3) we exploit domain knowledge about the relationship between features when available. With a quantitative study, we demonstrate that variability mining can efficiently support developers in locating features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6613490","Variability;reverse engineering;mining;feature;software product line;LEADT;feature location","Feature extraction;Software;Context;Data mining;Java;Companies;Educational institutions","data mining;reverse engineering;software product lines","variability mining;consistent semi automatic detection;product line features;software product line engineering;product line approach;legacy code;semi automatic tool support;reverse engineering;variability aware type systems","","25","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Defining and applying measures of distance between specifications","L. L. Jilani; J. Desharnais; A. Mili","Inst. de Recherches en Sci., Inf. et Telecommun., Ariana, Tunisia; NA; NA","IEEE Transactions on Software Engineering","","2001","27","8","673","703","Echoing Louis Pasteur's quote, we submit the premise that it is advantageous to define measures of distance between requirements specifications because such measures open up a wide range of possibilities both in theory and in practice. The authors present a mathematical basis for measuring distances between specifications and show how their measures of distance can be used to address concrete problems that arise in the practice of software engineering.","0098-5589;1939-3520;2326-3881","","10.1109/32.940565","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=940565","","Software measurement;Application software;Computer Society;Software engineering;Lattices;Software libraries;Concrete;Upper bound;Kernel;Arithmetic","formal specification;software metrics;systems analysis","specification distance measures;requirements specifications;mathematical basis;concrete problems;software engineering","","17","","61","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Quality, productivity, and learning in framework-based development: an exploratory case study","Chia Hung Kao","National Cheng Kung University","IEEE Transactions on Software Engineering","","2003","29","3","288","288","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183941","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183941","","Productivity;Computer aided software engineering;Statistical analysis;Performance evaluation;Testing;Error correction;Cities and towns","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Estimating bounds on the reliability of diverse systems","P. Popov; L. Strigini; J. May; S. Kuball","Centre for Software Reliability, City Univ., London, UK; Centre for Software Reliability, City Univ., London, UK; NA; NA","IEEE Transactions on Software Engineering","","2003","29","4","345","359","We address the difficult problem of estimating the reliability of multiple-version software. The central issue is the degree of statistical dependence between failures of diverse versions. Previously published models of failure dependence described what behavior could be expected ""on average"" from a pair of ""independently generated"" versions. We focus instead on predictions using specific information about a given pair of versions. The concept of ""variation of difficulty"" between situations to which software may be subject is central to the previous models cited, and it turns out to be central for our question as well. We provide new understanding of various alternative imprecise estimates of system reliability and some results of practical use, especially with diverse systems assembled from pre-existing (e.g., ""off-the-shelf"") subsystems. System designers, users, and regulators need useful bounds on the probability of system failure. We discuss how to use reliability data about the individual diverse versions to obtain upper bounds and other useful information for decision making. These bounds are greatly affected by how the versions' probabilities of failure vary between subdomains of the demand space or between operating regimes-it is even possible in some cases to demonstrate, before operation, upper bounds that are very close to the true probability of failure of the system-and by the level of detail with which these variations are documented in the data.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1191798","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1191798","","Software safety;Regulators;Upper bound;Computer errors;Application software;Hardware;Reliability;Assembly systems;Decision making;Fault tolerant systems","redundancy;software reliability","diverse system reliability bounds estimation;multiple-version software reliability;multiversion software reliability;statistical dependence;software failures;failure dependence;difficulty variation;system designers;system users;system regulators;decision making","","23","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Integrating formal verification and conformance testing for reactive systems","C. Constant; T. Jéron; H. Marchand; V. Rusu","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","8","558","574","In this paper, we describe a methodology integrating verification and conformance testing. A specification of a system - an extended input-output automaton, which may be infinite-state - and a set of safety properties (""nothing bad ever happens"") and possibility properties (""something good may happen"") are assumed. The properties are first tentatively verified on the specification using automatic techniques based on approximated state-space exploration, which are sound, but, as a price to pay for automation, are not complete for the given class of properties. Because of this incompleteness and of state-space explosion, the verification may not succeed in proving or disproving the properties. However, even if verification did not succeed, the testing phase can proceed and provide useful information about the implementation. Test cases are automatically and symbolically generated from the specification and the properties and are executed on a black-box implementation of the system. The test execution may detect violations of conformance between implementation and specification; in addition, it may detect violation/satisfaction of the properties by the implementation and by the specification. In this sense, testing completes verification. The approach is illustrated on simple examples and on a bounded retransmission protocol.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70707","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267026","","Formal verification;System testing;Safety;Formal specifications;Automata;Automation;Explosions;Automatic testing;Protocols;Performance evaluation","automata theory;conformance testing;formal specification;formal verification","formal verification;conformance testing;reactive systems;formal specification;extended input-output automaton;infinite-state;safety properties;possibility properties;approximated state-space exploration;bounded retransmission protocol","","28","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A decentralized self-adaptation mechanism for service-based applications in the cloud","V. Nallur; R. Bahsoon","University of Birmingham, Birmingham; University of Birmingham, Birmingham","IEEE Transactions on Software Engineering","","2013","39","5","591","612","Cloud computing, with its promise of (almost) unlimited computation, storage, and bandwidth, is increasingly becoming the infrastructure of choice for many organizations. As cloud offerings mature, service-based applications need to dynamically recompose themselves to self-adapt to changing QoS requirements. In this paper, we present a decentralized mechanism for such self-adaptation, using market-based heuristics. We use a continuous double-auction to allow applications to decide which services to choose, among the many on offer. We view an application as a multi-agent system and the cloud as a marketplace where many such applications self-adapt. We show through a simulation study that our mechanism is effective for the individual application as well as from the collective perspective of all applications adapting at the same time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6249687","Self-adaptation;market based;multi-agent systems","Quality of service;Pricing;Reliability;Resource management;Measurement;Adaptation models;Cloud computing","cloud computing;electronic commerce;multi-agent systems;quality of service","decentralized self-adaptation mechanism;service-based applications;cloud computing;QoS requirements;market-based heuristics;continuous double-auction;multiagent system","","35","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction to the Special Section from the International Conference on Software Maintenance and Evolution","D. Binkley; R. Koschke; S. Mancoridis","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","797","798","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70765","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4375378","","Software maintenance;Testing;Conferences;Computer industry;Government;Predictive models;Software engineering;Software systems;Buildings;Computer architecture","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"A classification of noncircular attribute grammars based on the look-ahead behavior","Wuu Yang","Dept. of Comput. & Inf. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan","IEEE Transactions on Software Engineering","","2002","28","3","210","227","We propose a family of static evaluators for subclasses of the well-defined (i.e., noncircular) attribute grammars. These evaluators augment the evaluator for the absolutely noncircular attribute grammars with look-ahead behaviors. Because this family covers exactly the set of all well-defined attribute grammars, well-defined attribute grammars may be classified into a hierarchy, called the NC hierarchy, according to their evaluators in the family. The location of a noncircular attribute grammar in the NC hierarchy is an intrinsic property of the grammar. The NC hierarchy confirms a result of Riis and Skyum (1981), which says that all well-defined attribute grammars allow a (static) pure multivisit evaluator by actually constructing such an evaluator. We also show that, for any finite m, an NC(m) attribute grammar can be transformed to an equivalent NC(0) grammar.","0098-5589;1939-3520;2326-3881","","10.1109/32.991318","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=991318","","Formal languages","attribute grammars;trees (mathematics)","static evaluators;noncircular attribute grammars;look-ahead behavior;syntax tree;NC hierarchy;pure multivisit evaluator;ordered attribute grammars;grammar classification","","","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Supertotal function definition in mathematics and software engineering","R. Boute","INTEC, Ghent Univ., Belgium","IEEE Transactions on Software Engineering","","2000","26","7","662","672","In engineering (including computing), mathematics and logic, expressions can arise that contain function applications where the argument is outside the function's domain. Such a situation need not represent a conceptual error, for instance, in conditional expressions, but it is traditionally considered a type error. Various solutions can be found in the literature based on the notion of partial function and/or a distinguished value undefined. However, these have rather pervasive effects, complicating function definition, sacrificing convenient algebraic laws of logical operators and/or Leibniz's rule, one of the most valuable assets in formal reasoning (especially in the calculational style). Other solutions have in common the realization that well-structured mathematical arguments are always guarded by conditions and that the value of A/spl rArr/B is not affected by domain violations in B in case-A. These solutions preserve Leibniz's rule and the standard meaning of the logical operators. In this second category, we propose the simplest possible solution, called supertotal function definition, and consisting of assigning the value false (or 0, depending on the preferred formalism) to any function application where the argument is outside the domain. This approach assumes the notion of function with which a domain is associated as a part of its specification. Ramifications regarding formal reasoning, use in software engineering (such as Parnas's predicate calculus) and in mathematical formulation in general are discussed. The proposed solution justifies formal reasoning as usual, but with increased freedom in expressions regarding types of function arguments. Hence, it can be adopted in existing formalisms with very minor changes to the latter, As a bonus, this discussion includes a very simple new view on conditional expressions, yielding unusually powerful and convenient calculational properties. Finally, differences and advantages w.r.t. other approaches are pointed out.","0098-5589;1939-3520;2326-3881","","10.1109/32.859534","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859534","","Mathematics;Software engineering;Application software;Calculus;Logic;Software standards","formal specification","function definition;software engineering;mathematics;formal methods;software specification;predicate calculus;calculational reasoning;functional mathematics;guarded formulas;conditional expressions;undefinedness;type correctness;subtyping","","1","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Formal methods application: an empirical tale of software development","A. E. K. Sobel; M. R. Clarkson","Dept. of Comput. Sci. & Syst. Anal., Miami Univ., Oxford, OH, USA; NA","IEEE Transactions on Software Engineering","","2002","28","3","308","320","The development of an elevator scheduling system by undergraduate students is presented. The development was performed by 20 teams of undergraduate students, divided into two groups. One group produced specifications by employing a formal method that involves only first-order logic. The other group used no formal analysis. The solutions of the groups are compared using the metrics of code correctness, conciseness, and complexity. Particular attention is paid to a subset of the formal methods group which provided a full verification of their implementation. Their results are compared to other published formal solutions. The formal methods group's solutions are found to be far more correct than the informal solutions.","0098-5589;1939-3520;2326-3881","","10.1109/32.991322","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=991322","","Application software;Programming","formal specification;object-oriented programming;computer science education;formal logic;software metrics","formal methods;software development;elevator scheduling system;undergraduate students;formal specifications;first-order logic;software metrics;object oriented design;code correctness;software engineering curriculum","","26","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Enhancing structured review with model-based verification","I. Traore; D. B. Aredo","Dept. of Electr. & Comput. Eng., Victoria Univ., BC, Canada; NA","IEEE Transactions on Software Engineering","","2004","30","11","736","753","We propose a development framework that extends the scope of structured review by supplementing the structured review with model-based verification. The proposed approach uses the Unified Modeling Language (UML) as a modeling notation. We discuss a set of correctness arguments that can be used in conjunction with formal verification and validation (V&V) in order to improve the quality and dependability of systems in a cost-effective way. Formal methods can be esoteric; consequently, their large scale application is hindered. We propose a framework based on the integration of lightweight formal methods and structured reviews. Moreover, we show that structured reviews enable us to handle aspects of V&V that cannot be fully automated. To demonstrate the feasibility of our approach, we have conducted a study on a security-critical system - a patient document service (PDS) system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359768","Index Terms- Structured review;formal methods;UML;prototype verification system (PVS);OCL;model-based verification;validation and verification.","Quality assurance;Unified modeling language;Software quality;Formal verification;Large-scale systems;Costs;Guidelines;Software prototyping;Prototypes","formal specification;formal verification;Unified Modeling Language;program testing;safety-critical software","structured review;model-based verification;Unified Modeling Language;formal verification;formal validation;security-critical system;patient document service system;prototype verification system;object constraint language","","11","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Monitoring Data Usage in Distributed Systems","D. Basin; M. Harvan; F. Klaedtke; E. Zalinescu","ETH Zurich, Zurich; ETH Zurich, Zurich; ETH Zurich, Zurich; ETH Zurich, Zurich","IEEE Transactions on Software Engineering","","2013","39","10","1403","1426","IT systems manage increasing amounts of sensitive data and there is a growing concern that they comply with policies that regulate data usage. In this paper, we use temporal logic to express policies and runtime monitoring to check system compliance. While well-established methods for monitoring linearly ordered system behavior exist, a major challenge is monitoring distributed and concurrent systems where actions are locally observed in the different system parts. These observations can only be partially ordered, while policy compliance may depend on the actions' actual order of appearance. Technically speaking, it is in general intractable to check compliance of partially ordered traces. We identify fragments of our policy specification language for which compliance can be checked efficiently, namely, by monitoring a single representative trace in which the observed actions are totally ordered. Through a case study we show that the fragments are capable of expressing nontrivial policies and that monitoring representative traces is feasible on real-world data.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6493331","Monitors;temporal logic;verification;distributed systems;regulation","Monitoring;Cost accounting;Periodic structures;Semantics;Distributed databases;Standards;Finite element analysis","concurrency control;formal verification;specification languages;temporal logic","data usage monitoring;distributed systems;IT systems;information technology systems;data usage regulation;temporal logic;runtime monitoring;system compliance;concurrent systems;policy compliance;policy specification language","","8","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Performance Specification and Evaluation with Unified Stochastic Probes and Fluid Analysis","R. A. Hayden; J. T. Bradley; A. Clark","Imperial College London, London; Imperial College London, London; University of Edinburgh, Edinburgh","IEEE Transactions on Software Engineering","","2013","39","1","97","118","Rapid and accessible performance evaluation of complex software systems requires two critical features: the ability to specify useful performance metrics easily and the capability to analyze massively distributed architectures, without recourse to large compute clusters. We present the unified stochastic probe, a performance specification mechanism for process algebra models that combines many existing ideas: state and action-based activation, location-based specification, many-probe specification, and immediate signaling. These features, between them, allow the precise and compositional construction of complex performance measurements. The paper shows how a subset of the stochastic probe language can be used to specify common response-time measures in massive process algebra models. The second contribution of the paper is to show how these response-time measures can be analyzed using so-called fluid techniques to produce rapid results. In doing this, we extend the fluid approach to incorporate immediate activities and a new type of response-time measure. Finally, we calculate various response-time measurements on a complex distributed wireless network of O(10<sup>129</sup>) states in size.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6133297","Performance modeling;performance evaluation tools;stochastic process algebra;measurement probes;fluid approximation;passage-time analysis","Probes;Stochastic processes;Analytical models;Algebra;Computational modeling;Semantics;Syntactics","formal specification;process algebra;software metrics;software performance evaluation","performance specification mechanism;performance evaluation mechanism;unified stochastic probes;fluid analysis;software system;performance metrics;process algebra model;state-based activation;action-based activation;location-based specification;many-probe specification;immediate signaling;stochastic probe language;common response-time measure;complex distributed wireless network","","9","","29","","","","","","IEEE","IEEE Journals & Magazines"
"A multilayer client-server queueing network model with synchronous and asynchronous messages","Sridhar Ramesh; H. G. Perros","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA","IEEE Transactions on Software Engineering","","2000","26","11","1086","1100","We analyze a multilayered queueing network that models a client-server system where clients and servers communicate via synchronous and asynchronous messages. The servers are organized in groups such that they form a multilayered hierarchical structure. The queueing network is approximately analyzed using a decomposition algorithm. Numerical tests show that the approximation algorithm has a good accuracy.","0098-5589;1939-3520;2326-3881","","10.1109/32.881719","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=881719","","Nonhomogeneous media;Queueing analysis;Client-server systems;Network servers;Software performance;Computer architecture;Software systems;Algorithm design and analysis;Testing;Approximation algorithms","client-server systems;message passing;queueing theory;software performance evaluation;multi-threading","multilayer client-server queueing network model;synchronous messages;asynchronous messages;multilayered queueing network;client-server system;multilayered hierarchical structure;decomposition algorithm;approximation algorithm","","18","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Performance Model Estimation and Tracking Using Optimal Filters","T. Zheng; C. M. Woodside; M. Litoiu","Carleton University, Ottawa; Carleton University, Ottawa; IBM Centre for Advanced Studeis, Toronto","IEEE Transactions on Software Engineering","","2008","34","3","391","406","To update a performance model, its parameter values must be updated, and in some applications (such as autonomic systems) tracked continuously over time. Direct measurement of many parameters during system operation requires instrumentation which is impractical. Kalman filter estimators can track such parameters using other data such as response times and utilizations, which are readily observable. This paper adapts Kalman filter estimators for performance model parameters, evaluates the approximations which must be made, and develops a systematic approach to setting up an estimator. The estimator converges under easily verified conditions. Different queueing-based models are considered here, and the extension for state-based models (such as stochastic Petri nets) is straightforward.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4515874","Modeling techniques;Measurement;Performance model;Kalman filtering;Parameter tracking;Modeling techniques;Measurement;Performance model;Kalman filtering;Parameter tracking","Filters;Parameter estimation;Delay;State estimation;Instruments;Stochastic processes;Time varying systems;Recursive estimation;Predictive models;Petri nets","Kalman filters;parameter estimation;software performance evaluation","optimal filters;performance model estimation;performance model tracking;Kalman filter estimators;queuing-based models","","56","","34","","","","","","IEEE","IEEE Journals & Magazines"
"On formalization of the whole-part relationship in the Unified Modeling Language","Hee Beng Kuan Tan; Lun Hao; Yong Yang","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","IEEE Transactions on Software Engineering","","2003","29","11","1054","1055","F. Barbier et al. (2003) introduced a formal definition for the semantics of the whole-part relationship in the Unified Modeling Language (UML). This paper reports some discrepancies that appeared previously and proposes solutions to these discrepancies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245307","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245307","","Unified modeling language;Mathematics;Qualifications","specification languages","whole-part relationship;Unified Modeling Language","","3","","3","","","","","","IEEE","IEEE Journals & Magazines"
"An automatic class generation mechanism by using method integration","K. Maruyama; K. I. Shima","Media Technol. Dev. Center, NTT Commun. Corp., Tokyo, Japan; NA","IEEE Transactions on Software Engineering","","2000","26","5","425","440","The paper presents a mechanism for automatically generating new classes from classes existing in a library by using their modification histories. To generate classes that are likely to meet a programmer's requirements and that are consistent with the existing classes, we propose three actors: a Specifier, a Finder, and an integrator. The Specifier records the history of modifications between methods with the same interface of a parent class and its heir. If the required method is not defined in the existing class which a programmer is referring to, the Finder retrieves classes similar to the referenced class and the Integrator applies the past modifications of similar classes to the referenced class. Classes are determined to be similar, based on their positions in a class hierarchy tree. Both the Specifier and Integrator are achieved by using a method integration algorithm based on object oriented bounded program slicing and class dependence graph matching. This mechanism enables programmers to reuse classes with little or no modification, and thus, easily create object oriented programs.","0098-5589;1939-3520;2326-3881","","10.1109/32.846300","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846300","","Programming profession;History;Mobile communication;Computer Society;Tree graphs;Costs;Object oriented programming;Software libraries;Laboratories","object-oriented programming;software libraries;program slicing;automatic programming;graph theory","automatic class generation mechanism;method integration;library classes;modification histories;Specifier;Finder;integrator;parent class;past modifications;referenced class;class hierarchy tree;method integration algorithm;object oriented bounded program slicing;class dependence graph matching;class reuse;object oriented programs","","1","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""An Interval Logic for Real-Time System Specification""","C. A. Furia; A. Morzenti; M. Pradella; M. G. Rossi","Dipt. di Elettronica e Informazione, Politecnico di Milano, Milan, Italy; Dipt. di Elettronica e Informazione, Politecnico di Milano, Milan, Italy; NA; NA","IEEE Transactions on Software Engineering","","2006","32","6","424","427","The paper ""An Interval Logic for Real-Time System Specification"" (Mattolini and Nesi, IEEE Trans. Software Eng., vol. 27, no. 3, pp. 208-227, Mar. 2001) presents the TILCO specification language and compares it to other existing similar languages. In this comment, we show that several of the logic formulas used for the comparison are flawed and/or overly complicated and we explain why, in this respect, the comparison is moot","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650216","Formal methods;temporal logic;real-time systems.","Logic;Real time systems;Resource management;Specification languages","formal specification;real-time systems;specification languages;temporal logic","interval logic;real-time system specification;TILCO specification language;logic formulas","","","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Clarifications on the Construction and Use of the ManyBugs Benchmark","C. Le Goues; Y. Brun; S. Forrest; W. Weimer","School of Computer Science, Carnegie Mellon University, Pittsburgh, PA; College of Information and Computer Science, University of Massachusetts at Amherst, Amherst, MA; Department of Computer Science, University of New Mexico, Albuquerque, NM; Computer Science and Engineering, University of Michigan, Ann Arbor, MI","IEEE Transactions on Software Engineering","","2017","43","11","1089","1090","Automated repair techniques produce variant php interpreters, which should naturally serve as the tested interpreters. However, the answer to the question of what should serve as the testing interpreter is less obvious. php's default test harness configuration uses the same version of the interpreter for both the tested and testing interpreter. However, php may be configured via a command-line argument to use a different interpreter, such as the unmodified defective version, or a separate, manually-repaired version.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2755651","US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8048536","","Maintenance engineering;Benchmark testing;Computer science;Electronic mail;Software engineering;Software","program debugging","ManyBugs benchmark;automated repair techniques;test harness configuration;testing interpreter","","","","7","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Addendum to ""Locating features in source code""","D. Bojic; T. Eisenbarth; R. Koschke; D. Simon; D. Velasevic","Fac. of Electr. Eng., Belgrade Univ., Serbia; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","2","140","","For original paper by T. Eisenbarth et al. see ibid., vol.29, no.3, p.210-24 (2003). We compare three approaches that apply formal concept analysis on execution profiles. This survey extends the discourse of related research by Bojic and Velasevic (2000).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265818","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265818","","Testing;Software architecture;Lattices;Software engineering;Information analysis;Unified modeling language;Software packages;Packaging","program diagnostics;software architecture;formal specification","formal concept analysis;feature location;source coding;program analysis;software architecture recovery","","1","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Model Checking Semantically Annotated Services","I. Di Pietro; F. Pagliarecci; L. Spalazzi","Universit&#x0E0; Politecnica delle Marche, Ancona; Universit&#x0E0; Politecnica delle Marche, Ancona; Universit&#x0E0; Politecnica delle Marche, Ancona","IEEE Transactions on Software Engineering","","2012","38","3","592","608","Model checking is a formal verification method widely accepted in the web service world because of its capability to reason about service behavior at process level. It has been used as a basic tool in several scenarios such as service selection, service validation, and service composition. The importance of semantics is also widely recognized. Indeed, there are several solutions to the problem of providing semantics to web services, most of them relying on some form of Description Logic. This paper presents an integration of model checking and semantic reasoning technologies in an efficient way. This can be considered the first step toward the use of semantic model checking in problems of selection, validation, and composition. The approach relies on a representation of services at process level that is based on semantically annotated state transition systems (asts) and a representation of specifications based on a semantically annotated version of computation tree logic (anctl). This paper proves that the semantic model checking algorithm is sound and complete and can be accomplished in polynomial time. This approach has been evaluated with several experiments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680919","Formal methods;model checking;temporal logic;description logic;intelligent web services;semantic web;web services.","Web services;Semantics;Ontologies;Switches;Computational modeling;Biological system modeling;Syntactics","computational complexity;formal verification;semantic Web;temporal logic;trees (mathematics);Web services","formal verification method;Web service;description logic;semantic reasoning technologies;semantic model checking;annotated state transition systems;computation tree logic;polynomial time;temporal logic;semantically annotated services","","11","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Ant Colony Optimization for Software Project Scheduling and Staffing with an Event-Based Scheduler","W. Chen; J. Zhang","Sun Yat-sen University, Guangzhou; Sun Yat-sen University, Guangzhou","IEEE Transactions on Software Engineering","","2013","39","1","1","17","Research into developing effective computer aided techniques for planning software projects is important and challenging for software engineering. Different from projects in other fields, software projects are people-intensive activities and their related resources are mainly human resources. Thus, an adequate model for software project planning has to deal with not only the problem of project task scheduling but also the problem of human resource allocation. But as both of these two problems are difficult, existing models either suffer from a very large search space or have to restrict the flexibility of human resource allocation to simplify the model. To develop a flexible and effective model for software project planning, this paper develops a novel approach with an event-based scheduler (EBS) and an ant colony optimization (ACO) algorithm. The proposed approach represents a plan by a task list and a planned employee allocation matrix. In this way, both the issues of task scheduling and employee allocation can be taken into account. In the EBS, the beginning time of the project, the time when resources are released from finished tasks, and the time when employees join or leave the project are regarded as events. The basic idea of the EBS is to adjust the allocation of employees at events and keep the allocation unchanged at nonevents. With this strategy, the proposed method enables the modeling of resource conflict and task preemption and preserves the flexibility in human resource allocation. To solve the planning problem, an ACO algorithm is further designed. Experimental results on 83 instances demonstrate that the proposed method is very promising.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6165315","Software project planning;project scheduling;resource allocation;workload assignment;ant colony optimization (ACO)","Software;Resource management;Planning;Humans;Project management;Job shop scheduling;Search problems","ant colony optimisation;human resource management;planning (artificial intelligence);project management;scheduling;software management","ant colony optimization algorithm;software project scheduling;software project staffing;event-based scheduler;computer aided techniques;software project planning;software engineering;project task scheduling problem;human resource allocation problem;EBS;ACO;task list;planned employee allocation matrix;resource conflict modeling;task preemption modeling","","68","","51","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient state space generation for the analysis of real-time systems","I. Kang; I. Lee; Young-Si Kim","Sch. of Comput., Soongsil Univ., Seoul, South Korea; NA; NA","IEEE Transactions on Software Engineering","","2000","26","5","453","477","State explosion is a well-known problem that impedes analysis and testing based on state-space exploration. This problem is particularly serious in real time systems because unbounded time values cause the state space to be infinite even for simple systems. The author presents an algorithm that produces a compact representation of the reachable state space of a real time system. The algorithm yields a small state space, but still retains enough information for analysis. To avoid the state explosion which can be caused by simply adding time values to states, our algorithm uses history equivalence and transition bisimulation to collapse states into equivalent classes. Through history equivalence, states are merged into an equivalence class with the same untimed executions up to the states. Using transition bisimulation, the states that have the same future behaviors are further collapsed. The resultant state space is finite and can be used to analyze real time properties. To show the effectiveness of our algorithm, we have implemented the algorithm and have analyzed several example applications.","0098-5589;1939-3520;2326-3881","","10.1109/32.846302","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846302","","State-space methods;Real time systems;Automata;Safety;Explosions;Reachability analysis;Logic testing;System testing;Algorithm design and analysis;History","state-space methods;real-time systems;systems analysis;formal specification;bisimulation equivalence;equivalence classes;reachability analysis","state space generation;real time systems analysis;state explosion;state-space exploration;unbounded time values;compact representation;reachable state space;time values;history equivalence;transition bisimulation;equivalent classes;untimed executions;future behaviors;real time properties","","11","","30","","","","","","IEEE","IEEE Journals & Magazines"
"The Effectiveness of Software Diversity in a Large Population of Programs","M. J. P. van der Meulen; M. A. Revilla","Det Norske Veritas, Høvik; University of Valladolid, Valladolid","IEEE Transactions on Software Engineering","","2008","34","6","753","764","In this paper, we first present an exploratory analysis of the aspects of multiple-version software diversity using 36,123, programs written to the same specification. We do so within the framework of the theories of Eckhardt and Lee and Littlewood and Miller. We analyse programming faults made, explore failure regions and difficulty functions, show how effective 1-out-of-2 diversity is and how language diversity increases this effectiveness. The second part of the paper generalizes the findings about 1-out-of-2 diversity, and its special case language diversity by performing statistical analyses of 89,402 programs written to 60 specifications. Most observations in the exploratory analysis are confirmed; however, although the benefit of language diversity can be observed, its effectiveness appears to be low.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4604670","Programming Techniques;Protection mechanisms;Design concepts;Quality analysis and evaluation;Software and System Safety;Reliability;Reliability;Performance measures;Programming Techniques;Protection mechanisms;Design concepts;Quality analysis and evaluation;Software and System Safety;Reliability;Reliability;Performance measures","Statistical analysis;Software reliability;Failure analysis;Functional programming;Fault tolerance;Software testing;Java;Algorithm design and analysis;Reliability engineering","software fault tolerance;statistical analysis","software diversity;exploratory analysis;programming fault;software failure;language diversity;1-out-of-2 diversity;statistical analysis;software reliability","","15","","12","","","","","","IEEE","IEEE Journals & Magazines"
"An introduction to rapid system prototyping","F. Kordon; Luqi","Comput. Sci. Dept., Univ. Pierre et Marie Curie, Paris, France; NA","IEEE Transactions on Software Engineering","","2002","28","9","817","821","The implementation and maintenance of industrial applications have continuously become more and more difficult. In this context, one problem is the evaluation of complex systems. The IEEE defines prototyping as a development approach promoting the implementation of a pilot version of the intended product. This approach is a potential solution to the early evaluation of a system. It can also be used to avoid the shift between the description/specification of a system and its implementation. This brief introduction to the special section on rapid system prototyping illustrates a current picture of prototyping.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033222","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033222","","Prototypes;Software prototyping;Hardware;Conferences;Application software;Textile industry;Design engineering;Computer industry;Computer bugs","software prototyping","rapid system prototyping;pilot version;system description;system specification","","25","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Whole Test Suite Generation","G. Fraser; A. Arcuri","Saarland University, Saarbrücken; Simula Research Laboratory, Lysaker","IEEE Transactions on Software Engineering","","2013","39","2","276","291","Not all bugs lead to program crashes, and not always is there a formal specification to check the correctness of a software test's outcome. A common scenario in software testing is therefore that test data are generated, and a tester manually adds test oracles. As this is a difficult task, it is important to produce small yet representative test sets, and this representativeness is typically measured using code coverage. There is, however, a fundamental problem with the common approach of targeting one coverage goal at a time: Coverage goals are not independent, not equally difficult, and sometimes infeasible-the result of test generation is therefore dependent on the order of coverage goals and how many of them are feasible. To overcome this problem, we propose a novel paradigm in which whole test suites are evolved with the aim of covering all coverage goals at the same time while keeping the total size as small as possible. This approach has several advantages, as for example, its effectiveness is not affected by the number of infeasible targets in the code. We have implemented this novel approach in the EvoSuite tool, and compared it to the common approach of addressing one goal at a time. Evaluated on open source libraries and an industrial case study for a total of 1,741 classes, we show that EvoSuite achieved up to 188 times the branch coverage of a traditional approach targeting single branches, with up to 62 percent smaller test suites.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6152257","Search-based software engineering;length;branch coverage;genetic algorithm;infeasible goal;collateral coverage","Software;Genetic algorithms;Search problems;Arrays;Genetic programming;Software testing","formal specification;program debugging;program testing","whole test suite generation;program crashes;formal specification;software testing;code coverage;EvoSuite tool;program debugging","","136","","52","","","","","","IEEE","IEEE Journals & Magazines"
"MOSES: A Framework for QoS Driven Runtime Adaptation of Service-Oriented Systems","V. Cardellini; E. Casalicchio; V. Grassi; S. Iannucci; F. L. Presti; R. Mirandola","University of Roma &#x0022;Tor Vergata&#x0022;, Roma; University of Roma &#x0022;Tor Vergata&#x0022;, Roma; University of Roma &#x0022;Tor Vergata&#x0022;, Roma; University of Roma &#x0022;Tor Vergata&#x0022;, Roma; University of Roma &#x0022;Tor Vergata&#x0022;, Roma; Politecnico di Milano, Milano","IEEE Transactions on Software Engineering","","2012","38","5","1138","1159","Architecting software systems according to the service-oriented paradigm and designing runtime self-adaptable systems are two relevant research areas in today's software engineering. In this paper, we address issues that lie at the intersection of these two important fields. First, we present a characterization of the problem space of self-adaptation for service-oriented systems, thus providing a frame of reference where our and other approaches can be classified. Then, we present MOSES, a methodology and a software tool implementing it to support QoS-driven adaptation of a service-oriented system. It works in a specific region of the identified problem space, corresponding to the scenario where a service-oriented system architected as a composite service needs to sustain a traffic of requests generated by several users. MOSES integrates within a unified framework different adaptation mechanisms. In this way it achieves greater flexibility in facing various operating environments and the possibly conflicting QoS requirements of several concurrent users. Experimental results obtained with a prototype implementation of MOSES show the effectiveness of the proposed approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5963694","Service-oriented architecture;runtime adaptation;quality of service","Service oriented architecture;Quality of service;Runtime;Concrete;Semiconductor optical amplifiers;Adaptation models;Software systems","service-oriented architecture","MOSES;QoS driven runtime adaptation;service oriented system;software system architecture;service oriented paradigm;runtime self adaptable system;software engineering;self adaptation;QoS-driven adaptation;service-oriented system","","68","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing Software Service Quality and Trustworthiness at Selection Time","N. Limam; R. Boutaba","POSTECH-Pohang University of Science and Technology, Pohang; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2010","36","4","559","574","The integration of external software in project development is challenging and risky, notably because the execution quality of the software and the trustworthiness of the software provider may be unknown at integration time. This is a timely problem and of increasing importance with the advent of the SaaS model of service delivery. Therefore, in choosing the SaaS service to utilize, project managers must identify and evaluate the level of risk associated with each candidate. Trust is commonly assessed through reputation systems; however, existing systems rely on ratings provided by consumers. This raises numerous issues involving the subjectivity and unfairness of the service ratings. This paper describes a framework for reputation-aware software service selection and rating. A selection algorithm is devised for service recommendation, providing SaaS consumers with the best possible choices based on quality, cost, and trust. An automated rating model, based on the expectancy-disconfirmation theory from market science, is also defined to overcome feedback subjectivity issues. The proposed rating and selection models are validated through simulations, demonstrating that the system can effectively capture service behavior and recommend the best possible choices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.2","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383370","Software as a service (SaaS);software selection;service utility;review and rating;trust and reputation;risk management;SLA monitoring.","Software quality;Risk management;Software maintenance;Costs;Software performance;Project management;Feedback;Monitoring;Business;Computer industry","quality of service;risk management;software architecture;software quality;software selection","software service quality;selection time;project development;software provider;SaaS model;reputation-aware software service selection;automated rating model;expectancy-disconfirmation theory;software service trustworthiness","","63","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""The Model Checker SPIN""","Ki-Seok Bang; Jin-Young Choi; Chuck Yoo","Dept. of Comput. Sci. & Eng., Korea Univ., Seoul, South Korea; NA; NA","IEEE Transactions on Software Engineering","","2001","27","6","573","576","The paper by G.J. Holzmann (see ibid., vol.23, no.5, p.279-95, 1997) describes how to apply SPIN to the verification of a synchronization algorithm (L.M. Ruane, 1990) in process scheduling of an operating system. We report an error in the verification model presented by G.J. Holzmann and present a revised model with verification result. Our result explains the reason why SPIN found the race condition in the synchronization algorithm. We also show that the suggested fix by G.J. Holzmann is incorrect.","0098-5589;1939-3520;2326-3881","","10.1109/32.926177","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=926177","","Software algorithms;Scheduling algorithm;Operating systems;Availability;Computer science","scheduling;operating systems (computers);synchronisation;hazards and race conditions;program verification","Model Checker;SPIN;verification model;verification result;race condition;synchronization algorithm;ACSR;LTL;process scheduling;operating system","","1","","7","","","","","","IEEE","IEEE Journals & Magazines"
"Modular Information Hiding and Type-Safe Linking for C","S. Srivastava; M. Hicks; J. S. Foster; P. Jenkins","University of Maryland at College Park, College Park; University of Maryland at College Park, College Park; University of Maryland at College Park, College Park; University of Maryland at College Park, College Park","IEEE Transactions on Software Engineering","","2008","34","3","357","376","This paper presents CMod, a novel tool that provides a sound module system for C. CMod works by enforcing four rules that are based on principles of modular reasoning and on current programming practice. CMod's rules flesh out the convention that .h header files are module interfaces and .c source files are module implementations. Although this convention is well-known, existing explanations of it are incomplete, omitting important subtleties needed for soundness. In contrast, we have proven formally that CMod's rules enforce both information hiding and type-safe linking. To use CMod, the programmer develops and builds their software as usual, redirecting the compiler and linker to CMod's wrappers. We evaluated CMod by applying it to 30 open source programs, totaling more than one million LoC. Violations to CMod's rules revealed more than a thousand information hiding errors, dozens of typing errors, and hundreds of cases that, although not currently bugs, make programming mistakes more likely as the code evolves. At the same time, programs generally adhere to the assumptions underlying CMod's rules, and so we could fix rule violations with a modest effort. We conclude that CMod can effectively support modular programming in C: it soundly enforces type-safe linking and information-hiding while being largely compatible with existing practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4497211","Coding Tools and Techniques;Modules;packages;Reliability;Code design;Information hiding;Coding Tools and Techniques;Modules;packages;Reliability;Code design;Information hiding","Joining processes;Programming profession;Program processors;Computer errors;Computer bugs;Open source software;Software packages;Packaging;Software reliability;Software safety","C language;data encapsulation;object-oriented programming;program compilers;public domain software;software reusability","modular information hiding;type-safe linking;CMOD;modular reasoning;compiler;open source programs;C language","","1","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Errata for ""Discovering Documentation for Java Container Classes"" [Aug 07 526-543]","J. Henkel; C. Reichenbach; A. Diwan","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","303","303","In the above titled paper (ibid., vol. 33, no. 8, pp. 526-543, Aug 07), there were several mistakes. The corrections are presented here.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4476755","","Documentation;Java;Containers;Equations;Computer science","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Learning Assumptions for CompositionalVerification of Timed Systems","S. Lin; É. André; Y. Liu; J. Sun; J. S. Dong","Temasek Laboratories, National University of Singapore, 5A, Engineering Drive 1, #09-02, Singapore; Université Paris 13, Sorbonne Paris Cité, Laboratoire d’Informatique de Paris-Nord (LIPN), A204,, Institut Galilée, 99 avenue Jean-Baptiste Clément, 93430 Villetaneuse, CNRS, UMR 7030, Villetaneuse, France; School of Computer Engineering , Nanyang Technological University, 50 Nanyang Avenue, Singapore; Singapore University of Technology and Design, BLK1, Level 3, West Wing, Room 9, 20 Dover Drive, Singapore; Computer Science Department, School of Computing, National University of Singapore, 13 Computing Drive, Singapore","IEEE Transactions on Software Engineering","","2014","40","2","137","153","Compositional techniques such as assume-guarantee reasoning (AGR) can help to alleviate the state space explosion problem associated with model checking. However, compositional verification is difficult to be automated, especially for timed systems, because constructing appropriate assumptions for AGR usually requires human creativity and experience. To automate compositional verification of timed systems, we propose a compositional verification framework using a learning algorithm for automatic construction of timed assumptions for AGR. We prove the correctness and termination of the proposed learning-based framework, and experimental results show that our method performs significantly better than traditional monolithic timed model checking.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6682903","Automatic assume-guarantee reasoning;model checking;timed systems","Model checking;Educational institutions;Explosions;Learning automata;Atomic clocks;Cognition","formal verification;inference mechanisms;learning (artificial intelligence)","monolithic timed model checking;learning-based framework;timed assumptions;learning algorithm;state space explosion problem;AGR techniques;assume-guarantee reasoning techniques;timed systems;compositional verification framework;learning assumptions","","5","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Tuning Temporal Features within the Stochastic π-Calculus","L. Pauleve; M. Magnin; O. Roux","IRCCyN, École Centrale de Nantes; IRCCyN, École Centrale de Nantes; IRCCyN, École Centrale de Nantes","IEEE Transactions on Software Engineering","","2011","37","6","858","871","The stochastic π-calculus is a formalism that has been used for modeling complex dynamical systems where the stochasticity and the delay of transitions are important features, such as in the case of biochemical reactions. Commonly, durations of transitions within stochastic π-calculus models follow an exponential law. The underlying dynamics of such models are expressed in terms of continuous-time Markov chains, which can then be efficiently simulated and model-checked. However, the exponential law comes with a huge variance, making it difficult to model systems with accurate temporal constraints. In this paper, a technique for tuning temporal features within the stochastic π-calculus is presented. This method relies on the introduction of a stochasticity absorption factor by replacing the exponential distribution with the Erlang distribution, which is a sum of exponential random variables. This paper presents a construction of the stochasticity absorption factor in the classical stochastic π-calculus with exponential rates. Tools for manipulating the stochasticity absorption factor and its link with timed intervals for firing transitions are also presented. Finally, the model-checking of such designed models is tackled by supporting the stochasticity absorption factor in a translation from the stochastic π-calculus to the probabilistic model checker PRISM.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.95","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5611556","Temporal parameters;\pi-calculus;model-checking;Markov processes;stochastic processes.","Stochastic processes;Exponential distribution;Random variables;Analytical models","exponential distribution;formal verification;pi calculus;stochastic processes","temporal feature tuning;stochastic π-calculus;complex dynamical system modeling;biochemical reactions;continuous-time Markov chains;stochasticity absorption factor;exponential distribution;Erlang distribution;exponential random variables;probabilistic model checker;PRISM","","1","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Time domain analysis of non-Markovian stochastic Petri nets with PRI transitions","A. Horvath; M. Telek","Dept. of Telecommun., Budapest Univ. of Technol. & Econ., Hungary; Dept. of Telecommun., Budapest Univ. of Technol. & Econ., Hungary","IEEE Transactions on Software Engineering","","2002","28","10","933","943","The time domain analysis of non-Markovian stochastic Petri nets with pre-emptive repeat identical (PRI) type transitions is considered in this paper. The set of ""time domain"" equations describing the evolution of the marking process is provided. The relation of the time domain and formerly available transform domain description is discussed. Based on the time domain description of the process, a simple numerical procedure is provided to analyze the transient behavior. Two examples are calculated to illustrate the proposed numerical method.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041050","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041050","","Time domain analysis;Stochastic processes;Petri nets;Transient analysis;Terminology;Steady-state;Laplace equations;Transforms;Queueing analysis","Petri nets;time-domain analysis;stochastic processes;queueing theory;differential equations;approximation theory","time domain analysis;nonMarkovian stochastic Petri nets;preemptive repeat identical transitions;marking process;transient behavior;queuing model;differential equations;first order approximation","","3","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient interprocedural array data-flow analysis for automatic program parallelization","Junjie Gu; Zhiyuan Li","Sun Microsyst. Inc., Palo Alto, CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","3","244","261","Since sequential languages such as Fortran and C are more machine-independent than current parallel languages, it is highly desirable to develop powerful parallelization tools which can generate parallel codes, automatically or semi-automatically, targeting different parallel architectures. Array data-flow analysis is known to be crucial to the success of automatic parallelization. Such an analysis should be performed interprocedurally and symbolically and it often needs to handle the predicates represented by IF conditions. Unfortunately, such a powerful program analysis can be extremely time-consuming if it is not carefully designed. How to enhance the efficiency of this analysis to a practical level remains an issue largely untouched to date. This paper presents techniques for efficient interprocedural array data-flow analysis and documents experimental results of its implementation in a research parallelizing compiler. Our techniques are based on guarded array regions and the resulting tool runs faster, by one or two orders of magnitude, than other similarly powerful tools.","0098-5589;1939-3520;2326-3881","","10.1109/32.842950","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842950","","Data analysis;Parallel languages;Parallel architectures;Performance analysis;Privatization;Computer Society;Power generation;Concurrent computing;Text analysis;Computer applications","arrays;data structures;data flow analysis;parallelising compilers;automatic programming;parallel programming;software performance evaluation","interprocedural array data-flow analysis;automatic program parallelization;parallel code generation;parallel architectures;symbolic analysis;predicates;IF conditions;efficiency enhancement;parallelizing compiler;guarded array regions","","5","","45","","","","","","IEEE","IEEE Journals & Magazines"
"A theory-based representation for object-oriented domain models","S. A. DeLoach; T. C. Hartrum","Dept. of Electr. & Comput. Eng., US Air Force Inst. of Technol., Wright-Patterson AFB, OH, USA; NA","IEEE Transactions on Software Engineering","","2000","26","6","500","517","Formal software specification has long been touted as a way to increase the quality and reliability of software; however, it remains an intricate, manually intensive activity. An alternative to using formal specifications directly is to translate graphically based, semiformal specifications into formal specifications. However, before this translation can take place, a formal definition of basic object oriented concepts must be found. The paper presents an algebraic model of object orientation that defines how object oriented concepts can be represented algebraically using an object oriented algebraic specification language O-SLANG. O-SLANG combines basic algebraic specification constructs with category theory operations to capture internal object class structure, as well as relationships between classes.","0098-5589;1939-3520;2326-3881","","10.1109/32.852740","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=852740","","Object oriented modeling;Formal specifications;Software quality;Software systems;Computer Society;Specification languages;Software engineering;Formal languages;Application software;Natural languages","object-oriented programming;object-oriented languages;algebraic specification;category theory","theory based representation;object oriented domain models;formal software specification;graphically based semiformal specification translation;formal definition;basic object oriented concepts;algebraic model;object orientation;object oriented concepts;object oriented algebraic specification language;O-SLANG;basic algebraic specification constructs;category theory operations;internal object class structure","","14","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A choice relation framework for supporting category-partition test case generation","T. Y. Chen; Pak-Lok Poon; T. H. Tse","Sch. of Inf. Technol., Swinburne Univ. of Technol., Hawthorn, Vic., Australia; NA; NA","IEEE Transactions on Software Engineering","","2003","29","7","577","593","We describe in this paper a choice relation framework for supporting category-partition test case generation. We capture the constraints among various values (or ranges of values) of the parameters and environment conditions identified from the specification, known formally as choices. We express these constraints in terms of relations among choices and combinations of choices, known formally as test frames. We propose a theoretical backbone and techniques for consistency checks and automatic deductions of relations. Based on the theory, algorithms have been developed for generating test frames from the relations. These test frames can then be used as the basis for generating test cases. Our algorithms take into consideration the resource constraints specified by software testers, thus maintaining the effectiveness of the test frames (and hence test cases) generated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214323","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214323","","Computer aided software engineering;Software testing;Software maintenance;Spine;Software algorithms;Costs;Humans;Information technology;Australia;Computer science","computer aided software engineering;program testing","choice relation framework;category- partition test case generation;category-partition test case generation;environment conditions;test frames;consistency checks;resource constraints;software testing","","34","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Architectural-level risk analysis using UML","K. Goseva-Popstojanova; A. Hassan; A. Guedem; W. Abdelmoez; D. E. M. Nassar; H. Ammar; A. Mili","Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; Dept. of Electr. & Comput. Eng., West Virginia Univ., Morgantown, WV, USA; NA","IEEE Transactions on Software Engineering","","2003","29","10","946","960","Risk assessment is an essential part in managing software development. Performing risk assessment during the early development phases enhances resource allocation decisions. In order to improve the software development process and the quality of software products, we need to be able to build risk analysis models based on data that can be collected early in the development process. These models will help identify the high-risk components and connectors of the product architecture, so that remedial actions may be taken in order to control and optimize the development process and improve the quality of the product. In this paper, we present a risk assessment methodology which can be used in the early phases of the software life cycle. We use the Unified Modeling Language (UML) and commercial modeling environment Rational Rose Real Time (RoseRT) to obtain UML model statistics. First, for each component and connector in software architecture, a dynamic heuristic risk factor is obtained and severity is assessed based on hazard analysis. Then, a Markov model is constructed to obtain scenarios risk factors. The risk factors of use cases and the overall system risk factor are estimated using the scenarios risk factors. Within our methodology, we also identify critical components and connectors that would require careful analysis, design, implementation, and more testing effort. The risk assessment methodology is applied on a pacemaker case study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237174","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237174","","Risk analysis;Unified modeling language;Risk management;Connectors;Programming;Software development management;Resource management;Software quality;Computer architecture;Statistics","software architecture;risk management;specification languages;Markov processes","software development;risk assessment;software architecture;dynamic coupling;Unified Modeling Language;UML;software life cycle;severity of failure","","76","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Toward formally-based design of message passing programs","S. Gorlatch","Passau Univ., Germany","IEEE Transactions on Software Engineering","","2000","26","3","276","288","Presents a systematic approach to the development of message passing programs. Our programming model is SPMD, with communications restricted to collective operations: scan, reduction, gather, etc. The design process in such an architecture-independent language is based on correctness-preserving transformation rules that are provable in a formal functional framework. We develop a set of design rules for composition and decomposition. For example, scan followed by reduction is replaced by a single reduction, and global reduction is decomposed into two faster operations. The impact of the design rules on the target performance is estimated analytically and tested in machine experiments. As a case study, we design two provably correct, efficient programs using the Message Passing Interface (MPI) for the famous maximum segment sum problem, starting from an intuitive, but inefficient, algorithm specification.","0098-5589;1939-3520;2326-3881","","10.1109/32.842952","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842952","","Message passing;Algorithm design and analysis;Process design;Skeleton;Design methodology;Performance analysis;Testing;Parallel programming;Programming profession;Parallel architectures","message passing;distributed programming;application program interfaces;formal specification;software performance evaluation","formally-based design;message passing programs;SPMD programming model;collective operations;scanning operation;reduction operation;gathering operation;architecture-independent language;correctness-preserving transformation rules;formal functional framework;design rules;composition;decomposition;performance;efficient programs;Message Passing Interface;MPI;maximum segment sum problem;algorithm specification;program transformations;systematic program design;homomorphisms;skeletons","","12","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Specifying timing constraints and composite events: an application in the design of electronic brokerages","A. K. Mok; P. Konana; Guangtian Liu; Chan-Gun Lee; Honguk Woo","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","12","841","858","Increasingly, business applications need to capture consumers' complex preferences interactively and monitor those preferences by translating them into event-condition-action (ECA) rules and syntactically correct processing specification. An expressive event model to specify primitive and composite events that may involve timing constraints among events is critical to such applications. Relying on the work done in active databases and real-time systems, this research proposes a new composite event model based on real-time logic (RTL). The proposed event model does not require fixed event consumption policies and allows the users to represent the exact correlation of event instances in defining composite events. It also supports a wide-range of domain-specific temporal events and constraints, such as future events, time-constrained events, and relative events. This event model is validated within an electronic brokerage architecture that unbundles the required functionalities into three separable components - business rule manager, ECA rule manager, and event monitor - with well-defined interfaces. A proof-of-concept prototype was implemented in the Java programming language to demonstrate the expressiveness of the event model and the feasibility of the architecture. The performance of the composite event monitor was evaluated by varying the number of rules, event arrival rates, and type of composite events.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.105","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377184","Index Terms- Active databases;real-time databases;electronic brokerages;event specification;timing constraints.","Timing;Event detection;Databases;Monitoring;Real time systems;Logic design;Prototypes;Java;Computer languages;Process control","active databases;electronic trading;Java;real-time systems;formal specification;temporal logic;electronic commerce","timing constraints;electronic brokerages architecture;business application;event-condition-action;correct processing specification;active databases;real-time systems;real-time logic;domain-specific temporal event;time-constrained event;business rule manager;ECA rule manager;event monitor;Java programming language;event arrival rates","","8","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Special issues for fm '99: the first world congress on formal methods in the development of computing systems","J. M. Wing; J. Woodcock","Carnegie Mellon University; NA","IEEE Transactions on Software Engineering","","2000","26","8","673","674","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.879806","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879806","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"A weakest precondition semantics for refinement of object-oriented programs","A. Cavalcanti; D. A. Naumann","Centro de Inf., Univ. Fed. de Pernambuco, Recife, Brazil; NA","IEEE Transactions on Software Engineering","","2000","26","8","713","728","We define a predicate-transformer semantics for an object oriented language that includes specification constructs from refinement calculi. The language includes recursive classes, visibility control, dynamic binding, and recursive methods. Using the semantics, we formulate notions of refinement. Such results are a first step toward a refinement calculus.","0098-5589;1939-3520;2326-3881","","10.1109/32.879810","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879810","","Java;Calculus;Object oriented modeling;Object oriented programming;Computer Society;Formal specifications;Terminology;Logic programming;Testing;Software algorithms","object-oriented programming;object-oriented languages;programming language semantics;formal specification;refinement calculus;type theory","weakest precondition semantics;object oriented program refinement;predicate-transformer semantics;object oriented language;specification constructs;refinement calculi;recursive classes;visibility control;dynamic binding;recursive methods","","24","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Validating the ISO/IEC 15504 measure of software requirements analysis process capability","K. El Emam; A. Birk","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; NA","IEEE Transactions on Software Engineering","","2000","26","6","541","566","ISO/IEC 15504 is an emerging international standard on software process assessment. It defines a number of software engineering processes and a scale for measuring their capability. One of the defined processes is software requirements analysis (SRA). A basic premise of the measurement scale is that higher process capability is associated with better project performance (i.e., predictive validity). The paper describes an empirical study that evaluates the predictive validity of SRA process capability. Assessments using ISO/IEC 15504 were conducted on 56 projects world-wide over a period of two years. Performance measures on each project were also collected using questionnaires, such as the ability to meet budget commitments and staff productivity. The results provide strong evidence of predictive validity for the SRA process capability measure used in ISO/IEC 15504, but only for organizations with more than 50 IT staff. Specifically, a strong relationship was found between the implementation of requirements analysis practices as defined in ISO/IEC 15504 and the productivity of software projects. For smaller organizations, evidence of predictive validity was rather weak. This can be interpreted in a number of different ways: that the measure of capability is not suitable for small organizations or that the SRA process capability has less effect on project performance for small organizations.","0098-5589;1939-3520;2326-3881","","10.1109/32.852742","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=852742","","ISO standards;IEC standards;Software measurement;Software quality;Capability maturity model;Software standards;Software engineering;Productivity;Military standards;Design engineering","ISO standards;IEC standards;software standards;software metrics;formal specification;software process improvement;software development management;program verification;human resource management","ISO/IEC 15504 measure validation;software requirements analysis process capability;international standard;software process assessment;software engineering processes;measurement scale;process capability;project performance;predictive validity;SRA process capability;performance measures;budget commitments;staff productivity;SRA process capability measure;IT staff;requirements analysis practices;software projects;small organizations","","72","","92","","","","","","IEEE","IEEE Journals & Magazines"
"A safe algorithm for resolving OR deadlocks","J. Villadangos; F. Farina; J. R. Gonzalez de Mendivil; J. R. Garitagoitia; A. Cordoba","Dept. de Automatica y Computacion, Univ. Publica de Navarra, Spain; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","7","608","622","Deadlocks in the OR model are usually resolved by aborting a deadlocked process. Prior algorithms for the same model sometimes abort nodes needlessly wasting computing resources. This paper presents a new deadlock resolution algorithm for the OR model that satisfies the following correctness criteria: (Safety) the algorithm does not resolve false deadlocks; (Liveness) the algorithm resolves all deadlocks in finite time. The communication cost of the algorithm is similar to that of previous nonsafe proposals. The theoretical cost has been validated by simulation. In addition, different algorithm initiation alternatives have been analyzed in order to reduce the latency of deadlocks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214325","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214325","","System recovery;Abortion;Computer Society;Costs;Algorithm design and analysis;Delay;Safety;Proposals;Distributed algorithms;Throughput","system recovery;distributed algorithms;program verification","OR model;OR deadlocks;deadlock resolution algorithm;correctness criteria;distributed algorithms","","5","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Foundations of the trace assertion method of module interface specification","R. Janicki; E. Sekerinski","McMaster Univ., Hamilton, Ont., Canada; NA","IEEE Transactions on Software Engineering","","2001","27","7","577","598","The trace assertion method is a formal state machine based method for specifying module interfaces. A module interface specification treats the module as a black-box, identifying all the module's access programs (i.e., programs that can be invoked from outside of the module) and describing their externally visible effects. In the method, both the module states and the behaviors observed are fully described by traces built from access program invocations and their visible effects. A formal model for the trace assertion method is proposed. The concept of step-traces is introduced and applied. The stepwise refinement of trace assertion specifications is considered. The role of nondeterminism, normal and exceptional behavior, value functions, and multiobject modules are discussed. The relationship with algebraic specifications is analyzed. A tabular notation for writing trace specifications to ensure readability is adapted.","0098-5589;1939-3520;2326-3881","","10.1109/32.935852","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=935852","","Algebra;Computer Society;Equations;Commutation;Writing;History;Tail","formal specification;automata theory","trace assertion method;formal state machine based method;module interface specification;black-box;access programs;module states;module behavior;access program invocations;externally visible effects;step traces;stepwise refinement;trace assertion specifications;nondeterminism;normal behavior;exceptional behavior;value functions;multiobject modules;algebraic specifications;tabular notation;readability;trace specification writing","","17","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Components of software development risk: how to address them? A project manager survey","J. Ropponen; K. Lyytinen","Finnish Evangelical Lutheran Mission, Helsinki, Finland; NA","IEEE Transactions on Software Engineering","","2000","26","2","98","112","Software risk management can be defined as an attempt to formalize risk oriented correlates of development success into a readily applicable set of principles and practices. By using a survey instrument we investigate this claim further. The investigation addresses the following questions: 1) What are the components of software development risk? 2) how does risk management mitigate risk components, and 3) what environmental factors if any influence them? Using principal component analysis we identify six software risk components: 1) scheduling and timing risks, 2) functionality risks, 3) subcontracting risks, 4) requirements management, 5) resource usage and performance risks, and 6) personnel management risks. By using one-way ANOVA with multiple comparisons we examine how risk management (or the lack of it) and environmental factors (such as development methods, manager's experience) influence each risk component. The analysis shows that awareness of the importance of risk management and systematic practices to manage risks have an effect on scheduling risks, requirements management risks, and personnel management risks. Environmental contingencies were observed to affect all risk components. This suggests that software risks can be best managed by combining specific risk management considerations with a detailed understanding of the environmental context and with sound managerial practices, such as relying on experienced and well-educated project managers and launching correctly sized projects.","0098-5589;1939-3520;2326-3881","","10.1109/32.841112","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=841112","","Programming;Risk management;Environmental management;Project management;Environmental factors;Personnel;Analysis of variance;Instruments;Principal component analysis;Software performance","risk management;software development management;principal component analysis","software development risk management;project manager survey;environmental factors;principal component analysis;timing risks;scheduling risks;functionality risks;subcontracting risks;requirements management;resource usage;performance risks;personnel management risks;one-way ANOVA;multiple comparisons;environmental contingencies","","162","","48","","","","","","IEEE","IEEE Journals & Magazines"
"The reference model for smooth growth of software systems revisited","W. M. Turski","Inst. of Inf., Warsaw Univ., Poland","IEEE Transactions on Software Engineering","","2002","28","8","814","815","Terms-Software evolution,The difference equation determining evolutionary growth of (some) software systems is generalized to a differential one. A hypothetical geometric model is derived and its possible uses are illustrated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027802","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027802","","Software systems;Difference equations;Solid modeling;History;Application software;Closed-form solution;Size measurement;Software measurement;Calendars;Shape","software engineering;difference equations","difference equation;evolutionary growth model;software evolution;system complexity;software systems;hypothetical geometric model","","38","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Specification and Verification of Normative Texts Using C-O Diagrams","G. Díaz; M. E. Cambronero; E. Martínez; G. Schneider","Department of Computer Science , University of Castilla-La Mancha, Albacete, Spain; Department of Computer Science , University of Castilla-La Mancha, Albacete, Spain; Department of Computer Science , University of Castilla-La Mancha, Albacete, Spain; Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Sweden","IEEE Transactions on Software Engineering","","2014","40","8","795","817","C-O diagrams have been introduced as a means to have a more visual representation of normative texts and electronic contracts, where it is possible to represent the obligations, permissions and prohibitions of the different signatories, as well as the penalties resulting from non-fulfillment of their obligations and prohibitions. In such diagrams we are also able to represent absolute and relative timing constraints. In this paper we present a formal semantics for C-O diagrams based on timed automata extended with information regarding the satisfaction and violation of clauses in order to represent different deontic modalities. As a proof of concept, we apply our approach to two different case studies, where the method presented here has successfully identified problems in the specification.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6657668","Normative documents;electronic contracts;deontic logic;formal verification;visual models;timed automata;C-O diagrams","Automata;Clocks;Contracts;Semantics;Cost accounting;Synchronization;Formal languages","automata theory;formal specification;formal verification;text analysis","normative texts;formal specification;formal verification;C-O diagrams;visual representation;electronic contracts;timing constraints;formal semantics;timed automata;deontic modalities","","3","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Formal development and verification of a distributed railway control system","A. E. Haxthausen; J. Peleska","Dept. of Inf. Technol., Tech. Univ., Lyngby, Denmark; NA","IEEE Transactions on Software Engineering","","2000","26","8","687","701","The authors introduce the concept for a distributed railway control system and present the specification and verification of the main algorithm used for safe distributed control. Our design and verification approach is based on the RAISE method, starting with highly abstract algebraic specifications which are transformed into directly implementable distributed control processes by applying a series of refinement and verification steps. Concrete safety requirements are derived from an abstract version that can be easily validated with respect to soundness and completeness. Complexity is further reduced by separating the system model into a domain model and a controller model. The domain model describes the physical system in absence of control and the controller model introduces the safety-related control mechanisms as a separate entity monitoring observables of the physical system to decide whether it is safe for a train to move or for a point to be switched.","0098-5589;1939-3520;2326-3881","","10.1109/32.879808","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879808","","Rail transportation;Distributed control;Control systems;Communication system control;Switches;Railway safety;Centralized control;Formal specifications;Mobile communication;Concrete","railways;rail traffic;traffic control;algebraic specification;program verification;distributed control;safety-critical software","formal development;distributed railway control system verification;formal specification;safe distributed control;verification approach;RAISE method;highly abstract algebraic specifications;directly implementable distributed control processes;verification steps;safety requirements;abstract version;soundness;completeness;domain model;controller model;safety-related control mechanisms","","52","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Documentation driven development for complex real-time systems","Luqi; L. Zhang; V. Berzins; Y. Qiao","Dept. of Comput. Sci., US Naval Postgraduate Sch., USA; Dept. of Comput. Sci., US Naval Postgraduate Sch., USA; Dept. of Comput. Sci., US Naval Postgraduate Sch., USA; Dept. of Comput. Sci., US Naval Postgraduate Sch., USA","IEEE Transactions on Software Engineering","","2004","30","12","936","952","This work presents a novel approach for development of complex real-time systems, called the documentation-driven development (DDD) approach. This approach can enhance integration of computer aided software development activities, which encompass the entire life cycle. DDD will provide a mechanism to monitor and quickly respond to changes in requirements and provide a friendly communication and collaboration environment to enable different stakeholders to be easily involved in development processes and, therefore, significantly improve the agility of software development for complex real-time systems. DDD will also support automated software generation based on a computational model and some relevant techniques. DDD includes two main parts: a documentation management system (DMS) and a process measurement system (PMS). DMS will create, organize, monitor, analyze, and transform all documentation associated with the software development process. PMS will monitor the frequent changes in requirements and assess the effort and success possibility of development. A case study was conducted by a tool set that realized part of the proposed approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.100","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377190","Index Terms- Software development;documentation;agility;information representation;complex systems;real-time systems.","Documentation;Real time systems;Programming;Software systems;Computerized monitoring;Collaborative software;Computational modeling;Information representation;Mission critical systems;Availability","real-time systems;computer aided software engineering;user interfaces;formal specification;formal verification","documentation driven development approach;real-time system;computer aided software development;software generation;computational model;documentation management system;process measurement system","","5","","56","","","","","","IEEE","IEEE Journals & Magazines"
"The model multiplicity problem: experimenting with real-time specification methods","M. Peleg; D. Dori","Sch. of Med., Stanford Univ., CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","8","742","759","The object-process methodology (OPM) specifies both graphically and textually the system's static-structural and behavioral-procedural aspects through a single unifying model. This model singularity is contrasted with the multimodel approach applied by existing object oriented system analysis methods. These methods usually employ at least three distinct models for specifying various system aspects: mainly structure, function, and behavior. Object modeling technique (OMT), the main ancestor of the unified modeling language (UML), extended with timed statecharts, represents a family of such multimodal object oriented methods. Two major open questions related to model multiplicity vs. model singularity have been: 1) whether or not a single model, rather than a combination of several models, enables the synthesis of a better system specification; and 2) which of the two alternative approaches yields a specification that is easier to comprehend. The authors address these questions through a double-blind controlled experiment. To obtain conclusive results, real time systems, which exhibit a more complex dynamic behavior than nonreal time systems were selected as the focus of the experiment. We establish empirically that a single model methodology, OPM, is more effective than a multimodel one, OMT, in terms of synthesis. We pinpoint specific issues in which significant diiferences between the two methodologies were found. The specification comprehension results show that there were significant differences between the two methods in specific issues.","0098-5589;1939-3520;2326-3881","","10.1109/32.879812","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879812","","Object oriented modeling;Real time systems;Unified modeling language;Control system synthesis;Biomedical engineering;Design methodology;Software engineering;Computer aided software engineering;Manuals;Information systems","formal specification;real-time systems;object-oriented methods;temporal logic","model multiplicity problem;real time specification methods;object-process methodology;OPM;behavioral-procedural aspects;static-structural aspects;unifying model;multimodel approach;object oriented system analysis methods;distinct models;system aspects;object modeling technique;unified modeling language;timed statecharts;multimodal object oriented methods;model multiplicity;model singularity;system specification;double-blind controlled experiment;real time systems;complex dynamic behavior;nonreal time systems;single model methodology;OMT;specification comprehension results","","40","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Response to Jiau et al.'s comments","T. C. Oliveira; P. S. C. Alencar; I. M. Filho; C. J. P. de Lucena; D. D. Cowan","Dept. de Inf., Pontificia Univ. Catolica do Rio de Janeiro, Brazil; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","10","708","","The consistency problems in UML models and related software processes can be very complex. First, although UML supports a modeling process that should yield inter and intraconsistent models, the meaning of the UML dependencies and their specializations is not precisely defined and, for this reason, many inconsistencies may appear in their models and processes. Precise definitions would form a basis for methods to detect and analyze consistency problems related to UML dependencies and relationships, as well as problems related to software processes described in UML. In addition, we are using the UML object constraint language (OCL) to describe constraints related to the framework instantiation process. OCL is recognized as a limited language in some aspects for expressing well-formedness rules. We have presented a method (T.C. Oliveira et al., 2004) that works in the case of consistent rules and consistent instantiation processes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339281","","Unified modeling language;Aggregates;Documentation","specification languages;formal specification","consistency problems;UML dependencies;UML object constraint language;instantiation process;software processes","","","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Clustering algorithm for parallelizing software systems in multiprocessors environment","D. Kadamuddi; J. J. P. Tsai","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; NA","IEEE Transactions on Software Engineering","","2000","26","4","340","361","A variety of techniques and tools exist to parallelize software systems on different parallel architectures (SIMD, MIMD). With the advances in high-speed networks, there has been a dramatic increase in the number of client/server applications. A variety of client/server applications are deployed today, ranging from simple telnet sessions to complex electronic commerce transactions. Industry standard protocols, like Secure Socket Layer (SSL), Secure Electronic Transaction (SET), etc., are in use for ensuring privacy and integrity of data, as well as for authenticating the sender and the receiver during message passing. Consequently, a majority of applications using parallel processing techniques are becoming synchronization-centric, i.e., for every message transfer, the sender and receiver must synchronize. However, more effective techniques and tools are needed to automate the clustering of such synchronization-centric applications to extract parallelism. The authors present a new clustering algorithm to facilitate the parallelization of software systems in a multiprocessor environment. The new clustering algorithm achieves traditional clustering objectives (reduction in parallel execution time, communication cost, etc.). Additionally, our approach: 1) reduces the performance degradation caused by synchronizations, and 2) avoids deadlocks during clustering. The effectiveness of our approach is depicted with the help of simulation results.","0098-5589;1939-3520;2326-3881","","10.1109/32.844493","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=844493","","Clustering algorithms;Software algorithms;Software systems;Application software;Network servers;Parallel architectures;High-speed networks;Electronic commerce;Electronics industry;Protocols","parallel programming;multiprocessing systems;workstation clusters;client-server systems;message passing;synchronisation;concurrency control","clustering algorithm;software system parallelization;multiprocessor environment;parallel architectures;high-speed networks;client/server applications;telnet sessions;electronic commerce transactions;industry standard protocols;Secure Socket Layer;Secure Electronic Transaction;data privacy;data integrity;message passing;user authentication;parallel processing techniques;message transfer;synchronization-centric applications;traditional clustering objectives;performance degradation;deadlocks","","11","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A Quantitative Approach to Input Generation in Real-Time Testing of Stochastic Systems","L. Carnevali; L. Ridi; E. Vicario","Università degli Studi di Firenze, Firenze; Università degli Studi di Firenze, Firenze; Università degli Studi di Firenze, Firenze","IEEE Transactions on Software Engineering","","2013","39","3","292","304","In the process of testing of concurrent timed systems, input generation identifies values of temporal parameters that let the Implementation Under Test (IUT) execute selected cases. However, when some parameters are not under control of the driver, test execution may diverge from the selected input and produce an inconclusive behavior. We formulate the problem on the basis of an abstraction of the IUT which we call partially stochastic Time Petri Net (psTPN), where controllable parameters are modeled as nondeterministic values and noncontrollable parameters as random variables with general (GEN) distribution. With reference to this abstraction, we derive the analytical form of the probability that the IUT runs along a selected behavior as a function of choices taken on controllable parameters. In the applicative perspective of real-time testing, this identifies a theoretical upper limit on the probability of a conclusive result, thus providing a means to plan the number of test repetitions that are necessary to guarantee a given probability of test-case coverage. It also provides a constructive technique for an optimal or suboptimal approach to input generation and a way to characterize the probability of conclusive testing under other suboptimal strategies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6226426","Real-time testing;input generation;Time Petri Nets;non-Markovian Stochastic Petri Nets;stochastic processes;Difference Bound Matrix","Stochastic processes;Timing;Real time systems;Testing;Tin;Vectors;Automata","Petri nets;program testing;real-time systems","quantitative approach;input generation;real-time testing;stochastic systems;concurrent timed systems;temporal parameters;implementation under test;IUT;test execution;inconclusive behavior;partially stochastic Time Petri Net;psTPN;controllable parameters;nondeterministic values;GEN distribution","","2","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Unpredication, unscheduling, unspeculation: reverse engineering Itanium executables","N. Snavely; S. Debray; G. R. Andrews","Dept. of Comput. Sci., Washington Univ., Seattle, WA, USA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","2","99","115","EPIC (explicitly parallel instruction computing) architectures, exemplified by the Intel Itanium, support a number of advanced architectural features, such as explicit instruction-level parallelism, instruction predication, and speculative loads from memory. However, compiler optimizations that take advantage of these features can profoundly restructure the program's code, making it potentially difficult to reconstruct the original program logic from an optimized Itanium executable. This paper describes techniques to undo some of the effects of such optimizations and thereby improve the quality of reverse engineering such executables.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401927","Index Terms- Reverse engineering;EPIC architectures;speculation;predication;code optimization.","Reverse engineering;Computer architecture;Parallel processing;Optimizing compilers;Computer aided instruction;Concurrent computing;Logic;Pipelines;Delay;Program processors","parallel architectures;reverse engineering;optimising compilers;parallel programming;scheduling;instruction sets","reverse engineering;Intel Itanium;EPIC architecture;explicitly parallel instruction computing;instruction-level parallelism;instruction predication;compiler optimization","","1","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""The confounding effect of class size on the validity of object-oriented metrics""","W. M. Evanco","Coll. of Inf. Sci. & Technol., Drexel Univ., Philadelphia, PA, USA","IEEE Transactions on Software Engineering","","2003","29","7","670","672","It has been proposed by El Emam et al. (ibid. vol.27 (7), 2001) that size should be taken into account as a confounding variable when validating object-oriented metrics. We take issue with this perspective since the ability to measure size does not temporally precede the ability to measure many of the object-oriented metrics that have been proposed. Hence, the condition that a confounding variable must occur causally prior to another explanatory variable is not met. In addition, when specifying multivariate models of defects that incorporate object-oriented metrics, entering size as an explanatory variable may result in misspecified models that lack internal consistency. Examples are given where this misspecification occurs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214331","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214331","","Size measurement;Software measurement;Object oriented modeling;Size control;Computer languages;Volume measurement;Veins;Software testing;Statistical analysis;Information science","software metrics;software reliability;object-oriented programming","object-oriented metrics;software detects;defect-proneness;statistical modeling;multivariate models;class size;validity","","14","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Applicability of Weyuker's Property 9 to object oriented metrics","Naveen Sharma; Padmaja Joshi; R. K. Joshi","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Mumbai, India; Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Mumbai, India; Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Mumbai, India","IEEE Transactions on Software Engineering","","2006","32","3","209","211","Weyuker's Property 9 has received a mixed response regarding its applicability to object oriented software metrics. Contrary to past beliefs, the relevance of this property to object oriented systems is brought out. In support of the new argument, counterexamples to earlier claims are formulated and two new metrics highlighting a notion of complexity that is capturable through Property 9 are also presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610611","Software metrics;object oriented design;Weyuker's properties;interaction complexity.","Software metrics;Software measurement;Testing;Q measurement;Corporate acquisitions;Ontologies;Programming profession;Memory management;Runtime","object-oriented programming;object-oriented methods;software metrics","Weyuker Property 9;object oriented software metrics;object oriented system design;software complexity measures","","11","","7","","","","","","IEEE","IEEE Journals & Magazines"
"An interval logic for real-time system specification","R. Mattolini; P. Nesi","Hewlett Packard, Italy; NA","IEEE Transactions on Software Engineering","","2001","27","3","208","227","Formal techniques for the specification of real time systems must be capable of describing system behavior as a set of relationships expressing the temporal constraints among events and actions, including properties of invariance, precedence, periodicity, liveness, and safety conditions. The paper describes a Temporal-Interval Logic with Compositional Operators (TILCO) designed expressly for the specification of real time systems. TILCO is a generalization of classical temporal logics based on the operators, eventually and henceforth; it allows both qualitative and quantitative specification of time relationships. TILCO is based on time intervals and can concisely express temporal constraints with time bounds, such as those needed to specify real time systems. This approach can be used to verify the completeness and consistency of specifications, as well as to validate system behavior against its requirements and general properties. TILCO has been formalized by using the theorem prover Isabelle/HOL. TILCO specifications satisfying certain properties are executable by using a modified version of the Tableaux algorithm. The paper defines TILCO and its axiomatization, highlights the tools available for proving properties of specifications and for their execution, and provides an example of system specification and validation.","0098-5589;1939-3520;2326-3881","","10.1109/32.910858","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910858","","Real time systems;Safety;Automatic logic units;Logic design;Time factors;Specification languages;Aerospace electronics;Robots;Process control;Gas insulated transmission lines","bibliographies;formal specification;real-time systems;temporal logic;program verification;theorem proving","interval logic;real time system specification;formal techniques;system behavior;temporal constraints;safety conditions;Temporal-Interval Logic with Compositional Operators;TILCO;classical temporal logics;eventually;henceforth;quantitative specification;time relationships;time intervals;completeness;consistency;theorem prover;Isabelle/HOL;TILCO specifications;Tableaux algorithm","","21","","69","","","","","","IEEE","IEEE Journals & Magazines"
"Semi-Proving: An Integrated Method for Program Proving, Testing, and Debugging","T. Y. Chen; T. H. Tse; Z. Q. Zhou","Swinburne University of Technology, Hawthorn; The University of Hong Kong, Hong Kong; University of Wollongong, Wollongong","IEEE Transactions on Software Engineering","","2011","37","1","109","125","We present an integrated method for program proving, testing, and debugging. Using the concept of metamorphic relations, we select necessary properties for target programs. For programs where global symbolic evaluation can be conducted and the constraint expressions involved can be solved, we can either prove that these necessary conditions for program correctness are satisfied or identify all inputs that violate the conditions. For other programs, our method can be converted into a symbolic-testing approach. Our method extrapolates from the correctness of a program for tested inputs to the correctness of the program for related untested inputs. The method supports automatic debugging through the identification of constraint expressions that reveal failures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5406529","Software/program verification;symbolic execution;testing and debugging.","Software testing;Automatic testing;Computer science;Built-in self-test;Software debugging;Costs;Automation;Australia Council;Communications technology;Software engineering","formal verification;program debugging;program testing","semiproving;program proving;program testing;program debugging;integrated method;metamorphic relation;symbolic evaluation;constraint expression;symbolic testing;automatic debugging;program verification","","37","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Performance evaluation of mobile processes via abstract machines","C. Nottegar; C. Priami; P. Degano","Dipt. di Inf., Verona Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","2001","27","10","867","889","We use a structural operational semantics which drives us in inferring quantitative measures on system evolution. The transitions of the system are labeled and we assign rates to them by only looking at these labels. The rates reflect the possibly distributed architecture on which applications run. We then map transition systems to Markov chains, and performance evaluation is carried out using standard tools. As a working example, we compare the performance of a conventional uniprocessor with a prefetch pipeline machine. We also consider two case studies from the literature involving mobile computation to show that our framework is feasible.","0098-5589;1939-3520;2326-3881","","10.1109/32.962559","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=962559","","Performance analysis;Resource management;Proposals;Computer architecture;Stochastic systems;Quality management;Humans;Calculus;Prefetching;Pipelines","performance evaluation;finite automata;Markov processes;process algebra;pipeline processing;distributed programming","structural operational semantics;quantitative measures;system evolution;distributed architecture;Markov chains;performance evaluation;uniprocessor;prefetch pipeline machine;mobile computation;calculi for mobility;formal methodology;stochastic models;abstract machines;mobile processes","","13","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Generation of execution sequences for modular time critical systems","P. S. Pietro; A. Morzenti; S. Morasca","Dipt. di Elettronica e Inf., Politecnico di Milano, Italy; NA; NA","IEEE Transactions on Software Engineering","","2000","26","2","128","149","We define methods for generating execution sequences for time-critical systems based on their modularized formal specification. An execution sequence represents a behavior of a time critical system and can be used, before the final system is built, to validate the system specification against the user requirements (specification validation) and, after the final system is built, to verify whether the implementation satisfies the specification (functional testing). Our techniques generate execution sequences in the large, in that we focus on the connections among the abstract interfaces of the modules composing a modular specification. Execution sequences in the large are obtained by composing execution sequences in the small for the individual modules. We abstract from the specification languages used for the individual modules of the system, so our techniques can also be used when the modules composing the system are specified with different formalisms. We consider the cases in which connections give rise to either circular or noncircular dependencies among specification modules. We show that execution sequence generation can be carried out successfully under rather broad conditions and we define procedures for efficient construction of execution sequences. These procedures can be taken as the basis for the implementation of (semi)automated tools that provide substantial support to the activity of specification validation and functional testing for industrially-sized time critical systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.841114","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=841114","","System testing;Software testing;Animation;Formal specifications;Sequential analysis;Computer Society;Time factors;Construction industry;Electrical equipment industry;Software prototyping","formal specification;specification languages;formal verification;program testing;sequences","execution sequence generation;modular time critical systems;modularized formal specification;user requirements;specification validation;abstract interfaces;specification languages;noncircular dependencies;circular dependencies;automated tools;functional testing","","3","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Authors’ Reply to “Comments on ‘Researcher Bias: The Use of Machine Learning in Software Defect Prediction’”","M. Shepperd; T. Hall; D. Bowes","Department of Computer Science, Brunel University London, Uxbridge, United Kingdom; Department of Computer Science, Brunel University London, Uxbridge, United Kingdom; University of Hertfordshire, Hatfield, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","11","1129","1131","In 2014 we published a meta-analysis of software defect prediction studies [1] . This suggested that the most important factor in determining results was Research Group, i.e., who conducts the experiment is more important than the classifier algorithms being investigated. A recent re-analysis [2] sought to argue that the effect is less strong than originally claimed since there is a relationship between Research Group and Dataset. In this response we show (i) the re-analysis is based on a small (21 percent) subset of our original data, (ii) using the same re-analysis approach with a larger subset shows that Research Group is more important than type of Classifier and (iii) however the data are analysed there is compelling evidence that who conducts the research has an effect on the results. This means that the problem of researcher bias remains. Addressing it should be seen as a matter of priority amongst those of us who conduct and publish experiments comparing the performance of competing software defect prediction systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2731308","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7990255","Software quality assurance;defect prediction;researcher bias","Software;NASA;Measurement;Analysis of variance;Data models;Predictive models;Analytical models","learning (artificial intelligence);pattern classification;program diagnostics","researcher bias;machine learning;meta-analysis;classifier algorithms;re-analysis approach;software defect prediction systems;research group","","","","5","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical investigation of an object-oriented software system","M. Cartwright; M. Shepperd","Dept. of Comput., Bournemouth Univ., UK; NA","IEEE Transactions on Software Engineering","","2000","26","8","786","796","The paper describes an empirical investigation into an industrial object oriented (OO) system comprised of 133000 lines of C++. The system was a subsystem of a telecommunications product and was developed using the Shlaer-Mellor method (S. Shlaer and S.J. Mellor, 1988; 1992). From this study, we found that there was little use of OO constructs such as inheritance, and therefore polymorphism. It was also found that there was a significant difference in the defect densities between those classes that participated in inheritance structures and those that did not, with the former being approximately three times more defect-prone. We were able to construct useful prediction systems for size and number of defects based upon simple counts such as the number of states and events per class. Although these prediction systems are only likely to have local significance, there is a more general principle that software developers can consider building their own local prediction systems. Moreover, we believe this is possible, even in the absence of the suites of metrics that have been advocated by researchers into OO technology. As a consequence, measurement technology may be accessible to a wider group of potential users.","0098-5589;1939-3520;2326-3881","","10.1109/32.879814","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879814","","Software systems;Computer industry;Testing;Lab-on-a-chip;Buildings;Computer languages;Java;Large-scale systems;Particle measurements;Software measurement","object-oriented programming;C++ language;telecommunication computing;software performance evaluation","empirical investigation;object oriented software system;industrial object oriented system;C++ code;telecommunications product;Shlaer-Mellor method;OO constructs;inheritance;polymorphism;defect densities;inheritance structures;prediction systems;local significance;software developers;local prediction systems;OO technology;measurement technology;potential users","","133","","19","","","","","","IEEE","IEEE Journals & Magazines"
"More success and failure factors in software reuse","T. Menzies; J. S. Di Stefano","Lane Dept. of Comput. Sci., West Virginia Univ., Morgantown, WV, USA; Lane Dept. of Comput. Sci., West Virginia Univ., Morgantown, WV, USA","IEEE Transactions on Software Engineering","","2003","29","5","474","477","Numerous discrepancies exist between expert opinion and empirical data reported in Morisio et al.'s recent TSE article. The differences related to what factors encouraged successful reuse in software organizations. This note describes how those differences were detected and comments on their methodological implications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199076","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199076","","Data mining;Decision trees;Project management;Data analysis;Machine learning;Failure analysis;Stress;Web sites;Association rules","software reusability","software reuse;machine learning;software organizations","","15","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Testability transformation","M. Harman; L. Hu; R. Hierons; J. Wegener; H. Sthamer; A. Baresel; M. Roper","Dept. of Inf. Syst. & Comput., Brunel Univ., Uxbridge, UK; Dept. of Inf. Syst. & Comput., Brunel Univ., Uxbridge, UK; Dept. of Inf. Syst. & Comput., Brunel Univ., Uxbridge, UK; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","1","3","16","A testability transformation is a source-to-source transformation that aims to improve the ability of a given test generation method to generate test data for the original program. We introduce testability transformation, demonstrating that it differs from traditional transformation, both theoretically and practically, while still allowing many traditional transformation rules to be applied. We illustrate the theory of testability transformation with an example application to evolutionary testing. An algorithm for flag removal is defined and results are presented from an empirical study which show how the algorithm improves both the performance of evolutionary test data generation and the adequacy level of the test data so-generated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265732","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265732","","Automatic testing;Application software;Software testing;Computer Society;Software engineering;Vehicles;Impedance;Information systems;Poles and towers","automatic test pattern generation;program testing;evolutionary computation","testability transformation;automated test data generation;evolutionary testing;flag removal algorithm;evolutionary test data generation;search-based software engineering","","114","","26","","","","","","IEEE","IEEE Journals & Magazines"
"The role of inspection in software quality assurance","D. L. Parnas; M. Lawford","Software Quality Res. Lab., Limerick Univ., Ireland; NA","IEEE Transactions on Software Engineering","","2003","29","8","674","676","Due to the complexity of the code, software is released with many errors. In response, both software practitioners and software researchers need to improve the reputation of the software. Inspection is the only way to improve the quality of software. Inspection methods can be more effective but success depends on having a sound and systematic procedure for conducting the inspection. The Workshop on Inspection in Software Engineering (WISE), a satellite event of the 2001 Computer Aided Verification (CAV '01) Conference, brought together researchers, practitioners, and regulators in the hope of finding effective approaches to software inspection. The workshop included invited lectures and paper presentations in the form of panel discussions on all aspects of software inspection. Submissions explained how practitioners and researchers were performing inspections, discussed the relevance of inspections, provided evidence of how inspections could be improved through refinement of the inspection process and computer aided tool support and explained how careful design of software could make inspections easier or more effective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223642","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223642","","Inspection;Software quality;Computer bugs;Software debugging;Software engineering;Satellites;Regulators;Software design;Software performance;Software tools","software quality;software development management;inspection;software process improvement;computer aided software engineering;program verification","software quality assurance;software process improvement;software inspection method;software practitioner;software researcher;Workshop on Inspection in Software Engineering;WISE;computer aided verification;CAV conference;computer aided tool support;software design","","13","","","","","","","","IEEE","IEEE Journals & Magazines"
"Corrections to ""the effectiveness of control structure diagrams in source code comprehension activities""","D. Hendrix; J. H. Cross; S. Maghsoodloo","Auburn University; NA; NA","IEEE Transactions on Software Engineering","","2002","28","6","624","624","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010064","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010064","","Java;Senior members;Back;Visualization;Computer science;Software engineering;Systems engineering and theory","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Detecting Arbitrary Stable Properties Using Efficient Snapshots","A. Kshemkalyani; B. Wu","Department of Computer Science, University of Illinois at Chicago, 851 South Morgan Street, Chicago, IL 60607; Department of Computer Science, University of Illinois at Chicago, 851 South Morgan Street, Chicago, IL 60607","IEEE Transactions on Software Engineering","","2007","33","5","330","346","A stable properly continues to hold in an execution once it becomes true. Detecting arbitrary stable properties efficiently in distributed executions is still an open problem. The known algorithms for detecting arbitrary stable properties and snapshot algorithms used to detect such stable properties suffer from drawbacks such as the following: They incur the overhead of a large number of messages per global snapshot, or alter application message headers, or use inhibition, or use the execution history, or assume a strong property such as causal delivery of messages in the system. We solve the problem of detecting an arbitrary stable property efficiently under the following assumptions: P1) the application messages should not be modified, not even by timestamps or message coloring. P2) no inhibition is allowed. P3) the algorithm should not use the message history. P4) any process can initiate the algorithm. This paper proposes a family of nonintrusive algorithms requiring 6(n - 1) control messages, where n is the number of processes. A three-phase strategy of uncoordinated observation of local states is used to give a consistent snapshot from which any stable property can be detected. A key feature of our algorithms is that they do not rely on the processes continually and pessimistically reporting their activity. Only the relevant activity that occurs in the thin slice during the algorithm execution needs to be examined.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1000","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160971","Distributed system;global state;stable predicate;stable property;distributed snapshot.","System recovery;History;Computer vision;Joining processes","distributed processing","distributed system snapshot;stable predicate;arbitrary stable property;nonintrusive algorithm","","10","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Semantics-Based Design for Secure Web Services","M. Bartoletti; P. Degano; G. Ferrari; R. Zunino","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","33","49","We outline a methodology for designing and composing services in a secure manner. In particular, we are concerned with safety properties of service behavior. Services can enforce security policies locally and can invoke other services that respect given security contracts. This call-by-contract mechanism offers a significant set of opportunities, each driving secure ways to compose services. We discuss how we can correctly plan service compositions in several relevant classes of services and security properties. With this aim, we propose a graphical modeling framework based on a foundational calculus called lambda <sup>req</sup> [13]. Our formalism features dynamic and static semantics, thus allowing for formal reasoning about systems. Static analysis and model checking techniques provide the designer with useful information to assess and fix possible vulnerabilities.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70740","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359467","Web services;call-by-contract;language-based security;static analysis;system verification;Web services;call-by-contract;language-based security;static analysis;system verification","Web services;Contracts;History;Information security;Calculus;Distributed computing;Design methodology;Safety;Information analysis;Computer networks","security of data;Web services","semantics-based design;secure Web services;security policies;call-by-contract mechanism;graphical modeling framework;foundational calculus;static semantics;formal reasoning;static analysis;model checking techniques","","36","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Analogy-X: Providing Statistical Inference to Analogy-Based Software Cost Estimation","J. W. Keung; B. A. Kitchenham; D. R. Jeffery","National ICT Australia Ltd. The University of New South Wales, Sydney; National ICT Australia, Sydney; National ICT Australia Ltd. The University of New South Wales, Sydney","IEEE Transactions on Software Engineering","","2008","34","4","471","484","Data-intensive analogy has been proposed as a means of software cost estimation as an alternative to other data intensive methods such as linear regression. Unfortunately, there are drawbacks to the method. There is no mechanism to assess its appropriateness for a specific dataset. In addition, heuristic algorithms are necessary to select the best set of variables and identify abnormal project cases. We introduce a solution to these problems based upon the use of the Mantel correlation randomization test called Analogy-X. We use the strength of correlation between the distance matrix of project features and the distance matrix of known effort values of the dataset. The method is demonstrated using the Desharnais dataset and two random datasets, showing (1) the use of Mantel's correlation to identify whether analogy is appropriate, (2) a stepwise procedure for feature selection, as well as (3) the use of a leverage statistic for sensitivity analysis that detects abnormal data points. Analogy-X, thus, provides a sound statistical basis for analogy, removes the need for heuristic search and greatly improves its algorithmic performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527255","Cost estimation;Management;Statistical methods;Software Engineering;Cost estimation;Management;Statistical methods;Software Engineering","Costs;Sensitivity analysis;Linear regression;Heuristic algorithms;Software algorithms;Input variables;Australia;Computer Society;Testing;Statistical analysis","correlation methods;matrix algebra;software cost estimation;software engineering;statistical analysis","statistical inference;analogy-based software cost estimation;data-intensive analogy;data intensive methods;linear regression;heuristic algorithms;Mantel correlation randomization test;Analogy-X;distance matrix;Desharnais dataset;sensitivity analysis;heuristic search","","61","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Cryptographic verification of test coverage claims","P. T. Devanbu; S. G. Stubblebine","Dept. of Comput. Sci., California Univ., Davis, CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","2","178","192","The market for software components is growing, driven on the ""demand side"" by the need for rapid deployment of highly functional products and, on the ""supply side"", by distributed object standards. As components and component vendors proliferate, there is naturally a growing concern about quality and the effectiveness of testing processes. White-box testing, particularly the use of coverage criteria, Is a widely used method for measuring the ""thoroughness"" of testing efforts. High levels of test coverage are used as indicators of good quality control procedures. Software vendors who can demonstrate high levels of test coverage have a credible claim to high quality. However, verifying such claims involves knowledge of the source code, test cases, build procedures, etc. In applications where reliability and quality are critical, it would be desirable to verify test coverage claims without forcing vendors to give up valuable technical secrets. In this paper, we explore cryptographic techniques that can be used to verify such claims. Our techniques have certain limitations, which we discuss in this paper. However, vendors who have done the hard work of developing high levels of test coverage can use these techniques (for a modest additional cost) to provide credible evidence of high coverage, while simultaneously reducing disclosure of intellectual property.","0098-5589;1939-3520;2326-3881","","10.1109/32.841116","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=841116","","Cryptography;Costs;Application software;Software testing;Software standards;Intellectual property;Particle measurements;Quality control;Software quality;Software safety","program testing;formal verification;quality control;software quality;cryptography;safety-critical software;industrial property","software components;test coverage claims;cryptographic verification;distributed object standards;white-box testing;quality control procedures;software vendors;reliability;intellectual property","","5","","39","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of API Change- and Fault-Proneness on the User Ratings of Android Apps","G. Bavota; M. Linares-Vásquez; C. E. Bernal-Cárdenas; M. D. Penta; R. Oliveto; D. Poshyvanyk","Department of Computer Science, Free University of Bozen-Bolzano, Bolzano, Italy; Department of Computer Science, The College of William and Mary, Williamsburg, VA; Department of Computer Science, The College of William and Mary, Williamsburg, VA; Department of Engineering, University of Sannio, Benevento, Italy; Department of Bioscience and Territory, University of Molise, Pesche (IS), Italy; Department of Computer Science, The College of William and Mary, Williamsburg, VA","IEEE Transactions on Software Engineering","","2015","41","4","384","407","The mobile apps market is one of the fastest growing areas in the information technology. In digging their market share, developers must pay attention to building robust and reliable apps. In fact, users easily get frustrated by repeated failures, crashes, and other bugs; hence, they abandon some apps in favor of their competition. In this paper we investigate how the fault- and change-proneness of APIs used by Android apps relates to their success estimated as the average rating provided by the users to those apps. First, in a study conducted on 5,848 (free) apps, we analyzed how the ratings that an app had received correlated with the fault- and change-proneness of the APIs such app relied upon. After that, we surveyed 45 professional Android developers to assess (i) to what extent developers experienced problems when using APIs, and (ii) how much they felt these problems could be the cause for unfavorable user ratings. The results of our studies indicate that apps having high user ratings use APIs that are less fault- and change-prone than the APIs used by low rated apps. Also, most of the interviewed Android developers observed, in their development experience, a direct relationship between problems experienced with the adopted APIs and the users' ratings that their apps received.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2367027","NSF; NSF; NSF; European Commission; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6945855","Mining software repositories;empirical studies;android;API changes;Mining software repositories;empirical studies;android;API changes","Androids;Humanoid robots;Software;History;Computer bugs;Educational institutions;Electronic mail","application program interfaces;data mining;mobile computing;program debugging;software fault tolerance;system recovery","API change-proneness;API fault-proneness;user ratings;Android Apps;mobile Apps market;information technology;software repository mining","","49","","70","","","","","","IEEE","IEEE Journals & Magazines"
"Modular verification of software components in C","S. Chaki; E. M. Clarke; A. Groce; S. Jha; H. Veith","Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","6","388","402","We present a new methodology for automatic verification of C programs against finite state machine specifications. Our approach is compositional, naturally enabling us to decompose the verification of large software systems into subproblems of manageable complexity. The decomposition reflects the modularity in the software design. We use weak simulation as the notion of conformance between the program and its specification. Following the counterexample guided abstraction refinement (CEGAR) paradigm, our tool MAGIC first extracts a finite model from C source code using predicate abstraction and theorem proving. Subsequently, weak simulation is checked via a reduction to Boolean satisfiability. MAGIC has been interfaced with several publicly available theorem provers and SAT solvers. We report experimental results with procedures from the Linux kernel, the OpenSSL toolkit, and several industrial strength benchmarks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321061","Software engineering;formal methods;verification.","Software engineering;Programming;Protocols;Automata;Software systems;Software design;Linux;Kernel;Unified modeling language;Visualization","formal specification;program verification;C language;finite state machines;theorem proving;computability;Boolean algebra","modular verification;software component;C program;finite state machine specification;software design;counterexample guided abstraction refinement;predicate abstraction;theorem proving;Boolean satisfiability;Linux kernel;software engineering;formal method","","74","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Current trends in exception handling","D. E. Perry; A. Romanovsky; A. Tripathi","University of Texas; NA; NA","IEEE Transactions on Software Engineering","","2000","26","10","921","922","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.879816","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879816","","Computer languages;Fault tolerant systems;Spreadsheet programs;Humans;Process design;Algorithm design and analysis;Computational modeling;Large-scale systems;Maintenance","","","","9","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Conservation of Information: Software’sHidden Clockwork?","L. Hatton","Faculty of Science, Engineering and Computing, Kingston University, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","5","450","460","In this paper it is proposed that the Conservation of Hartley-Shannon Information (hereafter contracted to H-S Information) plays the same role in discrete systems as the Conservation of Energy does in physical systems. In particular, using a variational approach, it is shown that the symmetry of scale-invariance, power-laws and the Conservation of H-S Information are intimately related and lead to the prediction that the component sizes of any software system assembled from components made from discrete tokens always asymptote to a scale-free power-law distribution in the unique alphabet of tokens used to construct each component. This is then validated to a very high degree of significance on some 100 million lines of software in seven different programming languages independently of how the software was produced, what it does, who produced it or what stage of maturity it has reached. A further implication of the theory presented here is that the average size of components depends only on their unique alphabet, independently of the package they appear in. This too is demonstrated on the main data set and also on 24 additional Fortran 90 packages.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2316158","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6784340","Information conservation;component size distribution;power-law;software systems","Software systems;Computer languages;Genomics;Bioinformatics;Genetic communication","information theory;programming languages;software engineering","software hidden clockwork;Hartley-Shannon information conservation;physical systems;discrete systems;energy conservation;variational approach;scale-invariance symmetry;H-S information conservation;software system;scale-free power-law distribution;discrete tokens;Fortran","","3","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Handling obstacles in goal-oriented requirements engineering","A. van Lamsweerde; E. Letier","Dept. d'Ingenierie Inf., Univ. Catholique de Louvain, Belgium; NA","IEEE Transactions on Software Engineering","","2000","26","10","978","1005","Requirements engineering is concerned with the elicitation of high-level goals to be achieved by the envisioned system, the refinement of such goals and their operationalization into specifications of services and constraints and the assignment of responsibilities for the resulting requirements to agents such as humans, devices and software. Requirements engineering processes often result in goals, requirements, and assumptions about agent behavior that are too ideal; some of them are likely not to be satisfied from time to time in the running system due to unexpected agent behavior. The lack of anticipation of exceptional behaviors results in unrealistic, unachievable, and/or incomplete requirements. As a consequence, the software developed from those requirements will not be robust enough and will inevitably result in poor performance or failures, sometimes with critical consequences on the environment. This paper presents formal techniques for reasoning about obstacles to the satisfaction of goals, requirements, and assumptions elaborated in the requirements engineering process. The techniques are based on a temporal logic formalization of goals and domain properties; they are integrated into an existing method for goal-oriented requirements elaboration with the aim of deriving more realistic, complete, and robust requirements specifications. A key principle is to handle exceptions at requirements engineering time and at the goal level, so that more freedom is left for resolving them in a satisfactory way. The various techniques proposed are illustrated and assessed in the context of a real safety-critical system.","0098-5589;1939-3520;2326-3881","","10.1109/32.879820","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879820","","Humans;Robustness;Software systems;Software performance;Logic;Software engineering;Stress;Face;Dispatching","formal specification;systems analysis;temporal logic;safety-critical software;exception handling","goal-oriented requirements engineering;formal specification;unexpected agent behavior;software performance;software failure;temporal logic;requirements specifications;exception handling;safety-critical system","","272","","88","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Test Case Generation as a Many-Objective Optimisation Problem with Dynamic Selection of the Targets","A. Panichella; F. M. Kifetew; P. Tonella","SnT, University of Luxembourg, Luxembourg, Esch-sur-Alzette, Luxembourg; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy","IEEE Transactions on Software Engineering","","2018","44","2","122","158","The test case generation is intrinsically a multi-objective problem, since the goal is covering multiple test targets (e.g., branches). Existing search-based approaches either consider one target at a time or aggregate all targets into a single fitness function (whole-suite approach). Multi and many-objective optimisation algorithms (MOAs) have never been applied to this problem, because existing algorithms do not scale to the number of coverage objectives that are typically found in real-world software. In addition, the final goal for MOAs is to find alternative trade-off solutions in the objective space, while in test generation the interesting solutions are only those test cases covering one or more uncovered targets. In this paper, we present Dynamic Many-Objective Sorting Algorithm (DynaMOSA), a novel many-objective solver specifically designed to address the test case generation problem in the context of coverage testing. DynaMOSA extends our previous many-objective technique Many-Objective Sorting Algorithm (MOSA) with dynamic selection of the coverage targets based on the control dependency hierarchy. Such extension makes the approach more effective and efficient in case of limited search budget. We carried out an empirical study on 346 Java classes using three coverage criteria (i.e., statement, branch, and strong mutation coverage) to assess the performance of DynaMOSA with respect to the whole-suite approach (WS), its archive-based variant (WSA) and MOSA. The results show that DynaMOSA outperforms WSA in 28 percent of the classes for branch coverage (+8 percent more coverage on average) and in 27 percent of the classes for mutation coverage (+11 percent more killed mutants on average). It outperforms WS in 51 percent of the classes for statement coverage, leading to +11 percent more coverage on average. Moreover, DynaMOSA outperforms its predecessor MOSA for all the three coverage criteria in 19 percent of the classes with +8 percent more code coverage on average.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2663435","National Research Fund; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7840029","Evolutionary testing;many-objective optimisation;automatic test case generation","Heuristic algorithms;Optimization;Testing;Software algorithms;Algorithm design and analysis;Sorting;Genetic algorithms","optimisation;program testing;search problems;sorting","dynamic target selection;Many-Objective Sorting Algorithm;search-based approaches;test sequence;test input data;branch coverage;many-objective solver;MOAs;many-objective optimisation algorithms;multiple test targets;multiobjective problem;Many-Objective optimisation problem;automated test case generation;DynaMOSA","","8","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Reasoning about the Reliability of Diverse Two-Channel Systems in Which One Channel Is ""Possibly Perfect""","B. Littlewood; J. Rushby","City University, London; SRI International, Menlo Park","IEEE Transactions on Software Engineering","","2012","38","5","1178","1194","This paper refines and extends an earlier one by the first author [1]. It considers the problem of reasoning about the reliability of fault-tolerant systems with two “channels” (i.e., components) of which one, A, because it is conventionally engineered and presumed to contain faults, supports only a claim of reliability, while the other, B, by virtue of extreme simplicity and extensive analysis, supports a plausible claim of “perfection.” We begin with the case where either channel can bring the system to a safe state. The reasoning about system probability of failure on demand (pfd) is divided into two steps. The first concerns aleatory uncertainty about 1) whether channel A will fail on a randomly selected demand and 2) whether channel B is imperfect. It is shown that, conditional upon knowing p<sub>A</sub>(the probability that A fails on a randomly selected demand) and p<sub>B</sub>(the probability that channel B is imperfect), a conservative bound on the probability that the system fails on a randomly selected demand is simply p<sub>A</sub>X p<sub>B</sub>. That is, there is conditional independence between the events “A fails” and “B is imperfect.” The second step of the reasoning involves epistemic uncertainty, represented by assessors' beliefs about the distribution of (p<sub>A</sub>, p<sub>B</sub>), and it is here that dependence may arise. However, we show that under quite plausible assumptions, a conservative bound on system pfd can be constructed from point estimates for just three parameters. We discuss the feasibility of establishing credible estimates for these parameters. We extend our analysis from faults of omission to those of commission, and then combine these to yield an analysis for monitored architectures of a kind proposed for aircraft.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.80","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975177","Software reliability;software fault tolerance;program correctness;assurance case;software diversity","Uncertainty;Software;Phase frequency detector;Cognition;Software reliability;Safety","aircraft;probability;reasoning about programs;software fault tolerance;uncertainty handling","diverse two-channel system;fault tolerant system;reasoning about the reliability;aleatory uncertainty;randomly selected demand;conditional independence;epistemic uncertainty;assessors belief;PFD;aircraft;probability of failure on demand","","9","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Alloy Meets the Algebra of Programming: A Case Study","J. N. Oliveira; M. A. Ferreira","University of Minho, Braga; Software Improvement Group, Amsterdam","IEEE Transactions on Software Engineering","","2013","39","3","305","326","Relational algebra offers to software engineering the same degree of conciseness and calculational power as linear algebra in other engineering disciplines. Binary relations play the role of matrices with similar emphasis on multiplication and transposition. This matches with Alloy's lemma “everything is a relation” and with the relational basis of the Algebra of Programming (AoP). Altogether, it provides a simple and coherent approach to checking and calculating programs from abstract models. In this paper, we put Alloy and the Algebra of Programming together in a case study originating from the Verifiable File System mini-challenge put forward by Joshi and Holzmann: verifying the refinement of an abstract file store model into a journaled (Flash) data model catering to wear leveling and recovery from power loss. Our approach relies on diagrams to graphically express typed assertions. It interweaves model checking (in Alloy) with calculational proofs in a way which offers the best of both worlds. This provides ample evidence of the positive impact in software verification of Alloy's focus on relations, complemented by induction-free proofs about data structures such as stores and lists.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6155724","Model checking;algebra of programming;software verification;grand challenges in computing","Metals;Software;Programming;Matrices;Calculus;Cognition","formal verification;mathematics computing;relational algebra;software engineering","relational algebra;software engineering;linear algebra;Alloys lemma;relational basis;algebra of programming;AoP;calculating programs;verifiable file system;model checking;software verification;data structures","","3","","53","","","","","","IEEE","IEEE Journals & Magazines"
"A test generation strategy for pairwise testing","Kuo-Chung Tai; Yu Lei","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA","IEEE Transactions on Software Engineering","","2002","28","1","109","111","Pairwise testing is a specification-based testing criterion which requires that for each pair of input parameters of a system, every combination of valid values of these two parameters be covered by at least one test case. The authors propose a novel test generation strategy for pairwise testing.","0098-5589;1939-3520;2326-3881","","10.1109/32.979992","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979992","","Testing","program testing;formal specification","test generation strategy;pairwise testing;specification-based testing criterion;input parameters;valid values;test case;software testing","","149","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Scaling step-wise refinement","D. Batory; J. N. Sarvela; A. Rauschmayer","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA","IEEE Transactions on Software Engineering","","2004","30","6","355","371","Step-wise refinement is a powerful paradigm for developing a complex program from a simple program by adding features incrementally. We present the AHEAD (algebraic hierarchical equations for application design) model that shows how step-wise refinement scales to synthesize multiple programs and multiple noncode representations. AHEAD shows that software can have an elegant, hierarchical mathematical structure that is expressible as nested sets of equations. We review a tool set that supports AHEAD. As a demonstration of its viability, we have bootstrapped AHEAD tools from equational specifications, refining Java and nonJava artifacts automatically; a task that was accomplished only by ad hoc means previously.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321059","Specification;design notations and documentation;representation;design concepts;methodologies;data abstraction;extensible languages;program synthesis;feature-oriented programming;refinement.","Equations;Packaging;Java;Refining;Collaboration;Unified modeling language;Jacobian matrices;Application software;Documentation;Design methodology","algebraic specification;data structures;Java;specification languages","step-wise refinement;algebraic hierarchical equation;application design;multiple noncode representation;hierarchical mathematical structure;Java;design notation;data abstraction;program synthesis;feature-oriented programming","","197","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm","N. Mittas; L. Angelis","Aristotle University of Thessaloniki, Thessaloniki; Aristotle University of Thessaloniki, Thessaloniki","IEEE Transactions on Software Engineering","","2013","39","4","537","551","Software Cost Estimation can be described as the process of predicting the most realistic effort required to complete a software project. Due to the strong relationship of accurate effort estimations with many crucial project management activities, the research community has been focused on the development and application of a vast variety of methods and models trying to improve the estimation procedure. From the diversity of methods emerged the need for comparisons to determine the best model. However, the inconsistent results brought to light significant doubts and uncertainty about the appropriateness of the comparison process in experimental studies. Overall, there exist several potential sources of bias that have to be considered in order to reinforce the confidence of experiments. In this paper, we propose a statistical framework based on a multiple comparisons algorithm in order to rank several cost estimation models, identifying those which have significant differences in accuracy, and clustering them in nonoverlapping groups. The proposed framework is applied in a large-scale setup of comparing 11 prediction models over six datasets. The results illustrate the benefits and the significant information obtained through the systematic comparison of alternative methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6235961","Cost estimation;management;metrics/measurement;statistical methods","Predictive models;Estimation;Accuracy;Measurement uncertainty;Prediction algorithms;Clustering algorithms;Systematics","pattern clustering;software cost estimation;software development management;statistical analysis","software cost estimation model;multiple comparisons algorithm;software project;project management;statistical framework;software cost estimation ranking;software cost estimation clustering","","44","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Software fault interactions and implications for software testing","D. R. Kuhn; D. R. Wallace; A. M. Gallo","Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA","IEEE Transactions on Software Engineering","","2004","30","6","418","421","Exhaustive testing of computer software is intractable, but empirical studies of software failures suggest that testing can in some cases be effectively exhaustive. We show that software failures in a variety of domains were caused by combinations of relatively few conditions. These results have important implications for testing. If all faults in a system can be triggered by a combination of n or fewer parameters, then testing all n-tuples of parameters is effectively equivalent to exhaustive testing, if software behavior is not dependent on complex event sequences and variables have a small set of discrete values.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321063","Statistical methods;testing strategies;test design.","Software testing;System testing;Databases;Software quality;History;Microwave ovens;Fault detection;Drugs;Embedded system","software fault tolerance;program testing;failure analysis;statistical analysis","computer software testing;software failure;software behavior;event sequence;discrete value;statistical method;test design","","315","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Integer Linear Programming-Based Property Checking for Asynchronous Reactive Systems","S. Leue; W. Wei","University of Konstanz, Konstanz; SAP AG, Darmstadt","IEEE Transactions on Software Engineering","","2013","39","2","216","236","Asynchronous reactive systems form the basis of a wide range of software systems, for instance in the telecommunications domain. It is highly desirable to rigorously show that these systems are correctly designed. However, traditional formal approaches to the verification of these systems are often difficult because asynchronous reactive systems usually possess extremely large or even infinite state spaces. We propose an integer linear program (ILP) solving-based property checking framework that concentrates on the local analysis of the cyclic behavior of each individual component of a system. We apply our framework to the checking of the buffer boundedness and livelock freedom properties, both of which are undecidable for asynchronous reactive systems with an infinite state space. We illustrate the application of the proposed checking methods to Promela, the input language of the SPIN model checker. While the precision of our framework remains an issue, we propose a counterexample guided abstraction refinement procedure based on the discovery of dependences among control flow cycles. We have implemented prototype tools with which we obtained promising experimental results on real-life system models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680910","Software verification;formal methods;property checking;integer linear programming;static analysis;abstraction;refinement;counterexamples;asynchronous communication;buffer boundedness;livelock freedom;control flow cycles;cycle dependences;UML;Promela","Unified modeling language;Complexity theory;Analytical models;Message passing;Integer linear programming;Mathematical model;Cost accounting","data structures;formal languages;formal verification;integer programming;linear programming;program diagnostics;state-space methods","integer linear programming-based property checking;asynchronous reactive systems;software systems;telecommunications domain;formal approaches;infinite state spaces;ILP solving-based property checking framework;cyclic behavior;individual component;buffer boundedness;livelock freedom properties;Promela;SPIN model checker input language;counterexample guided abstraction refinement procedure;control flow cycles;real-life system models","","","","73","","","","","","IEEE","IEEE Journals & Magazines"
"Designing process replication and activation: a quantitative approach","M. Litoiu; J. Rolia; G. Serazzi","IBM Canada Ltd., Toronto, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2000","26","12","1168","1178","Distributed application systems are composed of classes of objects with instances that interact to accomplish common goals. Such systems can have many classes of users with many types of requests. Furthermore, the relative load of these classes can shift throughout the day, causing changes to system behavior and bottlenecks. When designing and deploying such systems, it is necessary to determine a process replication and threading policy for the server processes that contain the objects, as well as process activation policies. To avoid bottlenecks, the policy must support all possible workload conditions. Licensing, implementation or resource constraints can limit the number of permitted replicas or threads of a server process. Process activation policies determine whether a server is persistent or should be created and terminated with each call. This paper describes quantitative techniques for choosing process replication or threading levels and process activation policies. Inappropriate policies can lead to unnecessary queuing delays for callers or unnecessarily high consumption of memory resources. The algorithms presented consider all workload conditions, are iterative in nature and are hybrid mathematical programming and analytic performance evaluation methods. An example is given to demonstrate the technique and describe how the results can be applied during software design and deployment.","0098-5589;1939-3520;2326-3881","","10.1109/32.888630","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888630","","Process design;Licenses;Delay;Iterative algorithms;Iterative methods;Mathematical programming;Performance analysis;Algorithm design and analysis;Software design","multi-threading;distributed programming;queueing theory;mathematical programming;software performance evaluation;distributed object management;systems analysis","process replication design;process activation design;quantitative approach;distributed application systems;object classes;common goals;user classes;user requests;relative load;system behavior;bottlenecks;threading policy;server processes;workload conditions;licensing constraints;implementation constraints;resource constraints;queuing delays;memory resource consumption;iterative algorithms;mathematical programming;analytic performance evaluation methods;software design;software deployment;closed queuing networks;performance analysis;performance modeling;distributed design;nonlinear programming;linear programming","","21","","18","","","","","","IEEE","IEEE Journals & Magazines"
"The use of proof in diversity arguments","B. Littlewood","Centre for Software Reliability, City Univ., London, UK","IEEE Transactions on Software Engineering","","2000","26","10","1022","1023","The limits to the reliability that can be claimed for a design-diverse fault-tolerant system are mainly determined by the dependence that must be expected in the failure behaviours of the different versions: claims for independence between version failure processes are not believable. We examine a different approach, in which a simple secondary system is used as a back-up to a more complex primary. The secondary system is sufficiently simple that claims for its perfection (with respect to design faults) are possible, but there is not complete certainty about such perfection. It is shown that assessment of the reliability of the overall fault-tolerant system in this case may take advantage of claims for independence that are more plausible than those involved in design diversity.","0098-5589;1939-3520;2326-3881","","10.1109/32.879822","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879822","","Safety;Protection;Phase frequency detector;Air traffic control;Aerospace control;Fault tolerant systems;Fault tolerance;Aircraft;Cultural differences;Battery powered vehicles","software fault tolerance","software reliability;software fault tolerance;version failure processes;design diversity","","8","","7","","","","","","IEEE","IEEE Journals & Magazines"
"A simulation study of the model evaluation criterion MMRE","T. Foss; E. Stensrud; B. Kitchenham; I. Myrtveit","Norwegian Sch. of Manage., Sandvika, Norway; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","11","985","995","The mean magnitude of relative error, MMRE, is probably the most widely used evaluation criterion for assessing the performance of competing software prediction models. One purpose of MMRE is to assist us to select the best model. In this paper, we have performed a simulation study demonstrating that MMRE does not always select the best model. Our findings cast some doubt on the conclusions of any study of competing software prediction models that use MMRE as a basis of model comparison. We therefore recommend not using MMRE to evaluate and compare prediction models. At present, we do not have any universal replacement for MMRE. Meanwhile, we therefore recommend using a combination of theoretical justification of the models that are proposed together with other metrics proposed in this paper.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245300","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245300","","Predictive models;Software engineering;Costs;Regression tree analysis;Computational modeling;Software performance;Regression analysis;Accuracy;Classification tree analysis;Analysis of variance","software metrics;software cost estimation;digital simulation","simulation study;model evaluation criterion MMRE;mean magnitude of relative error;performance assessment;software prediction models;software metrics;software cost estimation;software engineering","","200","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Using spanning sets for coverage testing","M. Marre; A. Bertolino","Departamento de Computacion, Buenos Aires Univ., Argentina; NA","IEEE Transactions on Software Engineering","","2003","29","11","974","984","A test coverage criterion defines a set E/sub r/ of entities of the program flowgraph and requires that every entity in this set is covered under some test Case. Coverage criteria are also used to measure the adequacy of the executed test cases. In this paper, we introduce the notion of spanning sets of entities for coverage testing. A spanning set is a minimum subset of E/sub r/, such that a test suite covering the entities in this subset is guaranteed to cover every entity in E/sub r/. When the coverage of an entity always guarantees the coverage of another entity, the former is said to subsume the latter. Based on the subsumption relation between entities, we provide a generic algorithm to find spanning sets for control flow and data flow-based test coverage criteria. We suggest several useful applications of spanning sets: They help reduce and estimate the number of test cases needed to satisfy coverage criteria. We also empirically investigate how the use of spanning sets affects the fault detection effectiveness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245299","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245299","","Testing;Fault detection","program testing;data flow graphs","spanning sets;coverage testing;program flowgraph;subsumption relation;generic algorithm;data flow-based test coverage;fault detection","","52","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A control-flow analysis for a calculus of concurrent objects","P. di Blasio; K. Fisher; C. Talcott","Arthur Andersen MBA, Rome, Italy; NA; NA","IEEE Transactions on Software Engineering","","2000","26","7","617","634","We present a set-based control flow analysis for an imperative, concurrent object calculus extending the Fisher-Honsell-Mitchell functional object-oriented calculus described in Fisher, Honsell and Mitchell, (1993). The analysis is shown to be sound with respect to a transition-system semantics.","0098-5589;1939-3520;2326-3881","","10.1109/32.859531","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859531","","Calculus;Performance analysis;Runtime;Object oriented programming;Information analysis;Software safety;Prototypes;Concurrent computing;Programming profession;Data flow computing","object-oriented programming;calculus;concurrency theory;program diagnostics","control-flow analysis;calculus of concurrent objects;concurrent object calculus;concurrency;object-oriented;soundness;prototype-based","","3","","28","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study of Test Case Filtering Techniques Based on Exercising Information Flows","W. Masri; A. Podgurski; D. Leon","IEEE Computer Society; IEEE Computer Society; NA","IEEE Transactions on Software Engineering","","2007","33","7","454","477","Some software defects trigger failures only when certain local or nonlocal program interactions occur. Such interactions are modeled by the closely related concepts of information flows, program dependences, and program slices. The latter concepts underlie a 78 variety of proposed test data adequacy criteria, and they form a potentially important basis for filtering existing test cases. We report the results of an empirical study of several test case filtering techniques that are based on exercising information flows. Both coverage-based and profile-distribution-based filtering techniques are considered. They are compared to filtering techniques based on exercising simpler program elements, such as basic blocks, branches, function calls, and call pairs, with respect to their effectiveness for revealing defects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1020","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4227828","Information flow;dynamic slicing;test case filtering;test suite minimization;coverage-based filtering;distribution-based filtering;software testing;empirical studies.","Information filtering;Information filters;Automatic testing;Software testing;Computer Society;Instruments;Computer science;Joining processes;Timing","program slicing;program testing;software fault tolerance","test case filtering;information flows;software defects;program interactions;program dependences;program slices;coverage-based filtering;profile-distribution-based filtering","","45","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Mutation Operators for Spreadsheets","R. Abraham; M. Erwig","Oregon State University, Corvallis; Oregon State University, Corvallis","IEEE Transactions on Software Engineering","","2009","35","1","94","108","Based on 1) research into mutation testing for general-purpose programming languages and 2) spreadsheet errors that have been reported in the literature, we have developed a suite of mutation operators for spreadsheets. We present an evaluation of the mutation adequacy of definition-use adequate test suites generated by a constraint-based automatic test-case generation system we have developed in previous work. The results of the evaluation suggest additional constraints that can be incorporated into the system to target mutation adequacy. In addition to being useful in mutation testing of spreadsheets, the operators can be used in the evaluation of error-detection tools and also for seeding spreadsheets with errors for empirical studies. We describe two case studies where the suite of mutation operators helped us carry out such empirical evaluations. The main contribution of this paper is a suite of mutation operators for spreadsheets that can be used for performing empirical evaluations of spreadsheet tools to indicate ways in which the tools can be improved.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4609389","Test coverage of code;Test design;Spreadsheets;Test coverage of code;Test design;Spreadsheets","Genetic mutations;System testing;Software testing;Computer languages;Automatic testing;Software engineering;Costs;Performance evaluation;Spreadsheet programs;Collaboration","program testing;spreadsheet programs","spreadsheet mutation operator;constraint-based automatic test-case generation system;error-detection tool;general purpose programming language;mutation testing","","31","","86","","","","","","IEEE","IEEE Journals & Magazines"
"Parametric fault tree for the dependability analysis of redundant systems and its high-level Petri net semantics","A. Bobbio; G. Franceschinis; R. Gaeta; L. Portinale","Dipt. di Informatica, Univ. del Piemonte Orientale, Alessandria, Italy; Dipt. di Informatica, Univ. del Piemonte Orientale, Alessandria, Italy; NA; NA","IEEE Transactions on Software Engineering","","2003","29","3","270","287","In order to cope efficiently with the dependability analysis of redundant systems with replicated units, a new, more compact fault-tree formalism, called Parametric Fault Tree (PFT), is defined. In a PFT formalism, replicated units are folded and indexed so that only one representative of the similar replicas is included in the model. From the PFT, a list of parametric cut sets can be derived, where only the relevant patterns leading to the system failure are evidenced regardless of the actual identity of the component in the cut set. The paper provides an algorithm to convert a PFT into a class of High-Level Petri Nets, called SWN. The purpose of this conversion is twofold: to exploit the modeling power and flexibility of the SWN formalism, allowing the analyst to include statistical dependencies that could not have been accommodated into the corresponding PFT and to exploit the capability of the SWN formalism to generate a lumped Markov chain, thus alleviating the state explosion problem. The search for the minimal cut sets (qualitative analysis) can be often performed by a structural T-invariant analysis on the generated SWN. The advantages that can be obtained from the translation of a PFT into a SWN are investigated considering a fault-tolerant multiprocessor system example.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183940","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183940","","Fault trees;Power system modeling;Independent component analysis;US Department of Transportation;Tellurium;Performance analysis;Failure analysis;Probability;Computer Society;Petri nets","Petri nets;redundancy;fault tolerant computing","Petri net semantics;dependability analysis;redundant systems;Parametric Fault Tree;SWN;lumped Markov chain;minimal cut sets;structural T-invariant analysis;fault-tolerant multiprocessor","","32","","31","","","","","","IEEE","IEEE Journals & Magazines"
"From UML to Petri Nets: The PCM-Based Methodology","S. Distefano; M. Scarpa; A. Puliafito","University of Messina, Sicily; University of Messina, Sicily; University of Messina, Sicily","IEEE Transactions on Software Engineering","","2011","37","1","65","79","In this paper, we present an evaluation methodology to validate the performance of a UML model, representing a software architecture. The proposed approach is based on open and well-known standards: UML for software modeling and the OMG Profile for Schedulability, Performance, and Time Specification for the performance annotations into UML models. Such specifications are collected in an intermediate model, called the Performance Context Model (PCM). The intermediate model is translated into a performance model which is subsequently evaluated. The paper is focused on the mapping from the PCM to the performance domain. More specifically, we adopt Petri nets as the performance domain, specifying a mapping process based on a compositional approach we have entirely implemented in the ArgoPerformance tool. All of the rules to derive a Petri net from a PCM and the performance measures assessable from the former are carefully detailed. To validate the proposed technique, we provide an in-depth analysis of a web application for music streaming.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5396344","Software engineering;performances evaluation;Petri nets;UML;software performance engineering.","Unified modeling language;Petri nets;Phase change materials;Software architecture;Software performance;Context modeling;Application software;Design engineering;Performance analysis;Stochastic processes","media streaming;Petri nets;software architecture;software metrics;software performance evaluation;Unified Modeling Language;Web services","UML;Petri nets;PCM;software architecture;software modeling;OMG profile;schedulability;time specification;performance context model;mapping process;ArgoPerformance tool;music streaming;Web application","","26","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Investigating reading techniques for object-oriented framework learning","F. Shull; F. Lanubile; V. R. Basili","Fraunhofer Center, Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","11","1101","1118","The empirical study described in the paper addresses software reading for construction: how application developers obtain an understanding of a software artifact for use in new system development. The study focuses on the processes that developers would engage in when learning and using object oriented frameworks. We analyzed 15 student software development projects using both qualitative and quantitative methods to gain insight into what processes occurred during framework usage. The contribution of the study is not to test predefined hypotheses but to generate well-supported hypotheses for further investigation. The main hypotheses produced are that example based techniques are well suited to use by beginning learners, while hierarchy based techniques are not, because of a larger learning curve. Other more specific hypotheses are proposed and discussed.","0098-5589;1939-3520;2326-3881","","10.1109/32.881720","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=881720","","Application software;Programming;Software engineering;Software libraries;Computer Society;Testing;Technical activities;User interfaces;Buildings;Skeleton","bibliographies;computer science education;teaching;object-oriented programming;reverse engineering","reading techniques;object oriented framework learning;software reading;application developers;software artifact;system development;object oriented frameworks;student software development projects;quantitative methods;framework usage;predefined hypotheses;example based techniques;beginning learners;hierarchy based techniques;learning curve","","33","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Problems with Precision: A Response to ""Comments on 'Data Mining Static Code Attributes to Learn Defect Predictors'""","T. Menzies; A. Dekhtyar; J. Distefano; J. Greenwald","Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV 26506; Department of Computer Science, California State Polytechnic University, San Luis Obispo, CA 93407; Integrated Software Metrics, 1000 Technology Drive, Suite 1215, Fairmont, WV 26554; Department of Computer Science, Portland State University, PO Box 751, Portland, OR 97207-0751","IEEE Transactions on Software Engineering","","2007","33","9","637","640","Zhang and Zhang argue that predictors are useless unless they have high precison&amp;recall. We have a different view, for two reasons. First, for SE data sets with large neg/pos ratios, it is often required to lower precision to achieve higher recall. Second, there are many domains where low precision detectors are useful.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70721","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288197","","Data mining;Detectors;Equations;Predictive models;Accuracy;Software engineering;NASA;Testing;Performance evaluation;Project management","data mining","data mining static code attributes;defect predictors","","97","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Deriving a Slicing Algorithm via FermaT Transformations","M. P. Ward; H. Zedan","De Montfort University, Leicester; De Montfort University, Leicester","IEEE Transactions on Software Engineering","","2011","37","1","24","47","In this paper, we present a case study in deriving an algorithm from a formal specification via FermaT transformations. The general method (which is presented in a separate paper) is extended to a method for deriving an implementation of a program transformation from a specification of the program transformation. We use program slicing as an example transformation since this is of interest outside the program transformation community. We develop a formal specification for program slicing in the form of a WSL specification statement which is refined into a simple slicing algorithm by applying a sequence of general purpose program transformations and refinements. Finally, we show how the same methods can be used to derive an algorithm for semantic slicing. The main novel contributions of this paper are: 1) developing a formal specification for slicing, 2) expressing the definition of slicing in terms of a WSL specification statement, and 3) by applying correctness preserving transformations to the specification, we can derive a simple slicing algorithm.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401170","Program slicing;program transformations;formal methods;algorithm derivation.","Formal specifications;Logic;Software algorithms;Reverse engineering;Assembly;High level languages","formal specification;program slicing","slicing algorithm;FermaT transformations;program transformation;program slicing;WSL specification statement;semantic slicing","","5","","73","","","","","","IEEE","IEEE Journals & Magazines"
"Specifying and Validating Data-Aware Temporal Web Service Properties","S. Halle; R. Villemaire; O. Cherkaoui","University of California, Santa Barbara, Santa Barbara; Universit&#x0E9; du Qu&#x0E9;bec &#x0E0; Montr&#x0E9;al, Montr&#x0E9;al; Universit&#x0E9; du Qu&#x0E9;bec &#x0E0; Montr&#x0E9;al, Montr&#x0E9;al","IEEE Transactions on Software Engineering","","2009","35","5","669","683","Most works that extend workflow validation beyond syntactical checking consider constraints on the sequence of messages exchanged between services. These constraints are expressed only in terms of message names and abstract away their actual data content. We provide examples of real-world ""data-aware"" Web service constraints where the sequence of messages and their content are interdependent. To this end, we present CTL-FO<sup>+</sup>, an extension over computation tree logic that includes first-order quantification on message content in addition to temporal operators. We show how CTL-FO<sup>+</sup> is adequate for expressing data-aware constraints, give a sound and complete model checking algorithm for CTL-FO<sup>+</sup>, and establish its complexity to be PSPACE-complete. A ""naive"" translation of CTL-FO<sup>+</sup> into CTL leads to a serious exponential blowup of the problem that prevents existing validation tools to be used. We provide an alternate translation of CTL-FO<sup>+</sup> into CTL, where the construction of the workflow model depends on the property to validate. We show experimentally how this translation is significantly more efficient for complex formulas and makes model checking of data-aware temporal properties on real-world Web service workflows tractable using off-the-shelf tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4907003","Web services;software/program verification;model checking;temporal logic.","Web services;Logic;Formal languages;Computer Society;Web and internet services;Guidelines;Contracts;Terminology;Simple object access protocol","computational complexity;formal specification;program verification;temporal logic;trees (mathematics);Web services","data-aware temporal Web service property;workflow validation;syntactical checking;messages exchange;CTL-FO<sup>+</sup>;computation tree logic;first-order quantification;model checking algorithm complexity;PSPACE-complete;formal specification","","22","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Respectful type converters","J. M. Wing; J. Ockerbloom","Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","7","579","593","In converting an object of one type to another, we expect some of the original object's behavior to remain the same and some to change. How can we state the relationship between the original object and converted object to characterize what information is preserved and what is lost after the conversion takes place? We answer this question by introducing the new relation, respects, and say that a type converter function C:A/spl rarr/B respects a type T. We formally define respects in terms of the Liskov and Wing behavioral notion of subtyping; types A and B are subtypes of T. We explain in detail the applicability of respectful type converters in the context of the Typed Object Model (TOM) Conversion Service, built at Carnegie Mellon and used on a daily basis throughout the world. We also briefly discuss how our respects relation addresses a similar question in two other contexts: type evolution and interoperability.","0098-5589;1939-3520;2326-3881","","10.1109/32.859529","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859529","","Information retrieval;Image converters;Displays;HTML;Context-aware services;Object oriented modeling;Context modeling;Internet;Web sites","type theory;object-oriented programming","type converters;respects;type converter function;subtyping;respectful type converters;Typed Object Model;type evolution;interoperability","","6","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting fault incidence using software change history","T. L. Graves; A. F. Karr; J. S. Marron; H. Siy","Los Alamos Nat. Lab., NM, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","7","653","661","This paper is an attempt to understand the processes by which software ages. We define code to be aged or decayed if its structure makes it unnecessarily difficult to understand or change and we measure the extent of decay by counting the number of faults in code in a period of time. Using change management data from a very large, long-lived software system, we explore the extent to which measurements from the change history are successful in predicting the distribution over modules of these incidences of faults. In general, process measures based on the change history are more useful in predicting fault rates than product metrics of the code: For instance, the number of times code has been changed is a better indication of how many faults it will contain than is its length. We also compare the fault rates of code of various ages, finding that if a module is, on the average, a year older than an otherwise similar module, the older module will have roughly a third fewer faults. Our most successful model measures the fault potential of a module as the sum of contributions from all of the times the module has been changed, with large, recent changes receiving the most weight.","0098-5589;1939-3520;2326-3881","","10.1109/32.859533","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859533","","History;Predictive models;Software systems;Aging;Time measurement;Software measurement;Length measurement;Software development management;Statistical analysis;Degradation","software maintenance;software metrics;software fault tolerance;management of change","fault incidence;software change history;change management data;change history;fault potential;code decay;metrics;statistical analysis","","319","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient evaluation of multifactor dependent system performance using fractional factorial design","T. Berling; P. Runeson","Ericsson Microwave Syst. AB, Molndal, Sweden; NA","IEEE Transactions on Software Engineering","","2003","29","9","769","781","Performance of computer-based systems may depend on many different factors, internal and external. In order to design a system to have the desired performance or to validate that the system has the required performance, the effect of the influencing factors must be known. Common methods give no or little guidance on how to vary the factors during prototyping or validation. Varying the factors in all possible combinations would be too expensive and too time-consuming. This paper introduces a systematic approach to the prototyping and the validation of a system's performance, by treating the prototyping or validation as an experiment, in which the fractional factorial design methodology is commonly used. To show that this is possible, a case study evaluating the influencing factors of the false and real target rate of a radar system is described. Our findings show that prototyping and validation of system performance become structured and effective when using the fractional factorial design. The methodology enables planning, performance, structured analysis, and gives guidance for appropriate test cases. The methodology yields not only main factors, but also interacting factors. The effort is minimized for finding the results, due to the methodology. The case study shows that after 112 test cases, of 1024 possible, the knowledge gained was enough to draw conclusions on the effects and interactions of 10 factors. This is a reduction with a factor 5-9 compared to alternative methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232283","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232283","","System performance;Testing;Prototypes;Design methodology;Radar;Performance analysis;Time measurement;Microcomputers;Filters;Temperature dependence","radar computing;software prototyping;software performance evaluation;formal verification","multifactor dependent system performance;fractional factorial design;prototyping;validation;systematic approach;target rate;radar system;planning","","25","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Finding Bugs in Web Applications Using Dynamic Test Generation and Explicit-State Model Checking","S. Artzi; A. Kiezun; J. Dolby; F. Tip; D. Dig; A. Paradkar; M. D. Ernst","Thomas J. Watson Research Center, Hawthorne; Women's Hospital/Harvard Medical School, Boston; Thomas J. Watson Research Center, Hawthorne; Thomas J. Watson Research Center, Hawthorne; University of Illinois at Urbana-Champaign, Urbana; Thomas J. Watson Research Center, Hawthorne; University of Washington, Seattle","IEEE Transactions on Software Engineering","","2010","36","4","474","494","Web script crashes and malformed dynamically generated webpages are common errors, and they seriously impact the usability of Web applications. Current tools for webpage validation cannot handle the dynamically generated pages that are ubiquitous on today's Internet. We present a dynamic test generation technique for the domain of dynamic Web applications. The technique utilizes both combined concrete and symbolic execution and explicit-state model checking. The technique generates tests automatically, runs the tests capturing logical constraints on inputs, and minimizes the conditions on the inputs to failing tests so that the resulting bug reports are small and useful in finding and fixing the underlying faults. Our tool Apollo implements the technique for the PHP programming language. Apollo generates test inputs for a Web application, monitors the application for crashes, and validates that the output conforms to the HTML specification. This paper presents Apollo's algorithms and implementation, and an experimental evaluation that revealed 673 faults in six PHP Web applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5416728","Software testing;Web applications;dynamic analysis;PHP;reliability;verification.","Computer bugs;Vehicle crash testing;Automatic testing;Logic testing;Computer crashes;Usability;Internet;Concrete;Computer languages;HTML","program debugging;program testing;program verification;software tools","dynamic test generation;explicit state model checking;Web script;Web pages;Apollo tool;PHP programming language;HTML specification;PHP Web applications;bugs;Internet","","41","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Design-level performance prediction of component-based applications","Y. Liu; I. Gorton; A. Fekete","Nat. ICT Australia, NSW, Australia; Nat. ICT Australia, NSW, Australia; NA","IEEE Transactions on Software Engineering","","2005","31","11","928","941","Server-side component technologies such as Enterprise JavaBeans (EJBs), .NET, and CORBA are commonly used in enterprise applications that have requirements for high performance and scalability. When designing such applications, architects must select suitable component technology platform and application architecture to provide the required performance. This is challenging as no methods or tools exist to predict application performance without building a significant prototype version for subsequent benchmarking. In this paper, we present an approach to predict the performance of component-based server-side applications during the design phase of software development. The approach constructs a quantitative performance model for a proposed application. The model requires inputs from an application-independent performance profile of the underlying component technology platform, and a design description of the application. The results from the model allow the architect to make early decisions between alternative application architectures in terms of their performance and scalability. We demonstrate the method using an EJB application and validate predictions from the model by implementing two different application architectures and measuring their performance on two different implementations of the EJB platform.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.127","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556552","Index Terms- Quality analysis and evaluation;software architectures;performance measures.","Application software;Scalability;Computer architecture;Java;Buildings;Software prototyping;Prototypes;Software design;Programming;Predictive models","object-oriented programming;software metrics;software performance evaluation;Java","design-level performance prediction;enterprise JavaBeans;.NET;CORBA;component-based server-side application;software development;quantitative performance model;application-independent performance profile","","42","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Transformation-based diagnosis of student programs for programming tutoring systems","Songwen Xu; Yam San Chee","Peoplesoft Inc., Santa Clara, CA, USA; NA","IEEE Transactions on Software Engineering","","2003","29","4","360","384","A robust technology that automates the diagnosis of students' programs is essential for programming tutoring systems. Such technology should be able to determine whether programs coded by a student are correct. If a student's program is incorrect, the system should be able to pinpoint errors in the program as well as explain and correct the errors. Due to the difficulty of this problem, no existing system performs this task entirely satisfactorily, and this problem still hampers the development of programming tutoring systems. This paper describes a transformation-based approach to automate the diagnosis of students' programs for programming tutoring systems. Improved control-flow analysis and data-flow analysis are used in program analysis. Automatic diagnosis of student programs is achieved by comparing the student program with a specimen program at the semantic level after both are standardized. The approach was implemented and tested on 525 real student programs for nine different programming tasks. Test results show that the method satisfies the requirements stated above. Compared to other existing approaches to automatic diagnosis of student programs, the approach developed here is more rigorous and safer in identifying student programming errors. It is also simpler to make use of in practice. Only specimen programs are needed for the diagnosis of student programs. The techniques of program standardization and program comparison developed here may also be useful for research in the fields of program understanding and software maintenance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1191799","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1191799","","Automatic programming;Error correction;Program processors;Data analysis;Testing;Computer Society;Robustness;Computer errors;Automatic control;Standardization","computer science education;computer aided instruction;program diagnostics","transformation-based program diagnosis;student programs;programming tutoring systems;control-flow analysis;data-flow analysis;student programming error identification;program standardization;program comparison;program understanding;software maintenance","","20","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A method for design and performance modeling of client/server systems","D. A. Menasce; H. Gomaa","Dept. of Comput. Sci., George Mason Univ., Fairfax, VA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","11","1066","1085","Designing complex distributed client/server applications that meet performance requirements may prove extremely difficult in practice if software developers are not willing or do not have the time to help software performance analysts. The paper advocates the need to integrate both design and performance modeling activities so that one can help the other. We present a method developed and used by the authors in the design of a fairly large and complex client/server application. The method is based on a software performance engineering language developed by one of the authors. Use cases were developed and mapped to a performance modeling specification using the language. A compiler for the language generates an analytic performance model for the system. Service demand parameters at servers, storage boxes, and networks are derived by the compiler from the system specification. A detailed model of DBMS query optimizers allows the compiler to estimate the number of I/Os and CPU time for SQL statements. The paper concludes with some results of the application that prompted the development of the method and language.","0098-5589;1939-3520;2326-3881","","10.1109/32.881718","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=881718","","Design methodology;Application software;Software performance;Network servers;Performance analysis;Computer Society;Databases;Unified modeling language;Mission critical systems;Costs","client-server systems;software performance evaluation;formal specification;program compilers;query processing","performance modeling;client/server systems design;distributed client/server applications;performance requirements;software developers;software performance analysts;performance modeling activities;software performance engineering language;use cases;performance modeling specification;compiler;analytic performance model;service demand parameters;storage boxes;system specification;DBMS query optimizers;CPU time;SQL statements","","37","","46","","","","","","IEEE","IEEE Journals & Magazines"
"A note on inconsistent axioms in Rushby's ""systematic formal verification for fault-tolerant time-triggered algorithms""","L. Pike","Galois Connections, Beaverton, OR, USA","IEEE Transactions on Software Engineering","","2006","32","5","347","348","We describe some inconsistencies in John Rushby's axiomatization of time-triggered algorithms that he presented in these transactions and that he formally specifies and verifies in the mechanical theorem-prover PVS. We present corrections for these inconsistencies that have been checked for consistency in PVS","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642681","Formal methods;formal verification;time-triggered algorithms;synchronous systems;PVS.","Formal verification;Fault tolerant systems;Clocks;Formal specifications;Real time systems;Conferences;Error correction;Floors","formal specification;formal verification;software fault tolerance;theorem proving","systematic formal verification;John Rushby axiomatization;fault-tolerant computing;formal specification;time-triggered algorithms;PVS mechanical theorem-prover","","7","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""On object systems and behavioral inheritance""","J. C. Chen; H. C. Jiau","Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","IEEE Transactions on Software Engineering","","2003","29","6","576","","After reviewing the regular paper ""On Object Systems and Behavioral Inheritance"" in IEEE Transactions on Software Engineering, vol. 28, no. 9, Sept. 2002, and discussing it with Professor H.C. Jiau, several errors were found and are stated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205185","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205185","","Computer science;Humans;Error correction;Mathematics","inheritance;object-oriented programming","object systems;behavioral inheritance","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing the Effectiveness of Sequence Diagrams in the Comprehension of Functional Requirements: Results from a Family of Five Experiments","S. Abrahão; C. Gravino; E. Insfran; G. Scanniello; G. Tortora","Universitat Politècnica de València, València; University of Salerno via Ponte Don Melillo, Salerno; Universitat Politècnica de València, València; University of Basilicata Viale DellAteneo, Macchia Romana, Potenza; University of Salerno via Ponte Don Melillo, Salerno","IEEE Transactions on Software Engineering","","2013","39","3","327","342","Modeling is a fundamental activity within the requirements engineering process and concerns the construction of abstract descriptions of requirements that are amenable to interpretation and validation. The choice of a modeling technique is critical whenever it is necessary to discuss the interpretation and validation of requirements. This is particularly true in the case of functional requirements and stakeholders with divergent goals and different backgrounds and experience. This paper presents the results of a family of experiments conducted with students and professionals to investigate whether the comprehension of functional requirements is influenced by the use of dynamic models that are represented by means of the UML sequence diagrams. The family contains five experiments performed in different locations and with 112 participants of different abilities and levels of experience with UML. The results show that sequence diagrams improve the comprehension of the modeled functional requirements in the case of high ability and more experienced participants.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6193111","Documentation;software engineering;requirements specifications","Unified modeling language;Object oriented modeling;Analytical models;Computational modeling;Software systems;Materials","formal specification;Unified Modeling Language","effectiveness assessment;UML sequence diagrams;functional requirements;family-of-five experiments;requirements engineering process;abstract descriptions;requirement interpretation;requirement validation;functional stakeholders;software engineering;requirements specifications;unified modeling language","","23","","55","","","","","","IEEE","IEEE Journals & Magazines"
"StakeRare: Using Social Networks and Collaborative Filtering for Large-Scale Requirements Elicitation","S. L. Lim; A. Finkelstein","University College London, London; University College London, London","IEEE Transactions on Software Engineering","","2012","38","3","707","735","Requirements elicitation is the software engineering activity in which stakeholder needs are understood. It involves identifying and prioritizing requirements-a process difficult to scale to large software projects with many stakeholders. This paper proposes StakeRare, a novel method that uses social networks and collaborative filtering to identify and prioritize requirements in large software projects. StakeRare identifies stakeholders and asks them to recommend other stakeholders and stakeholder roles, builds a social network with stakeholders as nodes and their recommendations as links, and prioritizes stakeholders using a variety of social network measures to determine their project influence. It then asks the stakeholders to rate an initial list of requirements, recommends other relevant requirements to them using collaborative filtering, and prioritizes their requirements using their ratings weighted by their project influence. StakeRare was evaluated by applying it to a software project for a 30,000-user system, and a substantial empirical study of requirements elicitation was conducted. Using the data collected from surveying and interviewing 87 stakeholders, the study demonstrated that StakeRare predicts stakeholder needs accurately and arrives at a more complete and accurately prioritized list of requirements compared to the existing method used in the project, taking only a fraction of the time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5740931","Requirements/specifications;elicitation methods;requirements prioritization;experimentation;human factors;recommender systems;social network analysis;stakeholder analysis.","Social network services;Collaboration;Filtering;Software;Size measurement;Software engineering;Business","collaborative filtering;data acquisition;project management;recommender systems;social networking (online);software management","social network;collaborative filtering;requirement elicitation;software engineering;stakeholder;StakeRare;recommender system;software project;data collection","","51","","113","","","","","","IEEE","IEEE Journals & Magazines"
"Controversies about the black and white diamonds","F. Barbier; B. Henderson-Sellers","Pau Univ., France; NA","IEEE Transactions on Software Engineering","","2003","29","11","1056","","F. Barbier et al. (2003) offered a formal definition for the semantics of the whole-part relationship in the Unified Modeling Language or UML. H.B.K. Tan et al. (2003) raised problems within some parts of the formalization. We discuss these problems and their remedies developed by H.B.K. Tan et al. (2003).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245308","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245308","","Unified modeling language;Navigation;Australia","specification languages","semantics;whole-part relationship;Unified Modeling Language;UML","","1","","2","","","","","","IEEE","IEEE Journals & Magazines"
"Reply to comments on ""An Interval Logic for Real-Time System Specification""","P. Bellini; P. Nesi; D. Rogai","Dept. of Syst. & Informatics, Florence Univ., Italy; Dept. of Syst. & Informatics, Florence Univ., Italy; Dept. of Syst. & Informatics, Florence Univ., Italy","IEEE Transactions on Software Engineering","","2006","32","6","428","431","The paper on Comments on ""An Interval Logic for Real-Time System Specification"" presents some remarks on the comparison examples from TILCO and other logics and some slips on the related examples. This paper gives evidence that such issues have no impact on the validity of the TILCO theory of paper and provides some further clarifications about some aspects of the comparison","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650217","Formal language;temporal logic;interval temporal logic;TILCO;conciseness.","Logic;Real time systems;Usability;Solids;Production;Timing","formal specification;real-time systems;specification languages;temporal logic","interval logic;real-time system specification;TILCO theory","","2","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Change Distilling:Tree Differencing for Fine-Grained Source Code Change Extraction","B. Fluri; M. Wuersch; M. PInzger; H. Gall","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","11","725","743","A key issue in software evolution analysis is the identification of particular changes that occur across several versions of a program. We present change distilling, a tree differencing algorithm for fine-grained source code change extraction. For that, we have improved the existing algorithm by Chawathe et al. for extracting changes in hierarchically structured data. Our algorithm extracts changes by finding both a match between the nodes of the compared two abstract syntax trees and a minimum edit script that can transform one tree into the other given the computed matching. As a result, we can identify fine-grained change types between program versions according to our taxonomy of source code changes. We evaluated our change distilling algorithm with a benchmark that we developed, which consists of 1,064 manually classified changes in 219 revisions of eight methods from three different open source projects. We achieved significant improvements in extracting types of source code changes: Our algorithm approximates the minimum edit script 45 percent better than the original change extraction approach by Chawathe et al. We are able to find all occurring changes and almost reach the minimum conforming edit script, that is, we reach a mean absolute percentage error of 34 percent, compared to the 79 percent reached by the original algorithm. The paper describes both our change distilling algorithm and the results of our evolution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70731","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339230","Source code change extraction;tree differencing algorithms;software repositories;software evolution analysis","Data mining;Taxonomy;Software maintenance;Programming profession;Software algorithms;Algorithm design and analysis;Software tools;Maintenance engineering;Software systems;History","software maintenance;software prototyping;tree data structures","minimum edit script;abstract syntax trees;software evolution analysis;fine-grained source code change extraction;change distilling tree differencing algorithm","","190","","47","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient distributed online algorithm to detect strong conjunctive predicates","Loon-Been Chen; I-Chen Wu","Dept. of Inf. Manage., Chin-Min Coll., Miao-Li, Taiwan; NA","IEEE Transactions on Software Engineering","","2002","28","11","1077","1084","Detecting strong conjunctive predicates is a fundamental problem in debugging and testing distributed programs. A strong conjunctive predicate is a logical statement to represent the desired event of the system. Therefore, if the predicate is not true, an error may occur because the desired event does not happen. Recently, several reported detection algorithms reveal the problem of unbounded state queue growth since the system may generate a huge number of execution states in a very short time. In order to solve this problem, this paper introduces the notion of removable states which can be disregarded in the sense that detection results still remain correct. A fully distributed algorithm is developed in this paper to perform the detection in an online manner. Based on the notion of removable states, the time complexity of the detection algorithm is improved as the number of states to be evaluated is reduced.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049405","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049405","","Debugging;Detection algorithms;Testing;Distributed algorithms;Size control;Distributed computing;Event detection;Performance evaluation;Algorithm design and analysis","computational complexity;program debugging;program testing;distributed programming;distributed algorithms","strong conjunctive predicate detection;debugging;testing;distributed online algorithm;distributed programs;logical statement;desired system event;error;unbounded state queue growth;execution states;removable states;time complexity","","2","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Impact of Budget and Schedule Pressure on Software Development Cycle Time and Effort","N. Nan; D. E. Harter","University of Oklahoma, Norman; Syracuse University, Syracuse","IEEE Transactions on Software Engineering","","2009","35","5","624","637","As excessive budget and schedule compression becomes the norm in today's software industry, an understanding of its impact on software development performance is crucial for effective management strategies. Previous software engineering research has implied a nonlinear impact of schedule pressure on software development outcomes. Borrowing insights from organizational studies, we formalize the effects of budget and schedule pressure on software cycle time and effort as U-shaped functions. The research models were empirically tested with data from a 25 billion/year international technology firm, where estimation bias is consciously minimized and potential confounding variables are properly tracked. We found that controlling for software process, size, complexity, and conformance quality, budget pressure, a less researched construct, has significant U-shaped relationships with development cycle time and development effort. On the other hand, contrary to our prediction, schedule pressure did not display significant nonlinear impact on development outcomes. A further exploration of the sampled projects revealed that the involvement of clients in the software development might have ldquoerodedrdquo the potential benefits of schedule pressure. This study indicates the importance of budget pressure in software development. Meanwhile, it implies that achieving the potential positive effect of schedule pressure requires cooperation between clients and software development teams.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815275","Cost estimation;time estimation;schedule and organizational issues;systems development.","Programming;Job shop scheduling;Software performance;Computer industry;Financial management;Software development management;Software engineering;Testing;Pressure control;Size control","budgeting;DP industry;project management;sampling methods;scheduling;software cost estimation;software development management;software metrics;software quality;statistical testing","budget pressure;schedule pressure;software development cycle time estimation;software development effort estimation;software industry;software development performance;software engineering research;nonlinear impact;organizational study;U-shaped function;empirical testing;international technology firm;potential confounding variable;software process control;software size control;software complexity control;software conformance quality control;sampled project management;potential positive effect;software development team management strategy;software cost estimation","","31","","74","","","","","","IEEE","IEEE Journals & Magazines"
"Special Section - On the International Conference on the Foundations of Software Engineering","","","IEEE Transactions on Software Engineering","","2003","29","10","0_1","0_2","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237167","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237167","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Centroidal Voronoi Tessellations- A New Approach to Random Testing","A. Shahbazi; A. F. Tappenden; J. Miller","University of Alberta, Edmonton; The King's University College, Edmonton; University of Alberta, Edmonton","IEEE Transactions on Software Engineering","","2013","39","2","163","183","Although Random Testing (RT) is low cost and straightforward, its effectiveness is not satisfactory. To increase the effectiveness of RT, researchers have developed Adaptive Random Testing (ART) and Quasi-Random Testing (QRT) methods which attempt to maximize the test case coverage of the input domain. This paper proposes the use of Centroidal Voronoi Tessellations (CVT) to address this problem. Accordingly, a test case generation method, namely, Random Border CVT (RBCVT), is proposed which can enhance the previous RT methods to improve their coverage of the input space. The generated test cases by the other methods act as the input to the RBCVT algorithm and the output is an improved set of test cases. Therefore, RBCVT is not an independent method and is considered as an add-on to the previous methods. An extensive simulation study and a mutant-based software testing investigation have been performed to demonstrate the effectiveness of RBCVT against the ART and QRT methods. Results from the experimental frameworks demonstrate that RBCVT outperforms previous methods. In addition, a novel search algorithm has been incorporated into RBCVT reducing the order of computational complexity of the new approach. To further analyze the RBCVT method, randomness analysis was undertaken demonstrating that RBCVT has the same characteristics as ART methods in this regard.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6165316","Adaptive random testing;centroidal Voronoi tessellation;P-measure;random testing;software testing;test case generation;test strategies","Subspace constraints;Software testing;Generators;Software algorithms;Power capacitors;Runtime","computational complexity;computational geometry;program testing","centroidal Voronoi tessellations;adaptive random testing method;ART method;quasi-random testing method;QRT method;test case generation method;random border CVT;RBCVT algorithm;mutant-based software testing;search algorithm;computational complexity;randomness analysis;software defects","","23","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Temporal logic query checking: a tool for model exploration","A. Gurfinkel; M. Chechik; B. Devereux","Dept. of Comput. Sci., Toronto Univ., Ont., Canada; Dept. of Comput. Sci., Toronto Univ., Ont., Canada; Dept. of Comput. Sci., Toronto Univ., Ont., Canada","IEEE Transactions on Software Engineering","","2003","29","10","898","914","Temporal logic query checking was first introduced by W. Chan in order to speed up design understanding by discovering properties not known a priori. A query is a temporal logic formula containing a special symbol ?/sub 1/, known as a placeholder. Given a Kripke structure and a propositional formula /spl phi/, we say that /spl phi/ satisfies the query if replacing the placeholder by /spl phi/ results in a temporal logic formula satisfied by the Kripke structure. A solution to a temporal logic query on a Kripke structure is the set of all propositional formulas that satisfy the query. Query checking helps discover temporal properties of a system and, as such, is a useful tool for model exploration. In this paper, we show that query checking is applicable to a variety of model exploration tasks, ranging from invariant computation to test case generation. We illustrate these using a Cruise Control System. Additionally, we show that query checking is an instance of a multi-valued model checking of Chechik et al. This approach enables us to build an implementation of a temporal logic query checker, TLQSolver, on top of our existing multi-valued model checker /sub /spl chi//Chek. It also allows us to decide a large class of queries and introduce witnesses for temporal logic queries-an essential notion for effective model exploration.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237171","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237171","","Multivalued logic;Logic design;Computer Society;Testing;Control systems;Buildings","temporal logic;multivalued logic;program diagnostics","temporal logic;model understanding;multi-valued logic;query;Kripke structure;propositional formula;Cruise Control System","","21","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Sloan research project","T. Bergin","NA","IEEE Transactions on Software Engineering","","2000","26","5","478","478","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.846303","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846303","","History;Computer languages;Software engineering;Computer science;Software tools;Collaborative tools;Writing;Permission;Programming profession","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Corrections to ""Avoiding packaging mismatch with flexible packaging""","R. DeLine","Carnegie Mellon University","IEEE Transactions on Software Engineering","","2001","27","6","577","577","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.926178","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=926178","","Packaging;Connectors","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Classifying Software Changes: Clean or Buggy?","S. Kim; E. J. Whitehead, Jr.; Y. Zhang","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","181","196","This paper introduces a new technique for predicting latent software bugs, called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent buggy change recall on average. Change classification has several desirable qualities: 1) The prediction granularity is small (a change to a single file), 2) predictions do not require semantic information about the source code, 3) the technique works for a broad array of project types and programming languages, and 4) predictions can be made immediately upon the completion of a change. Contributions of this paper include a description of the change classification approach, techniques for extracting features from the source code and change histories, a characterization of the performance of change classification across 12 open source projects, and an evaluation of the predictive power of different groups of features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70773","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4408585","Software maintenance;Metrics/Measurement;Clustering;classification;and association rules;Configuration Management;Data mining;Software maintenance;Metrics/Measurement;Clustering;classification;and association rules;Configuration Management;Data mining","Computer bugs;Machine learning;Open source software;Classification algorithms;History;Data mining;Feature extraction;Software debugging;Project management;Computer languages","data mining;feature extraction;learning (artificial intelligence);program debugging;software maintenance;software metrics","change classification;machine learning classifier;software change;software project;software configuration management repository;programming languages;feature extraction;open source projects;software metrics;software maintenance;association rule","","191","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Template semantics for model-based notations","Jianwei Niu; J. M. Atlee; N. A. Day","Sch. of Comput. Sci., Waterloo Univ., Ont., Canada; Sch. of Comput. Sci., Waterloo Univ., Ont., Canada; Sch. of Comput. Sci., Waterloo Univ., Ont., Canada","IEEE Transactions on Software Engineering","","2003","29","10","866","882","We propose a template-based approach to structuring the semantics of model-based specification notations. The basic computation model is a nonconcurrent, hierarchical state-transition machine (HTS), whose execution semantics are parameterized. Semantics that are common among notations (e.g., the concept of an enabled transition) are captured in the template, and a notation's distinct semantics (e.g., which states can enable transitions) are specified as parameters. The template semantics of composition operators define how multiple HTSs execute concurrently and how they communicate and synchronize with each other by exchanging events and data. The definitions of these operators use the template parameters to preserve notation-specific behavior in composition. Our template is sufficient to capture the semantics of basic transition systems, CSP, CCS, basic LOTOS, a subset of SDL88, and a variety of statecharts notations. We believe that a description of a notation's semantics using our template can be used as input to a tool that automatically generates formal analysis tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237169","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237169","","Carbon capture and storage;Concurrent computing;Computer Society;Computational modeling;High temperature superconductors;Algebra;Logic;Data mining;Thyristors","programming language semantics;software engineering;parallel programming;communicating sequential processes;calculus of communicating systems","specification notations;hierarchical state-transition machine;HTS;execution semantics;concurrency;computation model;template semantics;software","","21","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on “Researcher Bias: The Use of Machine Learning in Software Defect Prediction”","C. Tantithamthavorn; S. McIntosh; A. E. Hassan; K. Matsumoto","Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; School of Computing, Queen's University, Kingston, ON, Canada; Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan","IEEE Transactions on Software Engineering","","2016","42","11","1092","1094","Shepperd et al. find that the reported performance of a defect prediction model shares a strong relationship with the group of researchers who construct the models. In this paper, we perform an alternative investigation of Shepperd et al.'s data. We observe that (a) research group shares a strong association with other explanatory variables (i.e., the dataset and metric families that are used to build a model); (b) the strong association among these explanatory variables makes it difficult to discern the impact of the research group on model performance; and (c) after mitigating the impact of this strong association, we find that the research group has a smaller impact than the metric family. These observations lead us to conclude that the relationship between the research group and the performance of a defect prediction model are more likely due to the tendency of researchers to reuse experimental components (e.g., datasets and metrics). We recommend that researchers experiment with a broader selection of datasets and metrics to combat any potential bias in their results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2553030","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7450669","Software quality assurance;defect prediction;researcher bias","Measurement;Interference;Analysis of variance;Predictive models;Analytical models;NASA;Data models","learning (artificial intelligence);software fault tolerance;software quality","researcher bias;machine learning;software defect prediction;software quality assurance","","10","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Reliability estimation for a software system with sequential independent reviews","N. E. Rallis; Z. F. Lansdowne","Math. Dept., Boston Coll., Chestnut Hill, MA, USA; NA","IEEE Transactions on Software Engineering","","2001","27","12","1057","1061","Suppose that several sequential test and correction cycles have been completed for the purpose of improving the reliability of a given software system. One way to quantify the success of these efforts is to estimate the probability that all faults are found by the end of the last cycle, We describe how to evaluate this probability both prior to and after observing the numbers of faults detected in each cycle and we show when these two evaluations would be the same.","0098-5589;1939-3520;2326-3881","","10.1109/32.988707","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988707","","Software systems;Fault detection;System testing;Software reliability;Fault diagnosis;Software testing;Sequential analysis;Bayesian methods;Performance evaluation;Software measurement","software reliability;program testing;program debugging","reliability estimation;software system;sequential independent reviews;fault probability;sequential test/correction cycles","","15","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Requirements-based monitors for real-time systems","D. K. Peters; D. L. Parnas","Fac. of Eng. & Appl. Sci., Memorial Univ. of Newfoundland, St. John's, Nfld., Canada; NA","IEEE Transactions on Software Engineering","","2002","28","2","146","158","Before designing safety- or mission-critical real-time systems, a specification of the required behavior of the system should be produced and reviewed by domain experts. After the system has been implemented, it should be thoroughly tested to ensure that it behaves correctly. This is best done using a monitor, a system that observes the behavior of a target system and reports if that behavior is consistent with the requirements. Such a monitor can be used both as an oracle during testing and as a supervisor during operation. Monitors should be based on the documented requirements of the system. If the target system is required to monitor or control real-valued quantities, then the requirements, which are expressed in terms of the monitored and controlled quantities, will allow a range of behaviors to account for errors and imprecision in observation and control of these quantities. Even if the controlled variables are discrete valued, the requirements must specify the timing tolerance. Because of the limitations of the devices used by the monitor to observe the environmental quantities, there is unavoidable potential for false reports, both negative and positive, This paper discusses design of monitors for real-time systems, and examines the conditions under which a monitor will produce false reports. We describe the conclusions that can be drawn when using a monitor to observe system behavior.","0098-5589;1939-3520;2326-3881","","10.1109/32.988496","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988496","","Real time systems;System testing;Control systems;Mission critical systems;Monitoring;Timing;Terminology;Computer displays;Error correction;Safety devices","real-time systems;system monitoring;formal specification;safety-critical software;program testing","requirements-based monitors;mission-critical real-time systems;safety-critical real-time systems;specification;oracle;testing;supervisor;documented requirements;errors;imprecision;timing tolerance;environmental quantities;false reports","","26","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic mapping of system of N-dimensional affine recurrence equations (SARE) onto distributed memory parallel systems","A. Marongiu; P. Palazzari","Dept. of Electron. Eng., Rome Univ., Italy; NA","IEEE Transactions on Software Engineering","","2000","26","3","262","275","The automatic extraction of parallelism from algorithms, and the consequent parallel code generation, is a challenging problem. We present a procedure for automatic parallel code generation in the case of algorithms described through a SARE (Set of Affine Recurrence Equations). Starting from the original SARE description in an N-dimensional iteration space, the algorithm is converted into a parallel code for an (eventually virtual) m-dimensional distributed memory parallel machine (m<N). We demonstrate some theorems which are the mathematical basis for the proposed parallel generation tool. The projection technique used in the tool is based on the polytope model. Some affine transformations are introduced to project the polytope from the original iteration space onto another polytope, preserving the SARE semantic, in the time-processor (t,p) space. Points in (t,p) are individuated through the m-dimensional p coordinate and the n-dimensional t coordinate, resulting in N=n+m. Along with polytope transformation, a methodology to generate the code within processors is given. Finally, a cost function, used to guide the heuristic search for the polytope transformation and derived from the actual implementation of the method on an MPP SIMD machine, is introduced.","0098-5589;1939-3520;2326-3881","","10.1109/32.842951","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842951","","Difference equations;Signal processing algorithms;Cost function;Parallel machines;Linear algebra;Terminology;Parallel architectures;Sufficient conditions;Solid modeling;Timing","equations;mathematics computing;parallelising compilers;distributed memory systems;heuristic programming;programming theory;parallel programming;iterative methods;search problems","automatic mapping;N-dimensional affine recurrence equations;distributed memory parallel machine;automatic parallelism extraction;automatic parallel code generation;SARE;N-dimensional iteration space;projection technique;polytope transformation;processor-time space;cost function;heuristic search;SIMD machine;massively parallel processor;automatic parallelization;affine functions","","6","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Automatic analysis of consistency between requirements and designs""","Hewijin Christine Jiau; Dung-Feng Yu","Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan; Dept. of Electr. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","IEEE Transactions on Software Engineering","","2006","32","4","279","280","This article comments on ""automatic analysis of consistency between requirements and designs"".","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628973","","Error correction;Algorithm design and analysis","formal specification;formal verification","requirements engineering;software design;consistency analysis","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Detecting Memory Leaks Statically with Full-Sparse Value-Flow Analysis","Y. Sui; D. Ye; J. Xue","Programming Language and Compilers Group, School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; Programming Language and Compilers Group, School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; Programming Language and Compilers Group, School of Computer Science and Engineering, University of New South Wales, Sydney, Australia","IEEE Transactions on Software Engineering","","2014","40","2","107","122","We introduce a static detector, Saber, for detecting memory leaks in C programs. Leveraging recent advances on sparse pointer analysis, Saber is the first to use a full-sparse value-flow analysis for detecting memory leaks statically. Saber tracks the flow of values from allocation to free sites using a sparse value-flow graph (SVFG) that captures def-use chains and value flows via assignments for all memory locations represented by both top-level and address-taken pointers. By exploiting field-, flow- and context-sensitivity during different phases of the analysis, Saber detects memory leaks in a program by solving a graph reachability problem on its SVFG. Saber, which is fully implemented in Open64, is effective at detecting 254 leaks in the 15 SPEC2000 C programs and seven applications, while keeping the false positive rate at 18.3 percent. Saber compares favorably with several static leak detectors in terms of accuracy (leaks and false alarms reported) and scalability (LOC analyzed per second). In particular, compared with Fastcheck (which analyzes allocated objects flowing only into top-level pointers) using the 15 SPEC2000 C programs, Saber detects 44.1 percent more leaks at a slightly higher false positive rate but is only a few times slower.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2302311","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6720116","Memory Leaks;sparse value-flow analysis;static analysis;pointer analysis","Detectors;Resource management;Accuracy;Scalability;Abstracts;Standards;Sensitivity","C language;program diagnostics;reachability analysis;storage management","memory leaks detection;full-sparse value-flow analysis;Saber static detector;sparse pointer analysis;sparse value-flow graph;SVFG;def-use chains;value flows;memory locations;top-level pointers;address-taken pointers;field-sensitivity;flow-sensitivity;context-sensitivity;graph reachability problem;Open64;SPEC2000 C programs;false positive rate;static leak detectors;Fastcheck","","13","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Interaction protocols as design abstractions for business processes","N. Desai; A. U. Mallya; A. K. Chopra; M. P. Singh","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","12","1015","1027","Business process modeling and enactment are notoriously complex, especially in open settings, where business partners are autonomous, requirements must be continually finessed, and exceptions frequently arise because of real-world or organizational problems. Traditional approaches, which attempt to capture processes as monolithic flows, have proven inadequate in addressing these challenges. We propose (business) protocols as components for developing business processes. A protocol is an abstract, modular, publishable specification of an interaction among different roles to be played by different participants. When instantiated with the participants' internal policies, protocols yield concrete business processes. Protocols are reusable and refinable, thus simplifying business process design. We show how protocols and their composition are theoretically founded in the phi;-calculus.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.140","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566604","Index Terms- Multiagent systems;software reuse;interaction-based modeling;software design methodologies;rule-based processing;\pi-calculus.","Protocols;Skeleton;OWL;Concrete;Process design;Software systems;Software design;Multiagent systems;Mirrors;Business communication","business data processing;calculus;multi-agent systems;organisational aspects;software reusability;systems analysis","rule-based processing;software design methodology;interaction-based modeling;software reuse;multiagent system;φ-calculus;publishable specification;business protocols;organizational problem;business process modeling;design abstraction;interaction protocols","","60","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Current trends in exception handling","D. E. Perry; A. Romanovsky; A. Tripathi","University of Texas at Austin; NA; NA","IEEE Transactions on Software Engineering","","2000","26","9","817","819","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.877843","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877843","","Computer languages;Operating systems;Object oriented modeling;Robustness;Computer science;Computer Society;Software design;Software systems;Reliability engineering","","","","3","","2","","","","","","IEEE","IEEE Journals & Magazines"
"Adaptive Multi-Objective Evolutionary Algorithms for Overtime Planning in Software Projects","F. Sarro; F. Ferrucci; M. Harman; A. Manna; J. Ren","University College London, CREST Centre, London, United Kingdom; University of Salerno, Fisciano, SA, Italy; University College London, CREST Centre, London, United Kingdom; University of Salerno, Fisciano, SA, Italy; Beihang University, Beijing, China","IEEE Transactions on Software Engineering","","2017","43","10","898","917","Software engineering and development is well-known to suffer from unplanned overtime, which causes stress and illness in engineers and can lead to poor quality software with higher defects. Recently, we introduced a multi-objective decision support approach to help balance project risks and duration against overtime, so that software engineers can better plan overtime. This approach was empirically evaluated on six real world software projects and compared against state-of-the-art evolutionary approaches and currently used overtime strategies. The results showed that our proposal comfortably outperformed all the benchmarks considered. This paper extends our previous work by investigating adaptive multi-objective approaches to meta-heuristic operator selection, thereby extending and (as the results show) improving algorithmic performance. We also extended our empirical study to include two new real world software projects, thereby enhancing the scientific evidence for the technical performance claims made in the paper. Our new results, over all eight projects studied, showed that our adaptive algorithm outperforms the considered state of the art multi-objective approaches in 93 percent of the experiments (with large effect size). The results also confirm that our approach significantly outperforms current overtime planning practices in 100 percent of the experiments (with large effect size).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2650914","EPSRC; Microsoft Azure Research; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7814340","Software engineering;management;planning;search-based software engineering;project scheduling;overtime;hyperheuristic;multi-objective evolutionary algorithms;NSGAII","Software;Planning;Software engineering;Search problems;Adaptive algorithms;Project management;Standards","decision support systems;DP management;evolutionary computation;project management;software development management;software quality","adaptive multiobjective evolutionary algorithms;multiobjective decision support approach;project risks;software engineers;evolutionary approaches;overtime strategies;adaptive multiobjective approaches;meta-heuristic operator selection;software projects;overtime planning practices","","5","","96","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic Software Project Scheduling through a Proactive-Rescheduling Method","X. Shen; L. L. Minku; R. Bahsoon; X. Yao","B-DAT & CICAEET, School of Information and Control, Nanjing University of Information Science and Technology, No.219, Ning-Liu Road, Pu-Kou District, Nanjing, P.R. China; Department of Computer Science, University of Leicester, University Road, Leicester, United Kingdom; CERCIA, University of Birmingham, Edgbaston, Birmingham, United Kingdom; CERCIA, University of Birmingham, Edgbaston, Birmingham, United Kingdom","IEEE Transactions on Software Engineering","","2016","42","7","658","686","Software project scheduling in dynamic and uncertain environments is of significant importance to real-world software development. Yet most studies schedule software projects by considering static and deterministic scenarios only, which may cause performance deterioration or even infeasibility when facing disruptions. In order to capture more dynamic features of software project scheduling than the previous work, this paper formulates the project scheduling problem by considering uncertainties and dynamic events that often occur during software project development, and constructs a mathematical model for the resulting multi-objective dynamic project scheduling problem (MODPSP), where the four objectives of project cost, duration, robustness and stability are considered simultaneously under a variety of practical constraints. In order to solve MODPSP appropriately, a multi-objective evolutionary algorithm based proactive-rescheduling method is proposed, which generates a robust schedule predictively and adapts the previous schedule in response to critical dynamic events during the project execution. Extensive experimental results on 21 problem instances, including three instances derived from real-world software projects, show that our novel method is very effective. By introducing the robustness and stability objectives, and incorporating the dynamic optimization strategies specifically designed for MODPSP, our proactive-rescheduling method achieves a very good overall performance in a dynamic environment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2512266","National Natural Science Foundation of China (NSFC); Natural Science Foundation of Jiangsu Province of China; EPSRC; DAASE: Dynamic Adaptive Automated Software Engineering; EPSRC; Evolutionary Computation for Dynamic Optimization in Network Environments; CERCIA; School of Computer Science, University of Birmingham, United Kingdom; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7365465","Schedule and organizational issues;dynamic software project scheduling;search-based software engineering;multi-objective evolutionary algorithms;mathematical modeling","Dynamic scheduling;Software;Schedules;Uncertainty;Robustness;Job shop scheduling","evolutionary computation;project management;scheduling;software development management","uncertain environments;dynamic environments;dynamic features;software project development;multiobjective dynamic software project scheduling problem;MODPSP;project cost;project duration;project robustness;project stability;multiobjective evolutionary algorithm based proactive-rescheduling method;critical dynamic events;project execution;dynamic optimization strategies","","7","","49","","","","","","IEEE","IEEE Journals & Magazines"
"The Oracle Problem in Software Testing: A Survey","E. T. Barr; M. Harman; P. McMinn; M. Shahbaz; S. Yoo","Department of Computer Science, University College London, Gower Street, London WC2R 2LS, London, United Kingdom; Department of Computer Science, University College London, Gower Street, London WC2R 2LS, London, United Kingdom; Department of Computer Science, University of Sheffield, Sheffield S1 4DP, South Yorkshire, United Kingdom; Department of Computer Science, University of Sheffield, Sheffield S1 4DP, South Yorkshire, United Kingdom; Department of Computer Science, University College London, Gower Street, London WC2R 2LS, London, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","5","507","525","Testing involves examining the behaviour of a system in order to discover potential faults. Given an input for a system, the challenge of distinguishing the corresponding desired, correct behaviour from potentially incorrect behavior is called the “test oracle problem”. Test oracle automation is important to remove a current bottleneck that inhibits greater overall test automation. Without test oracle automation, the human has to determine whether observed behaviour is correct. The literature on test oracles has introduced techniques for oracle automation, including modelling, specifications, contract-driven development and metamorphic testing. When none of these is completely adequate, the final source of test oracle information remains the human, who may be aware of informal specifications, expectations, norms and domain specific information that provide informal oracle guidance. All forms of test oracles, even the humble human, involve challenges of reducing cost and increasing benefit. This paper provides a comprehensive survey of current approaches to the test oracle problem and an analysis of trends in this important area of software testing research and practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2372785","Engineering and Physical Sciences Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6963470","Test oracle;Automatic testing;Testing formalism;Test oracle;automatic testing;testing formalism","Probabilistic logic;Licenses;Automation;Software testing;Market research;Reliability","formal specification;program testing","test oracle problem;test oracle information;informal specifications;domain specific information;informal oracle guidance;software testing research;software testing practice;oracle automation;contract-driven development;metamorphic testing;oracle automation","","129","","211","","","","","","IEEE","IEEE Journals & Magazines"
"Safer User Interfaces: A Case Study in Improving Number Entry","H. Thimbleby","Department of Computer Science, Swansea University, Swansea SA2 0SF, Wales, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","7","711","729","Numbers are used in critical applications, including finance, healthcare, aviation, and of course in every aspect of computing. User interfaces for number entry in many devices (calculators, spreadsheets, infusion pumps, mobile phones, etc.) have bugs and design defects that induce unnecessary use errors that compromise their dependability. Focusing on Arabic key interfaces, which use digit keys 0-9-· usually augmented with correction keys, this paper introduces a method for formalising and managing design problems. Since number entry and devices such as calculators have been the subject of extensive user interface research since at least the 1980s, the diverse design defects uncovered imply that user evaluation methodologies are insufficient for critical applications. Likewise, formal methods are not being applied effectively. User interfaces are not trivial and more attention should be paid to their correct design and implementation. The paper includes many recommendations for designing safer number entry user interfaces.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2383396","Engineering and Physical Sciences Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6991548","Error processing;Software/Software Engineering;User interfaces;Human Factors in Software Design;User Interfaces;Information Interfaces;Representation (HCI);Error processing;software/software engineering;user interfaces;human factors in software design;user interfaces;information interfaces and representation (HCI)","User interfaces;Calculators;Computer bugs;Safety;Pressing;Software;Debugging","user interfaces","safer user interfaces;number entry;Arabic key interfaces;correction keys;design problem formalization;design problem management","","9","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Who Will Stay in the FLOSS Community? Modeling Participant’s Initial Behavior","M. Zhou; A. Mockus","School of Electronics Engineering and Computer Science, Peking University and Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China; Department of Electrical Engineering and Computer Science, University of Tennessee, Min H. Kao Building, Room 613, 1520 Middle Drive, Knoxville, TN","IEEE Transactions on Software Engineering","","2015","41","1","82","99","Motivation: To survive and succeed, FLOSS projects need contributors able to accomplish critical project tasks. However, such tasks require extensive project experience of long term contributors (LTCs). Aim: We measure, understand, and predict how the newcomers' involvement and environment in the issue tracking system (ITS) affect their odds of becoming an LTC. Method: ITS data of Mozilla and Gnome, literature, interviews, and online documents were used to design measures of involvement and environment. A logistic regression model was used to explain and predict contributor's odds of becoming an LTC. We also reproduced the results on new data provided by Mozilla. Results: We constructed nine measures of involvement and environment based on events recorded in an ITS. Macro-climate is the overall project environment while micro-climate is person-specific and varies among the participants. Newcomers who are able to get at least one issue reported in the first month to be fixed, doubled their odds of becoming an LTC. The macro-climate with high project popularity and the micro-climate with low attention from peers reduced the odds. The precision of LTC prediction was 38 times higher than for a random predictor. We were able to reproduce the results with new Mozilla data without losing the significance or predictive power of the previously published model. We encountered unexpected changes in some attributes and suggest ways to make analysis of ITS data more reproducible. Conclusions: The findings suggest the importance of initial behaviors and experiences of new participants and outline empirically-based approaches to help the communities with the recruitment of contributors for long-term participation and to help the participants contribute more effectively. To facilitate the reproduction of the study and of the proposed measures in other contexts, we provide the data we retrieved and the scripts we wrote at https://www.passion-lab.org/projects/developerfluency.html.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2349496","National Basic Research Program of China; National Natural Science Foundation of China; National Hi-Tech Research and Development Program of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6880395","Long term contributor;open source software;issue tracking system;mining software repository;extent of involvement;interaction with environment;initial behavior","Communities;Atmospheric measurements;Particle measurements;Predictive models;Data models;Data mining;Electronic mail","behavioural sciences;project management;public domain software","Free-Libre and/or open source software projects;open source software;Mozilla data;microclimate;macroclimate;logistic regression model;Gnome;ITS data;issue tracking system;LTC;long term contributors;critical project;FLOSS community","","23","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Specialising Software for Different Downstream Applications Using Genetic Improvement and Code Transplantation","J. Petke; M. Harman; W. B. Langdon; W. Weimer","University College London, London, United Kingdom; University College London, London, United Kingdom; University College London, London, United Kingdom; University of Virginia, Charlottesville, VA","IEEE Transactions on Software Engineering","","2018","44","6","574","594","Genetic improvement uses automated search to find improved versions of existing software. Genetic improvement has previously been concerned with improving a system with respect to all possible usage scenarios. In this paper, we show how genetic improvement can also be used to achieve specialisation to a specific set of usage scenarios. We use genetic improvement to evolve faster versions of a C++ program, a Boolean satisfiability solver called MiniSAT, specialising it for three different applications, each with their own characteristics. Our specialised solvers achieve between 4 and 36 percent execution time improvement, which is commensurate with efficiency gains achievable using human expert optimisation for the general solver. We also use genetic improvement to evolve faster versions of an image processing tool called ImageMagick, utilising code from GraphicsMagick, another image processing tool which was forked from it. We specialise the format conversion functionality to greyscale images and colour images only. Our specialised versions achieve up to 3 percent execution time improvement.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2702606","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7962212","Genetic improvement;GI;code transplants;code specialisation;SAT;ImageMagick;GraphicsMagick","Software;Software engineering;Image processing;C++ languages;Genetic programming;Optimization","Boolean algebra;C++ language;computability;genetic algorithms;image colour analysis;source code (software)","genetic improvement;execution time improvement;code transplantation;C++ program;Boolean satisfiability solver;MiniSAT;software specialisation;downstream application;image processing tool;ImageMagick;GraphicsMagick;greyscale images;colour images","","","","97","","","","","","IEEE","IEEE Journals & Magazines"
"The Role of Method Chains and Comments in Software Readability and Comprehension—An Experiment","J. Börstler; B. Paech","Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Computer Science, Heidelberg University, Heidelberg, Germany","IEEE Transactions on Software Engineering","","2016","42","9","886","898","Software readability and comprehension are important factors in software maintenance. There is a large body of research on software measurement, but the actual factors that make software easier to read or easier to comprehend are not well understood. In the present study, we investigate the role of method chains and code comments in software readability and comprehension. Our analysis comprises data from 104 students with varying programming experience. Readability and comprehension were measured by perceived readability, reading time and performance on a simple cloze test. Regarding perceived readability, our results show statistically significant differences between comment variants, but not between method chain variants. Regarding comprehension, there are no significant differences between method chain or comment variants. Student groups with low and high experience, respectively, show significant differences in perceived readability and performance on the cloze tests. Our results do not show any significant relationships between perceived readability and the other measures taken in the present study. Perceived readability might therefore be insufficient as the sole measure of software readability or comprehension. We also did not find any statistically significant relationships between size and perceived readability, reading time and comprehension.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2527791","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7404062","Software readability;software comprehension;software measurement;comments;method chains;experiment","Software;Guidelines;Software measurement;Software engineering;Programming;Complexity theory;Object oriented modeling","software maintenance;software metrics","software readability;software comprehension;software maintenance;software measurement;method chains;code comments;cloze tests","","3","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Self-Adaptive and Online QoS Modeling for Cloud-Based Software Services","T. Chen; R. Bahsoon","CERCIA, School of Computer Science, University of Birmingham, Birmingham, United Kingdom; CERCIA, School of Computer Science, University of Birmingham, Birmingham, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","5","453","475","In the presence of scale, dynamism, uncertainty and elasticity, cloud software engineers faces several challenges when modeling Quality of Service (QoS) for cloud-based software services. These challenges can be best managed through self-adaptivity because engineers' intervention is difficult, if not impossible, given the dynamic and uncertain QoS sensitivity to the environment and control knobs in the cloud. This is especially true for the shared infrastructure of cloud, where unexpected interference can be caused by co-located software services running on the same virtual machine; and co-hosted virtual machines within the same physical machine. In this paper, we describe the related challenges and present a fully dynamic, self-adaptive and online QoS modeling approach, which grounds on sound information theory and machine learning algorithms, to create QoS model that is capable to predict the QoS value as output over time by using the information on environmental conditions, control knobs and interference as inputs. In particular, we report on in-depth analysis on the correlations of selected inputs to the accuracy of QoS model in cloud. To dynamically selects inputs to the models at runtime and tune accuracy, we design self-adaptive hybrid dual-learners that partition the possible inputs space into two sub-spaces, each of which applies different symmetric uncertainty based selection techniques; the results of sub-spaces are then combined. Subsequently, we propose the use of adaptive multi-learners for building the model. These learners simultaneously allow several learning algorithms to model the QoS function, permitting the capability for dynamically selecting the best model for prediction on the fly. We experimentally evaluate our models in the cloud environment using RUBiS benchmark and realistic FIFA 98 workload. The results show that our approach is more accurate and effective than state-of-the-art modelings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2608826","EPSRC Grant; DAASE: Dynamic Adaptive Automated Software Engineering; The PhD scholarship from the School of Computer Science; University of Birmingham; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7572219","Software quality;search-based software engineering;self-adaptive systems;machine learning;cloud computing;performance modeling","Quality of service;Cloud computing;Interference;Adaptation models;Sensitivity;Uncertainty","cloud computing;learning (artificial intelligence);quality of service;virtual machines","cloud-based software services;quality of service;self-adaptivity;uncertain QoS sensitivity;cloud infrastructure;colocated software service;virtual machine;fully dynamic self-adaptive online QoS modeling;information theory;machine learning algorithm;environmental conditions;control knobs;self-adaptive hybrid dual-learners;input space partitioning;symmetric uncertainty based selection technique;QoS function;cloud environment;RUBiS benchmark;FIFA 98 workload","","7","","54","","","","","","IEEE","IEEE Journals & Magazines"
"An I/O Efficient Approach for Detecting All Accepting Cycles","L. Wu; K. Su; S. Cai; X. Zhang; C. Zhang; S. Wang","School of Computer Science and Engineering, University of Electronic Science and Technology, Chengdu, China; Institute for Integrated and Intelligent Systems, Griffith University, Brisbane, Australia; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Engineering, University of Electronic Science and Technology, Chengdu, China; School of Information Technology and Electrical Engineering, University of Queensland, Brisbane, Australia; School of Computer Science and Engineering, University of Electronic Science and Technology, Chengdu, China","IEEE Transactions on Software Engineering","","2015","41","8","730","744","Existing algorithms for I/O Linear Temporal Logic (LTL) model checking usually output a single counterexample for a system which violates the property. However, in real-world applications, such as diagnosis and debugging in software and hardware system designs, people often need to have a set of counterexamples or even all counterexamples. For this purpose, we propose an I/O efficient approach for detecting all accepting cycles, called Detecting All Accepting Cycles (DAAC), where the properties to be verified are in LTL. Different from other algorithms for finding all cycles, DAAC first searches for the accepting strongly connected components (ASCCs), and then finds all accepting cycles of every ASCC, which can avoid searching for a great many paths that are impossible to be extended to accepting cycles. In order to further lower DAAC's I/O complexity and improve its performance, we propose an intersection computation technique and a dynamic path management technique, and exploit a minimal perfect hash function (MPHF). We carry out both complexity and experimental comparisons with the state-of-the-art algorithms including Detect Accepting Cycle (DAC), Maximal Accepting Predecessors (MAP) and Iterative-Deepening Depth-First Search (IDDFS). The comparative results show that our approach is better on the whole in terms of I/O complexity and practical performance, despite the fact that it finds all counterexamples.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2411284","National Natural Science Foundation of China; China National; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7056483","Model Checking;Detection of All Accepting Cycles;Breath-First Search;Model checking;detection of all accepting cycles;state space explosion;accepting strongly connected component;breath-first search","Complexity theory;Model checking;Algorithm design and analysis;Educational institutions;Heuristic algorithms;Software algorithms;Software","formal verification;temporal logic","input-output efficient approach;I/O linear temporal logic;LTL model checking;DAAC approach;detecting all accepting cycles approach;accepting strongly connected components;ASCC;I/O complexity;intersection computation technique;dynamic path management technique;minimal perfect hash function;MPHF;detect accepting cycle algorithm;DAC algorithm;maximal accepting predecessors algorithm;iterative-deepening depth-first search algorithm;IDDFS algorithm","","","","36","","","","","","IEEE","IEEE Journals & Magazines"
"A Survey of App Store Analysis for Software Engineering","W. Martin; F. Sarro; Y. Jia; Y. Zhang; M. Harman","Department of Computer Science, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","9","817","847","App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2630689","EPRSC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7765038","App store;analysis;mining;API;feature;release planning;requirements engineering;reviews;security;ecosystem","Software;Security;Software engineering;Market research;Ecosystems;Mobile communication;Google","program testing;software engineering","app store analysis;software engineering;software deployment;software repositories;software development;requirements engineering;release planning;software design;software security;software testing","","15","","262","","","","","","IEEE","IEEE Journals & Magazines"
"The Value of Exact Analysis in Requirements Selection","L. Li; M. Harman; F. Wu; Y. Zhang","Department of Computer Science, CREST, University College London, Gower Street, London, United Kingdom; Department of Computer Science, CREST, University College London, Gower Street, London, United Kingdom; Department of Computer Science, CREST, University College London, Gower Street, London, United Kingdom; Department of Computer Science, CREST, University College London, Gower Street, London, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","6","580","596","Uncertainty is characterised by incomplete understanding. It is inevitable in the early phase of requirements engineering, and can lead to unsound requirement decisions. Inappropriate requirement choices may result in products that fail to satisfy stakeholders' needs, and might cause loss of revenue. To overcome uncertainty, requirements engineering decision support needs uncertainty management. In this research, we develop a decision support framework METRO for the Next Release Problem (NRP) to manage algorithmic uncertainty and requirements uncertainty. An exact NRP solver (NSGDP) lies at the heart of METRO. NSGDP's exactness eliminates interference caused by approximate existing NRP solvers. We apply NSGDP to three NRP instances, derived from a real world NRP instance, RALIC, and compare with NSGA-II, a widely-used approximate (inexact) technique. We find the randomness of NSGA-II results in decision makers missing up to 99.95 percent of the optimal solutions and obtaining up to 36.48 percent inexact requirement selection decisions. The chance of getting an inexact decision using existing approximate approaches is negatively correlated with the implementation cost of a requirement (Spearman r up to -0.72). Compared to the inexact existing approach, NSGDP saves 15.21 percent lost revenue, on average, for the RALIC dataset.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2615100","China Scholarship Council (CSC); EPSRC; DAASE; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7582553","Software engineering;exact multi-objective optimisation;simulation optimisation;next release problem","Uncertainty;Stakeholders;Robustness;Optimization;Software;Software engineering;Software algorithms","decision support systems;formal specification","requirements engineering decision support;METRO;next-release problem;algorithmic uncertainty management;requirements uncertainty management;exact NRP solver;NSGDP;NRP solvers;optimal solutions;inexact requirement selection decisions;RALIC dataset","","1","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Parallel Performance Problems on Shared-Memory Multicore Systems: Taxonomy and Observation","R. Atachiants; G. Doherty; D. Gregg","Trinity College Dublin, Ireland; Trinity College Dublin, Ireland; Trinity College Dublin, Ireland","IEEE Transactions on Software Engineering","","2016","42","8","764","785","The shift towards multicore processing has led to a much wider population of developers being faced with the challenge of exploiting parallel cores to improve software performance. Debugging and optimizing parallel programs is a complex and demanding task. Tools which support development of parallel programs should provide salient information to allow programmers of multicore systems to diagnose and distinguish performance problems. Appropriate design of such tools requires a systematic analysis of the problems which might be identified, and the information used to diagnose them. Building on the literature, we put forward a potential taxonomy of parallel performance problems, and an observational model which links measurable performance data to these problems. We present a validation of this model carried out with parallel programming experts, identifying areas of agreement and disagreement. This is accompanied with a survey of the prevalence of these problems in software development. From this we can identify contentious areas worthy of further exploration, as well as those with high prevalence and strong agreement, which are natural candidates for initial moves towards better tool support.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2519346","Science Foundation Ireland; Irish Software Research Centre; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7386691","Parallel programming, multicore, multi-threaded, optimization, performance problems, performance analysis, diagnosis, debugging, taxonomy.","Multicore processing;Software;Taxonomy;Computers;Context;Parallel programming;Hardware","parallel programming;performance evaluation;program debugging;shared memory systems;software tools","shared-memory multicore systems;parallel performance problems;multicore processing;software performance;parallel program debugging;parallel program optimization;tool support","","1","","97","","","","","","IEEE","IEEE Journals & Magazines"
"<sc>SymbexNet</sc>: Testing Network Protocol Implementations with Symbolic Execution and Rule-Based Specifications","J. Song; C. Cadar; P. Pietzuch","Department of Computer and Information Security, Sejong University, Seoul, Republic of Korea; Department of Computing, Imperial College London, London, SW7 2AZ, U.K.; Department of Computer and Information Security, Sejong University, Seoul, Republic of Korea","IEEE Transactions on Software Engineering","","2014","40","7","695","709","Implementations of network protocols, such as DNS, DHCP and Zeroconf, are prone to flaws, security vulnerabilities and interoperability issues caused by developer mistakes and ambiguous requirements in protocol specifications. Detecting such problems is not easy because (i) many bugs manifest themselves only after prolonged operation; (ii) reasoning about semantic errors requires a machine-readable specification; and (iii) the state space of complex protocol implementations is large. This article presents a novel approach that combines symbolic execution and rule-based specifications to detect various types of flaws in network protocol implementations. The core idea behind our approach is to (1) automatically generate high-coverage test input packets for a network protocol implementation using single- and multi-packet exchange symbolic execution (targeting stateless and stateful protocols, respectively) and then (2) use these packets to detect potential violations of manual rules derived from the protocol specification, and check the interoperability of different implementations of the same network protocol. We present a system based on these techniques, SymbexNet, and evaluate it on multiple implementations of two network protocols: Zeroconf, a service discovery protocol, and DHCP, a network configuration protocol. SymbexNet is able to discover non-trivial bugs as well as interoperability problems, most of which have been confirmed by the developers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2323977","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6815719","Symbolic execution;network security;testing;interoperability testing","Protocols;IP networks;Interoperability;Servers;Concrete;Testing;Computer bugs","formal specification;open systems;program debugging;program testing","SymbexNet;testing network protocol implementations;symbolic execution;rule based specifications;DNS;DHCP;Zeroconf;interoperability issues;security vulnerabilities;protocol specifications;semantic errors;machine readable specification;protocol implementations;protocol specification;network protocol;interoperability problems","","5","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Understanding Diverse Usage Patterns from Large-Scale Appstore-Service Profiles","X. Liu; H. Li; X. Lu; T. Xie; Q. Mei; F. Feng; H. Mei","Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, Beijing, China; Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, Beijing, China; Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, Beijing, China; University of Illinois Urbana-Champaign, Champaign, IL; University of Michigan, Ann Arbor, MI; Wandoujia, Beijing, China; Beijing Institute of Technology","IEEE Transactions on Software Engineering","","2018","44","4","384","411","The prevalence of smart mobile devices has promoted the popularity of mobile applications (a.k.a. apps). Supporting mobility has become a promising trend in software engineering research. This article presents an empirical study of behavioral service profiles collected from millions of users whose devices are deployed with Wandoujia, a leading Android app-store service in China. The dataset of Wandoujia service profiles consists of two kinds of user behavioral data from using 0.28 million free Android apps, including (1) app management activities (i.e., downloading, updating, and uninstalling apps) from over 17 million unique users and (2) app network usage from over 6 million unique users. We explore multiple aspects of such behavioral data and present patterns of app usage. Based on the findings as well as derived knowledge, we also suggest some new open opportunities and challenges that can be explored by the research community, including app development, deployment, delivery, revenue, etc.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2685387","National Basic Research Program (973) of China; Natural Science Foundation of China; National Science Foundation; National Science Foundation; MCubed; University of Michigan; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7883939","Mobile apps;app store;user behavior analysis","Androids;Humanoid robots;Software;Biological system modeling;Mobile communication;Electronic mail;Software engineering","Android (operating system);mobile computing;public domain software;software engineering","Wandoujia service profiles;user behavioral data;app usage;app development;diverse usage patterns;large-scale appstore-service profiles;smart mobile devices;mobile applications;software engineering research;behavioral service profiles;leading Android app-store service;free Android apps;app management activities;app network usage","","1","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Detecting Trivial Mutant Equivalences via Compiler Optimisations","M. Kintis; M. Papadakis; Y. Jia; N. Malevris; Y. Le Traon; M. Harman","Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Esch-sur-Alzette 4365, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Esch-sur-Alzette 4365, Luxembourg; CREST Centre, University College London, London, United Kingdom; Athens University of Economics and Business, Athens, Greece; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Esch-sur-Alzette 4365, Luxembourg; CREST Centre, University College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","4","308","333","Mutation testing realises the idea of fault-based testing, i.e., using artificial defects to guide the testing process. It is used to evaluate the adequacy of test suites and to guide test case generation. It is a potentially powerful form of testing, but it is well-known that its effectiveness is inhibited by the presence of equivalent mutants. We recently studied Trivial Compiler Equivalence (TCE) as a simple, fast and readily applicable technique for identifying equivalent mutants for C programs. In the present work, we augment our findings with further results for the Java programming language. TCE can remove a large portion of all mutants because they are determined to be either equivalent or duplicates of other mutants. In particular, TCE equivalent mutants account for 7.4 and 5.7 percent of all C and Java mutants, while duplicated mutants account for a further 21 percent of all C mutants and 5.4 percent Java mutants, on average. With respect to a benchmark ground truth suite (of known equivalent mutants), approximately 30 percent (for C) and 54 percent (for Java) are TCE equivalent. It is unsurprising that results differ between languages, since mutation characteristics are language-dependent. In the case of Java, our new results suggest that TCE may be particularly effective, finding almost half of all equivalent mutants.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2684805","Research Centre of Athens University of Economics and Business (RC/AUEB); National Research Fund, Luxembourg; Microsoft Azure; UK EPSRC projects; Centre for Research on Evolution Search and Testing (CREST); UCL; EPSRC project; Microsoft Azure; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7882714","Mutation testing;equivalent mutants;duplicated mutants;compiler optimisation","Java;Testing;Optimization;Syntactics;Program processors;Electronic mail","Java;program compilers;program testing","mutation testing;test case generation;Java programming language;TCE equivalent mutants;trivial mutant equivalences detection;fault-based testing;TCE duplicated mutants;Java mutants;compiler equivalence","","","","75","","","","","","IEEE","IEEE Journals & Magazines"
"Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering","S. L. Lim; P. J. Bentley; N. Kanakam; F. Ishikawa; S. Honiden","Department of Computer Science, University College, London; Department of Computer Science, University College London; Department of Clinical, Education and Health Psychology, University College, London; Digital Content and Media Sciences Research Division, National Institute of Informatics, Japan; National Institute of Informatics, Japan","IEEE Transactions on Software Engineering","","2015","41","1","40","64","Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2360674","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6913003","Requirements/specifications;market-driven software engineering;mobile application development;user requirements;survey research;app user behavior;software product lines;software ecosystems;Requirements/specifications;market-driven software engineering;mobile application development;user requirements;survey research;app user behavior;software product lines;software ecosystems","Mobile communication;Software;Smart phones;Software engineering;Data mining;Educational institutions","consumer behaviour;mobile computing;smart phones;software engineering","market-driven software engineering;medical applications;data analysis;South Korea;South;Mexico;Australia;Spain;Canada;India;Russia;Italy;United Kingdom;Brazil;France;Germany;Japan;China;USA;applications stores;mobile devices;user behavior;mobile application","","37","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Improved Evolutionary Algorithm Design for the Project Scheduling Problem Based on Runtime Analysis","L. L. Minku; D. Sudholt; X. Yao","CERCIA, School of Computer Science , The University of Birmingham, Birmingham B15 2TT, United Kingdom; Department of Computer Science , University of Sheffield, Sheffield S1 4DP, United Kingdom; CERCIA, School of Computer Science , The University of Birmingham, Birmingham B15 2TT, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","1","83","102","Several variants of evolutionary algorithms (EAs) have been applied to solve the project scheduling problem (PSP), yet their performance highly depends on design choices for the EA. It is still unclear how and why different EAs perform differently. We present the first runtime analysis for the PSP, gaining insights into the performance of EAs on the PSP in general, and on specific instance classes that are easy or hard. Our theoretical analysis has practical implications-based on it, we derive an improved EA design. This includes normalizing employees' dedication for different tasks to ensure they are not working overtime; a fitness function that requires fewer pre-defined parameters and provides a clear gradient towards feasible solutions; and an improved representation and mutation operator. Both our theoretical and empirical results show that our design is very effective. Combining the use of normalization to a population gave the best results in our experiments, and normalization was a key component for the practical effectiveness of the new design. Not only does our paper offer a new and effective algorithm for the PSP, it also provides a rigorous theoretical analysis to explain the efficiency of the algorithm, especially for increasingly large projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6648326","Schedule and organizational issues;evolutionary algorithms;software project scheduling;software project management;search-based software engineering;runtime analysis","Software;Schedules;Scheduling;Algorithm design and analysis;Software algorithms;Resource management;Software engineering","evolutionary computation;project management;scheduling;software development management","improved evolutionary algorithm;project scheduling problem;runtime analysis;PSP;improved EA design;employee dedication;fitness function;representation operator;mutation operator;population normalization;software project scheduling","","16","","45","","","","","","IEEE","IEEE Journals & Magazines"
"Practical Combinatorial Interaction Testing: Empirical Findings on Efficiency and Early Fault Detection","J. Petke; M. B. Cohen; M. Harman; S. Yoo","Computer Science Department, University College London, London, United Kingdom; Computer Science & Engineering Department, University of Nebraska-Lincoln, Lincoln, Nebraska, United States; Computer Science Department, University College London, London, United Kingdom; Computer Science Department, University College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","9","901","924","Combinatorial interaction testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. Simulated Annealing (SA) and Greedy Algorithms have been widely used to find CIT test suites. From the literature, there is a widely-held belief that SA is slower, but produces more effective tests suites than Greedy and that SA cannot scale to higher strength coverage. We evaluated both algorithms on seven real-world subjects for the well-studied two-way up to the rarely-studied six-way interaction strengths. Our findings present evidence to challenge this current orthodoxy: real-world constraints allow SA to achieve higher strengths. Furthermore, there was no evidence that Greedy was less effective (in terms of time to fault revelation) compared to SA; the results for the greedy algorithm are actually slightly superior. However, the results are critically dependent on the approach adopted to constraint handling. Moreover, we have also evaluated a genetic algorithm for constrained CIT test suite generation. This is the first time strengths higher than 3 and constraint handling have been used to evaluate GA. Our results show that GA is competitive only for pairwise testing for subjects with a small number of constraints.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2421279","National Science Foundation; Air Force Office of Scientific Research; UK Engineering and Physical Sciences Research Council (EPSRC); DAASE: Dynamic Adaptive Automated Software Engineering; GISMO: Genetic Improvement of Software for Multiple Objectives; CREST: Centre for Research on Evolution, Search and Testing; DAASE; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7081752","Combinatorial Interaction Testing;Prioritisation;Empirical Studies;Software Testing;Combinatorial interaction testing;prioritisation;empirical studies;software testing","Testing;Simulated annealing;Genetic algorithms;Fault detection;Greedy algorithms;Turning;Flexible printed circuits","genetic algorithms;greedy algorithms;program testing;simulated annealing;software fault tolerance","combinatorial interaction testing;early fault detection;software system configuration space;simulated annealing;SA;greedy algorithm;CIT test suite generation;constraint handling;pairwise testing;genetic algorithm","","29","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Symbolic Crosschecking of Data-Parallel Floating-Point Code","P. Collingbourne; C. Cadar; P. H. J. Kelly","Google Inc,; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","7","710","737","We present a symbolic execution-based technique for cross-checking programs accelerated using SIMD or OpenCL against an unaccelerated version, as well as a technique for detecting data races in OpenCL programs. Our techniques are implemented in KLEE-CL, a tool based on the symbolic execution engine KLEE that supports symbolic reasoning on the equivalence between expressions involving both integer and floating-point operations. While the current generation of constraint solvers provide effective support for integer arithmetic, the situation is different for floating-point arithmetic, due to the complexity inherent in such computations. The key insight behind our approach is that floating-point values are only reliably equal if they are essentially built by the same operations. This allows us to use an algorithm based on symbolic expression matching augmented with canonicalisation rules to determine path equivalence. Under symbolic execution, we have to verify equivalence along every feasible control-flow path. We reduce the branching factor of this process by aggressively merging conditionals, if-converting branches into select operations via an aggressive phi-node folding transformation. To support the Intel Streaming SIMD Extension (SSE) instruction set, we lower SSE instructions to equivalent generic vector operations, which in turn are interpreted in terms of primitive integer and floating-point operations. To support OpenCL programs, we symbolically model the OpenCL environment using an OpenCL runtime library targeted to symbolic execution. We detect data races by keeping track of all memory accesses using a memory log, and reporting a race whenever we detect that two accesses conflict. By representing the memory log symbolically, we are also able to detect races associated with symbolically-indexed accesses of memory objects. We used KLEE-CL to prove the bounded equivalence between scalar and data-parallel versions of floating-point programs and find a number of issues in a variety of open source projects that use SSE and OpenCL, including mismatches between implementations, memory errors, race conditions and a compiler bug.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.2297120","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6698391","Data-parallel code;floating point;symbolic execution;SIMD;OpenCL;KLEE-CL","Vectors;Kernel;Computational modeling;Computer architecture;Semantics;Programming;Parallel processing","data handling;floating point arithmetic;parallel processing;program debugging","symbolic crosschecking;data parallel floating point code;symbolic execution based technique;crosschecking programs;SIMD;OpenCL programs;KLEE-CL;symbolic execution engine;symbolic reasoning;floating-point operations;integer operations;integer arithmetic;floating point arithmetic;symbolic expression matching;phi node folding transformation;intel streaming SIMD extension;SSE instruction set;equivalent generic vector operations;floating point operations;primitive integer operations;OpenCL environment;memory accesses;memory log;open source projects;floating-point programs;compiler bug;memory errors;race conditions","","9","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Software Repair: A Survey","L. Gazzola; D. Micucci; L. Mariani","Department of Informatics, Systems and Communication (DISCo), University of Milano Bicocca, Milano, Italy; Department of Informatics, Systems and Communication (DISCo), University of Milano Bicocca, Milano, Italy; Department of Informatics, Systems and Communication (DISCo), University of Milano Bicocca, Milano, Italy","IEEE Transactions on Software Engineering","","2019","45","1","34","67","Despite their growing complexity and increasing size, modern software applications must satisfy strict release requirements that impose short bug fixing and maintenance cycles, putting significant pressure on developers who are responsible for timely producing high-quality software. To reduce developers workload, repairing and healing techniques have been extensively investigated as solutions for efficiently repairing and maintaining software in the last few years. In particular, repairing solutions have been able to automatically produce useful fixes for several classes of bugs that might be present in software programs. A range of algorithms, techniques, and heuristics have been integrated, experimented, and studied, producing a heterogeneous and articulated research framework where automatic repair techniques are proliferating. This paper organizes the knowledge in the area by surveying a body of 108 papers about automatic software repair techniques, illustrating the algorithms and the approaches, comparing them on representative examples, and discussing the open challenges and the empirical evidence reported so far.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2755013","EU H2020; ERC Consolidator; MIUR; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8089448","Automatic program repair;generate and validate;search-based;semantics-driven repair;correct by construction;program synthesis;self-repairing","Software;Maintenance engineering;Debugging;Computer bugs;Software algorithms;Fault diagnosis;Conferences","program debugging;software maintenance;software quality","modern software applications;maintenance cycles;software programs;heterogeneous research framework;articulated research framework;automatic software repair techniques;high-quality software","","1","","176","","","","","","IEEE","IEEE Journals & Magazines"
"A Templating System to Generate Provenance","L. Moreau; B. V. Batlajery; T. D. Huynh; D. Michaelides; H. Packer","Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom; Department of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","2","103","121","PROV-TEMPLATEIS a declarative approach that enables designers and programmers to design and generate provenance compatible with the PROV standard of the World Wide Web Consortium. Designers specify the topology of the provenance to be generated by composing templates, which are provenance graphs containing variables, acting as placeholders for values. Programmers write programs that log values and package them up in sets of bindings, a data structure associating variables and values. An expansion algorithm generates instantiated provenance from templates and sets of bindings in any of the serialisation formats supported by PROV. A quantitative evaluation shows that sets of bindings have a size that is typically 40 percent of that of expanded provenance templates and that the expansion algorithm is suitably tractable, operating in fractions of milliseconds for the type of templates surveyed in the article. Furthermore, the approach shows four significant software engineering benefits: separation of responsibilities, provenance maintenance, potential runtime checks and static analysis, and provenance consumption. The article gathers quantitative data and qualitative benefits descriptions from four different applications making use of PROV-TEMPLATE. The system is implemented and released in the open-source library ProvToolbox for provenance processing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2659745","EPSRC SOCIAM; ORCHID; FP7 SmartSociety; ESRC eBook; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7909036","Provenance;prov;provenance generation;template","Electronic publishing;Instruments;Standards;Maintenance engineering;Runtime;Libraries;Automobiles","data structures;graph theory;Internet;software engineering;software maintenance","data structure;PROV-TEMPLATE;provenance generation;open-source library ProvToolbox;software engineering;templating system;provenance processing;quantitative data;provenance consumption;provenance maintenance;expanded provenance templates;expansion algorithm;provenance graphs;World Wide Web Consortium;PROV standard","","2","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Overcoming the Equivalent Mutant Problem: A Systematic Literature Review and a Comparative Experiment of Second Order Mutation","L. Madeyski; W. Orzeszyna; R. Torkar; M. Józala","Institute of Informatics, Wroclaw University of Technology, Wyb. Wyspianskiego 27, Poland; Institute of Informatics, Wroclaw University of Technology, Poland; Division of Software Engineering , Department of Computer Science and Engineering, Chalmers University of Technology , SE-41296, Sweden; Institute of Informatics, Wroclaw University of Technology, Wyb. Wyspianskiego 27, Poland","IEEE Transactions on Software Engineering","","2014","40","1","23","42","Context. The equivalent mutant problem (EMP) is one of the crucial problems in mutation testing widely studied over decades. Objectives. The objectives are: to present a systematic literature review (SLR) in the field of EMP; to identify, classify and improve the existing, or implement new, methods which try to overcome EMP and evaluate them. Method. We performed SLR based on the search of digital libraries. We implemented four second order mutation (SOM) strategies, in addition to first order mutation (FOM), and compared them from different perspectives. Results. Our SLR identified 17 relevant techniques (in 22 articles) and three categories of techniques: detecting (DEM); suggesting (SEM); and avoiding equivalent mutant generation (AEMG). The experiment indicated that SOM in general and JudyDiffOp strategy in particular provide the best results in the following areas: total number of mutants generated; the association between the type of mutation strategy and whether the generated mutants were equivalent or not; the number of not killed mutants; mutation testing time; time needed for manual classification. Conclusions . The results in the DEM category are still far from perfect. Thus, the SEM and AEMG categories have been developed. The JudyDiffOp algorithm achieved good results in many areas.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6613487","Mutation testing;equivalent mutant problem;higher order mutation;second order mutation","Testing;Systematics;Educational institutions;Databases;Libraries;Java;Informatics","digital libraries;program testing","JudyDiffOp strategy;SEM;DEM;AEMG;avoiding equivalent mutant generation;suggesting;detecting;FOM;first order mutation;SOM strategies;digital libraries;SLR;mutation testing;EMP;second order mutation;comparative experiment;systematic literature review;equivalent mutant problem","","42","","79","","","","","","IEEE","IEEE Journals & Magazines"
"Mapping Bug Reports to Relevant Files: A Ranking Model, a Fine-Grained Benchmark, and Feature Evaluation","X. Ye; R. Bunescu; C. Liu","School of Electrical Engineering and Computer Science, Ohio University, Athens, OH; School of Electrical Engineering and Computer Science, Ohio University, Athens, OH; School of Electrical Engineering and Computer Science, Ohio University, Athens, OH","IEEE Transactions on Software Engineering","","2016","42","4","379","402","When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and improve productivity. This paper introduces an adaptive ranking approach that leverages project knowledge through functional decomposition of source code, API descriptions of library components, the bug-fixing history, the code change history, and the file dependency graph. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluate the ranking system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the learning-to-rank approach outperforms three recent state-of-the-art methods. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70 percent of the bug reports in the Eclipse Platform and Tomcat projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2479232","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7270328","Bug reports;software maintenance;learning to rank;Bug reports;software maintenance;learning to rank","Software;History;Computational modeling;Computer bugs;Collaboration;Benchmark testing;Standards","","","","8","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Effectively Incorporating Expert Knowledge in Automated Software Remodularisation","M. Hall; N. Walkinshaw; P. McMinn","Department of Computer Science, University of Sheffield, Sheffield, United Kingdom; Department of Informatics, University of Leicester, Leicester, United Kingdom; Department of Computer Science, University of Sheffield, Sheffield, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","7","613","630","Remodularising the components of a software system is challenging: sound design principles (e.g., coupling and cohesion) need to be balanced against developer intuition of which entities conceptually belong together. Despite this, automated approaches to remodularisation tend to ignore domain knowledge, leading to results that can be nonsensical to developers. Nevertheless, suppling such knowledge is a potentially burdensome task to perform manually. A lot information may need to be specified, particularly for large systems. Addressing these concerns, we propose the SUpervised reMOdularisation (SUMO) approach. SUMO is a technique that aims to leverage a small subset of domain knowledge about a system to produce a remodularisation that will be acceptable to a developer. With SUMO, developers refine a modularisation by iteratively supplying corrections. These corrections constrain the type of remodularisation eventually required, enabling SUMO to dramatically reduce the solution space. This in turn reduces the amount of feedback the developer needs to supply. We perform a comprehensive systematic evaluation using 100 real world subject systems. Our results show that SUMO guarantees convergence on a target remodularisation with a tractable amount of user interaction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2786222","EPSRC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8259332","Software remodularisation;domain knowledge;set partitioning","Clustering algorithms;Tools;Software algorithms;Software systems;Algorithm design and analysis","knowledge based systems;learning (artificial intelligence);reverse engineering;software maintenance","automated software remodularisation;domain knowledge;SUMO;target remodularisation;expert knowledge;supervised remodularisation approach","","","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Refactoring of OCL Constraints with Search","H. Lu; S. Wang; T. Yue; s. Ali; J. F. Nygård","Simula Research Laboratory, Fornebu, Norway; Simula Research Laboratory, Fornebu, Norway; Simula Research Laboratory, Fornebu, Norway; Simula Research Laboratory, Fornebu, Norway; Cancer Registry of Norway, Oslo, Norway","IEEE Transactions on Software Engineering","","2019","45","2","148","170","Object Constraint Language (OCL) constraints are typically used to provide precise semantics to models developed with the Unified Modeling Language (UML). When OCL constraints evolve regularly, it is essential that they are easy to understand and maintain. For instance, in cancer registries, to ensure the quality of cancer data, more than one thousand medical rules are defined and evolve regularly. Such rules can be specified with OCL. It is, therefore, important to ensure the understandability and maintainability of medical rules specified with OCL. To tackle such a challenge, we propose an automated <underline>s</underline>earch-<underline>b</underline>ased <underline>O</underline>CL constraint <underline>r</underline>efactoring <underline>a</underline>pproach (SBORA) by defining and applying four semantics-preserving refactoring operators (i.e., <italic>Context Change</italic>, <italic>Swap</italic>, <italic>Split</italic> and <italic>Merge</italic>) and three OCL quality metrics (<italic>Complexity</italic>, <italic>Coupling,</italic> and <italic>Cohesion</italic>) to measure the understandability and maintainability of OCL constraints. We evaluate SBORA along with six commonly used multi-objective search algorithms (e.g., Indicator-Based Evolutionary Algorithm (IBEA)) by employing four case studies from different domains: healthcare (i.e., cancer registry system from Cancer Registry of Norway (CRN)), Oil&Gas (i.e., subsea production systems), warehouse (i.e., handling systems), and an open source case study named SEPA. Results show: 1) IBEA achieves the best performance among all the search algorithms and 2) the refactoring approach along with IBEA can manage to reduce on average 29.25 percent <italic>Complexity</italic> and 39 percent <italic>Coupling</italic> and improve 47.75 percent <italic>Cohesion</italic>, as compared to the original OCL constraint set from CRN. To further test the performance of SBORA, we also applied it to refactor an OCL constraint set specified on the UML 2.3 metamodel and we obtained positive results. Furthermore, we conducted a controlled experiment with 96 subjects and results show that the understandability and maintainability of the original constraint set can be improved significantly from the perspectives of the 96 participants of the controlled experiment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2774829","RFF Hovedstaden; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8114267","Constraints;metrics/measurement;methodologies;CASE","Cancer;Unified modeling language;Measurement;Couplings;Complexity theory;Semantics;Computational modeling","","","","","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Developer Micro Interaction Metrics for Software Defect Prediction","T. Lee; J. Nam; D. Han; S. Kim; H. Peter In","Korea University, Seoul, South Korea; University of Waterloo, ON, Canada; University Colleage London, London, United Kingdom; Hong Kong University of Science and Technology, Hong Kong, China; Korea University, Seoul, South Korea","IEEE Transactions on Software Engineering","","2016","42","11","1015","1035","To facilitate software quality assurance, defect prediction metrics, such as source code metrics, change churns, and the number of previous defects, have been actively studied. Despite the common understanding that developer behavioral interaction patterns can affect software quality, these widely used defect prediction metrics do not consider developer behavior. We therefore propose micro interaction metrics (MIMs), which are metrics that leverage developer interaction information. The developer interactions, such as file editing and browsing events in task sessions, are captured and stored as information by Mylyn, an Eclipse plug-in. Our experimental evaluation demonstrates that MIMs significantly improve overall defect prediction accuracy when combined with existing software measures, perform well in a cost-effective manner, and provide intuitive feedback that enables developers to recognize their own inefficient behaviors during software development.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2550458","Next-Generation Information Computing Development Program; National Research Foundation of Korea; Ministry of Education, Science and Technology; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7447797","Defect prediction;software quality;software metrics;developer interaction;Mylyn","Software quality;Software metrics;Quality assurance;Complexity theory","software maintenance;software metrics;software quality","developer microinteraction metrics;software quality assurance;software defect prediction;defect prediction metrics;developer behavioral interaction patterns;MIM metric;developer interaction information;Mylyn plug-in;Eclipse plug-in;software development","","10","","66","","","","","","IEEE","IEEE Journals & Magazines"
"A taxonomy and catalog of runtime software-fault monitoring tools","N. Delgado; A. Q. Gates; S. Roach","Microsoft, Bellevue, WA, USA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","12","859","872","A goal of runtime software-fault monitoring is to observe software behavior to determine whether it complies with its intended behavior. Monitoring allows one to analyze and recover from detected faults, providing additional defense against catastrophic failure. Although runtime monitoring has been in use for over 30 years, there is renewed interest in its application to fault detection and recovery, largely because of the increasing complexity and ubiquitous nature of software systems. We present taxonomy that developers and researchers can use to analyze and differentiate recent developments in runtime software fault-monitoring approaches. The taxonomy categorizes the various runtime monitoring research by classifying the elements that are considered essential for building a monitoring system, i.e., the specification language used to define properties; the monitoring mechanism that oversees the program's execution; and the event handler that captures and communicates monitoring results. After describing the taxonomy, the paper presents the classification of the software-fault monitoring systems described in the literature.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377185","Index Terms- Assertion checkers;runtime monitors;specification;specification language;survey;software/program verification.","Taxonomy;Runtime;Condition monitoring;Computerized monitoring;Fault detection;Application software;Software systems;Specification languages;Software testing;Computer Society","software fault tolerance;program verification;specification languages;system monitoring;system recovery","runtime software-fault monitoring tools;software behavior;catastrophic failure;runtime monitoring;software system;specification language;program verification","","178","","76","","","","","","IEEE","IEEE Journals & Magazines"
"Quality, productivity, and learning in framework-based development: an exploratory case study","M. Morisio; D. Romano; I. Stamelos","Dipt. di Autom. e Inf., Politecnico di Torino, Italy; NA; NA","IEEE Transactions on Software Engineering","","2002","28","9","876","888","This paper presents an empirical study in an industrial context on the production of software using a framework. Frameworks are semicomplete applications, usually implemented as a hierarchy of classes. The framework is developed first, then several applications are derived from it. Frameworks are a reuse technique that supports the engineering of product lines. In the study, we compare quality (in the sense of rework effort) and productivity in traditional and framework-based software production. We observe that the latter is characterized by better productivity and quality, as well as a massive increase in productivity over time, that we attribute to the effect of learning the framework. Although we cannot extrapolate the results outside the local environment, enough evidence has been accumulated to stimulate future research work.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033227","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033227","","Productivity;Computer aided software engineering;Application software;Object oriented modeling;Software quality;Production;Software design;Investments;Computer Society;Computer industry","software quality;software reusability;software development management","software production;framework-based development;semicomplete applications;reuse technique;product line engineering;quality;productivity;learning","","22","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Discovering Services during Service-Based System Design Using UML","G. Spanoudakis; A. Zisman","City University, London; City University, London","IEEE Transactions on Software Engineering","","2010","36","3","371","389","Recently, there has been a proliferation of service-based systems, i.e., software systems that are composed of autonomous services but can also use software code. In order to support the development of these systems, it is necessary to have new methods, processes, and tools. In this paper, we describe a UML-based framework to assist with the development of service-based systems. The framework adopts an iterative process in which software services that can provide functional and nonfunctional characteristics of a system being developed are discovered, and the identified services are used to reformulate the design models of the system. The framework uses a query language to represent structural, behavioral, and quality characteristics of services to be identified, and a query processor to match the queries against service registries. The matching process is based on distance measurements between the queries and service specifications. A prototype tool has been implemented. The work has been evaluated in terms of recall, precision, and performance measurements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374424","Design notations and documentation;software process models;search discovery language;service discovery engine.","Unified modeling language;Software systems;Database languages;Quality of service;Computer Society;Distance measurement;Software prototyping;Prototypes;Documentation;Engines","pattern matching;query languages;query processing;software prototyping;Unified Modeling Language;Web services","service-based system design;software systems;software code;UML-based framework;iterative process;software services;query processor;matching process;distance measurements;query language","","23","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Is proof more cost-effective than testing?","S. King; J. Hammond; R. Chapman; A. Pryor","Dept. of Comput. Sci., York Univ., UK; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","8","675","686","The paper describes the use of formal development methods on an industrial safety-critical application. The Z notation was used for documenting the system specification and part of the design, and the SPARK subset of Ada was used for coding. However, perhaps the most distinctive nature of the project lies in the amount of proof that was carried out: proofs were carried out both at the Z level (approximately 150 proofs in 500 pages) and at the SPARK code level (approximately 9000 verification conditions generated and discharged). The project was carried out under UK Interim Defence Standards 00-55 and 00-56, which require the use of formal methods on safety-critical applications. It is believed to be the first to be completed against the rigorous demands of the 1991 version of these standards. The paper includes comparisons of proof with the various types of testing employed, in terms of their efficiency at finding faults. The most striking result is that the Z proof appears to be substantially more efficient at finding faults than the most efficient testing phase. Given the importance of early fault detection, we believe this helps to show the significant benefit and practicality of large-scale proof on projects of this kind.","0098-5589;1939-3520;2326-3881","","10.1109/32.879807","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879807","","Sparks;Helicopters;Formal specifications;Marine vehicles;Information systems;Fault detection;Large-scale systems;Software testing;Computer industry;Code standards","safety-critical software;formal specification;theorem proving;Ada;program testing;military computing","formal development methods;industrial safety-critical application;Z notation;system specification;SPARK subset;Ada;Z level;SPARK code level;verification conditions;UK Interim Defence Standards;00-55;00-56;formal methods;safety-critical applications;rigorous demands;Z proof;testing phase;fault detection;large-scale proof","","35","","","","","","","","IEEE","IEEE Journals & Magazines"
"Validation of an approach for improving existing measurement frameworks","M. G. Mendonca; V. R. Basili","Comput. Networks Res. Group, Salvador Univ., Brazil; NA","IEEE Transactions on Software Engineering","","2000","26","6","484","499","Software organizations are in need of methods to understand, structure, and improve the data their are collecting. We have developed an approach for use when a large number of diverse metrics are already being collected by a software organization (M.G. Mendonca et al., 1998; M.G. Mendonca, 1997). The approach combines two methods. One looks at an organization's measurement framework in a top-down goal-oriented fashion and the other looks at it in a bottom-up data-driven fashion. The top-down method is based on a measurement paradigm called Goal-Question-Metric (GQM). The bottom-up method is based on a data mining technique called Attribute Focusing (AF). A case study was executed to validate this approach and to assess its usefulness in an industrial environment. The top-down and bottom-up methods were applied in the customer satisfaction measurement framework at the IBM Toronto Laboratory. The top-down method was applied to improve the customer satisfaction (CUSTSAT) measurement from the point of view of three data user groups. It identified several new metrics for the interviewed groups, and also contributed to better understanding of the data user needs. The bottom-up method was used to gain new insights into the existing CUSTSAT data. Unexpected associations between key variables prompted new business insights, and revealed problems with the process used to collect and analyze the CUSTSAT data. The paper uses the case study and its results to qualitatively compare our approach against current ad hoc practices used to improve existing measurement frameworks.","0098-5589;1939-3520;2326-3881","","10.1109/32.852739","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=852739","","Software measurement;Customer satisfaction;Data mining;Laboratories;Software maintenance;Data analysis;Current measurement;Knowledge management;Software development management;Time measurement","data mining;DP industry;software metrics;software quality","business insights;data user needs;CUSTSAT measurement;customer satisfaction measurement framework;industrial environment;case study;Attribute Focusing;data mining technique;Goal-Question-Metric;measurement paradigm;bottom-up data-driven method;top-down goal-oriented method;diverse metrics;software organizations;measurement frameworks","","34","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Investigating the defect detection effectiveness and cost benefit of nominal inspection teams","S. Biffl; M. Halling","Inst. for Software Technol., Vienna Univ. of Technol., Austria; NA","IEEE Transactions on Software Engineering","","2003","29","5","385","397","Inspection is an effective but also expensive quality assurance activity to find defects early during software development. The defect detection process, team size, and staff hours invested can have a considerable impact on the defect detection effectiveness and cost-benefit of an inspection. In this paper, we use empirical data and a probabilistic model to estimate this impact for nominal (noncommunicating) inspection teams in an experiment context. Further, the analysis investigates how cutting off the inspection after a certain time frame would influence inspection performance. Main findings of the investigation are: 1) Using combinations of different reading techniques in a team is considerably more effective than using the best single technique only (regardless of the observed level of effort). 2) For optimizing the inspection performance, determining the optimal process mix in a team is more important than adding an inspector (above a certain team size) in our model. 3) A high level of defect detection effectiveness is much more costly to achieve than a moderate level since the average cost for the defects found by the inspector last added to a team increases more than linearly with growing effort investment. The work provides an initial baseline of inspection performance with regard to process diversity and effort in inspection teams. We encourage further studies on the topic of time usage with defect detection techniques and its effect on inspection effectiveness in a variety of inspection contexts to support inspection planning with limited resources.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199069","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199069","","Inspection;Programming;Quality assurance;Cost function;Investments;Context modeling;Performance analysis;Software engineering;Systems engineering and theory;Humans","software development management;program debugging;inspection;optimisation","defect detection effectiveness;cost-benefit analysis;nominal inspection teams;quality assurance;software development;noncommunicating inspection teams;reading technique combinations;optimal process mix","","32","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Key establishment in large dynamic groups using one-way function trees","A. T. Sherman; D. A. McGrew","Departynent of Comput. Sci. & Electr. Eng., Maryland Univ., Baltimore, MD, USA; NA","IEEE Transactions on Software Engineering","","2003","29","5","444","458","We present, implement, and analyze a new scalable centralized algorithm, called OFT, for establishing shared cryptographic keys in large, dynamically changing groups. Our algorithm is based on a novel application of one-way function trees. In comparison with the top-down logical key hierarchy (LKH) method of Wallner et al., our bottom-up algorithm approximately halves the number of bits that need to be broadcast to members in order to rekey after a member is added or evicted. The number of keys stored by group members, the number of keys broadcast to the group when new members are added or evicted, and the computational efforts of group members, are logarithmic in the number of group members. Among the hierarchical methods, OFT is the first to achieve an approximate halving in broadcast length, an idea on which subsequent algorithms have built. Our algorithm provides complete forward and backward security: Newly admitted group members cannot read previous messages, and evicted members cannot read future messages, even with collusion by arbitrarily many evicted members. In addition, and unlike LKH, our algorithm has the option of being member contributory in that members can be allowed to contribute entropy to the group key. Running on a Pentium II, our prototype has handled groups with up to 10 million members. This algorithm offers a new scalable method for establishing group session keys for secure large-group applications such as broadcast encryption, electronic conferences, multicast sessions, and military command and control.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199073","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199073","","Satellite broadcasting;Cryptography;TV broadcasting;Security;Multicast algorithms;Conference management;Project management;Radio broadcasting;Algorithm design and analysis;Entropy","cryptography;protocols","shared cryptographic keys;encryption;conference keying;cryptography;cryptographic protocols;Dynamic Cryptographic Context Management;group keying;key agreement;key establishment;key management;logical key hierarchy;one-way functions;one-way function tree;secure conferences;secure group applications","","275","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Recomputing Coverage Information to Assist Regression Testing","P. K. Chittimalli; M. J. Harrold","Tata Research Development &amp; Design Centre, India; Georgia Institute of Technology, Atlanta","IEEE Transactions on Software Engineering","","2009","35","4","452","469","This paper presents a technique that leverages an existing regression test selection algorithm to compute accurate, updated coverage data on a version of the software, P<sub>i+1</sub>, without rerunning any test cases that do not execute the changes from the previous version of the software, P<sub>i</sub> to P<sub>i+1</sub>. The technique also reduces the cost of running those test cases that are selected by the regression test selection algorithm by performing a selective instrumentation that reduces the number of probes required to monitor the coverage data. Users of our technique can avoid the expense of rerunning the entire test suite on P<sub>i+1</sub> or the inaccuracy produced by previous approaches that estimate coverage data for P<sub>i+1</sub> or that reuse outdated coverage data from Pi. This paper also presents a tool, RECOVER, that implements our technique, along with a set of empirical studies on a set of subjects that includes several industrial programs, versions, and test cases. The studies show the inaccuracies that can exist when an application-regression test selection-uses estimated or outdated coverage data. The studies also show that the overhead incurred by selective instrumentation used in our technique is negligible and overall our technique provides savings over earlier techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4760153","Regression testing;regression test selection;testing;maintenance.","Software testing;Costs;Performance evaluation;Instruments;Software quality;Software algorithms;Probes;Monitoring;Software performance;Error correction","program testing;regression analysis","coverage information;regression testing;software testing;industrial programs","","21","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Understanding Exception Handling: Viewpoints of Novices and Experts","H. Shah; C. Gorg; M. J. Harrold","Georgia Institute of Technology, Atlanta; Georgia Institute of Technology, Atlanta; Georgia Institute of Technology, Atlanta","IEEE Transactions on Software Engineering","","2010","36","2","150","161","Several recent studies indicate that many industrial applications exhibit poor quality in the design of exception-handling. To improve the quality of error-handling, we need to understand the problems and obstacles that developers face when designing and implementing exception-handling. In this paper, we present our research on understanding the viewpoint of developers-novices and experts-toward exception-handling. First, we conducted a study with novice developers in industry. The study results reveal that novices tend to ignore exceptions because of the complex nature of exception-handling. Then, we conducted a second study with experts in industry to understand their perspective on exception-handling. The study results show that, for experts, exception-handling is a crucial part in the development process. Experts also confirm the novices' approach of ignoring exception-handling and provide insights as to why novices do so. After analyzing the study data, we identified factors that influence experts' strategy selection process for handling exceptions and then built a model that represents a strategy selection process experts use to handle exceptions. Our model is based on interacting modules and fault scope. We conclude with some recommendations to help novices improve their understanding of exception-handling.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383375","Exception handling;user study;software developers.","Software performance;Application software;Debugging;Data analysis;Software tools;Functional programming;Programming profession;Performance evaluation;Visualization","error handling;software engineering","exception handling design;error handling quality;software development process;expert strategy selection process","","24","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Tool support for verifying UML activity diagrams","R. Eshuis; R. Wieringa","Dept. of Technol. Manage., Eindhoven Univ. of Technol., Netherlands; NA","IEEE Transactions on Software Engineering","","2004","30","7","437","447","We describe a tool that supports verification of workflow models specified in UML activity diagrams. The tool translates an activity diagram into an input format for a model checker according to a mathematical semantics. With the model checker, arbitrary propositional requirements can be checked against the input model. If a requirement fails to hold, an error trace is returned by the model checker, which our tool presents by highlighting a corresponding path in the activity diagram. We summarize our formal semantics, discuss the techniques used to reduce an infinite state space to a finite one, and motivate the need for strong fairness constraints to obtain realistic results. We define requirement-preserving rules for state space reduction. Finally, we illustrate the whole approach with a few example verifications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318605","Analysis;tools;software/program verification;model checking;state diagrams;workflow management.","Unified modeling language;Logic;Mathematical model;State-space methods;Software tools;Computer industry;Software standards;Software design;Workflow management software;Software systems","formal specification;specification languages;workflow management software;program verification","workflow model verification;UML activity diagrams;mathematical semantics;formal semantics;requirement-preserving rules;program verification;model checking;state diagrams;workflow management","","51","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis of the effects of software reuse on customer satisfaction in an RPG environment","G. Succi; L. Benedicenti; T. Vernazza","Dept. of Electr. & Comput. Eng., Alberta Univ., Edmonton, Alta., Canada; NA; NA","IEEE Transactions on Software Engineering","","2001","27","5","473","479","This paper reports on empirical research based on two software products. The research goal is to ascertain the impact of the adoption of a reuse policy on customer satisfaction. The results show that when a systematic reuse policy is implemented, such as the adoption of a domain specific library: reuse is significantly positively correlated with customer satisfaction; and there is a significant increase in customer satisfaction. The results have been extended to the underlying populations, supposed normal.","0098-5589;1939-3520;2326-3881","","10.1109/32.922717","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=922717","","Customer satisfaction;Software libraries;Software measurement;Productivity;Density measurement;Frequency measurement;Size measurement;Computer Society;Software metrics;Software engineering","software reusability;software metrics;software libraries","software reuse;customer satisfaction;software products;domain specific library;software metrics;software engineering","","18","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Deadline analysis of interrupt-driven software","D. Brylow; J. Palsberg","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; NA","IEEE Transactions on Software Engineering","","2004","30","10","634","655","Real-time, reactive, and embedded systems are increasingly used throughout society (e.g., flight control, railway signaling, vehicle management, medical devices, and many others). For real-time, interrupt-driven software, timely interrupt handling is part of correctness. It is vital for software verification in such systems to check that all specified deadlines for interrupt handling are met. Such verification is a daunting task because of the large number of different possible interrupt arrival scenarios. For example, for a Z86-based microcontroller, there can be up to six interrupt sources and each interrupt can arrive during any clock cycle. Verification of such systems has traditionally relied upon lengthy and tedious testing; even under the best of circumstances, testing is likely to cover only a fraction of the state space in interrupt-driven systems. This paper presents the Zilog architecture resource bounding infrastructure (ZARBI), a tool for deadline analysis of interrupt-driven Z86-based software. The main idea is to use static analysis to significantly decrease the required testing effort by automatically identifying and isolating the segments of code that need the most testing. Our tool combines multiresolution static analysis and testing oracles in such a way that only the oracles need to be verified by testing. Each oracle specifies the worst-case execution time from one program point to another, which is then used by the static analysis to improve precision. For six commercial microcontroller systems, our experiments show that a moderate number of testing oracles are sufficient to do precise deadline analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339276","Index Terms- Real time;multiresolution static analysis;testing oracles.","System testing;Microcontrollers;Automatic testing;Real time systems;Embedded system;Aerospace control;Rail transportation;Vehicles;Clocks;State-space methods","embedded systems;interrupts;program diagnostics;microcontrollers;program testing;formal verification;program compilers","embedded systems;interrupt handling;software verification;Z86-based microcontroller;Zilog architecture resource bounding infrastructure;deadline analysis;interrupt-driven Z86-based software;multiresolution static analysis;testing oracles;worst-case execution time","","10","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Fast, centralized detection and resolution of distributed deadlocks in the generalized model","Soojung Lee","Dept. of Comput. Educ., GyeongIn Nat. Univ. of Educ., Inchon, South Korea","IEEE Transactions on Software Engineering","","2004","30","9","561","573","In the literature, only a few studies have been performed on the distributed deadlock detection and resolution problem in the generalized request model. Most of the studies are based on the diffusing computation technique where propagation of probes and backward propagation of replies are required to detect deadlock. The replies carry the dependency information between processes for the initiator of the algorithm to determine deadlock. Since fast detection of deadlock is critical, we take a centralized approach that removes the need of backward propagation of replies, but sends the dependency information directly to the initiator of the algorithm. This enables reduction of time cost for deadlock detection to half of that of the existing distributed algorithms. The algorithm is extended to handle concurrent executions in order to further improve deadlock detection time, whereas the current algorithms focus only on a single execution. Simulation experiments are performed to see the effectiveness of this centralized approach as compared to previous distributed algorithms. It is found that our algorithm shows better results in several performance metrics especially in deadlock latency and algorithm execution time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324644","Index Terms- Deadlock detection;deadlock resolution;distributed algorithms;distributed systems;wait-for graph.","System recovery;Probes;Distributed algorithms;Computer Society;Costs;Measurement;Delay;Resumes;Resource management;Operating systems","distributed algorithms;system recovery;operating systems (computers);directed graphs;computational complexity;message passing","distributed deadlock detection;distributed deadlock resolution;backward propagation;dependency information;centralized approach;algorithm initiator;distributed algorithm;concurrent execution;performance metrics;deadlock latency;algorithm execution time;distributed systems;wait-for graph","","19","","21","","","","","","IEEE","IEEE Journals & Magazines"
"The Effects of Test-Driven Development on External Quality and Productivity: A Meta-Analysis","Y. Rafique; V. B. Mišić","Ryerson University, Toronto; Ryerson University, Toronto","IEEE Transactions on Software Engineering","","2013","39","6","835","856","This paper provides a systematic meta-analysis of 27 studies that investigate the impact of Test-Driven Development (TDD) on external code quality and productivity. The results indicate that, in general, TDD has a small positive effect on quality but little to no discernible effect on productivity. However, subgroup analysis has found both the quality improvement and the productivity drop to be much larger in industrial studies in comparison with academic studies. A larger drop of productivity was found in studies where the difference in test effort between the TDD and the control group's process was significant. A larger improvement in quality was also found in the academic studies when the difference in test effort is substantial; however, no conclusion could be derived regarding the industrial studies due to the lack of data. Finally, the influence of developer experience and task size as moderator variables was investigated, and a statistically significant positive correlation was found between task size and the magnitude of the improvement in quality.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6197200","Test-driven development;meta-analysis;code quality;programmer productivity;agile software development","Productivity;Computational modeling;Testing;Process control;Programming;Size measurement","program testing;software development management;software quality","test driven development;systematic meta analysis;code quality;code productivity;TDD;subgroup analysis;quality improvement","","31","","71","","","","","","IEEE","IEEE Journals & Magazines"
"Service-Level Agreements for Electronic Services","J. Skene; F. Raimondi; W. Emmerich","The University of Auckland, Auckland; Middlesex University, London; University College London, London","IEEE Transactions on Software Engineering","","2010","36","2","288","304","The potential of communication networks and middleware to enable the composition of services across organizational boundaries remains incompletely realized. In this paper, we argue that this is in part due to outsourcing risks and describe the possible contribution of Service-Level Agreements (SLAs) to mitigating these risks. For SLAs to be effective, it should be difficult to disregard their original provisions in the event of a dispute between the parties. Properties of understandability, precision, and monitorability ensure that the original intent of an SLA can be recovered and compared to trustworthy accounts of service behavior to resolve disputes fairly and without ambiguity. We describe the design and evaluation of a domain-specific language for SLAs that tend to exhibit these properties and discuss the impact of monitorability requirements on service-provision practices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5210121","Service-level agreements;electronic services;contracts;domain-specific languages;model-driven engineering.","Outsourcing;Cloud computing;Consumer electronics;Middleware;Service oriented architecture;Domain specific languages;Distributed computing;Computer Society;Communication networks;Monitoring","client-server systems;high level languages;Internet;outsourcing","service level agreements;electronic services;communication networks;middleware;outsourcing risks;SLA;domain specific language","","18","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Call-Stack Coverage for GUI Test Suite Reduction","S. McMaster; A. Memon","NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","99","115","Graphical user interfaces (GUIs) are used as front ends to most of today's software applications. The event-driven nature of GUIs presents new challenges for testing. One important challenge is test suite reduction. Conventional reduction techniques/tools based on static analysis are not easily applicable due to the increased use of multilanguage GUI implementations, callbacks for event handlers, virtual function calls, reflection, and multithreading. Moreover, many existing techniques ignore code in libraries and fail to consider the context in which event handlers execute. Consequently, they yield GUI test suites with seriously impaired fault-detection abilities. This paper presents a reduction technique based on the call-stack coverage criterion. Call stacks may be collected for any executing program with very little overhead. Empirical studies in this paper compare reduction based on call-stack coverage to reduction based on line, method, and event coverage, including variations that control for the size and optional consideration of library methods. These studies show that call-stack-based reduction provides unique trade-offs between the reduction in test suite size and the loss of fault detection effectiveness, which may be valuable in practice. Additionally, an analysis of the relationship between coverage requirements and fault-revealing test cases is presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70756","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4378345","Testing strategies;Test coverage of code;Test management;Testing tools;Testing strategies;Test coverage of code;Test management;Testing tools","Graphical user interfaces;Application software;Software testing;Reflection;Multithreading;Libraries;Fault detection;Computer Society;User interfaces;Size control","computer testing;graphical user interfaces","graphical user interfaces;test suite reduction;call-stack coverage criterion","","39","","32","","","","","","IEEE","IEEE Journals & Magazines"
"DEC: Service Demand Estimation with Confidence","A. Kalbasi; D. Krishnamurthy; J. Rolia; S. Dawson","University of Calgary, Calgary; University of Calgary, Calgary; Hewlett Packard Labs, Bristol; SAP Research Center Belfast, Belfast","IEEE Transactions on Software Engineering","","2012","38","3","561","578","We present a new technique for predicting the resource demand requirements of services implemented by multitier systems. Accurate demand estimates are essential to ensure the efficient provisioning of services in an increasingly service-oriented world. The demand estimation technique proposed in this paper has several advantages compared with regression-based demand estimation techniques, which many practitioners employ today. In contrast to regression, it does not suffer from the problem of multicollinearity, it provides more reliable aggregate resource demand and confidence interval predictions, and it offers a measurement-based validation test. The technique can be used to support system sizing and capacity planning exercises, costing and pricing exercises, and to predict the impact of changes to a service upon different service customers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728829","Benchmarking;resource demand prediction;statistical regression.","Benchmark testing;Equations;Software;Mathematical model;Estimation;Frequency modulation;Computers","multiprocessing systems;regression analysis;service-oriented architecture","DEC;service demand estimation technique;resource demand requirements;multitier systems;service-oriented world;regression-based demand estimation techniques;multicollinearity;system sizing;capacity planning","","20","","33","","","","","","IEEE","IEEE Journals & Magazines"
"STAR: Stack Trace Based Automatic Crash Reproduction via Symbolic Execution","N. Chen; S. Kim","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","IEEE Transactions on Software Engineering","","2015","41","2","198","220","Software crash reproduction is the necessary first step for debugging. Unfortunately, crash reproduction is often labor intensive. To automate crash reproduction, many techniques have been proposed including record-replay and post-failure-process approaches. Record-replay approaches can reliably replay recorded crashes, but they incur substantial performance overhead to program executions. Alternatively, post-failure-process approaches analyse crashes only after they have occurred. Therefore they do not incur performance overhead. However, existing post-failure-process approaches still cannot reproduce many crashes in practice because of scalability issues and the object creation challenge. This paper proposes an automatic crash reproduction framework using collected crash stack traces. The proposed approach combines an efficient backward symbolic execution and a novel method sequence composition approach to generate unit test cases that can reproduce the original crashes without incurring additional runtime overhead. Our evaluation study shows that our approach successfully exploited 31 (59.6 percent) of 52 crashes in three open source projects. Among these exploitable crashes, 22 (42.3 percent) are useful reproductions of the original crashes that reveal the crash triggering bugs. A comparison study also demonstrates that our approach can effectively outperform existing crash reproduction approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2363469","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6926857","Crash reproduction;static analysis;symbolic execution;test case generation;optimization;Crash reproduction;static analysis;symbolic execution;test case generation;optimization","Computer crashes;Arrays;Indexes;Color;Optimization;Explosions;Software","program debugging;program testing;project management;public domain software;system recovery","STAR;stack trace based automatic crash reproduction;software crash reproduction;debugging;record-replay approach;post-failure-process approach;scalability issues;object creation challenge;crash stack traces;backward symbolic execution;method sequence composition approach;unit test case generation;open source projects","","12","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Information-theoretic software clustering","P. Andritsos; V. Tzerpos","Dept. of Comput. Sci., Toronto Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","2005","31","2","150","165","The majority of the algorithms in the software clustering literature utilize structural information to decompose large software systems. Approaches using other attributes, such as file names or ownership information, have also demonstrated merit. At the same time, existing algorithms commonly deem all attributes of the software artifacts being clustered as equally important, a rather simplistic assumption. Moreover, no method that can assess the usefulness of a particular attribute for clustering purposes has been presented in the literature. In this paper, we present an approach that applies information theoretic techniques in the context of software clustering. Our approach allows for weighting schemes that reflect the importance of various attributes to be applied. We introduce LIMBO, a scalable hierarchical clustering algorithm based on the minimization of information loss when clustering a software system. We also present a method that can assess the usefulness of any nonstructural attribute in a software clustering context. We applied LIMBO to three large software systems in a number of experiments. The results indicate that this approach produces clusterings that come close to decompositions prepared by system experts. Experimental results were also used to validate our usefulness assessment method. Finally, we experimented with well-established weighting schemes from information retrieval, Web search, and data clustering. We report results as to which weighting schemes show merit in the decomposition of software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401930","Index Terms- Reverse engineering;reengineering;architecture reconstruction;clustering;information theory.","Software systems;Clustering algorithms;Software algorithms;Computer Society;Computer architecture;Reverse engineering;Software engineering;Minimization methods;Information retrieval;Web search","reverse engineering;software metrics;software architecture;software maintenance;systems re-engineering;pattern clustering;information retrieval","software clustering;software system;information retrieval;Web search;data clustering","","115","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Simulation-verification: biting at the state explosion problem","D. A. Stuart; M. Brockmeyer; A. K. Mok; F. Jahanian","Boeing Co., St. Louis, MO, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","7","599","617","Simulation and verification are two conventional techniques for the analysis of specifications of real-time systems. While simulation is relatively inexpensive in terms of execution time, it only validates the behavior of a system for one particular computation path. On the other hand, verification provides guarantees over the entire set of computation paths of a system, but is, in general, very expensive due to the state-space explosion problem. We introduce a new technique: simulation-verification combines the best of both worlds by synthesizing an intermediate analysis method. This method uses simulation to limit the generation of a computation graph to that set of computations consistent with the simulation. This limited computation graph, called a simulation-verification graph, can be one or more orders of magnitude smaller than the full computation graph. A tool, XSVT, is described which implements simulation-verification graphs. Three paradigms for using the new technique are proposed. The paper illustrates the application of the proposed technique via an example of a robot controller for a manufacturing assembly line.","0098-5589;1939-3520;2326-3881","","10.1109/32.935853","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=935853","","Explosions;Computational modeling;Analytical models;Context modeling;Costs;Real time systems;Robotic assembly;Specification languages;Robot control;Pulp manufacturing","real-time systems;virtual machines;formal verification;formal specification","state explosion problem;specifications;real-time systems;execution time;computation paths;simulation-verification;intermediate analysis method;computation graph;simulation-verification graph;XSVT tool;robot controller;manufacturing assembly line","","17","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Exploiting the Essential Assumptions of Analogy-Based Effort Estimation","E. Kocaguneli; T. Menzies; A. Bener; J. W. Keung","West Virginia University, Morgantown; West Virginia University, Morgantown; Ryerson University, Toronto; The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Software Engineering","","2012","38","2","425","438","Background: There are too many design options for software effort estimators. How can we best explore them all? Aim: We seek aspects on general principles of effort estimation that can guide the design of effort estimators. Method: We identified the essential assumption of analogy-based effort estimation, i.e., the immediate neighbors of a project offer stable conclusions about that project. We test that assumption by generating a binary tree of clusters of effort data and comparing the variance of supertrees versus smaller subtrees. Results: For 10 data sets (from Coc81, Nasa93, Desharnais, Albrecht, ISBSG, and data from Turkish companies), we found: 1) The estimation variance of cluster subtrees is usually larger than that of cluster supertrees; 2) if analogy is restricted to the cluster trees with lower variance, then effort estimates have a significantly lower error (measured using MRE, AR, and Pred(25) with a Wilcoxon test, 95 percent confidence, compared to nearest neighbor methods that use neighborhoods of a fixed size). Conclusion: Estimation by analogy can be significantly improved by a dynamic selection of nearest neighbors, using only the project data from regions with small variance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728833","Software cost estimation;analogy;k-NN.","Estimation;Training;Software;Training data;Linear regression;Euclidean distance;Humans","pattern clustering;program testing;project management;software cost estimation;trees (mathematics)","analogy-based effort estimation;software effort estimator design;essential assumption;supertree variance;subtree variance;Coc81 data set;Nasa93 data set;Desharnais data set;Albrecht data set;ISBSG data set;Turkish companies;estimation variance;binary cluster tree;cluster subtrees;dynamic selection;nearest neighbor selection;project data","","81","","69","","","","","","IEEE","IEEE Journals & Magazines"
"Event Logs for the Analysis of Software Failures: A Rule-Based Approach","M. Cinque; D. Cotroneo; A. Pecchia","Universitá degli Studi di Napoli Federico II, Naples; Universitá degli Studi di Napoli Federico II, Naples; Universitá degli Studi di Napoli Federico II, Naples","IEEE Transactions on Software Engineering","","2013","39","6","806","821","Event logs have been widely used over the last three decades to analyze the failure behavior of a variety of systems. Nevertheless, the implementation of the logging mechanism lacks a systematic approach and collected logs are often inaccurate at reporting software failures: This is a threat to the validity of log-based failure analysis. This paper analyzes the limitations of current logging mechanisms and proposes a rule-based approach to make logs effective to analyze software failures. The approach leverages artifacts produced at system design time and puts forth a set of rules to formalize the placement of the logging instructions within the source code. The validity of the approach, with respect to traditional logging mechanisms, is shown by means of around 12,500 software fault injection experiments into real-world systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6320555","Event log;logging mechanism;rule-based logging;error detection;software failures","Unified modeling language;Failure analysis;Analytical models;Systematics;Proposals;Software systems","software fault tolerance","event logs;software failures;rule-based approach;logging mechanism;log-based failure analysis;system design time","","17","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Synchronizability of conversations among Web services","X. Fu; T. Bultan; J. Su","Sch. of Comput. & Inf. Sci., Georgia Southwestern State Univ., Americus, GA, USA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","12","1042","1055","We present a framework for analyzing interactions among Web services that communicate with asynchronous messages. We model the interactions among the peers participating in a composite Web service as conversations, the global sequences of messages exchanged among the peers. This naturally leads to the following model checking problem: Given an LTL property and a composite Web service, do the conversations generated by the composite Web service satisfy the property? We show that asynchronous messaging leads to state space explosion for bounded message queues and undecidability of the model checking problem for unbounded message queues. We propose a technique called synchronizability analysis to tackle this problem. If a composite Web service is synchronizable, its conversation set remains the same when asynchronous communication is replaced with synchronous communication. We give a set of sufficient conditions that guarantee synchronizability and that can be checked statically. Based on our synchronizability results, we show that a large class of composite Web services with unbounded message queues can be verified completely using a finite state model checker such as SPIN. We also show that synchronizability analysis can be used to check the reliability of top-down conversation specifications and we contrast the conversation model with the Message Sequence Charts. We integrated synchronizability analysis to a tool we developed for analyzing composite Web services.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.141","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566606","Index Terms- Web services;asynchronous communication;conversations;model checking;verification;synchronizability;realizability.","Web services;XML;Asynchronous communication;Application software;Simple object access protocol;Communication standards;Data communication;Buffer storage;State-space methods;Explosions","message passing;formal specification;formal verification;synchronisation;finite state machines;Internet","Web service;messages exchange;model checking problem;synchronizability analysis;asynchronous communication;synchronous communication;SPIN finite state model checker;top-down conversation specification;message sequence charts","","59","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Reversible debugging using program instrumentation","Shyh-Kwei Chen; W. K. Fuchs; Jen-Yao Chung","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","8","715","727","Reversible execution has not been fully exploited in symbolic debuggers. Debuggers that can undo instructions usually incur a significant performance penalty during a debugging session. We describe an efficient reversible debugging mechanism based on program instrumentation. The approach enables repetitive debugging sessions with selectable reversible routines and recording modes. Experimental results indicate that the execution penalty can be significantly reduced with moderate code growth.","0098-5589;1939-3520;2326-3881","","10.1109/32.940726","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=940726","","Debugging;Instruments;History;Runtime;Computer languages;Checkpointing;Emulation;Databases;Program processors;Assembly","program debugging;reverse engineering;assembly language","reversible debugging;program instrumentation;reversible execution;symbolic debuggers;performance penalty;debugging session;repetitive debugging sessions;selectable reversible routines;recording modes;execution penalty;moderate code growth","","9","","35","","","","","","IEEE","IEEE Journals & Magazines"
"A Theoretical and Empirical Study of Diversity-Aware Mutation Adequacy Criterion","D. Shin; S. Yoo; D. Bae","KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea","IEEE Transactions on Software Engineering","","2018","44","10","914","931","Diversity has been widely studied in software testing as a guidance towards effective sampling of test inputs in the vast space of possible program behaviors. However, diversity has received relatively little attention in mutation testing. The traditional mutation adequacy criterion is a one-dimensional measure of the total number of killed mutants. We propose a novel, diversity-aware mutation adequacy criterion called distinguishing mutation adequacy criterion, which is fully satisfied when each of the considered mutants can be identified by the set of tests that kill it, thereby encouraging inclusion of more diverse range of tests. This paper presents the formal definition of the distinguishing mutation adequacy and its score. Subsequently, an empirical study investigates the relationship among distinguishing mutation score, fault detection capability, and test suite size. The results show that the distinguishing mutation adequacy criterion detects 1.33 times more unseen faults than the traditional mutation adequacy criterion, at the cost of a 1.56 times increase in test suite size, for adequate test suites that fully satisfies the criteria. The results show a better picture for inadequate test suites; on average, 8.63 times more unseen faults are detected at the cost of a 3.14 times increase in test suite size.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2732347","Information & communications Technology Promotion (IITP); Korea government (MSIP); Software R&D for Model-based Analysis and Verification of Higher-order Large Complex System; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7994647","Mutation testing;test adequacy criteria;diversity","Fault detection;Software engineering;Software testing;Correlation;Indexes;Subspace constraints","fault diagnosis;program testing;software engineering","empirical study;diversity-aware mutation adequacy criterion;software testing;test inputs;mutation testing;distinguishing mutation score;distinguishing mutation adequacy criterion;adequate test suites;inadequate test suites;theoretical study;program behaviors;killed mutants;fault detection capability","","2","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Two controlled experiments assessing the usefulness of design pattern documentation in program maintenance","L. Prechelt; B. Unger-Lamprecht; M. Philippsen; W. F. Tichy","abaXX Technol., Stuttgart, Germany; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","6","595","606","Using design patterns is claimed to improve programmer productivity and software quality. Such improvements may manifest both at construction time (in faster and better program design) and at maintenance time (in faster and more accurate program comprehension). The paper focuses on the maintenance context and reports on experimental tests of the following question: does it help the maintainer if the design patterns in the program code are documented explicitly (using source code comments) compared to a well-commented program without explicit reference to design patterns? Subjects performed maintenance tasks on two programs ranging from 360 to 560 LOC including comments. The experiments tested whether pattern comment lines (PCL) help during maintenance if patterns are relevant and sufficient program comments are already present. This question is a challenge for the experimental methodology: A setup leading to relevant results is quite difficult to find. We discuss these issues in detail and suggest a general approach to such situations. A conservative analysis of the results supports the hypothesis that pattern-relevant maintenance tasks were completed faster or with fewer errors if redundant design pattern information was provided. The article provides the first controlled experiment results on design pattern usage and it presents a solution approach to an important class of experiment design problems for experiments regarding documentation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010061","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010061","","Documentation;Computer Society;Programming profession;Testing;Productivity;Software design;Software tools;Software quality;Lab-on-a-chip;Java","software maintenance;system documentation;object-oriented programming;human factors;user interfaces","controlled experiments;design pattern documentation;program maintenance;programmer productivity;software quality;program comprehension;experimental tests;source code comments;pattern comment lines;PCL;program comments;Java;German graduate students;C++;American undergraduate students;pattern-relevant maintenance tasks;design pattern information;design pattern usage;experiment design problems","","69","","31","","","","","","IEEE","IEEE Journals & Magazines"
"A large-scale empirical study of just-in-time quality assurance","Y. Kamei; E. Shihab; B. Adams; A. E. Hassan; A. Mockus; A. Sinha; N. Ubayashi","Kyushu University, Fukuoka; Rochester Institute of Technology, Rochester; École Polytechnique de Montréal, Montréal; Queen's University, Kingston; Avaya Labs Research, Basking Ridge; Research In Motion, Waterloo; Kyushu University, Fukuoka","IEEE Transactions on Software Engineering","","2013","39","6","757","773","Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone (“risky”) software changes instead of files or packages. We refer to this type of quality assurance activity as “Just-In-Time Quality Assurance,” because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the characteristics of a software change, such as the number of added lines, and developer experience. A large-scale study of six open source and five commercial projects from multiple domains shows that our models can predict whether or not a change will lead to a defect with an average accuracy of 68 percent and an average recall of 64 percent. Furthermore, when considering the effort needed to review changes, we find that using only 20 percent of the effort it would take to inspect all changes, we can identify 35 percent of all defect-inducing changes. Our findings indicate that “Just-In-Time Quality Assurance” may provide an effort-reducing way to focus on the most risky changes and thus reduce the costs of developing high-quality software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6341763","Maintenance;software metrics;mining software repositories;defect prediction;just-in-time prediction","Measurement;Quality assurance;Predictive models;Software;Entropy;Object oriented modeling;Accuracy","program testing;software maintenance;software metrics;software quality","just-in-time quality assurance;defect prediction models;defect-prone file identification;defect-prone package identification;software systems;risk model;open source projects;commercial projects;risky changes;cost reduction;defect-prone software change identification;software metrics;software repository mining;software quality assurance activities;source code inspection;unit testing","","80","","63","","","","","","IEEE","IEEE Journals & Magazines"
"The exception handling effectiveness of POSIX operating systems","P. Koopman; J. DeVale","Dept. of Electr. & Comput. Eng., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","9","837","848","Operating systems form a foundation for robust application software, making it important to understand how effective they are at handling exceptional conditions. The Ballista testing system was used to characterize the handling of exceptional input parameter values for up to 233 POSIX functions and system calls on each of 15 widely used operating system (OS) implementations. This identified ways to crash systems with a single call, ways to cause task hangs within OS code, ways to cause abnormal task termination within OS and library code, failures to implement defined POSIX functionality, and failures to report unsuccessful operations. Overall, only 55 percent to 76 percent of the exceptional tests performed generated error codes, depending on the operating system being tested. Approximately 6 percent to 19 percent of tests failed to generate any indication of error despite exceptional inputs. Approximately 1 percent to 3 percent of tests revealed failures to implement defined POSIX functionality for unusual, but specified, situations. Between 18 percent and 33 percent of exceptional tests caused the abnormal termination of an OS system call or library function, and five systems were completely crashed by individual system calls with exceptional parameter values. The most prevalent sources of these robustness failures were illegal pointer values, numeric overflows, and end-of-file overruns.","0098-5589;1939-3520;2326-3881","","10.1109/32.877845","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877845","","Operating systems;Robustness;Application software;System testing;Vehicle crash testing;Computer crashes;Libraries;Telecommunication computing;Programming profession;Performance evaluation","exception handling;operating systems (computers);program testing","exception handling effectiveness;POSIX operating systems;application software;Ballista testing system;system calls;system crash;task hangs;abnormal task termination;library code;error codes;illegal pointer values;numeric overflows;end-of-file overruns;robustness failures","","56","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Measuring Program Comprehension: A Large-Scale Field Study with Professionals","X. Xia; L. Bao; D. Lo; Z. Xing; A. E. Hassan; S. Li","Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Singapore Management University, Singapore; Australian National University, Canberra, ACT, Australia; Queen’s University, Kingston, ON, Canada; Zhejiang University, Hangzhou, China","IEEE Transactions on Software Engineering","","2018","44","10","951","976","During software development and maintenance, developers spend a considerable amount of time on program comprehension activities. Previous studies show that program comprehension takes up as much as half of a developer's time. However, most of these studies are performed in a controlled setting, or with a small number of participants, and investigate the program comprehension activities only within the IDEs. However, developers' program comprehension activities go well beyond their IDE interactions. In this paper, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. We then measure the comprehension time by calculating the time that developers spend on program comprehension, e.g., inspecting console and breakpoints in IDE, or reading and understanding tutorials in web browsers. Using this approach, we can perform a more realistic investigation of program comprehension activities, through a field study of program comprehension in practice across a total of seven real projects, on 78 professional developers, and amounting to 3,148 working hours. Our study leverages interaction data that is collected across many applications by the developers. Our study finds that on average developers spend ~58 percent of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension, and we find senior developers spend significantly less percentages of time on program comprehension than junior developers. Our study also highlights the importance of several research directions needed to reduce program comprehension time, e.g., building automatic detection and improvement of low quality code and documentation, construction of software-engineering-specific search engines, designing better IDEs that help developers navigate code and browse information more efficiently, etc.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2734091","NSFC; National Key Technology R&D Program; Ministry of Science and Technology of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7997917","Program comprehension;field study;inference model","Navigation;Software;Time measurement;Browsers;Maintenance engineering;Programming;Debugging","human computer interaction;Internet;program compilers;reverse engineering;search engines;software maintenance","program comprehension activities;program comprehension time;developers time;software development;software maintenance;IDE interactions;ActivitySpace framework;human computer interaction;Web browsers;programming language;project phase;software-engineering;search engines","","1","","63","","","","","","IEEE","IEEE Journals & Magazines"
"On the automatic modularization of software systems using the Bunch tool","B. S. Mitchell; S. Mancoridis","Dept. of Comput. Sci., Drexel Univ., Philadelphia, PA, USA; Dept. of Comput. Sci., Drexel Univ., Philadelphia, PA, USA","IEEE Transactions on Software Engineering","","2006","32","3","193","208","Since modern software systems are large and complex, appropriate abstractions of their structure are needed to make them more understandable and, thus, easier to maintain. Software clustering techniques are useful to support the creation of these abstractions by producing architectural-level views of a system's structure directly from its source code. This paper examines the Bunch clustering system which, unlike other software clustering tools, uses search techniques to perform clustering. Bunch produces a subsystem decomposition by partitioning a graph of the entities (e.g., classes) and relations (e.g., function calls) in the source code. Bunch uses a fitness function to evaluate the quality of graph partitions and uses search algorithms to find a satisfactory solution. This paper presents a case study to demonstrate how Bunch can be used to create views of the structure of significant software systems. This paper also outlines research to evaluate the software clustering results produced by Bunch.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610610","Clustering;reverse engineering;reengineering;program comprehension;optimization;maintainability.","Software systems;Software tools;Software maintenance;Clustering algorithms;Reverse engineering;Software quality;Search problems;Software performance;Partitioning algorithms;Chaos","software architecture;software maintenance;reverse engineering;systems re-engineering;software tools;search problems","automatic modularization;Bunch software clustering tool;software system abstraction;software system architectural-level view;graph partition;search algorithm;reverse engineering;software system reengineering;program comprehension;software maintainability","","207","","56","","","","","","IEEE","IEEE Journals & Magazines"
"How Software Designers Interact with Sketches at the Whiteboard","N. Mangano; T. D. LaToza; M. Petre; A. van der Hoek","Molimur, Mission Viejo, CA; Department of Informatics, Donald Bren School Information and Computer Sciences, University of California, Irvine, CA; Faculty of Mathematics and Computing, The Open University, Milton Keynes, United Kingdom; Department of Informatics, Donald Bren School Information and Computer Sciences, University of California, Irvine, CA","IEEE Transactions on Software Engineering","","2015","41","2","135","156","Whiteboard sketches play a crucial role in software development, helping to support groups of designers in reasoning about a software design problem at hand. However, little is known about these sketches and how they support design `in the moment', particularly in terms of the relationships among sketches, visual syntactic elements within sketches, and reasoning activities. To address this gap, we analyzed 14 hours of design activity by eight pairs of professional software designers, manually coding over 4000 events capturing the introduction of visual syntactic elements into sketches, focus transitions between sketches, and reasoning activities. Our findings indicate that sketches serve as a rich medium for supporting design conversations. Designers often use general-purpose notations. Designers introduce new syntactic elements to record aspects of the design, or re-purpose sketches as the design develops. Designers constantly shift focus between sketches, using groups of sketches together that contain complementary information. Finally, sketches play an important role in supporting several types of reasoning activities (mental simulation, review of progress, consideration of alternatives). But these activities often leave no trace and rarely lead to sketch creation. We discuss the implications of these and other findings for the practice of software design at the whiteboard and for the creation of new electronic software design sketching tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2362924","National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6922572","Interaction styles;systems analysis and design;user-centered design;Interaction styles;systems analysis and design;user-centered design","Encoding;Cognition;Software design;Visualization;Syntactics;Videos","software engineering","software design;whiteboard sketch;software development;visual syntactic elements;reasoning activity","","6","","60","","","","","","IEEE","IEEE Journals & Magazines"
"A survey of controlled experiments in software engineering","D. I. K. Sjoeberg; J. E. Hannay; O. Hansen; V. B. Kampenes; A. Karahasanovic; N. -. Liborg; A. C. Rekdal","Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; NA; NA","IEEE Transactions on Software Engineering","","2005","31","9","733","753","The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.97","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514443","Index Terms- Controlled experiments;survey;research methodology;empirical software engineering.","Software engineering;Software maintenance;Application software;Conference proceedings;Computer Society;Recruitment;Computer industry;Programming;Software systems;Software metrics","software engineering","controlled experiment survey;software engineering;scientific articles","","266","","52","","","","","","IEEE","IEEE Journals & Magazines"
"A Synthetic Workload Generation Technique for Stress Testing Session-Based Systems","D. Krishnamurthy; J. A. Rolia; S. Majumdar","Department of Electrical and Computer Engineering, University of Calgary, 2500 University Drive NW, Calgary, AB, T2N 1N4, Canada; Enterprise Software and Systems Lab, Hewlett Packard Labs, 1501 Page Mill Road, Palo Alto, CA 94304; Department of Systems and Computer Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, ON, K1S 5B6, Canada","IEEE Transactions on Software Engineering","","2006","32","11","868","882","Enterprise applications are often business critical but lack effective synthetic workload generation techniques to evaluate performance. These workloads are characterized by sessions of interdependent requests that often cause and exploit dynamically generated responses. Interrequest dependencies must be reflected in synthetic workloads for these systems to exercise application functions correctly. This poses significant challenges for automating the construction of representative synthetic workloads and manipulating workload characteristics for sensitivity analyses. This paper presents a technique to overcome these problems. Given request logs for a system under study, the technique automatically creates a synthetic workload that has specified characteristics and maintains the correct interrequest dependencies. The technique is demonstrated through a case study involving a TPC-W e-commerce system. Results show that incorrect performance results can be obtained by neglecting interrequest dependencies, thereby highlighting the value of our technique. The study also exploits our technique to investigate the impact of several workload characteristics on system performance. Results establish that high variability in the distributions of session length, session idle times, and request service times can cause increased contention among sessions, leading to poor system responsiveness. To the best of our knowledge, these are the first results of this kind for a session-based system. We believe our technique is of value for studies where fine control over workload is essential","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.106","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015510","Performance of systems;measurement techniques;modeling techniques;software engineering;testing tools;Internet applications;electronic commerce;Web servers.","System testing;Application software;Sensitivity analysis;Delay;Web server;Stress control;Occupational stress;Computer Society;Character generation;System performance","electronic commerce;performance evaluation;program testing","synthetic workload generation technique;stress testing session-based system;enterprise application;e-commerce system;sensitivity analyses;TPC-W e-commerce system;performance evaluation","","49","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Better reliability assessment and prediction through data clustering","J. Tian","Dept. of Comput. Sci. & Eng., Southern Methodist Univ., Dallas, TX, USA","IEEE Transactions on Software Engineering","","2002","28","10","997","1007","This paper presents a new approach to software reliability modeling by grouping data into clusters of homogeneous failure intensities. This series of data clusters associated with different time segments can be directly used as a piecewise linear model for reliability assessment and problem identification, which can produce meaningful results early in the testing process. The dual model fits traditional software reliability growth models (SRGMs) to these grouped data to provide long-term reliability assessments and predictions. These models were evaluated in the testing of two large software systems from IBM. Compared with existing SRGMs fitted to raw data, our models are generally more stable over time and produce more consistent and accurate reliability assessments and predictions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041055","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041055","","Software systems;Predictive models;Software reliability;Software testing;System testing;Failure analysis;Piecewise linear techniques;Fluctuations;Data analysis","software reliability;statistical analysis;failure analysis;reliability theory","data clustering;reliability assessment;data grouping;input domain reliability models;data cluster based reliability models;piecewise linear model;identification;software reliability growth models","","11","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Design of multi-invariant data structures for robust shared accesses in multiprocessor systems","I-Ling Yen; F. B. Bastani; D. J. Taylor","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","193","207","Multiprocessor systems are widely used in many application programs to enhance system reliability and performance. However, reliability does not come naturally with multiple processors. We develop a multi-invariant data structure approach to ensure efficient and robust access to shared data structures in multiprocessor systems. Essentially, the data structure is designed to satisfy two invariants, a strong invariant, and a weak invariant. The system operates at its peak performance when the strong invariant is true. The system will operate correctly even when only the weak invariant is true, though perhaps at a lower performance level. The design ensures that the weak invariant will always be true in spite of fail-stop processor failures during the execution. By allowing the system to converge to a state satisfying only the weak invariant, the overhead for incorporating fault tolerance can be reduced. We present the basic idea of multi-invariant data structures. We also develop design rules that systematically convert fault-intolerant data abstractions into corresponding fault-tolerant versions. In this transformation, we augment the data structure and access algorithms to ensure that the system always converges to the weak invariant, even in the presence of fail-stop processor failures. We also design methods for the detection of integrity violations and for restoring the strong invariant. Two data structures, namely binary search tree and double-linked list, are used to illustrate the concept of multi-invariant data structures.","0098-5589;1939-3520;2326-3881","","10.1109/32.910857","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910857","","Data structures;Robustness;Multiprocessing systems;Fault tolerant systems;Protocols;Reliability;Tree data structures;Intelligent robots;Concurrent computing;System performance","shared memory systems;data structures;tree searching;fault tolerant computing;transaction processing;data integrity","multi-invariant data structure design;robust shared accesses;multiprocessor systems;application programs;system reliability;multiple processors;multi-invariant data structure approach;robust access;shared data structures;data structure;strong invariant;weak invariant;peak performance;performance level;fail-stop processor failures;fault tolerance;multi-invariant data structures;design rules;fault-intolerant data abstractions;fault-tolerant versions;access algorithms;integrity violations;binary search tree;double-linked list","","","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Explaining software developer acceptance of methodologies: a comparison of five theoretical models","C. K. Riemenschneider; B. C. Hardgrave; F. D. Davis","Sam M. Walton Coll. of Bus. Inf., Arkansas Univ., Fayetteville, AR, USA; Sam M. Walton Coll. of Bus. Inf., Arkansas Univ., Fayetteville, AR, USA; Sam M. Walton Coll. of Bus. Inf., Arkansas Univ., Fayetteville, AR, USA","IEEE Transactions on Software Engineering","","2002","28","12","1135","1145","Many organizations attempt to deploy methodologies intended to improve software development processes. However, resistance by individual software developers against using such methodologies often obstructs their successful deployment. To better explain why individual developers accept or resist methodologies, five theoretical models of individual intentions to accept information technology tools were examined. In a field study of 128 developers in a large organization that implemented a methodology, each model explained significant variance in developers' intentions to use the methodology. Similar to findings from the tool adoption context, we found that, if a methodology is not regarded as useful by developers, its prospects for successful deployment may be severely undermined. In contrast to the typical pattern of findings in a tool context, however, we found that methodology adoption intentions are driven by: 1) the presence of an organizational mandate to use the methodology, 2) the compatibility of the methodology with how developers perform their work, and 3) the opinions of developers' coworkers and supervisors toward using the methodology. Collectively, these results provide surprising new insights into why software developers accept or resist methodologies and suggest what software engineering managers might do to overcome developer resistance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158287","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158287","","Programming;Information technology;Software engineering;Productivity;Resists;Software development management;Computer Society;Engineering management;Technological innovation;Production","software engineering;information technology","software developer acceptance;software development processes;information technology tools;tool adoption context;software developers;software engineering managers","","91","","70","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study using task assignment patterns to improve the accuracy of software effort estimation","R. K. Smith; J. E. Hale; A. S. Parrish","Math., Comput. & Inf. Sci. Dept., Jacksonville State Univ., AL, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","264","271","In most software development organizations, there is seldom a one-to-one mapping between software developers and development tasks. It is frequently necessary to concurrently assign individuals to multiple tasks and to assign more than one individual to work cooperatively on a single task. A principal goal in making such assignments should be to minimize the effort required to complete each task. But what impact does the manner in which developers are assigned to tasks have on the effort requirements? This paper identifies four task assignment factors: team size, concurrency, intensity, and fragmentation. These four factors are shown to improve the predictive ability of the well-known intermediate COCOMO cost estimation model. A parsimonious effort estimation model is also derived that utilizes a subset of the task assignment factors and unadjusted function points. For the data examined, this parsimonious model is shown to have goodness of fit and quality of estimation superior to that of the COCOMO model, while utilizing fewer cost factors.","0098-5589;1939-3520;2326-3881","","10.1109/32.910861","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910861","","Costs;Predictive models;Concurrent computing;Computer Society;Finishing;Project management;Productivity;Programming profession;Software engineering;Information science","software cost estimation;software development management;software metrics","task assignment patterns;software effort estimation;software development organizations;team size;COCOMO;cost estimation model;unadjusted function points","","30","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Plat_Forms: A Web Development Platform Comparison by an Exploratory Experiment Searching for Emergent Platform Properties","L. Prechelt","Freie Universität Berlin, Berlin","IEEE Transactions on Software Engineering","","2011","37","1","95","108","Background: For developing Web-based applications, there exist several competing and widely used technological platforms (consisting of a programming language, framework(s), components, and tools), each with an accompanying development culture and style. Research question: Do Web development projects exhibit emergent process or product properties that are characteristic and consistent within a platform, but show relevant substantial differences across platforms or do team-to-team individual differences outweigh such differences, if any? Such a property could be positive (i.e., a platform advantage), negative, or neutral, and it might be unobvious which is which. Method: In a nonrandomized, controlled experiment, framed as a public contest called “Plat_Forms,” top-class teams of three professional programmers competed to implement the same requirements for a Web-based application within 30 hours. Three different platforms (Java EE, PHP, or Perl) were used by three teams each. We compare the resulting nine products and process records along many dimensions, both external (usability, functionality, reliability, security, etc.) and internal (size, structure, modifiability, etc.). Results: The various results obtained cover a wide spectrum: First, there are results that many people would have called “obvious” or “well known,” say, that Perl solutions tend to be more compact than Java solutions. Second, there are results that contradict conventional wisdom, say, that our PHP solutions appear in some (but not all) respects to be actually at least as secure as the others. Finally, one result makes a statement we have not seen discussed previously: Along several dimensions, the amount of within-platform variation between the teams tends to be smaller for PHP than for the other platforms. Conclusion: The results suggest that substantial characteristic platform differences do indeed exist in some dimensions, but possibly not in others.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5406528","Emergent properties;usability;functionality;reliability;security;product size;design structure;modifiability;Java;PHP;Perl.","Java;Computer languages;Usability;Security;Libraries;Programming profession;Product design;Buildings;Cascading style sheets;Ecosystems","emergent phenomena;Internet;Java;Perl;Web design","Web development platform;emergent platform properties;Web based applications;Java EE;Perl solutions;PHP solutions","","3","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Oracle Data Selection Support","G. Gay; M. Staats; M. Whalen; M. P. E. Heimdahl","Department of Computer Science & Engineering, University of South Carolina; Google, Inc.; Department of Computer Science and Engineering, University of Minnesota; Department of Computer Science and Engineering, University of Minnesota","IEEE Transactions on Software Engineering","","2015","41","11","1119","1137","The choice of test oracle-the artifact that determines whether an application under test executes correctly-can significantly impact the effectiveness of the testing process. However, despite the prevalence of tools that support test input selection, little work exists for supporting oracle creation. We propose a method of supporting test oracle creation that automatically selects the oracle data-the set of variables monitored during testing-for expected value test oracles. This approach is based on the use of mutation analysis to rank variables in terms of fault-finding effectiveness, thus automating the selection of the oracle data. Experimental results obtained by employing our method over six industrial systems (while varying test input types and the number of generated mutants) indicate that our method-when paired with test inputs generated either at random or to satisfy specific structural coverage criteria-may be a cost-effective approach for producing small, effective oracle data sets, with fault finding improvements over current industrial best practice of up to 1,435 percent observed (with typical improvements of up to 50 percent).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2436920","NASA; NSF; US National Science Foundation (NSF); Fonds National de la Recherche, Luxembourg; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7112189","Testing;Test Oracles;Oracle Data;Oracle Selection;Verification;Testing;test oracles;oracle data;oracle selection;verification","Testing;Monitoring;Software;Aerospace electronics;Training;Electronic mail;Computer crashes","program testing;program verification","automated oracle data selection support;mutation analysis;software testing;test oracle;oracle creation;specific structural coverage criteria","","5","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Size-Constrained Regression Test Case Selection Using Multicriteria Optimization","S. Mirarab; S. Akhlaghi; L. Tahvildari","University of Texas at Austin, Austin; Shahed University, Tehran; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2012","38","4","936","956","To ensure that a modified software system has not regressed, one approach is to rerun existing test cases. However, this is a potentially costly task. To mitigate the costs, the testing effort can be optimized by executing only a selected subset of the test cases that are believed to have a better chance of revealing faults. This paper proposes a novel approach for selecting and ordering a predetermined number of test cases from an existing test suite. Our approach forms an Integer Linear Programming problem using two different coverage-based criteria, and uses constraint relaxation to find many close-to-optimal solution points. These points are then combined to obtain a final solution using a voting mechanism. The selected subset of test cases is then prioritized using a greedy algorithm that maximizes minimum coverage in an iterative manner. The proposed approach has been empirically evaluated and the results show significant improvements over existing approaches for some cases and comparable results for the rest. Moreover, our approach provides more consistency compared to existing approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928351","Software regression testing;test case selection;integer programming;Pareto optimality","Testing;Software;Time factors;Fault detection;Optimization;Estimation;IP networks","greedy algorithms;integer programming;linear programming;program testing;regression analysis","size constrained regression test case selection;multicriteria optimization;modified software system;integer linear programming problem;voting mechanism;greedy algorithm;iterative manner","","28","","61","","","","","","IEEE","IEEE Journals & Magazines"
"A Lightweight System for Detecting and Tolerating Concurrency Bugs","M. Zhang; Y. Wu; S. Lu; S. Qi; J. Ren; W. Zheng","Tsinghua National Laboratory for Information Science and Technology (TNLIST), the Department of Computer Science and Technology, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology (TNLIST), the Department of Computer Science and Technology, Tsinghua University, Beijing, China; University of Chicago, Chicago, IL; UBER growth team, San Francisco, CA; Tsinghua National Laboratory for Information Science and Technology (TNLIST), the Department of Computer Science and Technology, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology (TNLIST), the Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Software Engineering","","2016","42","10","899","917","Along with the prevalence of multi-threaded programs, concurrency bugs have become one of the most important sources of software bugs. Even worse, due to the non-deterministic nature of concurrency bugs, these bugs are both difficult to detect and fix even after the detection. As a result, it is highly desired to develop an all-around approach that is able to not only detect them during the testing phase but also tolerate undetected bugs during production runs. However, existing bug-detecting and bug-tolerating tools are usually either<italic>1)</italic>constrained in types of bugs they can handle or<italic>2)</italic>requiring specific hardware supports for achieving an acceptable overhead. In this paper, we present a novel program invariant, name Anticipating Invariant (<sc>Ai</sc>), that can detect most types of concurrency bugs. More importantly,<sc>Ai</sc>can be used to anticipate many concurrency bugs before any irreversible changes have been made. Thus it enables us to develop a software-only system that is able to forestall failures with a simple thread stalling technique, which does not rely on execution roll-back and hence has good performance. Experiments with 35 real-world concurrency bugs demonstrate that<sc>Ai</sc>is capable of detecting and tolerating many important types of concurrency bugs, including both atomicity and order violations. It has also exposed two new bugs (confirmed by developers) that were never reported before in the literature. Performance evaluation with 6 representative parallel programs shows that<sc>Ai</sc>incurs negligible overhead (<inline-formula><tex-math notation=""LaTeX"">$ < 1\%$</tex-math><alternatives><inline-graphic xlink:href=""wu-ieq1-2531666.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>) for many nontrivial desktop and server applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2531666","Natural Science Foundation of China; National Basic Research (973) Program of China; National High-Tech R&D (863) Program of China; Chinese Special Project of Science and Technology; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7412768","Concurrency bugs;software reliability;bug tolerating","Computer bugs;Concurrent computing;Artificial intelligence;Turning;Testing;Hardware","","","","1","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Software Module Clustering as a Multi-Objective Search Problem","K. Praditwong; M. Harman; X. Yao","The University of Birmingham, Birmingham; University College London, London; The University of Birmingham, Birmingham","IEEE Transactions on Software Engineering","","2011","37","2","264","282","Software module clustering is the problem of automatically organizing software units into modules to improve program structure. There has been a great deal of recent interest in search-based formulations of this problem in which module boundaries are identified by automated search, guided by a fitness function that captures the twin objectives of high cohesion and low coupling in a single-objective fitness function. This paper introduces two novel multi-objective formulations of the software module clustering problem, in which several different objectives (including cohesion and coupling) are represented separately. In order to evaluate the effectiveness of the multi-objective approach, a set of experiments was performed on 17 real-world module clustering problems. The results of this empirical study provide strong evidence to support the claim that the multi-objective approach produces significantly better solutions than the existing single-objective approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5406532","SBSE;module clustering;multi-objective optimization;evolutionary computation.","Search problems;Computer science;Performance evaluation;Software engineering;Clustering algorithms;Computational intelligence;Testing;Educational institutions;Computer applications;Application software","optimisation;pattern clustering;search problems;software engineering","software module clustering;multi-objective search problem;program structure","","140","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Prioritizing test cases for regression testing","G. Rothermel; R. H. Untch; Chengyun Chu; M. J. Harrold","Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","10","929","948","Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detection, a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test and let software engineers begin correcting faults earlier than might otherwise be possible. One application of prioritization techniques involves regression testing, the retesting of software following modifications; in this context, prioritization techniques can take advantage of information gathered about the previous execution of test cases to obtain test case orderings. We describe several techniques for using test execution information to prioritize test cases for regression testing, including: 1) techniques that order test cases based on their total coverage of code components; 2) techniques that order test cases based on their coverage of code components not previously covered; and 3) techniques that order test cases based on their estimated ability to reveal faults in the code components that they cover. We report the results of several experiments in which we applied these techniques to various test suites for various programs and measured the rates of fault detection achieved by the prioritized test suites, comparing those rates to the rates achieved by untreated, randomly ordered, and optimally ordered suites.","0098-5589;1939-3520;2326-3881","","10.1109/32.962562","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=962562","","Computer aided software engineering;Software testing;Fault detection;Costs;Computer Society;System testing;Software maintenance;Processor scheduling;Feedback;Application software","program testing;program debugging","test case prioritization;regression testing;software testing;test case scheduling;software fault detection rate;software fault correction;code component coverage;experiments;cost-benefit analysis","","527","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Language-Independent and Automated Software Composition: The FeatureHouse Experience","S. Apel; C. Kästner; C. Lengauer","University of Passau, Passau; Philipps University Marburg, Marburg; University of Passau, Passau","IEEE Transactions on Software Engineering","","2013","39","1","63","79","Superimposition is a composition technique that has been applied successfully in many areas of software development. Although superimposition is a general-purpose concept, it has been (re)invented and implemented individually for various kinds of software artifacts. We unify languages and tools that rely on superimposition by using the language-independent model of feature structure trees (FSTs). On the basis of the FST model, we propose a general approach to the composition of software artifacts written in different languages. Furthermore, we offer a supporting framework and tool chain, called FEATUREHOUSE. We use attribute grammars to automate the integration of additional languages. In particular, we have integrated Java, C#, C, Haskell, Alloy, and JavaCC. A substantial number of case studies demonstrate the practicality and scalability of our approach and reveal insights into the properties that a language must have in order to be ready for superimposition. We discuss perspectives of our approach and demonstrate how we extended FEATUREHOUSE with support for XML languages (in particular, XHTML, XMI/UML, and Ant) and alternative composition approaches (in particular, aspect weaving). Rounding off our previous work, we provide here a holistic view of the FEATUREHOUSE approach based on rich experience with numerous languages and case studies and reflections on several years of research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.120","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6095570","FeatureHouse;feature structure trees;software composition;superimposition;language independence","Software;Java;Grammar;Databases;Printers;Latches;Unified modeling language","attribute grammars;C++ language;Java;software tools;Unified Modeling Language;XML","language-independent software composition;automated software composition;FEATUREHOUSE;superimposition;software development;general-purpose concept;language-independent model;feature structure tree;software artifact composition;supporting framework;tool chain;attribute grammar;integrated Java;C#;Haskell;Alloy;JavaCC;XML language;XHTML;XMI/UML;Ant","","21","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Rapid software development through team collocation","S. D. Teasley; L. A. Covi; M. S. Krishnan; J. S. Olson","Sch. of Inf., Michigan Univ., Ann Arbor, MI, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","7","671","683","In a field study conducted at a leading Fortune 100 company, we examined how having development teams reside in their own large room (an arrangement called radical collocation) affected system development. The collocated projects had significantly higher productivity and shorter schedules than both the industry benchmarks and the performance of past similar projects within the firm. The teams reported high satisfaction about their process, and both customers and project sponsors were similarly highly satisfied. The analysis of questionnaire, interview and observational data from these teams showed that being ""at hand,"" i.e. both visible and available, helped them to coordinate their work better and learn from each other. Radical collocation seems to be one of the factors leading to high productivity in these teams.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019481","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019481","","Programming;Productivity;Job shop scheduling;Computer Society;Software quality;Costs;Processor scheduling;Software engineering;Application software;Quality management","software prototyping;software development management;project management;human resource management","rapid software development;team collocation;field study;Fortune 100 company;software development teams;team room;radical collocation;system development;collocated projects;project productivity;project schedules;industry benchmarks;project performance;satisfaction;questionnaire data;interview data;observational data;team visibility;team availability;work coordination;learning;war rooms;software metrics;software engineering","","62","","47","","","","","","IEEE","IEEE Journals & Magazines"
"On the Asymptotic Behavior of Adaptive Testing Strategy for Software Reliability Assessment","J. Lv; B. Yin; K. Cai","School of Automation Science and Electrical Engineering, Bejing, China; School of Automation Science and Electrical Engineering, Bejing, China; School of Automation Science and Electrical Engineering, Bejing, China","IEEE Transactions on Software Engineering","","2014","40","4","396","412","In software reliability assessment, one problem of interest is how to minimize the variance of reliability estimator, which is often considered as an optimization goal. The basic idea is that an estimator with lower variance makes the estimates more predictable and accurate. Adaptive Testing (AT) is an online testing strategy, which can be adopted to minimize the variance of software reliability estimator. In order to reduce the computational overhead of decision-making, the implemented AT strategy in practice deviates from its theoretical design that guarantees AT's local optimality. This work aims to investigate the asymptotic behavior of AT to improve its global performance without losing the local optimality. To this end, a new AT strategy named Adaptive Testing with Gradient Descent method (AT-GD) is proposed. Theoretical analysis indicates that AT-GD, a locally optimal testing strategy, converges to the globally optimal solution as the assessment process proceeds. Simulation and experiments are set up to validate AT-GD's effectiveness and efficiency. Besides, sensitivity analysis of AT-GD is also conducted in this study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2310194","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6762895","Adaptive testing;operational profile;software reliability;testing strategy","Testing;Software reliability;Software;Global Positioning System;Aircraft;Reliability theory","decision making;gradient methods;optimisation;program testing;sensitivity analysis;software reliability","asymptotic behavior;adaptive testing strategy;software reliability assessment;optimization goal;online testing strategy;software reliability estimator;computational overhead reduction;decision-making;global performance improvement;adaptive testing with gradient descent method;AT-GD;locally optimal testing strategy;sensitivity analysis","","16","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Concept analysis for module restructuring","P. Tonella","Centro per la Ricerca Sci. e Tecnologica, Trento, Italy","IEEE Transactions on Software Engineering","","2001","27","4","351","363","Low coupling between modules and high cohesion inside each module are the key features of good software design. This paper proposes a new approach to using concept analysis for module restructuring, based on the computation of extended concept subpartitions. Alternative modularizations, characterized by high cohesion around the internal structures that are being manipulated, can be determined by such a method. To assess the quality of the restructured modules, the trade-off between encapsulation violations and decomposition is considered, and proper measures for both factors are defined. Furthermore, the cost of restructuring is evaluated through a measure of distance between the original and the new modularizations. Concept subpartitions were determined for a test suite of 20 programs of variable size: 10 public-domain and 10 industrial applications. The trade-off between encapsulation and decomposition was measured on the resulting module candidates, together with an estimate of the cost of restructuring. Moreover, the ability of concept analysis to determine meaningful modularizations was assessed in two ways. First, programs without encapsulation violations were used as oracles, assuming the absence of violations as an indicator of careful decomposition. Second, the suggested restructuring interventions were actually implemented in some case studies to evaluate the feasibility of restructuring and to deeply investigate the code organization before and after the intervention. Concept analysis was experienced to be a powerful tool supporting module restructuring.","0098-5589;1939-3520;2326-3881","","10.1109/32.917524","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917524","","Encapsulation;Data structures;Costs;Software design;Computer languages;Testing;Software engineering;Engineering management;Degradation;Preventive maintenance","subroutines;program control structures;systems re-engineering;data encapsulation;software engineering","concept analysis;module restructuring;module coupling;module cohesion;software design;extended concept subpartitions;modularization;internal structures;module quality;encapsulation violations;decomposition;cost evaluation;distance measure;concept subpartitions;program size;public-domain applications;industrial applications;oracles;restructuring interventions;case studies;code organization;abstract data types;legacy systems;reengineering","","77","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Input-Sensitive Profiling","E. Coppa; C. Demetrescu; I. Finocchi","Department of Computer Science, Sapienza University of Rome, Italy; Department of Computer, Control, and Management Engineering, Sapienza University of Rome, Italy; Department of Computer Science, Sapienza University of Rome, Italy","IEEE Transactions on Software Engineering","","2014","40","12","1185","1205","In this article we present a building block technique and a toolkit towards automatic discovery of workload-dependentperformance bottlenecks. From one or more runs of a program, our profiler automatically measures how the performance of individual routines scales as a function of the input size, yielding clues to their growth rate. The output of the profiler is, for each executed routine of the program, a set of tuples that aggregate performance costs by input size. The collected profiles can be used to produceperformance plots and derive trend functions by statistical curve fitting techniques. A key feature of our method is the ability toautomatically measure the size of the input given to a generic code fragment: to this aim, we propose an effective metric for estimating the input size of a routine and show how to compute it efficiently. We discuss several examples, showing that our approach can reveal asymptotic bottlenecks that other profilers may fail to detect and can provide useful characterizations of the workload and behavior of individual routines in the context of mainstream applications, yielding several code optimizations as well as algorithmic improvements. To prove the feasibility of our techniques, we implemented a Valgrind tool called aprof and performed an extensive experimentalevaluation on the SPEC CPU2006 benchmarks. Our experiments show that aprof delivers comparable performance to otherprominent Valgrind tools, and can generate informative plots even from single runs on typical workloads for mostalgorithmically-critical routines.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339825","Italian Ministry of Education, University, and Research (MIUR); “AMANDA-Algorithmics for MAssive and Networked DAta”; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6858059","Performance profiling;asymptotic analysis;dynamic program analysis;instrumentation","Context modeling;Algorithm design and analysis;Market research;Benchmark testing","curve fitting;program diagnostics;software performance evaluation;software tools;statistical analysis","input-sensitive profiling;building block technique;automatic workload-dependent performance bottleneck discovery;growth rate;program executed routine;tuples;performance plots;statistical curve fitting techniques;generic code fragment;code optimizations;aprof;experimental evaluation;SPEC CPU2006 benchmarks;Valgrind tools","","5","","60","","","","","","IEEE","IEEE Journals & Magazines"
"NLP-KAOS for Systems Goal Elicitation: Smart Metering System Case Study","E. Casagrande; S. Woldeamlak; W. L. Woon; H. H. Zeineldin; D. Svetinovic","Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE; Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE; Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE; Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE; Department of Electrical Engineering and Computer Science, Masdar Institute of Science and Technology, Abu Dhabi, UAE","IEEE Transactions on Software Engineering","","2014","40","10","941","956","This paper presents a computational method that employs Natural Language Processing (NLP) and text mining techniques to support requirements engineers in extracting and modeling goals from textual documents. We developed a NLP-based goal elicitation approach within the context of KAOS goal-oriented requirements engineering method. The hierarchical relationships among goals are inferred by automatically building taxonomies from extracted goals. We use smart metering system as a case study to investigate the proposed approach. Smart metering system is an important subsystem of the next generation of power systems (smart grids). Goals are extracted by semantically parsing the grammar of goal-related phrases in abstracts of research publications. The results of this case study show that the developed approach is an effective way to model goals for complex systems, and in particular, for the research-intensive complex systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339811","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6857327","Requirements engineering;goal elicitation;NLP;data mining;bibliometrics","Data mining;Taxonomy;Ontologies;Data models;Natural language processing;Abstracts;Data collection","data mining;formal specification;natural language processing;smart meters","NLP-KAOS;systems goal elicitation;smart metering system case study;computational method;natural language processing;text mining techniques;requirements engineers;textual documents;NLP-based goal elicitation approach;KAOS goal-oriented requirements engineering method;hierarchical relationships;power systems;smart grids;goal-related phrases;research publications;complex systems","","10","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Improving Source Code Lexicon via Traceability and Information Retrieval","A. De Lucia; M. Di Penta; R. Oliveto","University of Salerno, Fisciano; University of Sannio, Benevento; University of Molise, Pesche","IEEE Transactions on Software Engineering","","2011","37","2","205","227","The paper presents an approach helping developers to maintain source code identifiers and comments consistent with high-level artifacts. Specifically, the approach computes and shows the textual similarity between source code and related high-level artifacts. Our conjecture is that developers are induced to improve the source code lexicon, i.e., terms used in identifiers or comments, if the software development environment provides information about the textual similarity between the source code under development and the related high-level artifacts. The proposed approach also recommends candidate identifiers built from high-level artifacts related to the source code under development and has been implemented as an Eclipse plug-in, called COde Comprehension Nurturant Using Traceability (COCONUT). The paper also reports on two controlled experiments performed with master's and bachelor's students. The goal of the experiments is to evaluate the quality of identifiers and comments (in terms of their consistency with high-level artifacts) in the source code produced when using or not using COCONUT. The achieved results confirm our conjecture that providing the developers with similarity between code and high-level artifacts helps to improve the quality of source code lexicon. This indicates the potential usefulness of COCONUT as a feature for software development environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5601742","Software traceability;source code comprehensibility;source code identifier quality;information retrieval;software development environments;empirical software engineering.","Documentation;Programming;Large scale integration;Semantics;Software quality;Couplings","information retrieval;program diagnostics;software quality","source code lexicon;information retrieval;textual similarity;high level artifact;software development;candidate identifier;code comprehension nurturant using traceability;COCONUT;bachelor student;master student","","28","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Covering arrays for efficient fault characterization in complex configuration spaces","C. Yilmaz; M. B. Cohen; A. A. Porter","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","1","20","34","Many modern software systems are designed to be highly configurable so they can run on and be optimized for a wide variety of platforms and usage scenarios. Testing such systems is difficult because, in effect, you are testing a multitude of systems, not just one. Moreover, bugs can and do appear in some configurations, but not in others. Our research focuses on a subset of these bugs that are ""option-related""-those that manifest with high probability only when specific configuration options take on specific settings. Our goal is not only to detect these bugs, but also to automatically characterize the configuration subspaces (i.e., the options and their settings) in which they manifest. To improve efficiency, our process tests only a sample of the configuration space, which we obtain from mathematical objects called covering arrays. This paper compares two different kinds of covering arrays for this purpose and assesses the effect of sampling strategy on fault characterization accuracy. Our results strongly suggest that sampling via covering arrays allows us to characterize option-related failures nearly as well as if we had tested exhaustively, but at a much lower cost. We also provide guidelines for using our approach in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583600","Software testing;distributed continuous quality assurance;fault characterization;covering arrays.","System testing;Sampling methods;Computer bugs;Software systems;Costs;Quality assurance;Predictive models;Software design;Design optimization;Guidelines","program testing;program debugging;software fault tolerance;configuration management","covering arrays;fault characterization;complex configuration spaces;software system testing;software system bug detection;option-related failure characterization;distributed continuous quality assurance","","123","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Synthesis of Partial Behavior Models from Properties and Scenarios","S. Uchitel; G. Brunet; M. Chechik","Imperial College London and FCEN-University of Buenos Aires; University of Toronto, Toronto; University of Toronto, Toronto","IEEE Transactions on Software Engineering","","2009","35","3","384","406","Synthesis of behavior models from software development artifacts such as scenario-based descriptions or requirements specifications helps reduce the effort of model construction. However, the models favored by existing synthesis approaches are not sufficiently expressive to describe both universal constraints provided by requirements and existential statements provided by scenarios. In this paper, we propose a novel synthesis technique that constructs behavior models in the form of modal transition systems (MTS) from a combination of safety properties and scenarios. MTSs distinguish required, possible, and proscribed behavior, and their elaboration not only guarantees the preservation of the properties and scenarios used for synthesis but also supports further elicitation of new requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.107","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4721439","Modal transition systems;merge;synthesis;partial behavior models.","Computer Society;Analytical models;Programming;Safety;Process design;Buildings;Animation;Upper bound;Educational institutions;Computer science","formal specification","partial behavior model;software development;modal transition system;safety properties","","59","","53","","","","","","IEEE","IEEE Journals & Magazines"
"How effective developers investigate source code: an exploratory study","M. P. Robillard; W. Coelho; G. C. Murphy","Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada; NA; NA","IEEE Transactions on Software Engineering","","2004","30","12","889","903","Prior to performing a software change task, developers must discover and understand the subset of the system relevant to the task. Since the behavior exhibited by individual developers when investigating a software system is influenced by intuition, experience, and skill, there is often significant variability in developer effectiveness. To understand the factors that contribute to effective program investigation behavior, we conducted a study of five developers performing a change task on a medium-size open source system. We isolated the factors related to effective program investigation behavior by performing a detailed qualitative analysis of the program investigation behavior of successful and unsuccessful developers. We report on these factors as a set of detailed observations, such as evidence of the phenomenon of inattention blindness by developers skimming source code. In general, our results support the intuitive notion that a methodical and structured approach to program investigation is the most effective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.101","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377187","Index Terms- Software evolution;empirical software engineering;program investigation;program understanding.","Software systems;Performance analysis;Programming;Software tools;Computer Society;Software performance;Blindness;Software engineering;Scattering;Inspection","software prototyping;reverse engineering;public domain software;open systems;programming environments","source code;software system investigation;program investigation behavior;medium-size open source system;software evolution;empirical software engineering;program understanding","","95","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Probabilistic Interface Automata","E. Pavese; V. Braberman; S. Uchitel","Departamento de Computación, Universidad de Buenos Aires; Departamento de Computación, Universidad de Buenos Aires and CONICET; Departamento de Computación, Universidad de Buenos Aires; Imperial College London and CONICET","IEEE Transactions on Software Engineering","","2016","42","9","843","865","System specifications have long been expressed through automata-based languages, which allow for compositional construction of complex models and enable automated verification techniques such as model checking. Automata-based verification has been extensively used in the analysis of systems, where they are able to provide yes/no answers to queries regarding their temporal properties. Probabilistic modelling and checking aim at enriching this binary, qualitative information with quantitative information, more suitable to approaches such as reliability engineering. Compositional construction of software specifications reduces the specification effort, allowing the engineer to focus on specifying individual component behaviour to then analyse the composite system behaviour. Compositional construction also reduces the validation effort, since the validity of the composite specification should be dependent on the validity of the components. These component models are smaller and thus easier to validate. Compositional construction poses additional challenges in a probabilistic setting. Numerical annotations of probabilistically independent events must be contrasted against estimations or measurements, taking care of not compounding this quantification with exogenous factors, in particular the behaviour of other system components. Thus, the validity of compositionally constructed system specifications requires that the validated probabilistic behaviour of each component continues to be preserved in the composite system. However, existing probabilistic automata-based formalisms do not support specification of non-deterministic and probabilistic component behaviour which, when observed through logics such as pCTL, is preserved in the composite system. In this paper we present a probabilistic extension to Interface Automata which preserves pCTL properties under probabilistic fairness by ensuring a probabilistic branching simulation between component and composite automata. The extension not only supports probabilistic behaviour but also allows for weaker prerequisites to interfacing composition, that supports delayed synchronisation that may be required because of internal component behaviour. These results are equally applicable as an extension to non-probabilistic Interface Automata.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2527000","ANPCyT PICT; ANPCyT PICT; ANPCyT PICT; CONICET PIP; CONICET PIP; UBACYT; UBACYT; MEALS; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7401103","Behaviour models;probability;interface automata;model checking","Probabilistic logic;Automata;Interconnected systems;Computational modeling;Model checking;Semantics;Synchronization","formal specification;probabilistic automata;program verification","probabilistic interface automata;system specifications;automata-based languages;compositional construction;complex models;automated verification techniques;model checking;automata-based verification;temporal properties;probabilistic modelling;probabilistic checking;binary qualitative information;quantitative information;reliability engineering;software specifications;component behaviour;composite system behaviour;numerical annotations;probabilistically independent events;exogenous factors;system components;compositionally constructed system specifications;nondeterministic behaviour;probabilistic component behaviour;pCTL;probabilistic fairness;probabilistic branching simulation;composite automata;component automata;internal component behaviour;nonprobabilistic interface automata","","","","42","","","","","","IEEE","IEEE Journals & Magazines"
"A Probabilistic Analysis of the Efficiency of Automated Software Testing","M. Böhme; S. Paul","Software Engineering Chair, Germany; School of Computing, National University of Singapore, Singapore","IEEE Transactions on Software Engineering","","2016","42","4","345","360","We study the relative efficiencies of the random and systematic approaches to automated software testing. Using a simple but realistic set of assumptions, we propose a general model for software testing and define sampling strategies for random (R) and systematic (S<sub>0</sub>) testing, where each sampling is associated with a sampling cost: 1 and c units of time, respectively. The two most important goals of software testing are: (i) achieving in minimal time a given degree of confidence x in a program's correctness and (ii) discovering a maximal number of errors within a given time bound n̂. For both (i) and (ii), we show that there exists a bound on c beyond which R performs better than S<sub>0</sub>on the average. Moreover for (i), this bound depends asymptotically only on x. We also show that the efficiency of R can be fitted to the exponential curve. Using these results we design a hybrid strategy H that starts with R and switches to S<sub>0</sub>when S<sub>0</sub>is expected to discover more errors per unit time. In our experiments we find that H performs similarly or better than the most efficient of both and that S<sub>0</sub>may need to be significantly faster than our bounds suggest to retain efficiency over R.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2487274","Singapore's Ministry of Education; ERC; SPECMATE; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7289448","Partition Testing;Random Testing;Error-based Partitioning;Efficient Testing;Testing Theory;Partition testing;random testing;error-based partitioning;efficient testing;testing theory","Systematics;Software testing;Software engineering;Input variables;Random variables;Color","probability;program testing","probabilistic analysis;automated software testing;sampling strategies;random testing;systematic testing;exponential curve;hybrid strategy","","7","","37","","","","","","IEEE","IEEE Journals & Magazines"
"An enhanced neural network technique for software risk analysis","D. E. Neumann","Gen. Dynamics Land Syst., Warren, MI, USA","IEEE Transactions on Software Engineering","","2002","28","9","904","912","An enhanced technique for risk categorization is presented. This technique, PCA-ANN, provides an improved capability to discriminate high-risk software. The approach draws on the combined strengths of pattern recognition, multivariate statistics and neural networks. Principal component analysis is utilized to provide a means of normalizing and orthogonalizing the input data, thus eliminating the ill effects of multicollinearity. A neural network is used for risk determination/classification. A significant feature of this approach is a procedure, herein termed cross-normalization. This procedure provides the technique with capability to discriminate data sets that include disproportionately large numbers of high-risk software modules.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033229","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033229","","Neural networks;Risk analysis;Government;Costs;Principal component analysis;Mathematical model;Predictive models;Programming;Contracts;Pattern recognition","neural nets;risk management;principal component analysis;pattern recognition;statistics;software engineering","enhanced neural network technique;software risk analysis;risk categorization;pattern recognition;multivariate statistics;principal component analysis;input data normalization;multicollinearity;cross-normalization;high-risk software modules","","59","","36","","","","","","IEEE","IEEE Journals & Magazines"
"On Event-Based Middleware for Location-Aware Mobile Applications","R. Meier; V. Cahill","Trinity College Dublin, Dublin and Lero&#x02014;The Irish Software Engineering Research Centre; Trinity College Dublin, Dublin and Lero&#x02014;The Irish Software Engineering Research Centre","IEEE Transactions on Software Engineering","","2010","36","3","409","430","As mobile applications become more widespread, programming paradigms and middleware architectures designed to support their development are becoming increasingly important. The event-based programming paradigm is a strong candidate for the development of mobile applications due to its inherent support for the loose coupling between components required by mobile applications. However, existing middleware that supports the event-based programming paradigm is not well suited to supporting location-aware mobile applications in which highly mobile components come together dynamically to collaborate at some location. This paper presents a number of techniques including location-independent announcement and subscription coupled with location-dependent filtering and event delivery that can be used by event-based middleware to support such collaboration. We describe how these techniques have been implemented in STEAM, an event-based middleware with a fully decentralized architecture, which is particularly well suited to deployment in ad hoc network environments. The cost of such location-based event dissemination and the benefits of distributed event filtering are evaluated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374426","Distributed systems;middleware;publish subscribe;event-based communication;mobile computing;collaborative and location-aware applications;wireless ad hoc networks.","Middleware;Collaboration;Mobile computing;Application software;Mobile communication;Unmanned aerial vehicles;Pervasive computing;Computer architecture;Filtering;Ad hoc networks","information dissemination;middleware;mobile computing;software architecture","event-based middleware;location-aware mobile applications;event-based programming paradigm;location-independent announcement;location-dependent filtering;STEAM;decentralized architecture;ad hoc network environments;location-based event dissemination;distributed event filtering","","15","","56","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of software project bidding","M. Jorgensen; G. J. Carelius","Simula Res. Lab., L.Ysaker, Norway; Simula Res. Lab., L.Ysaker, Norway","IEEE Transactions on Software Engineering","","2004","30","12","953","969","The study described in this paper reports from a real-life bidding process in which 35 companies were bidding for the same contract. The bidding process consisted of two separate phases: a prestudy phase and a bidding phase. In the prestudy phase, 17 of the 35 bidding companies provided rough price indications based on a brief, incomplete description of user requirements. In the bidding phase, all 35 companies provided bids based on a more complete requirement specification that described a software system with substantially more functionality than the system indicated in the prestudy phase. The main result of the study is that the 17 companies involved in the prestudy phase presented bids that were, on average, about 70 percent higher than the bids of the other companies, although all companies based their bids on the same requirement specification. We propose an explanation for this difference that is consistent with the ""prospect theory"" and the ""precautionary bidding effect."" A possible implication of our findings is that software clients should not request early price indications based on limited and uncertain information when the final bids can be based on more complete and reliable information.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.92","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377191","Index Terms- Cost estimation;risk management;software psychology.","Costs;Contracts;Software systems;Reliability theory;Risk management;Psychology;Companies;Guidelines;Software quality;Scheduling","software management;project management;DP industry;contracts;formal specification;software cost estimation;risk management","software project bidding;prestudy phase;bidding phase;software system;requirement specification;software client","","24","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Software Component Models","K. Lau; Z. Wang","NA; NA","IEEE Transactions on Software Engineering","","2007","33","10","709","724","Component-based development (CBD) is an important emerging topic in software engineering, promising long-sought-after benefits like increased reuse, reduced time to market, and, hence, reduced software production cost. The cornerstone of a CBD technology is its underlying software component model, which defines components and their composition mechanisms. Current models use objects or architectural units as components. These are not ideal for component reuse or systematic composition. In this paper, we survey and analyze current component models and classify them into a taxonomy based on commonly accepted desiderata for CBD. For each category in the taxonomy, we describe its key characteristics and evaluate them with respect to these desiderata.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70726","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302781","software components;software component models;component life-cycle;component syntax;component semantics;component composition","Object oriented modeling;Costs;Taxonomy;Software engineering;Time to market;Software reusability;Application software;Software systems;Production systems;Computer industry","cost reduction;object-oriented programming;software cost estimation","software component models;component-based development;software engineering;software production cost","","145","","63","","","","","","IEEE","IEEE Journals & Magazines"
"A Quantitative Investigation of the Acceptable Risk Levels of Object-Oriented Metrics in Open-Source Systems","R. Shatnawi","Jordan University of Science and Technology, Irbid","IEEE Transactions on Software Engineering","","2010","36","2","216","225","Object-oriented metrics have been validated empirically as measures of design complexity. These metrics can be used to mitigate potential problems in the software complexity. However, there are few studies that were conducted to formulate the guidelines, represented as threshold values, to interpret the complexity of the software design using metrics. Classes can be clustered into low and high risk levels using threshold values. In this paper, we use a statistical model, derived from the logistic regression, to identify threshold values for the Chidamber and Kemerer (CK) metrics. The methodology is validated empirically on a large open-source system-the Eclipse project. The empirical results indicate that the CK metrics have threshold effects at various risk levels. We have validated the use of these thresholds on the next release of the Eclipse project-Version 2.1-using decision trees. In addition, the selected threshold values were more accurate than those were selected based on either intuitive perspectives or on data distribution parameters. Furthermore, the proposed model can be exploited to find the risk level for an arbitrary threshold value. These findings suggest that there is a relationship between risk levels and object-oriented metrics and that risk levels can be used to identify threshold effects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383377","Object-oriented programming;product metrics;CK metrics;threshold values;open-source software.","Open source software;Object oriented modeling;Software metrics;Software quality;Software testing;Software design;Predictive models;Quality assurance;Probability;Fault diagnosis","decision trees;object-oriented programming;public domain software;software fault tolerance;software maintenance;software metrics;statistical analysis","object-oriented metrics;open source systems;software complexity;software design;software metrics;statistical model;logistic regression;Chidamber and Kemerer metrics;Eclipse project version 2.1;decision trees;threshold values;data distribution parameters","","44","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Abstract communication model for distributed systems","U. Glasser; Y. Gurevich; M. Veanes","Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada; NA; NA","IEEE Transactions on Software Engineering","","2004","30","7","458","472","In some distributed and mobile communication models, a message disappears in one place and miraculously appears in another. In reality, of course, there are no miracles. A message goes from one network to another; it can be lost or corrupted in the process. Here, we present a realistic but high-level communication model where abstract communicators represent various nets and subnets. The model was originally developed in the process of specifying a particular network architecture, namely, the Universal Plug and Play architecture. But, it is general. Our contention is that every message-based distributed system, properly abstracted, gives rise to a specialization of our abstract communication model. The purpose of the abstract communication model is not to design a new kind of network; rather, it is to discover the common part of all message-based communication networks. The generality of the model has been confirmed by its successful reuse for very different distributed architectures. The model is based on distributed abstract state machines. It is implemented in the specification language AsmL and is used for testing distributed systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318607","Abstract state machines;communication protocols;computer networks;distributed systems;requirement specification;system modeling;testing of distributed systems.","Protocols;Mobile communication;Plugs;Communication networks;Computer architecture;Specification languages;System testing;Programming;Documentation;Context modeling","protocols;computer networks;specification languages;formal specification;open systems;program compilers;message passing","mobile communication models;computer network architecture;Universal Plug and Play architecture;message-based distributed system;abstract communication model;message-based communication networks;distributed architectures;distributed abstract state machines;AsmL specification language;communication protocols;requirement specification;system modeling;distributed communication models","","20","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Two-Phase Assessment Approach to Improve the Efficiency of Refactoring Identification","A. Han; S. Cha","Department of Computer Science and Engineering, Korea University, Sungbuk-gu, Seoul, South Korea; Department of Computer Science and Engineering, Korea University, Sungbuk-gu, Seoul, South Korea","IEEE Transactions on Software Engineering","","2018","44","10","1001","1023","To automate the refactoring identification process, a large number of candidates need to be compared. Such an overhead can make the refactoring approach impractical if the software size is large and the computational load of a fitness function is substantial. In this paper, we propose a two-phase assessment approach to improving the efficiency of the process. For each iteration of the refactoring process, refactoring candidates are preliminarily assessed using a lightweight, fast delta assessment method called the Delta Table. Using multiple Delta Tables, candidates to be evaluated with a fitness function are selected. A refactoring can be selected either interactively by the developer or automatically by choosing the best refactoring, and the refactorings are applied one after another in a stepwise fashion. The Delta Table is the key concept enabling a two-phase assessment approach because of its ability to quickly calculate the varying amounts of maintainability provided by each refactoring candidate. Our approach has been evaluated for three large-scale open-source projects. The results convincingly show that the proposed approach is efficient because it saves a considerable time while still achieving the same amount of fitness improvement as the approach examining all possible candidates.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2731853","Basic Science Research Program; National Research Foundation of Korea (NRF); Ministry of Education; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7990580","Refactoring assessment;refactoring identification;maintainability improvement","Measurement;Couplings;Symmetric matrices;Open source software;Computational efficiency;System analysis and design","iterative methods;public domain software;software maintenance","two-phase assessment approach;refactoring identification process;fitness function;computational load;delta assessment method;lightweight method;delta table;large-scale open-source projects","","","","60","","","","","","IEEE","IEEE Journals & Magazines"
"SubCM: a tool for improved visibility of software change in an industrial setting","H. Volzer; A. MacDonald; B. Atchison; A. Hanlon; P. Lindsay; P. Strooper","Lubeck Univ., Germany; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","10","675","693","Software configuration management is the discipline of managing large collections of software development artefacts from which software products are built. Software configuration management tools typically deal with artefacts at fine levels of granularity - such as individual source code files - and assist with coordination of changes to such artefacts. This paper describes a lightweight tool, designed to be used on top of a traditional file-based configuration management system. The add-on tool support enables users to flexibly define new hierarchical views of product structure, independent of the underlying artefact-repository structure. The tool extracts configuration and change data with respect to the user-defined hierarchy, leading to improved visibility of how individual subsystems have changed. The approach yields a range of new capabilities for build managers, and verification and validation teams. The paper includes a description of our experience using the tool in an organization that builds large embedded software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339278","Index Terms- Software configuration management;software maintenance;verification and validation.","Software tools;Computer industry;Software development management;Programming;Software maintenance;Project management;Testing;Computational Intelligence Society;Australia;Data mining","configuration management;software maintenance;formal verification;embedded systems;software tools","software change;software configuration management;software development artefacts;software products;source code files;file-based configuration management system;add-on tool support;artefact-repository structure;embedded software systems;software verification;software maintenance","","5","","36","","","","","","IEEE","IEEE Journals & Magazines"
"SPARTACAS: automating component reuse and adaptation","B. Morel; P. Alexander","Inf. & Telecommun. Technol., Kansas Univ., Lawrence, KS, USA; Inf. & Telecommun. Technol., Kansas Univ., Lawrence, KS, USA","IEEE Transactions on Software Engineering","","2004","30","9","587","600","A continuing challenge for software designers is to develop efficient and cost-effective software implementations. Many see software reuse as a potential solution; however, the cost of reuse tends to outweigh the potential benefits. The costs of software reuse include establishing and maintaining a library of reusable components, searching for applicable components to be reused in a design, as well as adapting components toward a proper implementation. We introduce SPARTACAS, a framework for automating specification-based component retrieval and adaptation that has been successfully applied to synthesis of software for embedded and digital signal processing systems. Using specifications to abstractly represent implementations allows automated theorem-provers to formally verify logical reusability relationships between specifications. These logical relationships are used to evaluate the feasibility of reusing the implementations of components to implement a problem. Retrieving a component that is a complete match to a problem is rare. It is more common to retrieve a component that partially satisfies the requirements of a problem. Such components have to be adapted. Rather than adapting components at the code level, SPARTACAS adapts the behavior of partial matches by imposing interactions with other components at the architecture level. A subproblem is synthesized that specifies the missing functionality required to complete the problem; the subproblem is used to query the library for components to adapt the partial match. The framework was implemented and evaluated empirically, the results suggest that automated adaptation using architectures successfully promotes software reuse, and hierarchically organizes a solution to a design problem.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324646","Index Terms- Reuse models;formal methods;programmer workbench;reuse library.","Software libraries;Computer architecture;Costs;Acoustical engineering;Design engineering;Software reusability;Programming;Software quality;Software design;Software maintenance","software reusability;software libraries;formal specification;program verification;software architecture;object-oriented programming","software design;cost-effective software implementation;software reuse;reusable component library;specification-based component retrieval;specification-based component adaptation;software synthesis;embedded system;digital signal processing system;automated theorem-provers;formal verification;formal methods;programmer workbench","","24","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Two Studies of Framework-Usage Templates Extracted from Dynamic Traces","A. Heydarnoori; K. Czarnecki; W. Binder; T. T. Bartolomei","University of Lugano, Lugano; University of Waterloo, Waterloo; University of Lugano, Lugano; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2012","38","6","1464","1487","Object-oriented frameworks are widely used to develop new applications. They provide reusable concepts that are instantiated in application code through potentially complex implementation steps such as subclassing, implementing interfaces, and calling framework operations. Unfortunately, many modern frameworks are difficult to use because of their large and complex APIs and frequently incomplete user documentation. To cope with these problems, developers often use existing framework applications as a guide. However, locating concept implementations in those sample applications is typically challenging due to code tangling and scattering. To address this challenge, we introduce the notion of concept-implementation templates, which summarize the necessary concept-implementation steps and identify them in the sample application code, and a technique, named FUDA, to automatically extract such templates from dynamic traces of sample applications. This paper further presents the results of two experiments conducted to evaluate the quality and usefulness of FUDA templates. The experimental evaluation of FUDA with 14 concepts in five widely used frameworks suggests that the technique is effective in producing templates with relatively few false positives and false negatives for realistic concepts by using two sample applications. Moreover, we observed in a user study with 28 programmers that the use of templates reduced the concept-implementation time compared to when documentation was used.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975174","Object-oriented application frameworks;framework comprehension;framework documentation;concept-implementation templates;application programming interface (API);dynamic analysis;concept location;feature identification","Dynamic programming;Feature extraction;Documentation;Java;Application programming interfaces;Runtime","application program interfaces;object-oriented methods","framework-usage template extraction;dynamic traces;object-oriented frameworks;application code;subclassing operation;interface implementation;calling framework operations;user documentation;code tangling;code scattering;concept-implementation templates;concept-implementation steps;FUDA templates;framework API understanding through dynamic analysis","","5","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Reasoning About Identifier Spaces: How to Make Chord Correct","P. Zave","AT&T Laboratories—Research, Bedminster, NJ","IEEE Transactions on Software Engineering","","2017","43","12","1144","1156","The Chord distributed hash table (DHT) is well-known and often used to implement peer-to-peer systems. Chord peers find other peers, and access their data, through a ring-shaped pointer structure in a large identifier space. Despite claims of proven correctness, i.e., eventual reachability, previous work has shown that the Chord ring-maintenance protocol is not correct under its original operating assumptions. Previous work has not, however, discovered whether Chord could be made correct under the same assumptions. The contribution of this paper is to provide the first specification of correct operations and initialization for Chord, an inductive invariant that is necessary and sufficient to support a proof of correctness, and two independent proofs of correctness. One proof is informal and intuitive, and applies to networks of any size. The other proof is based on a formal model in Alloy, and uses fully automated analysis to prove the assertions for networks of bounded size. The two proofs complement each other in several important ways.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2655056","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7823003","Computers and information processing;distributed computing;peer-to-peer computing;software engineering;formal verification","Peer-to-peer computing;Formal verification;Information processing;Analytical models;Structural rings;Distributed processing","peer-to-peer computing;protocols;telecommunication network topology","hash table;peer-to-peer systems;Chord peers;ring-shaped pointer structure;identifier space;Chord ring-maintenance protocol","","1","","33","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Analyzing the Effect of Gain Time on Soft-Task Scheduling Policies in Real-Time Systems","L. Búrdalo; A. Terrasa; A. Espinosa; A. García-Fornes","Universitat Politèecnica de València, Valencia; Universitat Politèecnica de València, Valencia; Universitat Politèecnica de València, Valencia; Universitat Politèecnica de València, Valencia","IEEE Transactions on Software Engineering","","2012","38","6","1305","1318","In hard real-time systems, gain time is defined as the difference between the Worst Case Execution Time (WCET) of a hard task and its actual processor consumption at runtime. This paper presents the results of an empirical study about how the presence of a significant amount of gain time in a hard real-time system questions the advantages of using the most representative scheduling algorithms or policies for aperiodic or soft tasks in fixed-priority preemptive systems. The work presented here refines and complements many other studies in this research area in which such policies have been introduced and compared. This work has been performed by using the authors' testing framework for soft scheduling policies, which produces actual, synthetic, randomly generated applications, executes them in an instrumented Real-Time Operating System (RTOS), and finally processes this information to obtain several statistical outcomes. The results show that, in general, the presence of a significant amount of gain time reduces the performance benefit of the scheduling policies under study when compared to serving the soft tasks in background, which is considered the theoretical worst case. In some cases, this performance benefit is so small that the use of a specific scheduling policy for soft tasks is questionable.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.95","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6025357","Real-time systems;RT-Linux;scheduling policies","Real time systems;Servers;Time factors;Generators;Scheduling;Heuristic algorithms;Decision support systems","operating systems (computers);real-time systems;scheduling","gain time;soft-task scheduling policies;hard real-time systems;worst case execution time;WCET;processor consumption;representative scheduling algorithms;aperiodic tasks;fixed-priority preemptive systems;author testing framework;instrumented real-time operating system;RTOS;statistical outcomes","","2","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Self-Organizing Roles on Agile Software Development Teams","R. Hoda; J. Noble; S. Marshall","The University of Auckland, Auckland; Victoria University of Wellington, Wellington; Victoria University of Wellington, Wellington","IEEE Transactions on Software Engineering","","2013","39","3","422","444","Self-organizing teams have been recognized and studied in various forms-as autonomous groups in socio-technical systems, enablers of organizational theories, agents of knowledge management, and as examples of complex-adaptive systems. Over the last decade, self-organizing teams have taken center stage in software engineering when they were incorporated as a hallmark of Agile methods. Despite the long and rich history of self-organizing teams and their recent popularity with Agile methods, there has been little research on the topic within software wngineering. Particularly, there is a dearth of research on how Agile teams organize themselves in practice. Through a Grounded Theory research involving 58 Agile practitioners from 23 software organizations in New Zealand and India over a period of four years, we identified informal, implicit, transient, and spontaneous roles that make Agile teams self-organizing. These roles-Mentor, Coordinator, Translator, Champion, Promoter, and Terminator-are focused toward providing initial guidance and encouraging continued adherence to Agile methods, effectively managing customer expectations and coordinating customer collaboration, securing and sustaining senior management support, and identifying and removing team members threatening the self-organizing ability of the team. Understanding these roles will help software development teams and their managers better comprehend and execute their roles and responsibilities as a self-organizing team.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6197202","Self-organizing;team roles;software engineering;Agile software development;grounded theory","Programming;Organizations;Collaboration;Software;Organizing;Software engineering","knowledge management;software management;software prototyping;team working","self-organizing roles;agile software development teams;self-organizing teams;autonomous groups;socio-technical systems;organizational theories enablers;knowledge management agents;complex-adaptive system examples;software engineering;grounded theory research;New Zealand;India;mentor role;coordinator role;translator role;champion role;promoter role;terminator role;customer expectation management;customer collaboration coordination;senior management support security;senior management support sustainability","","46","","115","","","","","","IEEE","IEEE Journals & Magazines"
"Exploiting Model Morphology for Event-Based Testing","F. Belli; M. Beyazıt","Department of Electrical Engineering and Information Technology, University of Paderborn, Paderborn, Germany; Department of Computer Engineering, Yaşar University, İzmir, Turkey","IEEE Transactions on Software Engineering","","2015","41","2","113","134","Model-based testing employs models for testing. Model-based mutation testing (MBMT) additionally involves fault models, called mutants, by applying mutation operators to the original model. A problem encountered with MBMT is the elimination of equivalent mutants and multiple mutants modeling the same faults. Another problem is the need to compare a mutant to the original model for test generation. This paper proposes an event-based approach to MBMT that is not fixed on single events and a single model but rather operates on sequences of events of length k ≥ 1 and invokes a sequence of models that are derived from the original one by varying its morphology based on k. The approach employs formal grammars, related mutation operators, and algorithms to generate test cases, enabling the following: (1) the exclusion of equivalent mutants and multiple mutants; (2) the generation of a test case in linear time to kill a selected mutant without comparing it to the original model; (3) the analysis of morphologically different models enabling the systematic generation of mutants, thereby extending the set of fault models studied in related literature. Three case studies validate the approach and analyze its characteristics in comparison to random testing and another MBMT approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2360690","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6915728","Model-based mutation testing;grammar-based testing;mutant selection;Model-based mutation testing;grammar-based testing;(model) morphology;mutant selection;test generation","Grammar;Testing;Unified modeling language;Production;Context;Morphology;Analytical models","computational complexity;grammars;program testing","model morphology;event-based testing;model-based mutation testing;MBMT;fault models;mutants;mutation operators;test generation;event-based approach;formal grammars;equivalent mutant;multiple mutants;linear time;random testing","","4","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Tracking mobile units for dependable message delivery","A. L. Murphy; G. -. Roman; G. Varghese","Rochester Univ., NY, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","5","433","448","As computing components get smaller and people become accustomed to having computational power at their disposal at any time, mobile computing is developing as an important research area. One of the fundamental problems in mobility is maintaining connectivity through message passing as the user moves through the network. An approach to this is to have a single home node constantly track the current location of the mobile unit and forward messages to this location. One problem with this approach is that, during the update to the home agent after movement, messages are often dropped, especially in the case of frequent movement. In this paper, we present a new algorithm which uses a home agent, but maintains information regarding a subnet within which the mobile unit must be present. We also present a reliable message delivery algorithm which is superimposed on the region maintenance algorithm. Our strategy is based on ideas from diffusing computations as first proposed by Dijkstra and Scholten. Finally, we present a second algorithm which limits the size of the subnet by keeping only a path from the home node to the mobile unit.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000448","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000448","","Maintenance;Mobile computing;Message passing","portable computers;mobile computing;message passing;software maintenance;computer communications software","mobile units tracking;dependable message delivery;mobile computing;connectivity;message passing;home agent;reliable message delivery algorithm;region maintenance algorithm","","2","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Model Checking Markov Chains with Actions and State Labels","C. Baier; L. Cloth; B. R. Haverkort; M. Kuntz; M. Siegle","Institute for Theoretical Computer Science, Technische Universita¨t Dresden, D-01062 Dresden, Germany; EWI/DACS, University of Twente, PO Box 217, 7500 AE Enschede, The Netherlands; EWI/DACS, University of Twente, PO Box 217, 7500 AE Enschede, The Netherlands; EWI/DACS, University of Twente, PO Box 217, 7500 AE Enschede, The Netherlands; Universita¨t der Bundeswehr Mu¨nchen, Institute fu¨r Technische Informatik, Fakulta¨t fu¨r Informatik, 85577 Neubiberg, Germany","IEEE Transactions on Software Engineering","","2007","33","4","209","224","In the past, logics of several kinds have been proposed for reasoning about discrete-time or continuous-time Markov chains. Most of these logics rely on either state labels (atomic propositions) or on transition labels (actions). However, in several applications it is useful to reason about both state properties and action sequences. For this purpose, we introduce the logic as CSL which provides a powerful means to characterize execution paths of Markov chains with actions and state labels. asCSL can be regarded as an extension of the purely state-based logic CSL (continuous stochastic logic). In asCSL, path properties are characterized by regular expressions over actions and state formulas. Thus, the truth value of path formulas depends not only on the available actions in a given time interval, but also on the validity of certain state formulas in intermediate states. We compare the expressive power of CSL and asCSL and show that even the state-based fragment of asCSL is strictly more expressive than CSL if time intervals starting at zero are employed. Using an automaton-based technique, an asCSL formula and a Markov chain with actions and state labels are combined into a product Markov chain. For time intervals starting at zero, we establish a reduction of the model checking problem for asCSL to CSL model checking on this product Markov chain. The usefulness of our approach is illustrated with an elaborate model of a scalable cellular communication system, for which several properties are formalized by means of asCSL formulas and checked using the new procedure","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123324","Protocol verification;performance of systems;model checking;automata;Markov processes.","Stochastic processes;Probabilistic logic;Algebra;Power system modeling;Law;Legal factors;Automata;Markov processes;Embedded system;Petri nets","automata theory;formal verification;Markov processes;temporal logic","state label;transition label;Markov chain;continuous stochastic logic;regular expression;automaton-based technique;model checking","","33","","33","","","","","","IEEE","IEEE Journals & Magazines"
"An Experience in Testing the Security of Real-World Electronic Voting Systems","D. Balzarotti; G. Banks; M. Cova; V. Felmetsger; R. Kemmerer; W. Robertson; F. Valeur; G. Vigna","Eurecom Institute, Sophia Antipolis, France; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara; University of California, Santa Barbara, Santa Barbara","IEEE Transactions on Software Engineering","","2010","36","4","453","473","Voting is the process through which a democratic society determines its government. Therefore, voting systems are as important as other well-known critical systems, such as air traffic control systems or nuclear plant monitors. Unfortunately, voting systems have a history of failures that seems to indicate that their quality is not up to the task. Because of the alarming frequency and impact of the malfunctions of voting systems, in recent years a number of vulnerability analysis exercises have been carried out against voting systems to determine if they can be compromised in order to control the results of an election. We have participated in two such large-scale projects, sponsored by the Secretaries of State of California and Ohio, whose goals were to perform the security testing of the electronic voting systems used in their respective states. As the result of the testing process, we identified major vulnerabilities in all of the systems analyzed. We then took advantage of a combination of these vulnerabilities to generate a series of attacks that would spread across the voting systems and would “steal” votes by combining voting record tampering with social engineering approaches. As a response to the two large-scale security evaluations, the Secretaries of State of California and Ohio recommended changes to improve the security of the voting process. In this paper, we describe the methodology that we used in testing the two real-world electronic voting systems we evaluated, the findings of our analysis, our attacks, and the lessons we learned.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5210119","Voting systems;security testing;vulnerability analysis.","Electronic equipment testing;System testing;Security;Electronic voting systems;Large-scale systems;Government;Air traffic control;History;Frequency;Control systems","data privacy;government data processing;security of data","security testing;electronic voting system;alarming frequency;vulnerability analysis exercise;California;Ohio;social engineering approache;large scale security evaluation","","18","","62","","","","","","IEEE","IEEE Journals & Magazines"
"The optimal class size for object-oriented software","K. El Emam; S. Benlarbi; N. Goel; W. Melo; H. Lounis; S. N. Rai","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","5","494","509","A growing body of literature suggests that there is an optimal size for software components. This means that components that are too small or too big will have a higher defect content (i.e., there is a U-shaped curve relating defect content to size). The U-shaped curve has become known as the ""Goldilocks Conjecture."" Recently, a cognitive theory has been proposed to explain this phenomenon and it has been expanded to characterize object-oriented software. This conjecture has wide implications for software engineering practice. It suggests 1) that designers should deliberately strive to design classes that are of the optimal size, 2) that program decomposition is harmful, and 3) that there exists a maximum (threshold) class size that should not be exceeded to ensure fewer faults in the software. The purpose of the current paper is to evaluate this conjecture for object-oriented systems. We first demonstrate that the claims of an optimal component/class size (1) above) and of smaller components/classes having a greater defect content (2) above) are due to a mathematical artifact in the analyses performed previously. We then empirically test the threshold effect claims of this conjecture (3) above). To our knowledge, the empirical test of size threshold effects for object-oriented systems has not been performed thus far. We performed an initial study with an industrial C++ system and repeated it twice on another C++ system and on a commercial Java application. Our results provide unambiguous evidence that there is no threshold effect of class size. We obtained the same result for three systems using four different size measures. These findings suggest that there is a simple continuous relationship between class size and faults, and that, optimal class size, smaller classes are better and threshold effects conjectures have no sound theoretical nor empirical basis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000452","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000452","","Software engineering;Performance analysis;System testing;Performance evaluation;Java;Size measurement","object-oriented programming;software quality;software metrics","optimal class size;object-oriented software;software components;U-shaped curve;program decomposition;object-oriented metrics;software quality","","46","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Balancing Privacy and Utility in Cross-Company Defect Prediction","F. Peters; T. Menzies; L. Gong; H. Zhang","West Virginia University, Morgantown; West Virginia University, Morgantown; Tsinghua University, Beijing; Tsinghua University, Beijing","IEEE Transactions on Software Engineering","","2013","39","8","1054","1068","Background: Cross-company defect prediction (CCDP) is a field of study where an organization lacking enough local data can use data from other organizations for building defect predictors. To support CCDP, data must be shared. Such shared data must be privatized, but that privatization could severely damage the utility of the data. Aim: To enable effective defect prediction from shared data while preserving privacy. Method: We explore privatization algorithms that maintain class boundaries in a dataset. CLIFF is an instance pruner that deletes irrelevant examples. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. CLIFF+MORPH are tested in a CCDP study among 10 defect datasets from the PROMISE data repository. Results: We find: 1) The CLIFFed+MORPHed algorithms provide more privacy than the state-of-the-art privacy algorithms; 2) in terms of utility measured by defect prediction, we find that CLIFF+MORPH performs significantly better. Conclusions: For the OO defect data studied here, data can be privatized and shared without a significant degradation in utility. To the best of our knowledge, this is the first published result where privatization does not compromise defect prediction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6419712","Privacy;classification;defect prediction","Testing;Software;Genetic algorithms;Sociology;Statistics;Search problems;Arrays","data privacy;program debugging","privacy balancing;utility balancing;cross-company defect prediction;CCDP;defect predictors;privacy preservation;privatization algorithms;class boundaries;pruner;data mutator;PROMISE data repository;CLIFFed-MORPHed algorithm;OO defect data","","42","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Knowledge-based repository scheme for storing and retrieving business components: a theoretical design and an empirical analysis","P. Vitharana; F. M. Zahedi; H. Jain","Sch. of Manage., Syracuse Univ., NY, USA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","7","649","664","Component-based development (CDB) promises to reduce complexity and cost of software development and maintenance through reuse. For CBD to be successful, a vibrant market for commercial business components is essential. One of the key requirements of an active market for business components is an effective scheme for classifying and describing them at various levels of detail, as well as a corresponding repository for storing and retrieving these components. Such a scheme needs to support various constituents such as business users, managers, and application assemblers. The scheme and repository should help users and managers to select components that match their requirements and aid application assemblers in identifying components most compatible with their deployment environment (such as the platform) and system inputs (such as data types). Drawing from the concepts of group technology and software reuse paradigm, this paper proposes a scheme for classifying and describing business components and the design of a knowledge-based repository for their storage and retrieval. The proposed scheme is implemented in a prototype repository. The effectiveness of the prototype and the underlying classification and coding scheme is assessed empirically through controlled experiments. Results support the assertion that the scheme is effective in enhancing the users' and analysts' ability to find the needed business components.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214328","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214328","","Application software;Software prototyping;Prototypes;Costs;Programming;Software maintenance;Business communication;Environmental management;Assembly systems;Group technology","business data processing;knowledge management;object-oriented programming;information retrieval;information storage;software libraries;hypermedia markup languages;classification","component-based development;software reuse;software library;component repository;commercial business components;group technology;knowledge-based repository;component storage;component retrieval","","30","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Reducing Features to Improve Code Change-Based Bug Prediction","S. Shivaji; E. James Whitehead; R. Akella; S. Kim","University of California, Santa Cruz, Santa Cruz; University of California, Santa Cruz, Santa Cruz; University of California, Santa Cruz, Santa Cruz; Hong Kong University of Science and, Hong Kong","IEEE Transactions on Software Engineering","","2013","39","4","552","569","Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine (SVM) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The SVM's improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6226427","Reliability;bug prediction;machine learning;feature selection","Software;Support vector machines;History;Machine learning;Feature extraction;Measurement;Computer bugs","belief networks;learning (artificial intelligence);pattern classification;program debugging;support vector machines","code change-based bug prediction;machine learning classifier;source code file;software history;classifier-based bug prediction;machine learned feature reduction;feature selection technique;classification performance;naive Bayes classifier;support vector machine;SVM classifier;software project;buggy F-measure","","75","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Retargeting sequential image-processing programs for data parallel execution","L. B. Baumstark; L. M. Wills","Dept. of Comput. Sci., State Univ. of West Georgia, Carollton, GA, USA; NA","IEEE Transactions on Software Engineering","","2005","31","2","116","136","New compact, low-power implementation technologies for processors and imaging arrays can enable a new generation of portable video products. However, software compatibility with large bodies of existing applications written in C prevents more efficient, higher performance data parallel architectures from being used in these embedded products. If this software could be automatically retargeted explicitly for data parallel execution, product designers could incorporate these architectures into embedded products. The key challenge is exposing the parallelism that is inherent in these applications but that is obscured by artifacts imposed by sequential programming languages. This paper presents a recognition-based approach for automatically extracting a data parallel program model from sequential image processing code and retargeting it to data parallel execution mechanisms. The explicitly parallel model presented, called multidimensional data flow (MDDF), captures a model of how operations on data regions (e.g., rows, columns, and tiled blocks) are composed and interact. To extract an MDDF model, a partial recognition technique is used that focuses on identifying array access patterns in loops, transforming only those program elements that hinder parallelization, while leaving the core algorithmic computations intact. The paper presents results of retargeting a set of production programs to a representative data parallel processor array to demonstrate the capacity to extract parallelism using this technique. The retargeted applications yield a potential execution throughput limited only by the number of processing elements, exceeding thousands of instructions per cycle in massively parallel implementations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401928","Index Terms- Reengineering;SIMD processors;data-level parallelization;explicitly parallel program representation;program recognition.","Data mining;Embedded software;Application software;Parallel processing;Software performance;Parallel architectures;Product design;Computer architecture;Computer languages;Image recognition","parallel programming;parallel architectures;video signal processing;data flow computing;systems re-engineering;imaging","sequential image-processing program;data parallel execution;imaging array;portable video product;data parallel architecture;sequential programming language;data parallel program model;multidimensional data flow;parallel processor array","","5","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Amorphous Slicing of Extended Finite State Machines","K. Androutsopoulos; D. Clark; M. Harman; R. M. Hierons; Z. Li; L. Tratt","University College London, London; University College London, London; University College London, London; Brunel University, Uxbridge, Middlesex; Beijing University of Chemical Technology, Beijing; King's Colledge London, London","IEEE Transactions on Software Engineering","","2013","39","7","892","909","Slicing is useful for many software engineering applications and has been widely studied for three decades, but there has been comparatively little work on slicing extended finite state machines (EFSMs). This paper introduces a set of dependence-based EFSM slicing algorithms and an accompanying tool. We demonstrate that our algorithms are suitable for dependence-based slicing. We use our tool to conduct experiments on 10 EFSMs, including benchmarks and industrial EFSMs. Ours is the first empirical study of dependence-based program slicing for EFSMs. Compared to the only previously published dependence-based algorithm, our average slice is smaller 40 percent of the time and larger only 10 percent of the time, with an average slice size of 35 percent for termination insensitive slicing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.72","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6374192","Slicing;extended finite state machines","Automata;Algorithm design and analysis;Approximation algorithms;Software algorithms;Unified modeling language;Educational institutions;Electronic mail","finite state machines;program slicing;software engineering","software engineering application;extended finite state machine slicing;dependence-based EFSM slicing algorithm;benchmarks EFSM;industrial EFSM;dependence-based program slicing;termination insensitive slicing;amorphous slicing","","8","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Linking Model-Driven Development and Software Architecture: A Case Study","A. Mattsson; B. Lundell; B. Lings; B. Fitzgerald","Combitech AB, Jönköping; University of Skövde, Skövde; University of Skövde, Skövde; Univerity of Limerick, Limerick","IEEE Transactions on Software Engineering","","2009","35","1","83","93","A basic premise of model driven development (MDD) is to capture all important design information in a set of formal or semi-formal models which are then automatically kept consistent by tools. The concept however is still relatively immature and there is little by way of empirically validated guidelines. In this paper we report on the use of MDD on a significant real-world project over several years. Our research found the MDD approach to be deficient in terms of modelling architectural design rules. Furthermore, the current body of literature does not offer a satisfactory solution as to how architectural design rules should be modelled. As a result developers have to rely on time-consuming and error-prone manual practices to keep a system consistent with its architecture. To realise the full benefits of MDD it is important to find ways of formalizing architectural design rules which then allow automatic enforcement of the architecture on the system model. Without this, architectural enforcement will remain a bottleneck in large MDD projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4657364","Software Architecture;Model-Driven Development;Case Study Research;Software Architecture;Model-Driven Development;Case Study Research","Joining processes;Software architecture;Computer architecture;Guidelines;Context modeling;Computer industry;Computer errors;Programming;Keyword search;Portals","formal verification;software architecture;systems analysis","model-driven development;software architecture;formal models;semi-formal models;architectural design rules","","22","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Toward a Tool-Based Development Methodology for Pervasive Computing Applications","D. Cassou; J. Bruneau; C. Consel; E. Balland","University of Bordeaux and INRIA, Talence; University of Bordeaux and INRIA, Talence; University of Bordeaux and INRIA, Talence; University of Bordeaux and INRIA, Talence","IEEE Transactions on Software Engineering","","2012","38","6","1445","1463","Despite much progress, developing a pervasive computing application remains a challenge because of a lack of conceptual frameworks and supporting tools. This challenge involves coping with heterogeneous devices, overcoming the intricacies of distributed systems technologies, working out an architecture for the application, encoding it in a program, writing specific code to test the application, and finally deploying it. This paper presents a design language and a tool suite covering the development life-cycle of a pervasive computing application. The design language allows us to define a taxonomy of area-specific building-blocks, abstracting over their heterogeneity. This language also includes a layer to define the architecture of an application, following an architectural pattern commonly used in the pervasive computing domain. Our underlying methodology assigns roles to the stakeholders, providing separation of concerns. Our tool suite includes a compiler that takes design artifacts written in our language as input and generates a programming framework that supports the subsequent development stages, namely, implementation, testing, and deployment. Our methodology has been applied on a wide spectrum of areas. Based on these experiments, we assess our approach through three criteria: expressiveness, usability, and productivity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.107","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6051438","Methodology;domain-specific language;generative programming;pervasive computing;toolkit;programming support;simulation","Pervasive computing;Taxonomy;Computer architecture;Programming;Domain specific languages;Computational modeling;Software architecture","program compilers;software architecture;ubiquitous computing","tool-based development methodology;pervasive computing applications;distributed systems technologies;development life-cycle;area-specific building-blocks;architectural pattern;compiler;design artifacts","","21","","53","","","","","","IEEE","IEEE Journals & Magazines"
"The confounding effect of class size on the validity of object-oriented metrics","K. El Emam; S. Benlarbi; N. Goel; S. N. Rai","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","7","630","650","Much effort has been devoted to the development and empirical validation of object-oriented metrics. The empirical validations performed thus far would suggest that a core set of validated metrics is close to being identified. However, none of these studies allow for the potentially confounding effect of class size. We demonstrate a strong size confounding effect and question the results of previous object-oriented metrics validation studies. We first investigated whether there is a confounding effect of class size in validation studies of object-oriented metrics and show that, based on previous work, there is reason to believe that such an effect exists. We then describe a detailed empirical methodology for identifying those effects. Finally, we perform a study on a large C++ telecommunications framework to examine if size is really a confounder. This study considered the Chidamber and Kemerer metrics and a subset of the Lorenz and Kidd metrics. The dependent variable was the incidence of a fault attributable to a field failure (fault-proneness of a class). Our findings indicate that, before controlling for size, the results are very similar to previous studies. The metrics that are expected to be validated are indeed associated with fault-proneness.","0098-5589;1939-3520;2326-3881","","10.1109/32.935855","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=935855","","Size control;Software quality;Testing;Telecommunication control;Helium;Software engineering;Maintenance;Guidelines;Object oriented modeling;Predictive models","software metrics;object-oriented programming;C++ language;software fault tolerance;software quality","class size;object-oriented metrics;software metrics validation;C++;telecommunications framework;fault-proneness;software quality","","193","","110","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional Verification for Hierarchical Scheduling of Real-Time Systems","L. Carnevali; A. Pinzuti; E. Vicario","Università di Firenze; Università di Firenze; Università di Firenze","IEEE Transactions on Software Engineering","","2013","39","5","638","657","Hierarchical Scheduling (HS) techniques achieve resource partitioning among a set of real-time applications, providing reduction of complexity, confinement of failure modes, and temporal isolation among system applications. This facilitates compositional analysis for architectural verification and plays a crucial role in all industrial areas where high-performance microprocessors allow growing integration of multiple applications on a single platform. We propose a compositional approach to formal specification and schedulability analysis of real-time applications running under a Time Division Multiplexing (TDM) global scheduler and preemptive Fixed Priority (FP) local schedulers, according to the ARINC-653 standard. As a characterizing trait, each application is made of periodic, sporadic, and jittering tasks with offsets, jitters, and nondeterministic execution times, encompassing intra-application synchronizations through semaphores and mailboxes and interapplication communications among periodic tasks through message passing. The approach leverages the assumption of a TDM partitioning to enable compositional design and analysis based on the model of preemptive Time Petri Nets (pTPNs), which is expressly extended with a concept of Required Interface (RI) that specifies the embedding environment of an application through sequencing and timing constraints. This enables exact verification of intra-application constraints and approximate but safe verification of interapplication constraints. Experimentation illustrates results and validates their applicability on two challenging workloads in the field of safety-critical avionic systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6264049","Real-time systems;hierarchical scheduling;ARINC-653;time division multiplexing;preemptive fixed priority;compositional verification;preemptive time Petri nets;symbolic state-space analysis","Real time systems;Complexity theory;Time division multiplexing;Job shop scheduling;Timing;Resource management;Petri nets","aerospace computing;formal verification;message passing;microprocessor chips;Petri nets;resource allocation;safety-critical software;scheduling","compositional verification;hierarchical scheduling;realtime system;HS technique;resource partitioning;compositional analysis;architectural verification;high-performance microprocessor;formal specification;schedulability analysis;time division multiplexing;TDM global scheduler;preemptive fixed priority local scheduler;FP local scheduler;ARINC-653 standard;periodic task;sporadic task;jittering task;nondeterministic execution time;intra-application synchronization;interapplication communication;message passing;semaphore;mailbox;compositional design;preemptive time Petri nets model;required interface concept;RI concept;sequencing constraint;timing constraint;safety-critical avionic system","","19","","41","","","","","","IEEE","IEEE Journals & Magazines"
"A logical theory of interfaces and objects","P. S. C. Alencar; D. D. Cowan; C. J. P. Lucena","Dept. of Comput. Sci., Waterloo Univ., Ont., Canada; Dept. of Comput. Sci., Waterloo Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","2002","28","6","548","575","This paper motivates and describes a logic-based approach to specifying and reasoning about interfaces and objects that focuses on separation of concerns issues. The approach is based on the abstract design view (ADV), a software design model for object-oriented systems. The model was originally introduced to characterize, in an informal and practical setting, a clear separation between objects, which we called abstract design objects and their interfaces (ADVs). The objects capture the basic concern, while the interfaces capture special concerns such as user interface, control, timing, and distribution. First, we analyze the ADV design model in order to precisely characterize the interfaces, their associated objects, and the relationship between them. Then, we present one possible approach to formalizing interfaces, objects, and the ""views-a"" relationship. The central mathematical tools used for this purpose are temporal logic and some tools from the category theory. The formal approach is illustrated by examples that show how the interface and related objects and the views-a relationship can be used in object-oriented specifications. We also show how the theory enables the designer to perform relevant analysis activities while modeling with separation of concerns in mind. The theory can be used to derive dynamic and structural properties of the interface objects and the views-a relationship. In particular, we can use the theory to derive global properties of interfaces that capture special concerns from the local properties of their related objects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010059","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010059","","Object oriented modeling;Application software;User interfaces;Computer interfaces;Distributed computing;Timing;Software design;Performance analysis;Logic design;Concurrent computing","object-oriented programming;category theory;temporal logic;user interfaces;formal specification;formal verification","abstract design views;user interfaces;concurrency;specification;verification;abstract design objects;temporal logic;category theory;object-oriented program;separation of concerns","","7","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Determining inspection cost-effectiveness by combining project data and expert opinion","B. Freimut; L. C. Briand; F. Vollei","Fraunhofer Inst. for Exp. Software, Kaiserslautern, Germany; Fraunhofer Inst. for Exp. Software, Kaiserslautern, Germany; Fraunhofer Inst. for Exp. Software, Kaiserslautern, Germany","IEEE Transactions on Software Engineering","","2005","31","12","1074","1092","There is a general agreement among software engineering practitioners that software inspections are an important technique to achieve high software quality at a reasonable cost. However, there are many ways to perform such inspections and many factors that affect their cost-effectiveness. It is therefore important to be able to estimate this cost-effectiveness in order to monitor it, improve it, and convince developers and management that the technology and related investments are worth while. This work proposes a rigorous but practical way to do so. In particular, a meaningful model to measure cost-effectiveness is proposed and a method to determine cost-effectiveness by combining project data and expert opinion is described. To demonstrate the feasibility of the proposed approach, the results of a large-scale industrial case study are presented and an initial validation is performed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.136","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566608","Index Terms- Software inspection;cost-effectiveness model;Monte Carlo simulation;case study;expert opinion.","Inspection;Costs;Software quality;Monitoring;Programming;Software engineering;Technology management;Investments;Particle measurements;Large-scale systems","inspection;software quality;software cost estimation","software engineering;software inspection;software quality;cost-effectiveness estimation;project data;expert opinion","","13","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Formal analysis of a space-craft controller using SPIN","K. Havelund; M. Lowry; J. Penix","Kestrel Technol., NASA Ames Res. Center, Moffett Field, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","8","749","765","The paper documents an application of the finite state model checker SPIN to formally analyze a multithreaded plan execution module. The plan execution module is one component of NASA's New Millennium Remote Agent, an artificial intelligence-based spacecraft control system architecture which launched in October of 1998 as part of the DEEP SPACE 1 mission. The bottom layer of the plan execution module architecture is a domain specific language, named ESL (Executive Support Language), implemented as an extension to multithreaded COMMON LISP. ESL supports the construction of reactive control mechanisms for autonomous robots and spacecraft. For the case study, we translated the ESL services for managing interacting parallel goal-and-event driven processes into the PROMELA input language of SPIN. A total of five previously undiscovered concurrency errors were identified within the implementation of ESL. According to the Remote Agent programming team, the effort has had a major impact, locating errors that would not have been located otherwise and, in one case, identifying a major design flaw. In fact, in a different part of the system, a concurrency bug identical to one discovered by this study escaped testing and caused a deadlock during an in-flight experiment, 96 million kilometers from Earth. The work additionally motivated the introduction of procedural abstraction in terms of inline procedures into SPIN.","0098-5589;1939-3520;2326-3881","","10.1109/32.940728","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=940728","","Space vehicles;Concurrent computing;Intelligent systems;Artificial intelligence;Intelligent agent;Intelligent control;Control systems;Space missions;Domain specific languages;Orbital robotics","space vehicles;aerospace control;multi-threading;intelligent control;program verification;software agents","formal analysis;spacecraft controller;SPIN;finite state model checker;multithreaded plan execution module;New Millennium Remote Agent;artificial intelligence-based spacecraft control system architecture;DEEP SPACE 1 mission;plan execution module architecture;domain specific language;Executive Support Language;multithreaded COMMON LISP;reactive control mechanisms;autonomous robots;ESL services;interacting parallel goal-and-event driven processes;PROMELA input language;concurrency errors;Remote Agent programming team;design flaw;concurrency bug;in-flight experiment;procedural abstraction;inline procedures","","59","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Learning a Metric for Code Readability","R. P. L. Buse; W. R. Weimer","University of Virginia, Charlottesville; University of Virginia, Charlottesville","IEEE Transactions on Software Engineering","","2010","36","4","546","558","In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from 120 human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80 percent effective and better than a human, on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with three measures of software quality: code changes, automated defect reports, and defect log messages. We measure these correlations on over 2.2 million lines of code, as well as longitudinally, over many releases of selected projects. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggest that comments, in and of themselves, are less important than simple blank lines to local judgments of readability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5332232","Software readability;program understanding;machine learning;software maintenance;code metrics;FindBugs.","Software quality;Humans;Software maintenance;Readability metrics;Documentation;Software measurement;Computer languages;Design engineering;Machine learning;Costs","human factors;software quality","code readability;software quality;local code features;human notions;code changes;automated defect reports;defect log messages;programming language design","","64","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Fixing of Programs with Contracts","Y. Pei; C. A. Furia; M. Nordio; Y. Wei; B. Meyer; A. Zeller","Chair of Software Engineering, Department of Computer Science, ETH Zürich, Switzerland; Chair of Software Engineering, Department of Computer Science, ETH Zürich, Switzerland; Chair of Software Engineering, Department of Computer Science, ETH Zürich, Switzerland; Constraint Reasoning Group, Microsoft Research Cambridge, United Kingdom; Chair of Software Engineering, Department of Computer Science, ETH Zürich, Switzerland; Software Engineering Chair, Saarland University, Germany","IEEE Transactions on Software Engineering","","2014","40","5","427","449","This paper describes AutoFix, an automatic debugging technique that can fix faults in general-purpose software. To provide high-quality fix suggestions and to enable automation of the whole debugging process, AutoFix relies on the presence of simple specification elements in the form of contracts (such as pre- and postconditions). Using contracts enhances the precision of dynamic analysis techniques for fault detection and localization, and for validating fixes. The only required user input to the AutoFix supporting tool is then a faulty program annotated with contracts; the tool produces a collection of validated fixes for the fault ranked according to an estimate of their suitability. In an extensive experimental evaluation, we applied AutoFix to over 200 faults in four code bases of different maturity and quality (of implementation and of contracts). AutoFix successfully fixed 42 percent of the faults, producing, in the majority of cases, corrections of quality comparable to those competent programmers would write; the used computational resources were modest, with an average time per fix below 20 minutes on commodity hardware. These figures compare favorably to the state of the art in automated program fixing, and demonstrate that the AutoFix approach is successfully applicable to reduce the debugging burden in real-world scenarios.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2312918","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6776507","Automatic program repair;contracts;dynamic analysis","Indexes;Contracts;Debugging;Libraries;Software engineering;Software;Automation","contracts;fault diagnosis;program debugging;program diagnostics;software tools","automated program fixing;contracts;automatic debugging technique;general-purpose software fault fixing;high-quality fix suggestions;debugging process;dynamic analysis techniques;fault localization;fault detection;AutoFix supporting tool;computational resources;commodity hardware;AutoFix approach","","34","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Cross versus Within-Company Cost Estimation Studies: A Systematic Review","B. A. Kitchenham; E. Mendes; G. H. Travassos","IEEE Computer Society; NA; NA","IEEE Transactions on Software Engineering","","2007","33","5","316","329","The objective of this paper is to determine under what circumstances individual organizations would be able to rely on cross-company-based estimation models. We performed a systematic review of studies that compared predictions from cross-company models with predictions from within-company models based on analysis of project data. Ten papers compared cross-company and within-company estimation models; however, only seven presented independent results. Of those seven, three found that cross-company models were not significantly different from within-company models, and four found that cross-company models were significantly worse than within-company models. Experimental procedures used by the studies differed making it impossible to undertake formal meta-analysis of the results. The main trend distinguishing study results was that studies with small within-company data sets (i.e., $20 projects) that used leave-one-out cross validation all found that the within-company model was significantly different (better) from the cross-company model. The results of this review are inconclusive. It is clear that some organizations would be ill-served by cross-company models whereas others would benefit. Further studies are needed, but they must be independent (i.e., based on different data bases or at least different single company data sets) and should address specific hypotheses concerning the conditions that would favor cross-company or within-company models. In addition, experimenters need to standardize their experimental procedures to enable formal meta-analysis, and recommendations are made in Section 3.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1001","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160970","Cost estimation;management;systematic review;software engineering.","Costs;Predictive models;Computer Society;Data analysis;Performance analysis;Engineering management;Software engineering;Proposals;Productivity;Accuracy","software cost estimation","software engineering;software cost estimation models;cross-company-based estimation","","152","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Process Aspects and Social Dynamics of Contemporary Code Review: Insights from Open Source Development and Industrial Practice at Microsoft","A. Bosu; J. C. Carver; C. Bird; J. Orbeck; C. Chockley","Department of Computer Science, Southern Illinois University, Carbondale, IL; Department of Computer Science, University of Alabama, Tuscaloosa, AL; Microsoft Research, Microsoft Corportation, Redmond, WA; Department of Computer Science, University of Alabama, Tuscaloosa, AL; Department of Computer Science, University of Alabama, Tuscaloosa, AL","IEEE Transactions on Software Engineering","","2017","43","1","56","75","Many open source and commercial developers practice contemporary code review, a lightweight, informal, tool-based code review process. To better understand this process and its benefits, we gathered information about code review practices via surveys of open source software developers and developers from Microsoft. The results of our analysis suggest that developers spend approximately 10-15 percent of their time in code reviews, with the amount of effort increasing with experience. Developers consider code review important, stating that in addition to finding defects, code reviews offer other benefits, including knowledge sharing, community building, and maintaining code quality. The quality of the code submitted for review helps reviewers form impressions about their teammates, which can influence future collaborations. We found a large amount of similarity between the Microsoft and OSS respondents. One interesting difference is that while OSS respondents view code review as an important method of impression formation, Microsoft respondents found knowledge dissemination to be more important. Finally, we found little difference between distributed and co-located Microsoft teams. Our findings identify the following key areas that warrant focused research: 1) exploring the non-technical benefits of code reviews, 2) helping developers in articulating review comments, and 3) assisting reviewers' program comprehension during code reviews.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2576451","US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7484733","Code review;open source;OSS;survey;peer impressions;commercial projects","Inspection;Organizations;Collaboration;Context;Instruments;Measurement;Human factors","public domain software;software engineering;software management;software reviews;team working","contemporary code review;open source software developers;code quality;OSS;knowledge dissemination;Microsoft teams","","2","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Constructing meta-CASE workbenches by exploiting visual language generators","G. Costagliola; V. Deufemia; F. Ferrucci; C. Gravino","Dipt. di Matematica e Inf., Salerno Univ., Italy; Dipt. di Matematica e Inf., Salerno Univ., Italy; Dipt. di Matematica e Inf., Salerno Univ., Italy; Dipt. di Matematica e Inf., Salerno Univ., Italy","IEEE Transactions on Software Engineering","","2006","32","3","156","175","In this paper, we propose an approach for the construction of meta-CASE workbenches, which suitably integrates the technology of visual language generation systems, UML metamodeling, and interoperability techniques based on the GXL (graph exchange language) format. The proposed system consists of two major components. Environments for single visual languages are generated by using the modeling language environment generator (MEG), which follows a metamodel/grammar-approach. The abstract syntax of a visual language is defined by UML class diagrams, which serve as a base for the grammar specification of the language. The workbench generator (WoG) allows designers to specify the target workbench by means of a process model given in terms of a suitable activity diagram. Starting from the supplied specification WoG generates the customized workbench by integrating the required environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610608","Meta-CASE tools;metamodeling techniques;visual languages;visual programming environment generators.","Computer aided software engineering;Power system modeling;Unified modeling language;Metamodeling;Programming environments;Software design;Collaborative work;Software tools;Employment;Debugging","visual languages;visual programming;automatic programming;programming environments;computer aided software engineering;formal specification;Unified Modeling Language;XML;grammars;open systems","meta-CASE workbench tool;visual language generation system;UML metamodeling;interoperability techniques;GXL;graph exchange language format;modeling language environment generator;MEG;metamodel/grammar-approach;UML class diagram;language grammar specification;workbench generator;WoG;visual programming environment generator","","10","","73","","","","","","IEEE","IEEE Journals & Magazines"
"Tranquility: A Low Disruptive Alternative to Quiescence for Ensuring Safe Dynamic Updates","Y. Vandewoude; P. Ebraert; Y. Berbers; T. D'Hondt","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","856","868","This paper revisits a problem that was identified by Kramer and Magee: placing a system in a consistent state before and after runtime changes. We show that their notion of quiescence as a necessary and sufficient condition for safe runtime changes is too strict and results in a significant disruption in the application being updated. In this paper, we introduce a weaker condition: tranquillity. We show that tranquillity is easier to obtain and less disruptive for the running application but still a sufficient condition to ensure application consistency. We present an implementation of our approach on a component middleware platform and experimentally verify the validity and practical applicability of our approach using data retrieved from a case study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70733","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359466","Distributed objects;Componentware;Application-aware adaptation;Distributed objects;components;containers;Distributed objects;Componentware;Application-aware adaptation;Distributed objects;components;containers","Application software;Runtime;Sufficient conditions;Software engineering;Middleware;Information retrieval;Computer languages;Operating systems;Software systems;Containers","middleware;object-oriented programming","tranquillity;quiescence;necessary and sufficient condition;runtime changes;application consistency;component middleware platform","","70","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Advancing candidate link generation for requirements tracing: the study of methods","J. H. Hayes; A. Dekhtyar; S. K. Sundaram","Dept. of Comput. Sci., Kentucky Univ., Lexington, KY, USA; Dept. of Comput. Sci., Kentucky Univ., Lexington, KY, USA; Dept. of Comput. Sci., Kentucky Univ., Lexington, KY, USA","IEEE Transactions on Software Engineering","","2006","32","1","4","19","This paper addresses the issues related to improving the overall quality of the dynamic candidate link generation for the requirements tracing process for verification and validation and independent verification and validation analysts. The contribution of the paper is four-fold: we define goals for a tracing tool based on analyst responsibilities in the tracing process, we introduce several new measures for validating that the goals have been satisfied, we implement analyst feedback in the tracing process, and we present a prototype tool that we built, RETRO (REquirements TRacing On-target), to address these goals. We also present the results of a study used to assess RETRO's support of goals and goal elements that can be measured objectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.3","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583599","Requirements tracing;dynamic link generation;Verification and Validation (V&amp;V);Independent Validation and Verification (IV&amp;V);information retrieval;TF-IDF;LSI;recall;precision.","Risk analysis;System testing;Spine;Computer Society;Feedback;Prototypes;Information retrieval;Large scale integration;Computer aided software engineering","formal verification;formal specification;software tools","dynamic candidate link generation;RETRO tool;Requirements Tracing On-target tool;independent verification analysis;independent validation analysis","","232","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Privately Finding Specifications","W. Weimer; N. Mishra","NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","21","32","Buggy software is a reality and automated techniques for discovering bugs are highly desirable. A specification describes the correct behavior of a program. For example, a file must eventually be closed once it has been opened. Specifications are learned by finding patterns in normal program execution traces versus erroneous ones. With more traces, more specifications can be learned more accurately. By combining traces from multiple parties that possess distinct programs but use a common library, it is possible to obtain sufficiently many traces. However, obtaining traces from competing parties is problematic: By revealing traces, it may be possible to learn that one party writes buggier code than another. We present an algorithm by which mutually distrusting parties can work together to learn program specifications while preserving their privacy. We use a perturbation algorithm to obfuscate individual trace values while still allowing statistical trends to be mined from the data. Despite the noise introduced to safeguard privacy, empirical evidence suggests that our algorithm learns specifications that find 85 percent of the bugs that a no-privacy approach would find.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70744","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359470","F.3.1.f Specification techniques;D.2.19 Software Quality/SQA;I.2.6 Learning;K.4.1.f Privacy;F.3.1.f Specification techniques;D.2.19 Software Quality/SQA;I.2.6 Learning;K.4.1.f Privacy","Computer bugs;Privacy;Software testing;Data security;Operating systems;Libraries;Software quality;Formal specifications;Information security;Concurrent computing","data mining;perturbation techniques;program debugging;software maintenance","buggy software;specification techniques;perturbation algorithm;data mining","","3","","40","","","","","","IEEE","IEEE Journals & Magazines"
"<italic>SITAR</italic>: GUI Test Script Repair","Z. Gao; Z. Chen; Y. Zou; A. M. Memon","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science, University of Maryland, College Park, MD, USA","IEEE Transactions on Software Engineering","","2016","42","2","170","186","System testing of a GUI-based application requires that test cases, consisting of sequences of user actions/events, be executed and the software's output be verified. To enable automated re-testing, such test cases are increasingly being coded as low-level test scripts, to be replayed automatically using test harnesses. Whenever the GUI changes-widgets get moved around, windows get merged-some scripts become unusable because they no longer encode valid input sequences. Moreover, because the software's output may have changed, their test oracles-assertions and checkpoints-encoded in the scripts may no longer correctly check the intended GUI objects. We present ScrIpT repAireR (SITAR), a technique to automatically repair unusable low-level test scripts. SITAR uses reverse engineering techniques to create an abstract test for each script, maps it to an annotated event-flow graph (EFG), uses repairing transformations and human input to repair the test, and synthesizes a new “repaired” test script. During this process, SITAR also repairs the reference to the GUI objects used in the checkpoints yielding a final test script that can be executed automatically to validate the revised software. SITAR amortizes the cost of human intervention across multiple scripts by accumulating the human knowledge as annotations on the EFG. An experiment using QTP test scripts suggests that SITAR is effective in that 41-89 percent unusable test scripts were repaired. Annotations significantly reduced human cost after 20 percent test scripts had been repaired.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2454510","National Basic Research Program of China; NSFC; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7214294","GUI testing;GUI test script;test script repair;human knowledge accumulation;GUI testing;GUI test script;test script repair;human knowledge accumulation","Maintenance engineering;Graphical user interfaces;Software;Testing;Automation;Computational modeling;Electronic mail","graph theory;graphical user interfaces;program testing;software maintenance","SITAR;test script repairer;graphical user interface;GUI-based application;system testing;automated retesting;event-flow graph;EFG;software validation","","9","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Resolving race conditions in asynchronous partial order scenarios","B. Mitchell","Dept. of Comput., Surrey Univ., Guildford, UK","IEEE Transactions on Software Engineering","","2005","31","9","767","784","Scenario-based requirements specifications are the industry norm for defining communication protocols. However, such scenarios often contain race conditions. A race condition occurs when events are specified to occur in a particular order, but in practice, this order cannot be guaranteed. The paper considers UML/MSC scenarios that can be described with standard partial order theoretic asynchronous behavioral semantics. We define these to be partial order scenarios. The paper proves there is a unique minimal generalization of a partial order scenario that is race free. The paper also proves there is a unique minimal race free refinement of the behavioral semantics of a partial order scenario. Unlike the generalization, the refinement cannot be realized in the form of a partial order scenario, although it can always be embedded in one. The paper, also proves the results can be generalized to a subclass of iterative scenarios.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.104","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514445","Index Terms- Requirements analysis;formal methods;distributed programming.","Unified modeling language;Protocols;Communication industry;Communication systems;Specification languages;Message passing;Creep;Inspection;Software tools","Unified Modeling Language;formal specification;distributed programming;programming language semantics","race condition;asynchronous partial order scenarios;formal specifications;UML;asynchronous behavioral semantics;distributed programming","","14","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Design Pattern Detection Using Similarity Scoring","N. Tsantalis; A. Chatzigeorgiou; G. Stephanides; S. T. Halkidis","NA; IEEE Computer Society; IEEE Computer Society; NA","IEEE Transactions on Software Engineering","","2006","32","11","896","909","The identification of design patterns as part of the reengineering process can convey important information to the designer. However, existing pattern detection methodologies generally have problems in dealing with one or more of the following issues: identification of modified pattern versions, search space explosion for large systems and extensibility to novel patterns. In this paper, a design pattern detection methodology is proposed that is based on similarity scoring between graph vertices. Due to the nature of the underlying graph algorithm, this approach has the ability to also recognize patterns that are modified from their standard representation. Moreover, the approach exploits the fact that patterns reside in one or more inheritance hierarchies, reducing the size of the graphs to which the algorithm is applied. Finally, the algorithm does not rely on any pattern-specific heuristic, facilitating the extension to novel design structures. Evaluation on three open-source projects demonstrated the accuracy and the efficiency of the proposed method","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.112","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015512","Patterns;object-oriented design methods;graph algorithms;restructuring;reverse engineering;reengineering.","Computer Society;Explosions;Design methodology;Open source software;Software systems;Space exploration;Clustering algorithms;Pattern recognition;Algorithm design and analysis;Reverse engineering","graph theory;object-oriented methods;object-oriented programming;systems re-engineering","design pattern detection;reengineering process;graph vertices;open-source project;graph algorithms","","195","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting Self-Adaptation via Quantitative Verification and Sensitivity Analysis at Run Time","A. Filieri; G. Tamburrelli; C. Ghezzi","Reliable Software Systems Group, University of Stuttgart, Stuttgart, Germany; Vrije University of Amsterdam, Netherlands; Dipartimento di Elettronica, Informazione e Bioingegneria at Politecnico di Milano, Milan, Italy","IEEE Transactions on Software Engineering","","2016","42","1","75","99","Modern software-intensive systems often interact with an environment whose behavior changes over time, often unpredictably. The occurrence of changes may jeopardize their ability to meet the desired requirements. It is therefore desirable to design software in a way that it can self-adapt to the occurrence of changes with limited, or even without, human intervention. Self-adaptation can be achieved by bringing software models and model checking to run time, to support perpetual automatic reasoning about changes. Once a change is detected, the system itself can predict if requirements violations may occur and enable appropriate counter-actions. However, existing mainstream model checking techniques and tools were not conceived for run-time usage; hence they hardly meet the constraints imposed by on-the-fly analysis in terms of execution time and memory usage. This paper addresses this issue and focuses on perpetual satisfaction of non-functional requirements, such as reliability or energy consumption. Its main contribution is the description of a mathematical framework for run-time efficient probabilistic model checking. Our approach statically generates a set of verification conditions that can be efficiently evaluated at run time as soon as changes occur. The proposed approach also supports sensitivity analysis, which enables reasoning about the effects of changes and can drive effective adaptation strategies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2421318","European Commission; Programme IDEAS-ERC; Project 227977-SMScom; Programme FP7-PEOPLE-2011-IEF; Project 302648-RunMore; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7083754","Self-adaptive Systems;Software Evolution;Non-functional Requirements;Discrete-Time Markov models;Rewards;Software Reliability;Costs;Probabilistic Model Checking;Models at Runtime;Self-adaptive systems;software evolution;non-functional requirements;discrete-time Markov models;rewards;software reliability;costs;probabilistic model checking;models at runtime","Model checking;Adaptation models;Software;Markov processes;Probabilistic logic;Computational modeling;Reliability","probability;program verification;software reliability","self-adaptation;quantitative verification;run time analysis;software-intensive systems;software design;software models;perpetual automatic reasoning;requirements violation;on-the-fly analysis;execution time;memory usage;perpetual satisfaction;nonfunctional requirements;mathematical framework;probabilistic model checking;sensitivity analysis","","24","","91","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Test Suites and Adequacy Criteria Using Simulation-Based Models of Distributed Systems","M. J. Rutherford; A. Carzaniga; A. L. Wolf","University of Colorado at Boulder, Boulder; University of Lugano, Lugano; Imperial College London, London","IEEE Transactions on Software Engineering","","2008","34","4","452","470","Test adequacy criteria provide the engineer with guidance on how to populate test suites. While adequacy criteria have long been a focus of research, existing testing methods do not address many of the fundamental characteristics of distributed systems, such as distribution topology, communication failure, and timing. Furthermore, they do not provide the engineer with a means to evaluate the relative effectiveness of different criteria, nor the relative effectiveness of adequate test suites satisfying a given criterion. This paper makes three contributions to the development and use of test adequacy criteria for distributed systems: (1) a testing method based on discrete-event simulations; (2) a fault-based analysis technique for evaluating test suites and adequacy criteria; and (3) a series of case studies that validate the method and technique. The testing method uses a discrete-event simulation as an operational specification of a system, in which the behavioral effects of distribution are explicitly represented. Adequacy criteria and test cases are then defined in terms of this simulation-based specification. The fault-based analysis involves mutation of the simulation-based specification to provide a foil against which test suites and the criteria that formed them can be evaluated. Three distributed systems were used to validate the method and technique, including DNS, the domain name system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527254","Specification;Test coverage of specifications;Specification;Test coverage of specifications","System testing;Discrete event simulation;Computational modeling;Computer Society;Domain Name System;Web server;Network servers;Computer simulation;Topology;Timing","distributed processing;program testing","distributed system;test adequacy criteria;discrete-event simulation;fault-based analysis technique;simulation-based specification","","13","","61","","","","","","IEEE","IEEE Journals & Magazines"
"Early Detection of Collaboration Conflicts and Risks","Y. Brun; R. Holmes; M. D. Ernst; D. Notkin","University of Massachusetts, Amherst; University of Waterloo, Waterloo; University of Washington, Seattle; University of Washington, Seattle","IEEE Transactions on Software Engineering","","2013","39","10","1358","1375","Conflicts among developers' inconsistent copies of a shared project arise in collaborative development and can slow progress and decrease quality. Identifying and resolving such conflicts early can help. Identifying situations which may lead to conflicts can prevent some conflicts altogether. By studying nine open-source systems totaling 3.4 million lines of code, we establish that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. Motivated by this finding, we develop a speculative analysis technique that uses previously unexploited information from version control operations to precisely diagnose important classes of conflicts. Then, we design and implement Crystal, a publicly available tool that helps developers identify, manage, and prevent conflicts. Crystal uses speculative analysis to make concrete advice unobtrusively available to developers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6520859","Collaborative development;collaboration conflicts;developer awareness;speculative analysis;version control;Crystal","Crystals;Collaboration;History;Open source software;Control systems;Terminology;Computer science","configuration management;groupware;program diagnostics;public domain software;software engineering","collaboration conflicts;collaborative development;open-source systems;overlapping textual edits;subsequent build-and-test failures;speculative analysis technique;version control operations;Crystal;publicly available tool;source code","","21","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Interface Mutation: an approach for integration testing","M. E. Delamaro; J. C. Maidonado; A. P. Mathur","Dept. of Inf., Maringa State Univ., Brazil; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","228","247","The need for test adequacy criteria is widely recognized. Several criteria have been proposed for the assessment of adequacy of tests at the unit level. However, there remains a lack of criteria for the assessment of the adequacy of tests generated during integration testing. We present a mutation based interprocedural criterion, named Interface Mutation (IM), suitable for use during integration testing. A case study to evaluate the proposed criterion is reported. In the study, the UNIX sort utility was seeded with errors and Interface Mutation evaluated by measuring the cost of its application and its error revealing effectiveness. Alternative IM criteria using different sets of Interface Mutation operators were also evaluated. While comparing the error revealing effectiveness of these Interface Mutation-based test sets with same size randomly generated test sets, we observed that in most cases Interface Mutation based test sets are superior. The results suggest that Interface Mutation offers a viable test adequacy criteria for use at the integration level.","0098-5589;1939-3520;2326-3881","","10.1109/32.910859","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910859","","Genetic mutations;Software testing;Software systems;System testing;Computer Society;Costs;Programming;Redundancy;Software design","program testing;Unix;sorting","Interface Mutation;integration testing;test adequacy criteria;unit level;mutation based interprocedural criterion;case study;UNIX sort utility;error revealing effectiveness;alternative IM criteria;Interface Mutation operators;test sets;randomly generated test sets;integration level","","113","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Ethical issues in empirical studies of software engineering","J. Singer; N. G. Vinson","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada","IEEE Transactions on Software Engineering","","2002","28","12","1171","1180","The popularity of empirical methods in software engineering research is on the rise. Surveys, experiments, metrics, case studies, and field studies are examples of empirical methods used to investigate both software engineering processes and products. The increased application of empirical methods has also brought about an increase in discussions about adapting these methods to the peculiarities of software engineering. In contrast, the ethical issues raised by empirical methods have received little, if any, attention in the software engineering literature. This article is intended to introduce the ethical issues raised by empirical research to the software engineering research community and to stimulate discussion of how best to deal with these ethical issues. Through a review of the ethical codes of several fields that commonly employ humans and artifacts as research subjects, we have identified major ethical issues relevant to empirical studies of software engineering. These issues are illustrated with real empirical studies of software engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158289","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158289","","Software engineering;Ethics;Humans;Collaborative work;Application software;Guidelines;Risk management;Law;Legal factors;Collaborative software","software engineering;professional aspects;legislation","ethical issues;empirical studies;software engineering;software engineering processes;software engineering products;legal issues","","48","","41","","","","","","IEEE","IEEE Journals & Magazines"
"On the Distribution of Bugs in the Eclipse System","G. Concas; M. Marchesi; A. Murgia; R. Tonelli; I. Turnu","University of Cagliari, Cagliari; University of Cagliari, Cagliari; University of Cagliari, Cagliari; University of Cagliari, Cagliari; University of Cagliari, Cagliari","IEEE Transactions on Software Engineering","","2011","37","6","872","877","The distribution of bugs in software systems has been shown to satisfy the Pareto principle, and typically shows a power-law tail when analyzed as a rank-frequency plot. In a recent paper, Zhang showed that the Weibull cumulative distribution is a very good fit for the Alberg diagram of bugs built with experimental data. In this paper, we further discuss the subject from a statistical perspective, using as case studies five versions of Eclipse, to show how log-normal, Double-Pareto, and Yule-Simon distributions may fit the bug distribution at least as well as the Weibull distribution. In particular, we show how some of these alternative distributions provide both a superior fit to empirical data and a theoretical motivation to be used for modeling the bug generation process. While our results have been obtained on Eclipse, we believe that these models, in particular the Yule-Simon one, can generalize to other software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928349","Software bug distribution;empirical research;object-oriented systems.","Computer bugs;Software systems;Data models;Computational modeling;Weibull distribution;Object oriented modeling","eclipses;Pareto analysis;Weibull distribution","software systems;Pareto principle;Weibull cumulative distribution;statistical perspective;eclipse system","","19","","22","","","","","","IEEE","IEEE Journals & Magazines"
"BURN: Enabling Workload Burstiness in Customized Service Benchmarks","G. Casale; A. Kalbasi; D. Krishnamurthy; J. Rolia","Imperial College London, London; University of Calgary, Calgary; University of Calgary, Calgary; HP Labs, Palo Alto","IEEE Transactions on Software Engineering","","2012","38","4","778","793","We introduce BURN, a methodology to create customized benchmarks for testing multitier applications under time-varying resource usage conditions. Starting from a set of preexisting test workloads, BURN finds a policy that interleaves their execution to stress the multitier application and generate controlled burstiness in resource consumption. This is useful to study, in a controlled way, the robustness of software services to sudden changes in the workload characteristics and in the usage levels of the resources. The problem is tackled by a model-based technique which first generates Markov models to describe resource consumption patterns of each test workload. Then, a policy is generated using an optimization program which sets as constraints a target request mix and user-specified levels of burstiness at the different resources in the system. Burstiness is quantified using a novel metric called overdemand, which describes in a natural way the tendency of a workload to keep a resource congested for long periods of time and across multiple requests. A case study based on a three-tier application testbed shows that our method is able to control and predict burstiness for session service demands at a fine-grained scale. Furthermore, experiments demonstrate that for any given request mix our approach can expose latency and throughput degradations not found with nonbursty workloads having the same request mix.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.58","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928353","Benchmarking;performance;burstiness;bottleneck migration;overdemand","Benchmark testing;Markov processes;Servers;Aggregates;Analytical models;Computational modeling;Linear regression","benchmark testing;Markov processes","BURN;workload burstiness;customized service benchmarks;customized benchmarks;multitier application;time-varying resource usage condition;controlled burstiness;software services;model-based technique;Markov models;resource consumption pattern;optimization program;target request mix;user-specified levels;three-tier application testbed;session service demands;fine-grained scale;latency;throughput degradation;nonbursty workloads","","8","","38","","","","","","IEEE","IEEE Journals & Magazines"
"A survey on software architecture analysis methods","L. Dobrica; E. Niemela","Fac. of Autom. Control & Comput., Univ. Politehnica of Bucharest, Romania; NA","IEEE Transactions on Software Engineering","","2002","28","7","638","653","The purpose of the architecture evaluation of a software system is to analyze the architecture to identify potential risks and to verify that the quality requirements have been addressed in the design. This survey shows the state of the research at this moment, in this domain, by presenting and discussing eight of the most representative architecture analysis methods. The selection of the studied methods tries to cover as many particular views of objective reflections as possible to be derived from the general goal. The role of the discussion is to offer guidelines related to the use of the most suitable method for an architecture assessment process. We will concentrate on discovering similarities and differences between these eight available methods by making classifications, comparisons and appropriateness studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019479","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019479","","Software architecture;Computer architecture;Software systems;Software quality;Risk analysis;Reflection;Software metrics;Computer Society;Guidelines;Computer industry","software architecture;software quality;reviews","survey;software architecture analysis methods;potential risk identification;software quality requirements;objective reflections;software architecture assessment process;classifications;appropriateness studies;software quality attributes;scenarios","","226","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Enhanced Code Conversion Approach for the Integrated Cross-Platform Mobile Development (ICPMD)","W. S. El-Kassas; B. A. Abdullah; A. H. Yousef; A. M. Wahba","Department of Computer and Systems Engineering, Faculty of Engineering, Ain Shams University, Cairo, Egypt; Department of Computer and Systems Engineering, Faculty of Engineering, Ain Shams University, Cairo, Egypt; Department of Computer and Systems Engineering, Faculty of Engineering, Ain Shams University, Cairo, Egypt; Department of Computer and Systems Engineering, Faculty of Engineering, Ain Shams University, Cairo, Egypt","IEEE Transactions on Software Engineering","","2016","42","11","1036","1053","Mobile development companies aim to maximize the return on investments by making their mobile applications (Apps) available on different mobile platforms. Consequently, the same App is developed several times; each time the developer uses the programming languages and development tools of a specific platform. Therefore, there is a need to have cross-platform mobile applications development solutions that enable the developers to develop the App once and run it everywhere. The Integrated Cross-Platform Mobile Applications Development (ICPMD) solution is one of the attempts that enables the developers to use the most popular programming languages like Java for Android and C# for Windows Phone 8 (WP8). ICPMD is used to transform both the source code and user interface to another language to generate full Apps on the target platform. This paper extends ICPMD by proposing a new code conversion approach based on XSLT and Regular Expressions to ease the conversion process. In addition, it provides the assessment method to compare the ICPMD efficiency with competing approaches. Several Apps are converted from WP8 to Android and vice versa. The ICPMD evaluation results show reasonable improvement over commercial cross-platform mobile development tools (Titanium and Xamarin).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2543223","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7442177","Cross-platform mobile development;code conversion;code reuse;generated apps;ICPMD, source code patterns","Mobile communication;Java;Runtime;Titanium;Application programming interfaces;Smart phones","Android (operating system);mobile computing;programming languages;smart phones;software tools;source code (software);user interfaces","code conversion;integrated cross-platform mobile development;ICPMD;mobile application development;mobile platform;programming language;development tool;source code;user interface;smart phone","","1","","39","","","","","","IEEE","IEEE Journals & Magazines"
"An Improved SDA Based Defect Prediction Framework for Both Within-Project and Cross-Project Class-Imbalance Problems","X. Jing; F. Wu; X. Dong; B. Xu","State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, China; State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, China; State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China","IEEE Transactions on Software Engineering","","2017","43","4","321","339","Background. Solving the class-imbalance problem of within-project software defect prediction (SDP) is an important research topic. Although some class-imbalance learning methods have been presented, there exists room for improvement. For cross-project SDP, we found that the class-imbalanced source usually leads to misclassification of defective instances. However, only one work has paid attention to this cross-project class-imbalance problem. Objective. We aim to provide effective solutions for both within-project and cross-project class-imbalance problems. Method. Subclass discriminant analysis (SDA), an effective feature learning method, is introduced to solve the problems. It can learn features with more powerful classification ability from original metrics. For within-project prediction, we improve SDA for achieving balanced subclasses and propose the improved SDA (ISDA) approach. For cross-project prediction, we employ the semi-supervised transfer component analysis (SSTCA) method to make the distributions of source and target data consistent, and propose the SSTCA+ISDA prediction approach. Results. Extensive experiments on four widely used datasets indicate that: 1) ISDA-based solution performs better than other state-of-the-art methods for within-project class-imbalance problem; 2) SSTCA+ISDA proposed for cross-project class-imbalance problem significantly outperforms related methods. Conclusion. Within-project and cross-project class-imbalance problems greatly affect prediction performance, and we provide a unified and effective prediction framework for both problems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2597849","National Nature Science Foundation of China; The Chinese 973 Program; Research Project of NJUPT; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7530877","Software defect prediction (SDP);within-project class-imbalance;cross-project class-imbalance;improved subclass discriminant analysis (ISDA);ISDA based defect prediction framework","Support vector machines;Learning systems;Predictive models;Software;Software engineering;Measurement","learning (artificial intelligence);software engineering","SDA based defect prediction;within-project class-imbalance problem;cross-project class-imbalance problem;software defect prediction;SDP;class-imbalance learning;subclass discriminant analysis;semisupervised transfer component analysis;SSTCA","","16","","78","","","","","","IEEE","IEEE Journals & Magazines"
"Quality Requirements in Industrial Practice—An Extended Interview Study at Eleven Companies","R. Berntsson Svensson; T. Gorschek; B. Regnell; R. Torkar; A. Shahrokni; R. Feldt","Lund University, Lund; Blekinge Institute of Technology, Karlskrona; Lund University, Lund; Blekinge Institute of Technology, Karlskrona; Chalmers University of Technology, Göteborg; Chalmers University of Technology, Göteborg","IEEE Transactions on Software Engineering","","2012","38","4","923","935","In order to create a successful software product and assure its quality, it is not enough to fulfill the functional requirements, it is also crucial to find the right balance among competing quality requirements (QR). An extended, previously piloted, interview study was performed to identify specific challenges associated with the selection, tradeoff, and management of QR in industrial practice. Data were collected through semistructured interviews with 11 product managers and 11 project leaders from 11 software companies. The contribution of this study is fourfold: First, it compares how QR are handled in two cases, companies working in business-to-business markets and companies that are working in business-to-consumer markets. These two are also compared in terms of impact on the handling of QR. Second, it compares the perceptions and priorities of QR by product and project management, respectively. Third, it includes an examination of the interdependencies among quality requirements perceived as most important by the practitioners. Fourth, it characterizes the selection and management of QR in downstream development activities.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5753901","Management;process;requirements/specifications","Companies;Interviews;Industries;Usability;Telecommunications;Reliability","DP industry;project management;software management;software quality","quality requirements;industrial practice;software product;software quality;QR selection;QR tradeoff;QR management;software company;business-to-business market;business-to-consumer market;project management;QR handling","","25","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting Change Impact Analysis Using a Recommendation System: An Industrial Case Study in a Safety-Critical Context","M. Borg; K. Wnuk; B. Regnell; P. Runeson","SICS Swedish ICT AB, Ideon Science Park, Building Beta 2, Scheelevägen 17, Lund, Sweden; Blekinge Institute of Technology, Karlskrona, Sweden; Lund University, Lund, Sweden; Lund University, Lund, Sweden","IEEE Transactions on Software Engineering","","2017","43","7","675","700","Change Impact Analysis (CIA) during software evolution of safety-critical systems is a labor-intensive task. Several authors have proposed tool support for CIA, but very few tools were evaluated in industry. We present a case study on ImpRec, a recommendation System for Software Engineering (RSSE), tailored for CIA at a process automation company. ImpRec builds on assisted tracing, using information retrieval solutions and mining software repositories to recommend development artifacts, potentially impacted when resolving incoming issue reports. In contrast to the majority of tools for automated CIA, ImpRec explicitly targets development artifacts that are not source code. We evaluate ImpRec in a two-phase study. First, we measure the correctness of ImpRec's recommendations by a simulation based on 12 years' worth of issue reports in the company. Second, we assess the utility of working with ImpRec by deploying the RSSE in two development teams on different continents. The results suggest that ImpRec presents about 40 percent of the true impact among the top-10 recommendations. Furthermore, user log analysis indicates that ImpRec can support CIA in industry, and developers acknowledge the value of ImpRec in interviews. In conclusion, our findings show the potential of reusing traceability associated with developers' past activities in an RSSE.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2620458","Embedded Applications Software Engineering; ORION; Knowledge Foundation; Lund University; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7637029","Case;maintenance management;software and system safety;tracing","Context;Industries;Software engineering;Unified modeling language;Automation;Software systems","program diagnostics;recommender systems;safety-critical software;software engineering","change impact analysis;safety-critical context;CIA;software evolution;safety-critical system;recommendation system for software engineering;RSSE;ImpRec;information retrieval solution;software repository mining","","2","","120","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Evaluation of Mutation Testing for Improving the Test Quality of Safety-Critical Software","R. Baker; I. Habli","Aero Engine Controls, Birmingham; University of York, York","IEEE Transactions on Software Engineering","","2013","39","6","787","805","Testing provides a primary means for assuring software in safety-critical systems. To demonstrate, particularly to a certification authority, that sufficient testing has been performed, it is necessary to achieve the test coverage levels recommended or mandated by safety standards and industry guidelines. Mutation testing provides an alternative or complementary method of measuring test sufficiency, but has not been widely adopted in the safety-critical industry. In this study, we provide an empirical evaluation of the application of mutation testing to airborne software systems which have already satisfied the coverage requirements for certification. Specifically, we apply mutation testing to safety-critical software developed using high-integrity subsets of C and Ada, identify the most effective mutant types, and analyze the root causes of failures in test cases. Our findings show how mutation testing could be effective where traditional structural coverage analysis and manual peer review have failed. They also show that several testing issues have origins beyond the test activity, and this suggests improvements to the requirements definition and coding process. Our study also examines the relationship between program characteristics and mutation survival and considers how program size can provide a means for targeting test areas most likely to have dormant faults. Industry feedback is also provided, particularly on how mutation testing can be integrated into a typical verification life cycle of airborne software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6298894","Mutation;safety-critical software;verification;testing;certification","Testing;Certification;Software systems;Safety;Industries;Guidelines","Ada;aerospace computing;C language;certification;integrated software;program testing;program verification;safety-critical software;software quality","software test quality;safety-critical software;test coverage level;safety standard;industry guideline;test sufficiency measurement;empirical evaluation;mutation testing;airborne software system;certification;coverage requirement satisfaction;C;Ada;software integration;mutant type;software failure;structural coverage analysis;coding process;verification life cycle","","18","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Logical clock requirements for reverse engineering scenarios from a distributed system","C. E. Hrischuk; C. M. Woodside","Rational Software, Woodinville, WA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","4","321","339","To reverse engineer scenarios from event traces, one must infer causal relationships between events. The inferences are usually based on a trace with sequence numbers or timestamps corresponding to some kind of logical clock. In practice, there is an explosion of potentially causal relationships in the trace, which limits one's ability to extract scenarios. This work defines a more parsimonious form of causality called scenario causality that concentrates on certain major causal relationships and ignores more subtle, potentially causal links. The influence of an event is restricted to the particular scenario it is part of. An event which is not a message reception is defined to be caused by the previous event in the same software object, while a message reception is caused by a sending event in another object. The events are ordered to form a scenario event graph where typed nodes are events and the typed edges are certain causal relationships. Intuitively, we might say that most logical clocks, which identify events which ""happened before"" a given event and, thus, are potentially causal, give an upper bound on the set of causal events; scenario causality identifies a lower bound. The much smaller lower bound set makes it possible to reverse engineer and automate the analysis of scenarios.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995416","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995416","","Clocks;Reverse engineering","reverse engineering;clocks;distributed programming;causality;program diagnostics;graph grammars;program debugging;software engineering","logical clock requirements;scenario reverse engineering;distributed system;event traces;causal relationship inference;trace sequence numbers;timestamps;parsimonious causality;scenario causality;message reception;software objects;sending event;scenario event graph;typed nodes;typed edges;upper bound;lower bound;automatic scenario analysis;causal order;software tracing;graph grammar;trace analysis;distributed programming;event labeling;debugging;World Wide Web services","","3","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Success and failure factors in software reuse","M. Morisio; M. Ezran; C. Tully","Dipt. Autom. e Inf., Torino Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","2002","28","4","340","357","This paper aims at identifying some of the key factors in adopting or running a company-wide software reuse program. Key factors are derived from empirical evidence of reuse practices, as emerged from a survey of projects for the introduction of reuse in European companies: 24 such projects performed from 1994 to 1997 were analyzed using structured interviews. The projects were undertaken in both large and small companies, working in a variety of business domains, and using both object-oriented and procedural development approaches. Most of them produce software with high commonality between applications, and have at least reasonably mature processes. Despite that apparent potential for success, around one-third of the projects failed. Three main causes of failure were not introducing reuse-specific processes, not modifying nonreuse processes, and not considering human factors. The root cause was a lack of commitment by top management, or nonawareness of the importance of those factors, often coupled with the belief that using the object-oriented approach or setting up a repository seamlessly is all that is necessary to achieve success in reuse. Conversely, successes were achieved when, given a potential for reuse because of commonality among applications, management committed to introducing reuse processes, modifying nonreuse processes, and addressing human factors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995420","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995420","","Companies;Human factors;Performance analysis;Application software","software reusability;object-oriented programming;software development management;human factors","company-wide software reuse program;survey;European companies;small companies;large companies;business;object-oriented programming;project failure;procedural development;human factors;top management","","106","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Composite Constant Propagation and its Application to Android Program Analysis","D. Octeau; D. Luchaup; S. Jha; P. McDaniel","Department of Computer Sciences, University of Wisconsin, Madison, WI; CyLab, Carnegie Mellon University, Pittsburgh, PA; Department of Computer Sciences, University of Wisconsin, Madison, WI; Department of Computer Science and Engineering, Pennsylvania State University, University Park, PA","IEEE Transactions on Software Engineering","","2016","42","11","999","1014","Many program analyses require statically inferring the possible values of composite types. However, current approaches either do not account for correlations between object fields or do so in an ad hoc manner. In this paper, we introduce the problem of composite constant propagation. We develop the first generic solver that infers all possible values of complex objects in an interprocedural, flow and context-sensitive manner, taking field correlations into account. Composite constant propagation problems are specified using COAL, a declarative language. We apply our COAL solver to the problem of inferring Android Inter-Component Communication (ICC) values, which is required to understand how the components of Android applications interact. Using COAL, we model ICC objects in Android more thoroughly than the state-of-the-art. We compute ICC values for 489 applications from the Google Play store. The ICC values we infer are substantially more precise than previous work. The analysis is efficient, taking two minutes per application on average. While this work can be used as the basis for many whole-program analyses of Android applications, the COAL solver can also be used to infer the values of composite objects in many other contexts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2550446","National Science Foundation; National Science Foundation; Google; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7447806","Composite constant;constant propagation;inter-component communication;ICC;Android application analysis","Androids;Humanoid robots;Coal;Correlation;Context;Object oriented modeling;Receivers","Android (operating system);mobile computing;program diagnostics","composite constant propagation;Android program analysis;COAL declarative language;Android inter-component communication value;ICC values;Google Play Store;whole-program analysis;Android applications","","5","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Fragment class analysis for testing of polymorphism in Java software","A. Rountev; A. Milanova; B. G. Ryder","Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","6","372","387","Testing of polymorphism in object-oriented software may require coverage of all possible bindings of receiver classes and target methods at call sites. Tools that measure this coverage need to use class analysis to compute the coverage requirements. However, traditional whole-program class analysis cannot be used when testing incomplete programs. To solve this problem, we present a general approach for adapting whole-program class analyses to operate on program fragments. Furthermore, since analysis precision is critical for coverage tools, we provide precision measurements for several analyses by determining which of the computed coverage requirements are actually feasible for a set of subject components. Our work enables the use of whole-program class analyses for testing of polymorphism in partial programs, and identifies analyses that potentially are good candidates for use in coverage tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321060","Program analysis;class analysis;test coverage;object-oriented software.","Software testing;Java;Computer Society;Collaboration;Programming profession;Contracts;Fault detection;Performance evaluation;Guidelines;Computer science","Java;program testing;object-oriented methods;program diagnostics","fragment class analysis;polymorphism testing;Java software;object-oriented software;whole-program class analysis;partial program","","19","","46","","","","","","IEEE","IEEE Journals & Magazines"
"BLISS: Improved Symbolic Execution by Bounded Lazy Initialization with SAT Support","N. Rosner; J. Geldenhuys; N. M. Aguirre; W. Visser; M. F. Frias","Department of Computer Science, FCEyN, Universidad de Buenos Aires, Buenos Aires, Argentina; Department of Computer Science, University of Stellenbosch, Stellenbosch, South Africa; Department of Computer Science, FCEFQyN, Universidad Nacional de Rio Cuarto, and CONICET, Río Cuarto, Argentina; Department of Computer Science, University of Stellenbosch, Stellenbosch, South Africa; Department of Software Engineering, Instituto Tecnológico de Buenos Aires, and CONICET, Buenos Aires, Argentina","IEEE Transactions on Software Engineering","","2015","41","7","639","660","Lazy Initialization (LI) allows symbolic execution to effectively deal with heap-allocated data structures, thanks to a significant reduction in spurious and redundant symbolic structures. Bounded lazy initialization (BLI) improves on LI by taking advantage of precomputed relational bounds on the interpretation of class fields in order to reduce the number of spurious structures even further. In this paper we present bounded lazy initialization with SAT support (BLISS), a novel technique that refines the search for valid structures during the symbolic execution process. BLISS builds upon BLI, extending it with field bound refinement and satisfiability checks. Field bounds are refined while a symbolic structure is concretized, avoiding cases that, due to the concrete part of the heap and the field bounds, can be deemed redundant. Satisfiability checks on refined symbolic heaps allow us to prune these heaps as soon as they are identified as infeasible, i.e., as soon as it can be confirmed that they cannot be extended to any valid concrete heap. Compared to LI and BLI, BLISS reduces the time required by LI by up to four orders of magnitude for the most complex data structures. Moreover, the number of partially symbolic structures obtained by exploring program paths is reduced by BLISS by over 50 percent, with reductions of over 90 percent in some cases (compared to LI). BLISS uses less memory than LI and BLI, which enables the exploration of states unreachable by previous techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2389225","NPRP; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7004061","Symbolic execution;lazy initialization;tight field bounds;Symbolic PathFinder","Concrete;Binary trees;Java;Periodic structures;Software","computability;data structures;program verification","BLISS;symbolic execution;bounded lazy initialization with SAT support;heap-allocated data structures;symbolic structures;relational bounds;class fields;field bound refinement;satisfiability check;symbolic heap;program path","","4","","26","","","","","","IEEE","IEEE Journals & Magazines"
"A Data Mining Approach for Detecting Higher-Level Clones in Software","H. A. Basit; S. Jarzabek","Lahore University of Management Sciences, Lahore; National University of Singapore, Singapore","IEEE Transactions on Software Engineering","","2009","35","4","497","514","Code clones are similar program structures recurring in variant forms in software system(s). Several techniques have been proposed to detect similar code fragments in software, so-called simple clones. Identification and subsequent unification of simple clones is beneficial in software maintenance. Even further gains can be obtained by elevating the level of code clone analysis. We observed that recurring patterns of simple clones often indicate the presence of interesting higher-level similarities that we call structural clones. Structural clones show a bigger picture of similarity situation than simple clones alone. Being logical groups of simple clones, structural clones alleviate the problem of huge number of clones typically reported by simple clone detection tools, a problem that is often dealt with postdetection visualization techniques. Detection of structural clones can help in understanding the design of the system for better maintenance and in reengineering for reuse, among other uses. In this paper, we propose a technique to detect some useful types of structural clones. The novelty of our approach includes the formulation of the structural clone concept and the application of data mining techniques to detect these higher-level similarities. We describe a tool called clone miner that implements our proposed technique. We assess the usefulness and scalability of the proposed techniques via several case studies. We discuss various usage scenarios to demonstrate in what ways the knowledge of structural clones adds value to the analysis based on simple clones alone.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4796208","Design concepts;maintainability;restructuring;reverse engineering;reengineering;reusable software.","Data mining;Cloning;Software systems;Software maintenance;Collaboration;Portals;Computer Society;Visualization;Scalability;Reverse engineering","data mining;software maintenance;software reusability","data mining approach;higher-level clone detection;program structures;software system;software maintenance;code clone analysis;postdetection visualization techniques;software reusability","","44","","53","","","","","","IEEE","IEEE Journals & Magazines"
"A Comprehensive Approach to Naming and Accessibility in Refactoring Java Programs","M. Schäfer; A. Thies; F. Steimann; F. Tip","IBM T.J. Watson Research Center, Hawthorne; Fernuniversität in Hagen, Hagen; Fernuniversität in Hagen, Hagen; IBM, Hawthorne","IEEE Transactions on Software Engineering","","2012","38","6","1233","1257","Automated tool support for refactoring is now widely available for mainstream programming languages such as Java. However, current refactoring tools are still quite fragile in practice and often fail to preserve program behavior or compilability. This is mainly because analyzing and transforming source code requires consideration of many language features that complicate program analysis, in particular intricate name lookup and access control rules. This paper introduces J<sub>L</sub>, a lookup-free, access control-free representation of Java programs. We present algorithms for translating Java programs into J<sub>L</sub>and vice versa, thereby making it possible to formulate refactorings entirely at the level of J<sub>L</sub>and to rely on the translations to take care of naming and accessibility issues. We demonstrate how complex refactorings become more robust and powerful when lifted to J<sub>L</sub>. Our approach has been implemented using the JastAddJ compiler framework, and evaluated by systematically performing two commonly used refactorings on an extensive suite of real-world Java applications. The evaluation shows that our tool correctly handles many cases where current refactoring tools fail to handle the complex rules for name binding and accessibility in Java.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6152131","Restructuring;reverse engineering;and reengineering;object-oriented languages;Java","Java;Access control;Feature extraction;Reverse engineering;Object oriented programming;Shadow mapping;Program processors","authorisation;Java;naming services;program compilers;program diagnostics;software maintenance","comprehensive approach;naming issues;accessibility issues;Java program refactoring;mainstream programming languages;source code analysis;source code transformation;language features;program analysis;name lookup;access control rules;JL;lookup-free access control-free representation;JastAddJ compiler framework","","8","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Xstream: a middleware for streaming XML contents over wireless environments","E. Y. C. Wong; A. T. S. Chan; Hong Va Leong","Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China; Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China; Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China","IEEE Transactions on Software Engineering","","2004","30","12","918","935","XML (extensible Markup Language) has been developed and deployed by domain-specific standardization bodies and commercial companies. Studies have been conducted on a wide variety of issues encompassing XML. In the use of XML for wireless computing, the focus has been on investigating ways to efficiently represent XML data for transmission over a wireless environment. We propose a middleware, Xstream (XML Streaming), for efficiently streaming XML contents over a wireless environment by leveraging the rich semantics and structural characteristics of XML documents and by flexibly managing units containing fragments of data into autonomous units, known as XDU (Xstream Data Unit) fragments. The concept of an XDU is fundamental to the operation of Xstream. It provides for the efficient transfer of documents across a wireless link and allows other issues and challenges pertaining to wireless transmission to be addressed. By fragmenting and organizing an XML document into XDU fragments, we are able to incrementally send fragments across a wireless link, while the receiver is able to perform look-ahead processing of the document without having to wait for the entire document to be downloaded. We propose a fragmenting strategy based on the value of the wireless link's Maximum Transfer Units (MTUs). In addition, we present and evaluate several packetizing strategies, i.e., strategies wherein a collection of XDUs are grouped into a packet to optimize packet delivery and processing. At the receiving end of this process, a reassembly strategy incrementally reconstructs the XML document as XDU fragments are being received, thereby facilitating client application implementation of look-ahead processing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.108","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377189","Index Terms- XML;streaming;multiresolution;wireless;fragmentation;packetizing;middleware.","Middleware;XML;Wireless application protocol;Markup languages;Mobile computing;Standardization;Content management;Environmental management;Organizing","XML;middleware;mobile computing;multimedia computing;formal specification","Xstream middleware;XML content streaming;wireless environments;extensible Markup Language;domain-specific standardization;wireless computing;XML documents;Xstream Data Unit fragments;Maximum Transfer Units;packetizing strategies","","5","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluation of Accuracy in Design Pattern Occurrence Detection","N. Pettersson; W. Lowe; J. Nivre","Växjö University, Växjö; Växjö University, Växjö; Växjö University, Växjö","IEEE Transactions on Software Engineering","","2010","36","4","575","590","Detection of design pattern occurrences is part of several solutions to software engineering problems, and high accuracy of detection is important to help solve the actual problems. The improvement in accuracy of design pattern occurrence detection requires some way of evaluating various approaches. Currently, there are several different methods used in the community to evaluate accuracy. We show that these differences may greatly influence the accuracy results, which makes it nearly impossible to compare the quality of different techniques. We propose a benchmark suite to improve the situation and a community effort to contribute to, and evolve, the benchmark suite. Also, we propose fine-grained metrics assessing the accuracy of various approaches in the benchmark suite. This allows comparing the detection techniques and helps improve the accuracy of detecting design pattern occurrences.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.92","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374428","Patterns;object-oriented design methods;measurement techniques;evaluation;reverse engineering;reengineering;restructuring.","Design methodology;Software systems;Computer science;Natural languages;Software engineering;Measurement techniques;Reverse engineering;Software tools;Application software;Software quality","object-oriented methods;software engineering;software metrics;software performance evaluation","design pattern occurrence detection;software engineering problem;fine grained metric;benchmark suite","","33","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Refactoring the aspectizable interfaces: an empirical assessment","P. Tonella; M. Ceccato","Centro per la Ricerca Scientifica e Tecnologica, ITC, Povo, Italy; Centro per la Ricerca Scientifica e Tecnologica, ITC, Povo, Italy","IEEE Transactions on Software Engineering","","2005","31","10","819","832","Aspect oriented programming aims at addressing the problem of the crosscutting concerns, i.e., those functionalities that are scattered among several modules in a given system. Aspects can be defined to modularize such concerns. In this work, we focus on a specific kind of crosscutting concerns, the scattered implementation of methods declared by interfaces that do not belong to the principal decomposition. We call such interfaces aspectizable. All the aspectizable interfaces identified within a large number of classes from the Java Standard Library and from three Java applications have been automatically migrated to aspects. To assess the effects of the migration on the internal and external quality attributes of these systems, we collected a set of metrics and we conducted an empirical study, in which some maintenance tasks were executed on the two alternative versions (with and without aspects) of the same system. In this paper, we report the results of such a comparison.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.115","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542065","Index Terms- Aspect oriented programming;refactoring;program transformations;empirical studies.","Scattering;Java;Functional programming;Libraries;Object oriented programming;Packaging;Containers","object-oriented programming;software metrics;Java;software libraries;software quality;software maintenance;application program interfaces","aspectizable interfaces;aspect oriented programming;crosscutting concerns;Java Standard Library;software metrics;software maintenance","","24","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Verifying the Evolution of Probability Distributions Governed by a DTMC","Y. Kwon; G. Agha","Microsoft Corporation, Redmond; University of Illinois at Urbana-Champaign, Urbana","IEEE Transactions on Software Engineering","","2011","37","1","126","141","We propose a new probabilistic temporal logic, iLTL, which captures properties of systems whose state can be represented by probability mass functions (pmfs). Using iLTL, we can specify reachability to a state (i.e., a pmf), as well as properties representing the aggregate (expected) behavior of a system. We then consider a class of systems whose transitions are governed by a Markov Chain-in this case, the set of states a system may be in is specified by the transitions of pmfs from all potential initial states to the final state. We then provide a model checking algorithm to check iLTL properties of such systems. Unlike existing model checking techniques, which either compute the portions of the computational paths that satisfy a specification or evaluate properties along a single path of pmf transitions, our model checking technique enables us to do a complete analysis on the expected behaviors of large-scale systems. Desirable system parameters may also be found as a counterexample of a negated goal. Finally, we illustrate the usefulness of iLTL model checking by means of two examples: assessing software reliability and ensuring the results of administering a drug.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.80","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5557891","Probabilistic model checking;linear temporal logic;Discrete Time Markov Chain;pharmacokinetics.","Markov processes;Limiting;Eigenvalues and eigenfunctions;Computational modeling;Transient analysis;Probability distribution;Steady-state","formal verification;Markov processes;statistical distributions;temporal logic","probability distribution;DTMC;temporal logic;iLTL;probability mass function;model checking;large scale system;discrete time Markov chain","","6","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Synthesis of behavioral models from scenarios","S. Uchitel; J. Kramer; J. Magee","Dept. of Comput., Imperial Coll., London, UK; Dept. of Comput., Imperial Coll., London, UK; Dept. of Comput., Imperial Coll., London, UK","IEEE Transactions on Software Engineering","","2003","29","2","99","115","Scenario-based specifications such as Message Sequence Charts (MSCs) are useful as part of a requirements specification. A scenario is a partial story, describing how system components, the environment, and users work concurrently and interact in order to provide system level functionality. Scenarios need to be combined to provide a more complete description of system behavior. Consequently, scenario synthesis is central to the effective use of scenario descriptions. How should a set of scenarios be interpreted? How do they relate to one another? What is the underlying semantics? What assumptions are made when synthesizing behavior models from multiple scenarios? In this paper, we present an approach to scenario synthesis based on a clear sound semantics, which can support and integrate many of the existing approaches to scenario synthesis. The contributions of the paper are threefold. We first define an MSC language with sound abstract semantics in terms of labeled transition systems and parallel composition. The language integrates existing approaches based on scenario composition by using high-level MSCs (hMSCs) and those based on state identification by introducing explicit component state labeling. This combination allows stakeholders to break up scenario specifications into manageable parts and reuse scenarios using hMCSs; it also allows them to introduce additional domain-specific information and general assumptions explicitly into the scenario specification using state labels. Second, we provide a sound synthesis algorithm which translates scenarios into a behavioral specification in the form of Finite Sequential Processes. This specification can be analyzed with the Labeled Transition System Analyzer using model checking and animation. Finally, we demonstrate how many of the assumptions embedded in existing synthesis approaches can be made explicit and modeled in our approach. Thus, we provide the basis for a common approach to scenario-based specification, synthesis, and analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178048","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178048","","Computer Society;Switches;Labeling;Animation;Software engineering;Unified modeling language","formal specification;specification languages","scenario-based specifications;message sequence charts;requirements specification;semantics;multiple scenarios;scenario synthesis;labeled transition systems;parallel composition;explicit component state labeling;synthesis algorithm;behavioral specification;finite sequential processes;labeled transition system analyzer","","124","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Reasons for software effort estimation error: impact of respondent role, information collection approach, and data analysis method","M. Jorgensen; K. Molokken-Ostvold","Simula Res. Lab., Lysaker, Norway; Simula Res. Lab., Lysaker, Norway","IEEE Transactions on Software Engineering","","2004","30","12","993","1007","This study aims to improve analyses of why errors occur in software effort estimation. Within one software development company, we collected information about estimation errors through: 1) interviews with employees in different roles who are responsible for estimation, 2) estimation experience reports from 68 completed projects, and 3) statistical analysis of relations between characteristics of the 68 completed projects and estimation error. We found that the role of the respondents, the data collection approach, and the type of analysis had an important impact on the reasons given for estimation error. We found, for example, a strong tendency to perceive factors outside the respondents' own control as important reasons for inaccurate estimates. Reasons given for accurate estimates, on the other hand, typically cited factors that were within the respondents' own control and were determined by the estimators' skill or experience. This bias in types of reason means that the collection only of project managers' viewpoints will not yield balanced models of reasons for estimation error. Unfortunately, previous studies on reasons for estimation error have tended to collect information from project managers only. We recommend that software companies combine estimation error information from in-depth interviews with stakeholders in all relevant roles, estimation experience reports, and results from statistical analyses of project characteristics","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.103","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377193","Index Terms- Cost estimation;review and evaluation;performance evaluation.","Estimation error;Data analysis;Project management;Statistical analysis;Computer errors;Information analysis;Programming;Error analysis;Information management","cost-benefit analysis;error handling;project management;software cost estimation;software development management;software performance evaluation","software effort estimation error;information collection approach;data analysis;software development;statistical analysis;project management;interviews;cost estimation;project evaluation;software review;performance evaluation","","40","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods","G. J. Pai; J. Bechta Dugan","Fraunhofer Institute for Experimental Software Engineering (IESE), Fraunhofer-Platz 1, 67663 Kaiserslautern, Germany; Charles L. Brown Department of Electrical and Computer Engineering, University of Virginia, 351 McCormick Road, PO Box 400743, Charlottesville, VA 22904-4743","IEEE Transactions on Software Engineering","","2007","33","10","675","686","We present a methodology for Bayesian analysis of software quality. We cast our research in the broader context of constructing a causal framework that can include process, product, and other diverse sources of information regarding fault introduction during the software development process. In this paper, we discuss the aspect of relating internal product metrics to external quality metrics. Specifically, we build a Bayesian network (BN) model to relate object-oriented software metrics to software fault content and fault proneness. Assuming that the relationship can be described as a generalized linear model, we derive parametric functional forms for the target node conditional distributions in the BN. These functional forms are shown to be able to represent linear, Poisson, and binomial logistic regression. The models are empirically evaluated using a public domain data set from a software subsystem. The results show that our approach produces statistically significant estimations and that our overall modeling method performs no worse than existing techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70722","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302779","Bayesian analysis;Bayesian networks;defects;fault proneness;metrics;object-oriented;regression;software quality","Bayesian methods;Software quality;Object oriented modeling;Software measurement;Programming;Quality assessment;Information resources;Software metrics;Logistics","Bayes methods;mathematics computing;object-oriented methods;regression analysis;software fault tolerance;software metrics;software quality","binomial logistic regression;Poisson logistic regression;linear logistic regression;object-oriented software metrics;Bayesian network;software quality;software fault proneness;software fault content","","86","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Quantitative analysis of faults and failures in a complex software system","N. E. Fenton; N. Ohlsson","Dept. of Comput. Sci., Queen Mary & Westfield Coll., London, UK; NA","IEEE Transactions on Software Engineering","","2000","26","8","797","814","The authors describe a number of results from a quantitative study of faults and failures in two releases of a major commercial software system. They tested a range of basic software engineering hypotheses relating to: the Pareto principle of distribution of faults and failures; the use of early fault data to predict later fault and failure data; metrics for fault prediction; and benchmarking fault data. For example, we found strong evidence that a small number of modules contain most of the faults discovered in prerelease testing and that a very small number of modules contain most of the faults discovered in operation. We found no evidence to support previous claims relating module size to fault density nor did we find evidence that popular complexity metrics are good predictors of either fault-prone or failure-prone modules. We confirmed that the number of faults discovered in prerelease testing is an order of magnitude greater than the number discovered in 12 months of operational use. The most important result was strong evidence of a counter-intuitive relationship between pre- and postrelease faults; those modules which are the most fault-prone prerelease are among the least fault-prone postrelease, while conversely, the modules which are most fault-prone postrelease are among the least fault-prone prerelease. This observation has serious ramifications for the commonly used fault density measure. Our results provide data-points in building up an empirical picture of the software development process.","0098-5589;1939-3520;2326-3881","","10.1109/32.879815","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879815","","Failure analysis;Software systems;Density measurement;Software engineering;Software testing;Computer industry;Benchmark testing;Programming;Software metrics;Phase measurement","software performance evaluation;software metrics;software reliability","quantitative analysis;complex software system faults;quantitative study;commercial software system;basic software engineering hypotheses;Pareto principle;early fault data;failure data;software metrics;fault prediction;benchmarking;prerelease testing;module size;fault density;complexity metrics;failure-prone modules;operational use;counter-intuitive relationship;postrelease faults;fault-prone prerelease;fault-prone postrelease;fault density measure;data-points;software development process","","319","","46","","","","","","IEEE","IEEE Journals & Magazines"
"A formally verified application-level framework for real-time scheduling on POSIX real-time operating systems","P. Li; Binoy Ravindran; S. Suhaib; S. Feizabadi","Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., USA; Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., USA; Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., USA; Dept. of Electr. & Comput. Eng., Virginia Polytech. Inst. & State Univ., USA","IEEE Transactions on Software Engineering","","2004","30","9","613","629","We present a framework, called meta scheduler, for implementing real-time scheduling algorithms. The meta scheduler is a portable middleware layer component designed for implementations over POSIX-compliant operating systems. It accommodates pluggable real-time scheduling algorithms while offering the flexibility of platform independence - the singular underlying OS requirement is the now nearly ubiquitous POSIX compliance. The versatility of pluggable schedulers positions the meta scheduler for deployment in an interoperable heterogeneous real-time environment. We present the design of the meta scheduler and outline its implementation. Furthermore, we present a mechanized correctness verification using the UPPAAL model checker. Prototype implementation of the meta scheduler over QNX Neutrino real-time operating system demonstrates high performance and a small footprint.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324648","Index Terms- Real-time scheduling;time/utility functions;utility accrual scheduling;Portable Operating System Interface (POSIX);model checking.","Real time systems;Operating systems;Scheduling algorithm;Job shop scheduling;Time factors;Processor scheduling;Middleware;Prototypes;Neutrino sources;Adaptive systems","real-time systems;middleware;formal verification;scheduling;network operating systems;distributed object management","meta scheduler;real-time scheduling;portable middleware layer component;interoperable heterogeneous real-time environment;formal verification;UPPAAL model checker;QNX Neutrino real-time operating system;utility functions;utility accrual scheduling;portable operating system interface","","36","","45","","","","","","IEEE","IEEE Journals & Magazines"
"What Do We Know about the Effectiveness of Software Design Patterns?","C. Zhang; D. Budgen","Durham University, Durham; Durham University, Durham","IEEE Transactions on Software Engineering","","2012","38","5","1213","1231","Context. Although research in software engineering largely seeks to improve the practices and products of software development, many practices are based upon codification of expert knowledge, often with little or no underpinning from objective empirical evidence. Software design patterns seek to codify expert knowledge to share experience about successful design structures. Objectives. To investigate how extensively the use of software design patterns has been subjected to empirical study and what evidence is available about how and when their use can provide an effective mechanism for knowledge transfer about design. Method. We conducted a systematic literature review in the form of a mapping study, searching the literature up to the end of 2009 to identify relevant primary studies about the use of the 23 patterns catalogued in the widely referenced book by the “Gang of Four.” These studies were then categorized according to the forms of study employed, the patterns that were studied, as well as the context within which the study took place. Results. Our searches identified 611 candidate papers. Applying our inclusion/exclusion criteria resulted in a final set of 10 papers that described 11 instances of “formal” experimental studies of object-oriented design patterns. We augmented our analysis by including seven “experience” reports that described application of patterns using less rigorous observational forms. We report and review the profiles of the empirical evidence for those patterns for which multiple studies exist. Conclusions. We could not identify firm support for any of the claims made for patterns in general, although there was some support for the usefulness of patterns in providing a framework for maintenance, and some qualitative indication that they do not help novices learn about design. For future studies we recommend that researchers use case studies that focus upon some key patterns, and seek to identify the impact that their use can have upon maintenance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975176","Design patterns;systematic literature review;empirical software engineering","Software engineering;Software design;Systematics;Search engines;Terminology;Maintenance engineering","object-oriented programming;software maintenance","software design patterns;software engineering;software development;design structures;knowledge transfer;Gang-of-Four;object-oriented design patterns","","33","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Static Analysis of Model Transformations","J. S. Cuadrado; E. Guerra; J. de Lara","Department of Languages and Systems, Universidad de Murcia, Murcia, Spain; Department of Computer Science, Universidad Autónoma de Madrid, Madrid, Spain; Department of Computer Science, Universidad Autónoma de Madrid, Madrid, Spain","IEEE Transactions on Software Engineering","","2017","43","9","868","897","Model transformations are central to Model-Driven Engineering (MDE), where they are used to transform models between different languages; to refactor and simulate models; or to generate code from models. Thus, given their prominent role in MDE, practical methods helping in detecting errors in transformations and automate their verification are needed. In this paper, we present a method for the static analysis of ATL model transformations. The method aims at discovering typing and rule errors, like unresolved bindings, uninitialized features or rule conflicts. It relies on static analysis and type inference, and uses constraint solving to assert whether a source model triggering the execution of a given problematic statement can possibly exist. Our method is supported by a tool that integrates seamlessly with the ATL development environment. To evaluate the usefulness of our method, we have used it to analyse a public repository of ATL transformations. The high number of errors discovered shows that static analysis of ATL transformations is needed in practice. Moreover, we have measured the precision and recall of the method by considering a synthetic set of transformations obtained by mutation techniques, and comparing with random testing. The experiment shows good overall results in terms of false positives and negatives.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2635137","Spanish MINECO; R&D programme of the Madrid Region; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7765073","Model-driven engineering;model transformation;ATL;static analysis;model finders;verification and testing","Unified modeling language;Analytical models;Testing;Model driven engineering;Transforms;Manuals;Computational modeling","constraint handling;error detection;formal verification;inference mechanisms;program diagnostics;random functions","static analysis;model-driven engineering;MDE;error detection;verification automation;ATL model transformations;typing errors;rule errors;unresolved bindings;uninitialized features;rule conflicts;type inference;constraint solving;public repository analysis;mutation techniques;random testing","","2","","70","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"What Industry Needs from Architectural Languages: A Survey","I. Malavolta; P. Lago; H. Muccini; P. Pelliccione; A. Tang","Università dell'Aquila, Italy; VU University Amsterdam, Amsterdam; Università dell'Aquila, Italy; Università dell'Aquila, Italy; Swinburne University of Technology, Melbourne","IEEE Transactions on Software Engineering","","2013","39","6","869","891","Many times we are faced with the proliferation of definitions, concepts, languages, and tools in certain (research) topics. But often there is a gap between what is provided by existing technologies and what is needed by their users. The strengths, limitations, and needs of the available technologies can be dubious. The same applies to software architectures, and specifically to languages designed to represent architectural models. Tens of different architectural languages have been introduced by the research and industrial communities in the last two decades. However, it is unclear if they fulfill the user's perceived needs in architectural description. As a way to plan for next generation languages for architectural description, this study analyzes practitioners' perceived strengths, limitations, and needs associated with existing languages for software architecture modeling in industry. We run a survey by interviewing 48 practitioners from 40 different IT companies in 15 countries. Each participant is asked to fill in a questionnaire of 51 questions. By analyzing the data collected through this study, we have concluded that 1) while practitioners are generally satisfied with the design capabilities provided by the languages they use, they are dissatisfied with the architectural language analysis features and their abilities to define extra-functional properties; 2) architectural languages used in practice mostly originate from industrial development instead of from academic research; 3) more formality and better usability are required of an architectural language.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.74","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6374194","Software architecture;architecture description languages;ADL;survey;empirical study","Unified modeling language;Software architecture;Computer architecture;Industries;Communities;Software;Google","data analysis;software architecture;specification languages","architectural languages;software architectures;architectural models;next generation languages;architectural description;data collection analysis;industrial development","","95","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Pair Programming with Respect to System Complexity and Programmer Expertise","E. Arisholm; H. Gallis; T. Dyba; D. I. K. Sjoberg","Simula Research Laboratory, PO Box 134, NO-1325 Lysaker, Norway; Simula Research Laboratory, PO Box 134, NO-1325 Lysaker, Norway; Simula Research Laboratory and SINTEF Information and Communication Technology, NO-7465 Trondheim, Norway; Simula Research Laboratory, PO Box 134, NO-1325 Lysaker, Norway","IEEE Transactions on Software Engineering","","2007","33","2","65","86","A total of 295 junior, intermediate, and senior professional Java consultants (99 individuals and 98 pairs) from 29 international consultancy companies in Norway, Sweden, and the UK were hired for one day to participate in a controlled experiment on pair programming. The subjects used professional Java tools to perform several change tasks on two alternative Java systems with different degrees of complexity. The results of this experiment do not support the hypotheses that pair programming in general reduces the time required to solve the tasks correctly or increases the proportion of correct solutions. On the other hand, there is a significant 84 percent increase in effort to perform the tasks correctly. However, on the more complex system, the pair programmers had a 48 percent increase in the proportion of correct solutions but no significant differences in the time taken to solve the tasks correctly. For the simpler system, there was a 20 percent decrease in time taken but no significant differences in correctness. However, the moderating effect of system complexity depends on the programmer expertise of the subjects. The observed benefits of pair programming in terms of correctness on the complex system apply mainly to juniors, whereas the reductions in duration to perform the tasks correctly on the simple system apply mainly to intermediates and seniors. It is possible that the benefits of pair programming will exceed the results obtained in this experiment for larger, more complex tasks and if the pair programmers have a chance to work together over a longer period of time","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052584","Empirical software engineering;pair programming;extreme programming;design principles;control styles;object-oriented programming;software maintainability;quasi-experiment.","Programming profession;Time measurement;Java;Software engineering;Software maintenance;Keyboards;Cost function;Power measurement;Computer industry","Java;object-oriented programming;task analysis;team working","pair programming;system complexity;programmer expertise;Java","","115","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Entropy Based Software Reliability Analysis of Multi-Version Open Source Software","V. B. Singh; M. Sharma; H. Pham","University of Delhi, Delhi, India; University of Delhi, Delhi, India; Department of Industrial and Systems Engineering, Rutgers, State University of New Jersey, Piscataway, NJ","IEEE Transactions on Software Engineering","","2018","44","12","1207","1223","The number of issues fixed in the current release of the software is one of the factors which decides the next release of the software. The source code files get changed during fixing of these issues. The uncertainty arises due to these changes is quantified using entropy based measures. We developed a Non-Homogeneous Poisson Process model for Open Source Software to understand the fixing of issues across releases. Based on this model, optimal release-updating using entropy and maximizing the active user's satisfaction level subject to fixing of issues up to a desired level, is investigated as well. The proposed models have been validated on five products of the Apache open source project. The optimal release time estimated from the proposed model is close to the observed release time at different active user's satisfaction levels. The proposed decision model can assist management to appropriately determine the optimal release-update time. The proposed entropy based model for issues estimation shows improvement in performance for 21 releases out of total 23 releases, when compared with well-known traditional software reliability growth models, namely GO model [1] and S-shaped model [2] . The proposed model is also found statistically significant.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2766070","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8081836","Entropy;feature improvement;new feature;release time problem;software repositories;cobb-douglas","Entropy;Software reliability;Software product lines;Computer bugs;Open source software","entropy;program diagnostics;public domain software;software maintenance;software reliability;source code (software);stochastic processes","multiversion Open Source Software;source code files;NonHomogeneous Poisson Process model;Apache open source project;GO model;S-shaped model;software reliability analysis;entropy","","","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Techniques for Classifying Executions of Deployed Software to Support Software Engineering Tasks","M. Haran; A. Karr; M. Last; A. Orso; A. A. Porter; A. Sanil; S. Fouche","Department of Statistics at Pennsylvania State University, University Park, PA 16802; National Institute of Statistical Sciences, Research Triangle Park, NC 27709-4006; National Institute of Statistical Sciences, Research Triangle Park, NC 27709-4006; College of Computing at the Georgia Institute of Technology, Atlanta, GA 30332-0765; Computer Science Department, University of Maryland, College Park, MD 20814; National Institute of Statistical Sciences, Research Triangle Park, NC 27709-4006; Computer Science Department, University of Maryland, College Park, MD 20814","IEEE Transactions on Software Engineering","","2007","33","5","287","304","There is an increasing interest in techniques that support analysis and measurement of fielded software systems. These techniques typically deploy numerous instrumented instances of a software system, collect execution data when the instances run in the field, and analyze the remotely collected data to better understand the system's in-the-field behavior. One common need for these techniques is the ability to distinguish execution outcomes (e.g., to collect only data corresponding to some behavior or to determine how often and under which condition a specific behavior occurs). Most current approaches, however, do not perform any kind of classification of remote executions and either focus on easily observable behaviors (e.g., crashes) or assume that outcomes' classifications are externally provided (e.g., by the users). To address the limitations of existing approaches, we have developed three techniques for automatically classifying execution data as belonging to one of several classes. In this paper, we introduce our techniques and apply them to the binary classification of passing and failing behaviors. Our three techniques impose different overheads on program instances and, thus, each is appropriate for different application scenarios. We performed several empirical studies to evaluate and refine our techniques and to investigate the trade-offs among them. Our results show that 1) the first technique can build very accurate models, but requires a complete set of execution data; 2) the second technique produces slightly less accurate models, but needs only a small fraction of the total execution data; and 3) the third technique allows for even further cost reductions by building the models incrementally, but requires some sequential ordering of the software instances' instrumentation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1004","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160968","Execution classification;remote analysis/measurement.","Software engineering;Instruments;Software systems;Software measurement;Computer crashes;Debugging;Performance evaluation;Costs;Data analysis;Failure analysis","software metrics;software performance evaluation","software engineering;binary classification;program instances;execution classification","","22","","34","","","","","","IEEE","IEEE Journals & Magazines"
"The development and evaluation of three diverse techniques for object-oriented code inspection","A. Dunsmore; M. Roper; M. Wood","Abbey Mill Bus. Centre, Formedix, Paisley, UK; NA; NA","IEEE Transactions on Software Engineering","","2003","29","8","677","686","We describe the development and evaluation of a rigorous approach aimed at the effective and efficient inspection of object-oriented (OO) code. Since the time that inspections were developed they have been shown to be powerful defect detection strategies. However, little research has been done to investigate their application to OO systems, which have very different structural and execution models compared to procedural systems. This suggests that inspection techniques may not be currently being deployed to their best effect in the context of large-scale OO systems. Work to date has revealed three significant issues that need to be addressed - the identification of chunks of code to be inspected, the order in which the code is read, and the resolution of frequent nonlocal references. Three techniques are developed with the aim of addressing these issues: one based on a checklist, one focused on constructing abstract specifications, and the last centered on the route that a use case takes through a system. The three approaches are evaluated empirically and, in this instance, it is suggested that the checklist is the most effective approach, but that the other techniques also have potential strengths. For the best results in a practical situation, a combination of techniques is recommended, one of which should focus specifically on the characteristics of OO.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223643","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223643","","Inspection;Computer Society;Application software;Object oriented modeling;Power system modeling;Large-scale systems;Industrial control;Testing;Encapsulation;Libraries","inspection;software reviews;object-oriented programming;formal specification;software performance evaluation;software quality;program testing","software inspection technique;software quality evaluation;object-oriented code inspection;OO system;defect detection strategy;code chunk identification;nonlocal reference resolution;checklist technique;code review;abstract specification;use case technique;empirical study","","28","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Context-Aware Adaptive Applications: Fault Patterns and Their Automated Identification","M. Sama; S. Elbaum; F. Raimondi; D. S. Rosenblum; Z. Wang","University College London, UK; University of Nebraska, Lincoln, NE; University College London, UK; University College, London, UK; University of Nebraska, Lincoln, NE","IEEE Transactions on Software Engineering","","2010","36","5","644","661","Applications running on mobile devices are intensely context-aware and adaptive. Streams of context values continuously drive these applications, making them very powerful but, at the same time, susceptible to undesired configurations. Such configurations are not easily exposed by existing validation techniques, thereby leading to new analysis and testing challenges. In this paper, we address some of these challenges by defining and applying a new model of adaptive behavior called an Adaptation Finite-State Machine (A-FSM) to enable the detection of faults caused by both erroneous adaptation logic and asynchronous updating of context information, with the latter leading to inconsistencies between the external physical context and its internal representation within an application. We identify a number of adaptation fault patterns, each describing a class of faulty behaviors. Finally, we describe three classes of algorithms to detect such faults automatically via analysis of the A-FSM. We evaluate our approach and the trade-offs between the classes of algorithms on a set of synthetically generated Context-Aware Adaptive Applications (CAAAs) and on a simple but realistic application in which a cell phone's configuration profile changes automatically as a result of changes to the user's location, speed, and surrounding environment. Our evaluation describes the faults our algorithms are able to detect and compares the algorithms in terms of their performance and storage requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5432224","Adaptation;context-awareness;fault detection;mobile computing;model-based analysis;model checking;ordered binary decision diagrams;symbolic verification;ubiquitous computing.","Fault diagnosis;Fault detection;Context modeling;Algorithm design and analysis;Handheld computers;Personal digital assistants;Data structures;Global Positioning System;Computer science;Lead","finite state machines;formal logic;mobile computing;program verification;software fault tolerance","context-aware adaptive applications;mobile devices;validation techniques;adaptation finite-state machine;A-FSM analysis;asynchronous information updating;fault pattern adaptation;cell phone;fault detection","","52","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Choosing Component Origins for Software Intensive Systems: In-House, COTS, OSS or Outsourcing?—A Case Survey","K. Petersen; D. Badampudi; S. M. A. Shah; K. Wnuk; T. Gorschek; E. Papatheocharous; J. Axelsson; S. Sentilles; I. Crnkovic; A. Cicchetti","Department of Software Engineering, Blekinge Institute of Technology, Campus Gräsvik, Karlskrona, Sweden; Blekinge Institute of Technology, Campus Gräsvik, Karlskrona, Sweden; SICS Swedish ICT AB, Kista, Sweden; Blekinge Institute of Technology, Campus Gräsvik, Karlskrona, Sweden; Blekinge Institute of Technology, Campus Gräsvik, Karlskrona, Sweden; SICS Swedish ICT AB, Kista, Sweden; SICS Swedish ICT AB, Kista, Sweden; Mälardalen University, Västerås, Sweden; Chalmers, Gothenberg, Sweden; Mälardalen University, Västerås, Sweden","IEEE Transactions on Software Engineering","","2018","44","3","237","261","The choice of which software component to use influences the success of a software system. Only a few empirical studies investigate how the choice of components is conducted in industrial practice. This is important to understand to tailor research solutions to the needs of the industry. Existing studies focus on the choice for off-the-shelf (OTS) components. It is, however, also important to understand the implications of the choice of alternative component sourcing options (CSOs), such as outsourcing versus the use of OTS. Previous research has shown that the choice has major implications on the development process as well as on the ability to evolve the system. The objective of this study is to explore how decision making took place in industry to choose among CSOs. Overall, 22 industrial cases have been studied through a case survey. The results show that the solutions specifically for CSO decisions are deterministic and based on optimization approaches. The non-deterministic solutions proposed for architectural group decision making appear to suit the CSO decision making in industry better. Interestingly, the final decision was perceived negatively in nine cases and positively in seven cases, while in the remaining cases it was perceived as neither positive nor negative.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2677909","ORION project; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7870688","Decision making;in-house;COTS;OSS;outsourcing","Decision making;Outsourcing;Companies;Computer architecture;Software;Industries","decision making;object-oriented programming;outsourcing;software architecture","CSO decision making;software intensive systems;COTS;OSS;Outsourcing;software component;software system;tailor research solutions;off-the-shelf components;OTS;alternative component sourcing options;CSOs;CSO decisions;nondeterministic solutions;architectural group decision;component origins;architectural group decision making","","3","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Provable Protection against Web Application Vulnerabilities Related to Session Data Dependencies","L. Desmet; P. Verbaeten; W. Joosen; F. Piessens","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","50","64","Web applications are widely adopted and their correct functioning is mission critical for many businesses. At the same time, Web applications tend to be error prone and implementation vulnerabilities are readily and commonly exploited by attackers. The design of countermeasures that detect or prevent such vulnerabilities or protect against their exploitation is an important research challenge for the fields of software engineering and security engineering. In this paper, we focus on one specific type of implementation vulnerability, namely, broken dependencies on session data. This vulnerability can lead to a variety of erroneous behavior at runtime and can easily be triggered by a malicious user by applying attack techniques such as forceful browsing. This paper shows how to guarantee the absence of runtime errors due to broken dependencies on session data in Web applications. The proposed solution combines development-time program annotation, static verification, and runtime checking to provably protect against broken data dependencies. We have developed a prototype implementation of our approach, building on the JML annotation language and the existing static verification tool ESC/Java2, and we successfully applied our approach to a representative J2EE-based e-commerce application. We show that the annotation overhead is very small, that the performance of the fully automatic static verification is acceptable, and that the performance overhead of the runtime checking is limited.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70742","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359468","Software/Program Verification;Security;Security and Protection;Reliability;Data sharing;Web-based services;Web technologies;Software/Program Verification;Security;Security and Protection;Reliability;Data sharing;Web-based services;Web technologies","Protection;Runtime;Application software;Data security;Computer bugs;Computer errors;Software engineering;Databases;Computer crime;Information retrieval","programming languages;Web services","provable protection;Web application vulnerabilities;session data dependencies;software engineering;security engineering;JML annotation language;J2EE-based e-commerce application","","2","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Test-suite reduction and prioritization for modified condition/decision coverage","J. A. Jones; M. J. Harrold","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Transactions on Software Engineering","","2003","29","3","195","209","Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software according to some criterion as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. This paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of empirical studies of these algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183927","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183927","","Software testing;Software algorithms;Costs;System testing;Software maintenance;Computer Society;Safety;FAA;Performance evaluation;Software performance","program testing;performance evaluation","critical software;software testing;commercial airborne systems;test suites;modified condition/decision coverage;test-suite prioritization","","152","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers","K. Dejaeger; T. Verbraken; B. Baesens","Katholieke Universiteit Leuven, Leuven; Katholieke Universiteit Leuven, Leuven; Katholieke Universiteit Leuven, Leuven","IEEE Transactions on Software Engineering","","2013","39","2","237","257","Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network (BN) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to BN theory, is investigated. The results, both in terms of the AUC and the recently introduced H-measure, are rigorously tested using the statistical framework of Demšar. It is concluded that simple and comprehensible networks with less nodes can be constructed using BN classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6175912","Software fault prediction;Bayesian networks;classification;comprehensibility","Software;Predictive models;Bayesian methods;Measurement;Capability maturity model;Probability distribution;Machine learning","belief networks;feature extraction;learning (artificial intelligence);Markov processes;pattern classification;prediction theory;program testing;software fault tolerance;statistical analysis","software fault prediction models;Bayesian network classifiers;software testing;software development;faulty software code;machine learning literature;Naive Bayes classifier;citing predictive performance;BN classifiers;Markov blanket principle;feature selection;BN theory;AUC;introduced H-measure;statistical framework;Demsar;predictive performance;model selection","","72","","109","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing Regulatory Rules for Privacy and Security Requirements","T. Breaux; A. Antón","NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","5","20","Information practices that use personal, financial, and health-related information are governed by US laws and regulations to prevent unauthorized use and disclosure. To ensure compliance under the law, the security and privacy requirements of relevant software systems must properly be aligned with these regulations. However, these regulations describe stakeholder rules, called rights and obligations, in complex and sometimes ambiguous legal language. These ""rules"" are often precursors to software requirements that must undergo considerable refinement and analysis before they become implementable. To support the software engineering effort to derive security requirements from regulations, we present a methodology for directly extracting access rights and obligations from regulation texts. The methodology provides statement-level coverage for an entire regulatory document to consistently identify and infer six types of data access constraints, handle complex cross references, resolve ambiguities, and assign required priorities between access rights and obligations to avoid unlawful information disclosures. We present results from applying this methodology to the entire regulation text of the US Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70746","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359472","Requirements/Specifications;Security and Privacy Protection;Legal Aspects of Computing;Requirements/Specifications;Security and Privacy Protection;Legal Aspects of Computing","Privacy;Law;Information security;Permission;Software systems;Legal factors;Software engineering;Data security;Data mining;Insurance","legislation;security of data;software engineering","software system security;software system privacy;software engineering;US Health Insurance Portability and Accountability Act Privacy Rule","","148","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings","S. Lessmann; B. Baesens; C. Mues; S. Pietsch","University of Hamburg, Hamburg; K.U.Leuven, Leuven; University of Southampton, Southampton; University of Hamburg, Hamburg","IEEE Transactions on Software Engineering","","2008","34","4","485","496","Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527256","Complexity measures;Data mining;Formal methods;Statistical methods;Complexity measures;Data mining;Formal methods;Statistical methods","Predictive models;Software quality;Software testing;Statistical analysis;Large-scale systems;NASA;Software systems;Benchmark testing;Fault diagnosis;Convergence","benchmark testing;software quality;statistical testing","benchmarking classification models;software defect prediction;software quality;testing efficiency;predictive classification models;code attributes;fault-prone modules;metric-based classification;proprietary data sets;statistical testing procedures","","390","","67","","","","","","IEEE","IEEE Journals & Magazines"
"A Comparative Study of Software Model Checkers as Unit Testing Tools: An Industrial Case Study","M. Kim; Y. Kim; H. Kim","KAIST, Daejon; KAIST, Daejon; Samsung Electronics, Suwon","IEEE Transactions on Software Engineering","","2011","37","2","146","160","Conventional testing methods often fail to detect hidden flaws in complex embedded software such as device drivers or file systems. This deficiency incurs significant development and support/maintenance cost for the manufacturers. Model checking techniques have been proposed to compensate for the weaknesses of conventional testing methods through exhaustive analyses. Whereas conventional model checkers require manual effort to create an abstract target model, modern software model checkers remove this overhead by directly analyzing a target C program, and can be utilized as unit testing tools. However, since software model checkers are not fully mature yet, they have limitations according to the underlying technologies and tool implementations, potentially critical issues when applied in industrial projects. This paper reports our experience in applying Blast and CBMC to testing the components of a storage platform software for flash memory. Through this project, we analyzed the strong and weak points of two different software model checking technologies in the viewpoint of real-world industrial application-counterexample-guided abstraction refinement with predicate abstraction and SAT-based bounded analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5510242","Embedded software verification;software model checking;bounded model checking;CEGAR-based model checking;flash file systems.","Software tools;Software testing;Computer industry;Manufacturing industries;System testing;Embedded software;File systems;Costs;Flash memory;Refining","C language;program testing;program verification;storage management","software model checkers;unit testing tools;complex embedded software;model checking techniques;abstract target model;C program;Blast;CBMC;storage platform software;flash memory","","18","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Coordination Challenges in Large-Scale Software Development: A Case Study of Planning Misalignment in Hybrid Settings","S. Bick; K. Spohrer; R. Hoda; A. Scheerer; A. Heinzl","SAP SE, Walldorf, Germany; University of Mannheim, Mannheim, Germany; University of Auckland, Auckland, New Zealand; SAP SE, Walldorf, Germany; University of Mannheim, Mannheim, Germany","IEEE Transactions on Software Engineering","","2018","44","10","932","950","Achieving effective inter-team coordination is one of the most pressing challenges in large-scale software development. Hybrid approaches of traditional and agile development promise combining the overview and predictability of long-term planning on an inter-team level with the flexibility and adaptability of agile development on a team level. It is currently unclear, however, why such hybrids often fail. Our case study within a large software development unit of 13 teams at a global enterprise software company explores how and why a combination of traditional planning on an inter-team level and agile development on a team level can result in ineffective coordination. Based on a variety of data, including interviews with scrum masters, product owners, architects and senior management, and using Grounded Theory data analysis procedures, we identify a lack of dependency awareness across development teams as a key explanation of ineffective coordination. Our findings show how a lack of dependency awareness emerges from misaligned planning activities of specification, prioritization, estimation and allocation between agile team and traditional inter-team levels and ultimately prevents effective coordination. Knowing about these issues, large-scale hybrid projects in similar contexts can try to better align their planning activities across levels to improve dependency awareness and in turn achieve more effective coordination.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2730870","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7990187","Large-scale software development;agile;hybrid;inter-team coordination;dependency awareness;planning alignment;information systems development","Software;Planning;Agile software development;Companies;Task analysis;Interviews","data analysis;project management;software development management;software prototyping;team working","large-scale software development;agile development;long-term planning;inter-team level;software development unit;global enterprise software company;ineffective coordination;agile team;large-scale hybrid projects;coordination challenges;planning misalignment;hybrid settings;effective inter-team coordination;pressing challenges;scrum masters;product owners;architects;senior management;grounded theory data analysis procedures","","1","","97","","","","","","IEEE","IEEE Journals & Magazines"
"A Comparison of Six UML-Based Languages for Software Process Modeling","R. Bendraou; J. Jezequel; M. Gervais; X. Blanc","University of Pierre and Marie Curie (UPMC), Paris; IRISA, INRIA-Rennes Bretagne Atlantique, Rennes; University of Paris Ouest Nanterre La Défense, Paris; University of Pierre and Marie Curie (UPMC), Paris","IEEE Transactions on Software Engineering","","2010","36","5","662","675","Describing and managing activities, resources, and constraints of software development processes is a challenging goal for many organizations. A first generation of Software Process Modeling Languages (SPMLs) appeared in the 1990s but failed to gain broad industrial support. Recently, however, a second generation of SPMLs has appeared, leveraging the strong industrial interest for modeling languages such as UML. In this paper, we propose a comparison of these UML-based SPMLs. While not exhaustive, this comparison concentrates on SPMLs most representative of the various alternative approaches, ranging from UML-based framework specializations to full-blown executable metamodeling approaches. To support the comparison of these various approaches, we propose a frame gathering a set of requirements for process modeling, such as semantic richness, modularity, executability, conformity to the UML standard, and formality. Beyond discussing the relative merits of these approaches, we also evaluate the overall suitability of these UML-based SPMLs for software process modeling. Finally, we discuss the impact of these approaches on the current state of the practice, and conclude with lessons we have learned in doing this comparison.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.85","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5593045","Metamodeling;process modeling and execution;software process modeling languages;UML.","Unified modeling language;Software;Object oriented modeling;Analytical models;Semantics;Programming;Computational modeling","software engineering;Unified Modeling Language","UML based language;software development process;software process modeling language;UML based SPML;metamodeling approach","","37","","78","","","","","","IEEE","IEEE Journals & Magazines"
"An Observe-Model-Exercise* Paradigm to Test Event-Driven Systems with Undetermined Input Spaces","B. N. Nguyen; A. M. Memon","Department of Computer Science, University of Maryland, College Park; Department of Computer Science, University of Maryland, College Park","IEEE Transactions on Software Engineering","","2014","40","3","216","234","System testing of software applications with a graphical-user interface (GUI) front-end requires that sequences of GUI events, that sample the application's input space, be generated and executed as test cases on the GUI. However, the context-sensitive behavior of the GUI of most of today's non-trivial software applications makes it practically impossible to fully determine the software's input space. Consequently, GUI testers-both automated and manual-working with undetermined input spaces are, in some sense, blindly navigating the GUI, unknowingly missing allowable event sequences, and failing to realize that the GUI implementation may allow the execution of some disallowed sequences. In this paper, we develop a new paradigm for GUI testing, one that we call Observe-Model-Exercise* (OME*) to tackle the challenges of testing context-sensitive GUIs with undetermined input spaces. Starting with an incomplete model of the GUI's input space, a set of coverage elements to test, and test cases, OME* iteratively observes the existence of new events during execution of the test cases, expands the model of the GUI's input space, computes new coverage elements, and obtains new test cases to exercise the new elements. Our experiment with 8 open-source software subjects, more than 500,000 test cases running for almost 1,100 machine-days, shows that OME* is able to expand the test space on average by 464.11 percent; it detected 34 faults that had never been detected before.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2300857","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6714448","Test generation;user interfaces;quality concepts","Graphical user interfaces;Computational modeling;Blogs;Testing;Software;Context;Layout","graphical user interfaces;program testing;public domain software","observe-model-exercise paradigm;test event-driven system;undetermined input spaces;software system testing;graphical-user interface front-end;GUI context-sensitive behavior;open-source software subjects;test generation","","17","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing Semi-Automated Clustering Methods for Persona Development","J. Brickey; S. Walczak; T. Burgess","US Army, Combating Terrorism Center, West Point; University of Colorado Denver, Denver; United States Military Academy, West Point","IEEE Transactions on Software Engineering","","2012","38","3","537","546","Current and future information systems require a better understanding of the interactions between users and systems in order to improve system use and, ultimately, success. The use of personas as design tools is becoming more widespread as researchers and practitioners discover its benefits. This paper presents an empirical study comparing the performance of existing qualitative and quantitative clustering techniques for the task of identifying personas and grouping system users into those personas. A method based on Factor (Principal Components) Analysis performs better than two other methods which use Latent Semantic Analysis and Cluster Analysis as measured by similarity to expert manually defined clusters.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928355","Clustering;interaction styles;personas;user-centered design;user interfaces.","Manuals;Clustering methods;Principal component analysis;Software;Interviews;Humans;Organizations","information systems;pattern clustering;principal component analysis","semi-automated clustering methods;persona development;information systems;design tools;quantitative clustering techniques;qualitative clustering techniques;principal components analysis;latent semantic analysis","","4","","47","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient distributed deadlock avoidance algorithm for the AND model","Hui Wu; Wei-Ngan Chin; J. Jaffar","Sch. of Comput., Nat. Univ. of Singapore, Singapore; NA; NA","IEEE Transactions on Software Engineering","","2002","28","1","18","29","A new rank-based distributed deadlock avoidance algorithm for the AND resource request model is presented. Deadlocks are avoided by dynamically maintaining an invariant Con(WFG): For each pair of processes p/sub i/ and p/sub j/, p/sub i/ is allowed to wait for process p/sub j/ iff the rank of p/sub j/ is greater than that of p/sub i/ for the WFG (Wait-For Graph). Our algorithm neither restricts the order of resource requests nor needs a priori information about resource requests nor causes unnecessary abortion of processes. Multidimensional ranks, which are partially ordered and dynamically modified are used to drastically reduce the cost of maintaining Con(WFG). Our simulation results show that the performance of our algorithm is better than that of existing algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/32.979987","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979987","","System recovery","concurrency control;resource allocation;processor scheduling;distributed algorithms;graph theory","distributed deadlock avoidance algorithm;partially ordered rank;AND resource request model;invariant;WFG;Wait-For Graph;unnecessary process abortion;multidimensional ranks;partially ordered ranks;dynamically modified ranks;concurrency control","","8","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Stack and queue integrity on hostile platforms","P. T. Devanbu; S. G. Stubblebine","Dept. of Comput. Sci., California Univ., Davis, CA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","1","100","108","When computationally intensive tasks have to be carried out on trusted, but limited platforms such as smart cards, it becomes necessary to compensate for the limited resources (memory, CPU speed) by off-loading implementations of data structures onto an available (but insecure, untrusted) fast coprocessor. However, data structures such as stacks, queues, RAMs, and hash tables can be corrupted (and made to behave incorrectly) by a potentially hostile implementation platform or by an adversary knowing or choosing data structure operations. The paper examines approaches that can detect violations of data structure invariants, while placing limited demands on the resources of the secure computing platform.","0098-5589;1939-3520;2326-3881","","10.1109/32.979991","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979991","","Data structures;Smart cards;Random access memory;Coprocessors","data integrity;data structures;security of data","queue integrity;stack integrity;computationally intensive tasks;smart cards;data structures;fast coprocessor;hash tables;hostile implementation platform;secure computing platform;security;software protection","","7","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Approximate Structural Context Matching: An Approach to Recommend Relevant Examples","R. Holmes; R. J. Walker; G. C. Murphy","Department of Computer Science, University of Calgary, 2500 University Dr. NW, Calgary, Alberta, Canada T2N 1N4; Department of Computer Science, University of Calgary, 2500 University Dr. NW, Calgary, Alberta, Canada T2N 1N4; Department of Computer Science, University of British Columbia, 201-2366 Main Mall, Vancouver, British Columbia, Canada V6T 1Z4","IEEE Transactions on Software Engineering","","2006","32","12","952","970","When coding to an application programming interface (API), developers often encounter difficulties, unsure of which class to subclass, which objects to instantiate, and which methods to call. Example source code that demonstrates the use of the API can help developers make progress on their task. This paper describes an approach to provide such examples in which the structure of the source code that the developer is writing is matched heuristically to a repository of source code that uses the API. The structural context needed to query the repository is extracted automatically from the code, freeing the developer from learning a query language or from writing their code in a particular style. The repository is generated automatically from existing applications, avoiding the need for handcrafted examples. We demonstrate that the approach is effective, efficient, and more reliable than traditional alternatives through four empirical studies","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.117","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016572","API usage;structural context;heuristic search;Strathcona;example recommendation.","Application software;Database languages;Writing;Computer Society;Software libraries;Protocols;Documentation;Investments;Code standards;Standards development","application program interfaces;pattern matching;query languages","approximate structural context matching;application programming interface;source code structure;query language","","66","","22","","","","","","IEEE","IEEE Journals & Magazines"
"DECOR: A Method for the Specification and Detection of Code and Design Smells","N. Moha; Y. Gueheneuc; L. Duchien; A. Le Meur","INRIA, IRISA, Universit&#x0E9; de Rennes 1, France; &#x0C9;cole Polytechnique de Montr&#x0E9;al, Qu&#x0E9;bec; LIFL, INRIA Lille-Nord Europe, Universit&#x0E9; de Lille, France; LIFL, INRIA Lille-Nord Europe, Universit&#x0E9; de Lille, France","IEEE Transactions on Software Engineering","","2010","36","1","20","36","Code and design smells are poor solutions to recurring implementation and design problems. They may hinder the evolution of a system by making it hard for software engineers to carry out changes. We propose three contributions to the research field related to code and design smells: (1) DECOR, a method that embodies and defines all the steps necessary for the specification and detection of code and design smells, (2) DETEX, a detection technique that instantiates this method, and (3) an empirical validation in terms of precision and recall of DETEX. The originality of DETEX stems from the ability for software engineers to specify smells at a high level of abstraction using a consistent vocabulary and domain-specific language for automatically generating detection algorithms. Using DETEX, we specify four well-known design smells: the antipatterns Blob, Functional Decomposition, Spaghetti Code, and Swiss Army Knife, and their 15 underlying code smells, and we automatically generate their detection algorithms. We apply and validate the detection algorithms in terms of precision and recall on XERCES v2.7.0, and discuss the precision of these algorithms on 11 open-source systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5196681","Antipatterns;design smells;code smells;specification;metamodeling;detection;Java.","Detection algorithms;Vocabulary;Domain specific languages;Algorithm design and analysis;Metamodeling;Java;Design engineering;Object oriented programming;Phase detection;Costs","formal specification;program verification;software quality","code specification;code detection;design smells;DECOR;DETEX;antipatterns Blob;functional decomposition;Spaghetti code;Swiss army knife;empirical validation;domain-specific language;open-source systems","","246","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ScottKnottESD in response to ""An empirical comparison of model validation techniques for defect prediction models""","S. Herbold","Institute of Computer Science, University of Goettingen, Goettingen, Germany","IEEE Transactions on Software Engineering","","2017","43","11","1091","1094","In this article, we discuss the ScottKnottESD test, which was proposed in a recent paper “An Empirical Comparison of Model Validation Techniques for Defect Prediction Models” that was published in this journal. We discuss the implications and the empirical impact of the proposed normality correction of ScottKnottESD and come to the conclusion that this correction does not necessarily lead to the fulfillment of the assumptions of the original Scott-Knott test and may cause problems with the statistical analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2748129","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8024011","Scott-knott test, log transformation, statistics","Analysis of variance;Measurement;Distributed databases;Predictive models;Sociology","program testing;software metrics;statistical analysis","model validation techniques;defect prediction models;ScottKnottESD test;empirical impact;statistical analysis","","","","23","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"A Game-Theoretic Foundation for the Maximum Software Resilience against Dense Errors","C. Huang; D. A. Peled; S. Schewe; F. Wang","Graduate Institute of Electronic Engineering, National Taiwan University, Taiwan, ROC; Department of Computer Science, Bar Ilan University, Ramat Gan, Israel; Department of Computer Science, University of Liverpool, Liverpool, United Kingdom; Department of Electrical Engineering, National Taiwan University, Taiwan, ROC","IEEE Transactions on Software Engineering","","2016","42","7","605","622","Safety-critical systems need to maintain their functionality in the presence of multiple errors caused by component failures or disastrous environment events. We propose a game-theoretic foundation for synthesizing control strategies that maximize the resilience of a software system in defense against a realistic error model. The new control objective of such a game is called<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq1-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>-resilience. In order to be<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq2-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>-resilient, a system needs to rapidly recover from infinitely many waves of a small number of up to<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq3-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>close errors provided that the blocks of up to<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq4-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>errors are separated by short time intervals, which can be used by the system to recover. We first argue why we believe this to be the right level of abstraction for safety critical systems when local faults are few and far between. We then show how the analysis of<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq5-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>-resilience problems can be formulated as a model-checking problem of a mild extension to the alternating-time<inline-formula><tex-math notation=""LaTeX"">$\mu$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq6-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>-calculus (AMC). The witness for<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq7-2510001.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>resilience, which can be provided by the model checker, can be used for providing control strategies that are optimal with respect to resilience. We show that the computational complexity of constructing such optimal control strategies is low and demonstrate the feasibility of our approach through an implementation and experimental results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2510001","ISF; Efficient Synthesis Method of Control for Concurrent Systems; Engineering and Physical Science Research Council (EPSRC); MOST; Research Center for Information Technology Innovation (CITI); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7360234","Fault tolerance;resilience;formal verification;model-checking;game;strategy;complexity","Resilience;Games;Software systems;Safety;Game theory;Computer science","","","","3","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Design and evaluation of a support service for mobile, wireless publish/subscribe applications","M. Caporuscio; A. Carzaniga; A. L. Wolf","Dipt. di Informatica, Universita degli Studi dell'Aquila, L'Aquila, Italy; NA; NA","IEEE Transactions on Software Engineering","","2003","29","12","1059","1071","This paper presents the design and evaluation of a support service for mobile, wireless clients of a distributed publish/subscribe system. A distributed publish/subscribe system is a networked communication infrastructure where messages are published by senders and then delivered to the receivers whose subscriptions match the messages. Communication therefore does not involve the use of explicit addresses, but rather emerges from the dynamic arrangement of publishers and subscribers. Such a communication mechanism is an ideal platform for a variety of Internet applications, including multiparty messaging, personal information management, information sharing, online news distribution, service discovery, and electronic auctions. Our goal is to support such applications on mobile, wireless host devices in such a way that the applications can, if they chose, be oblivious to the mobility and intermittent connectivity of their hosts as they move from one publish/subscribe access point to another. In this paper, we describe a generic, value-added service that can be used in conjunction with publish/subscribe systems to achieve these goals. We detail the implementation of the service and present the results of our evaluation of the service in both wireline and emulated wireless environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265521","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265521","","Application software;Mobile communication;Mobile computing;Subscriptions;Web and internet services;Information management;Network servers;Switches;Personal digital assistants;Wireless networks","middleware;message passing;mobile computing;wireless LAN;cellular radio;synchronisation","support service;mobile wireless publish/subscribe applications;distributed publish/subscribe system;networked communication infrastructure;Internet applications;multiparty messaging;personal information management;information sharing;online news distribution;service discovery;electronic auctions;mobile wireless host devices;wireline environments;emulated wireless environments","","70","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Dynamic Software Update Safety Using Systematic Testing","C. M. Hayden; E. K. Smith; E. A. Hardisty; M. Hicks; J. S. Foster","University of Maryland, College Park, College Park; University of Maryland, College Park, College Park; University of Maryland, College Park, College Park; University of Maryland, College Park, College Park; University of Maryland, College Park, College Park","IEEE Transactions on Software Engineering","","2012","38","6","1340","1354","Dynamic software updating (DSU) systems patch programs on the fly without incurring downtime. To avoid failures due to the updating process itself, many DSU systems employ timing restrictions. However, timing restrictions are theoretically imperfect, and their practical effectiveness is an open question. This paper presents the first significant empirical evaluation of three popular timing restrictions: activeness safety (AS), which prevents updates to active functions, con-freeness safety (CFS), which only allows modifications to active functions when doing so is provably type-safe, and manual identification of the event-handling loops during which an update may occur. We evaluated these timing restrictions using a series of DSU patches to three programs: OpenSSH, vsftpd, and ngIRCd. We systematically applied updates at each distinct update point reached during execution of a suite of system tests for these programs to determine which updates pass and which fail. We found that all three timing restrictions prevented most failures, but only manual identification allowed none. Further, although CFS and AS allowed many more update points, manual identification still supported updates with minimal delay. Finally, we found that manual identification required the least developer effort. Overall, we conclude that manual identification is most effective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.101","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6035725","Dynamic software updating (DSU);hot-swapping;software reliability;testing;program tracing","Software reliability;Software testing;Servers","program testing;safety-critical software;software fault tolerance;software maintenance","dynamic software updating safety evaluation;systematic testing;DSU systems;timing restrictions;activeness safety;AS;active functions;con-freeness safety;CFS;event-handling loop identification;OpenSSH;vsftpd;ngIRCd;manual identification;failure prevention","","10","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Instance Generator and Problem Representation to Improve Object Oriented Code Coverage","A. Sakti; G. Pesant; Y. Guéhéneuc","Department of Computer and Software Engineering, École Polytechnique de Montral, Montral, QC, Canada; Department of Computer and Software Engineering, École Polytechnique de Montral, Montral, QC, Canada; Department of Computer and Software Engineering, École Polytechnique de Montral, Montral, QC, Canada","IEEE Transactions on Software Engineering","","2015","41","3","294","313","Search-based approaches have been extensively applied to solve the problem of software test-data generation. Yet, test-data generation for object-oriented programming (OOP) is challenging due to the features of OOP, e.g., abstraction, encapsulation, and visibility that prevent direct access to some parts of the source code. To address this problem we present a new automated search-based software test-data generation approach that achieves high code coverage for unit-class testing. We first describe how we structure the test-data generation problem for unit-class testing to generate relevant sequences of method calls. Through a static analysis, we consider only methods or constructors changing the state of the class-under-test or that may reach a test target. Then we introduce a generator of instances of classes that is based on a family of means-of-instantiation including subclasses and external factory methods. It also uses a seeding strategy and a diversification strategy to increase the likelihood to reach a test target. Using a search heuristic to reach all test targets at the same time, we implement our approach in a tool, JTExpert, that we evaluate on more than a hundred Java classes from different open-source libraries. JTExpert gives better results in terms of search time and code coverage than the state of the art, EvoSuite, which uses traditional techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2363479","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6926828","Automatic Test Data Generation;Search Based Software Testing;Unit Class Testing;Seeding Strategy;Diversification Strategy;Java Testing;Automatic test data generation;search based software testing;unit class testing;seeding strategy;diversification strategy;Java testing","Testing;Complexity theory;Generators;Search problems;Java;Production facilities;Libraries","Java;object-oriented programming;program diagnostics;program testing;public domain software","instance generator;problem representation;object oriented code coverage;search-based approach;object-oriented programming;OOP;abstraction;encapsulation;visibility;source code;automated search-based software test-data generation approach;unit-class testing;method call sequences;static analysis;class-under-test;means-of-instantiation;seeding strategy;diversification strategy;search heuristic;JTExpert;Java class evaluation;open-source libraries;search time;EvoSuite","","25","","53","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Educational Background on the Effectiveness of Requirements Inspections: An Empirical Study","J. C. Carver; N. Nagappan; A. Page","The University of Alabama, Tuscaloosa; Microsoft Research, Redmond; Microsoft Research, Redmond","IEEE Transactions on Software Engineering","","2008","34","6","800","812","While the inspection of various software artifacts increases the quality of the end product, the effectiveness of an inspection depends largely on the individual inspectors involved. To address that issue, a large-scale controlled inspection experiment with over 70 professionals was conducted at Microsoft Corporation that focused on the relationship between an inspector's background and their effectiveness during a requirements inspection. The results of the study showed that inspectors with university degrees in majors not related to computer science found significantly more defects than those with degrees in computer science majors. We also observed that level of education (Masters, PhD), prior industrial experience or other job related experiences did not significantly impact the effectiveness of an inspector. The only other type of experience that had a significant impact on effectiveness was experience in writing requirements, i.e. professionals with prior experience writing requirements found statistically significant more defects than their counterparts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4564472","Requirements/Specifications;Software Quality/SQA;Metrics/Measurement;Requirements/Specifications;Software Quality/SQA;Metrics/Measurement","Inspection;Software quality;Computer science;Writing;Bioreactors;Large-scale systems;Computer science education;Software measurement;Programming","formal specification;software quality","software quality;software artifacts;educational background;requirements inspections","","18","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A Formal and Tool-Equipped Approach for the Integration of State Diagrams and Formal Datatypes","C. Attiogbe; P. Poizat; G. Salaun","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","3","157","170","Separation of concerns or aspects is a way to deal with the increasing complexity of systems. The separate design of models for different aspects also promotes a better reusability level. However, an important issue is then to define means to integrate them into a global model. We present a formal and tool-equipped approach for the integration of dynamic models (behaviors expressed using state diagrams) and static models (formal data types) with the benefit to share advantages of both: graphical user-friendly models for behaviors, formal and abstract models for data types. Integration is achieved in a generic way so that it can deal with both different static specification languages (algebraic specifications, Z, B) and different dynamic specification semantics","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084134","Formal methods;languages;integrated environments;state diagrams;specification techniques;operational semantics;tools.","Specification languages;Proposals;Unified modeling language;Software architecture;Concurrent computing;Communication system control;Software engineering;Formal languages","algebraic specification;formal languages;formal verification;programming language semantics;specification languages;visual languages","formal approach;tool-equipped approach;state diagram;formal datatypes;reusability level;graphical user-friendly model;static specification language;algebraic specification;dynamic specification semantics","","4","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Guided Mutation Testing for JavaScript Web Applications","S. Mirshokraie; A. Mesbah; K. Pattabiraman","Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC, Canada","IEEE Transactions on Software Engineering","","2015","41","5","429","444","Mutation testing is an effective test adequacy assessment technique. However, there is a high computational cost in executing the test suite against a potentially large pool of generated mutants. Moreover, there is much effort involved in filtering out equivalent mutants. Prior work has mainly focused on detecting equivalent mutants after the mutation generation phase, which is computationally expensive and has limited efficiency. We propose an algorithm to select variables and branches for mutation as well as a metric, called FunctionRank, to rank functions according to their relative importance from the application's behaviour point of view. We present a technique that leverages static and dynamic analysis to guide the mutation generation process towards parts of the code that are more likely to influence the program's output. Further, we focus on the JavaScript language, and propose a set of mutation operators that are specific to Web applications. We implement our approach in a tool called MUTANDIS. The results of our empirical evaluation show that (1) more than 93 percent of generated mutants are non-equivalent, and (2) more than 75 percent of the surviving non-equivalent mutants are in the top 30 percent of the ranked functions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2371458","NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6960094","mutation testing;JavaScript;equivalent mutants;guided mutation generation;web applications;Mutation testing;JavaScript;equivalent mutants;guided mutation generation;web applications","Testing;Measurement;Heuristic algorithms;Complexity theory;Performance analysis;Instruments;IEEE Computer Society","Java;program diagnostics;program testing","guided mutation testing;JavaScript Web applications;test adequacy assessment technique;computational cost;test suite execution;equivalent mutants;mutation generation phase;variable selection;FunctionRank;function ranking;relative function importance;application behaviour;static analysis;dynamic analysis;program output;mutation operators;nonequivalent mutants;empirical evaluation;MUTANDIS tool;Web applications","","5","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling Human-in-the-Loop Security Analysis and Decision-Making Processes","M. A. Schumann; D. Drusinsky; J. B. Michael; D. Wijesekera","KEYW Corporation, 7740 Milestone Pkwy, Suite 400, Hanover; Department of Computer Science , Naval Postgraduate School, Monterey; Departments of Computer Science and Electrical & Computer Engineering, Naval Postgraduate School, 900 N Glebe Road, Arlington; Department of Computer Science , George Mason University, Fairfax","IEEE Transactions on Software Engineering","","2014","40","2","154","166","This paper presents a novel application of computer-assisted formal methods for systematically specifying, documenting, statically and dynamically checking, and maintaining human-centered workflow processes. This approach provides for end-to-end verification and validation of process workflows, which is needed for process workflows that are intended for use in developing and maintaining high-integrity systems. We demonstrate the technical feasibility of our approach by applying it on the development of the US government's process workflow for implementing, certifying, and accrediting cross-domain computer security solutions. Our approach involves identifying human-in-the-loop decision points in the process activities and then modeling these via statechart assertions. We developed techniques to specify and enforce workflow hierarchies, which was a challenge due to the existence of concurrent activities within complex workflow processes. Some of the key advantages of our approach are: it results in development of a model that is executable, supporting both upfront and runtime checking of process-workflow requirements; aids comprehension and communication among stakeholders and process engineers; and provides for incorporating accountability and risk management into the engineering of process workflows.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2302433","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6727512","Formal methods;information assurance;process modeling;software engineering;statechart assertions;verification and validation","Unified modeling language;Object oriented modeling;Software;Runtime;Formal specifications;Analytical models;Business","decision making;formal specification;formal verification;government data processing;security of data;workflow management software","human-in-the-loop security analysis;decision-making process;computer-assisted formal methods;human-centered workflow process;process specification;process documentation;process statically checking;process dynamically checking;process maintenance;end-to-end verification;end-to-end validation;high-integrity systems;US government process workflow;United States;cross-domain computer security solutions;human-in-the-loop decision points;process activities;statechart assertions;workflow hierarchies;accountability;risk management;process workflows engineering","","1","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Comparison and Evaluation of Clone Detection Tools","S. Bellon; R. Koschke; G. Antoniol; J. Krinke; E. Merlo","Axivion GmbH, Nobelstr. 15, 70569 Stuttgart, Germany; Universitat Bremen, Fachbereich 03, Postfach 33 04 40, 28334 Bremen, Germany; Departement de Genie Informatique, Ecole Polytechnique de Montreal. Pavillons Lassonde, Mac-Lassonde, 2500, chemin de Polytechnique, Montreal (Quebec), Canada, H3T 1J4; Fern-Universitat in Hagen, Universitatsstr. 27, 58097 Hagen, Germany; Department of Computer Engineering, Ecole Polytechnique of Montreal, PO Box 6079, Station Downtown, Montreal (Quebec), Canada, H3C 3A7","IEEE Transactions on Software Engineering","","2007","33","9","577","591","Many techniques for detecting duplicated source code (software clones) have been proposed in the past. However, it is not yet clear how these techniques compare in terms of recall and precision as well as space and time requirements. This paper presents an experiment that evaluates six clone detectors based on eight large C and Java programs (altogether almost 850 KLOC). Their clone candidates were evaluated by one of the authors as an independent third party. The selected techniques cover the whole spectrum of the state-of-the-art in clone detection. The techniques work on text, lexical and syntactic information, software metrics, and program dependency graphs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70725","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288192","Redundant code;duplicated code;software clones","Cloning;Computer Society;Detectors;Java;Software metrics;Concrete;Software tools;Visualization;Fingerprint recognition;Abstracts","program diagnostics;program verification;software metrics","clone detection tool;duplicated source code detection;text information;lexical information;syntactic information;software metrics;program dependency graphs","","263","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Black-Box String Test Case Generation through a Multi-Objective Optimization","A. Shahbazi; J. Miller","Department of Electrical and Computer Engineering, Edmonton, AB, Canada; Department of Electrical and Computer Engineering, Edmonton, AB, Canada","IEEE Transactions on Software Engineering","","2016","42","4","361","378","String test cases are required by many real-world applications to identify defects and security risks. Random Testing (RT) is a low cost and easy to implement testing approach to generate strings. However, its effectiveness is not satisfactory. In this research, black-box string test case generation methods are investigated. Two objective functions are introduced to produce effective test cases. The diversity of the test cases is the first objective, where it can be measured through string distance functions. The second objective is guiding the string length distribution into a Benford distribution based on the hypothesis that the population of strings is right-skewed within its range. When both objectives are applied via a multi-objective optimization algorithm, superior string test sets are produced. An empirical study is performed with several real-world programs indicating that the generated string test cases outperform test cases generated by other methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2487958","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7293669","Adaptive random testing;automated test case generation;black-box testing;mutation;random testing;software testing;string distance;string test cases;Adaptive random testing;automated test case generation;black-box testing;mutation;random testing;software testing;string distance;string test cases","Sociology;Statistics;Biological cells;Subspace constraints;Testing;Power capacitors;Genetic algorithms","optimisation;program testing;security of data","black-box string test case generation;security risks;random testing;RT;objective functions;string distance functions;Benford distribution;multiobjective optimization algorithm","","10","","76","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing the evolutionary history of the logical design of object-oriented software","Z. Xing; E. Stroulia","Dept. of Comput. Sci., Alberta Univ., Edmonton, Alta., Canada; Dept. of Comput. Sci., Alberta Univ., Edmonton, Alta., Canada","IEEE Transactions on Software Engineering","","2005","31","10","850","868","Today, most object-oriented software systems are developed using an evolutionary process model. Therefore, understanding the phases that the system's logical design has gone through and the style of their evolution can provide valuable insights in support of consistently maintaining and evolving the system, without compromising the integrity and stability of its architecture. In this paper, we present a method for analyzing the evolution of object-oriented software systems from the point of view of their logical design. This method relies on UMLDiff, a UML-structure differencing algorithm, which, given a sequence of UML class models corresponding to the logical design of a sequence of system code releases, produces a sequence of ""change records"" that describe the design-level changes between subsequent system releases. This change-records sequence is subsequently analyzed from the perspective of each individual system class, to produce the class-evolution profile, i.e., a class-specific change-records' sequence. Three types of longitudinal analyses - phasic, gamma, and optimal matching analysis - are applied to the class-evolution profiles to recover a high-level abstraction of distinct evolutionary phases and their corresponding styles and to identify class clusters with similar evolution trajectories. The recovered knowledge facilitates the overall understanding of system evolution and the planning of future maintenance activities. We report on one real-world case study evaluating our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.106","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542067","Index Terms- Restructuring;reverse engineering;and reengineering.","History;Software design;Software systems;Object oriented modeling;Algorithm design and analysis;Surges;Quality management;Project management;Visualization;Documentation","object-oriented programming;object-oriented methods;Unified Modeling Language;software maintenance;software architecture;reverse engineering","object-oriented software system;evolutionary process model;system logical design;UMLDiff;UML-structure differencing algorithm;phasic analysis;gamma analysis;optimal matching analysis;software maintenance","","39","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Software productivity measurement using multiple size measures","B. Kitchenham; E. Mendes","Nat. ICT Australia, Alexandria, NSW, Australia; NA","IEEE Transactions on Software Engineering","","2004","30","12","1023","1035","Productivity measures based on a simple ratio of product size to project effort assume that size can be determined as a single measure. If there are many possible size measures in a data set and no obvious model for aggregating the measures into a single measure, we propose using the expression AdjustedSize/Effort to measure productivity. AdjustedSize is defined as the most appropriate regression-based effort estimation model, where all the size measures selected for inclusion in the estimation model have a regression parameter significantly different from zero (p<0.05). This productivity measurement method ensures that each project has an expected productivity value of one. Values between zero and one indicate lower than expected productivity, values greater than one indicate higher than expected productivity. We discuss the assumptions underlying this productivity measurement method and present an example of its use for Web application projects. We also explain the relationship between effort prediction models and productivity models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.104","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377195","Index Terms- Software productivity measurement;software cost estimation.","Size measurement;Software measurement;Productivity;Predictive models;Costs;Equations;Computer science;Computer Society;Application software;Production","software cost estimation;software metrics;productivity;project management;regression analysis","software productivity measurement;software cost estimation;product size;project effort;regression-based effort estimation;parameter estimation","","63","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Systematic reliability analysis of a class of application-specific embedded software framework","Sung Kim; F. B. Bastani; I-Ling Yen; I. -. Chen","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; NA","IEEE Transactions on Software Engineering","","2004","30","4","218","230","Dramatic advances in computer and communication technologies have made it economically feasible to extend the use of embedded computer systems to more and more critical applications. At the same time, these embedded computer systems are becoming more complex and distributed. As the bulk of the complex application-specific logic of these systems is realized by software, the need for certifying software systems has grown substantially. While relatively mature techniques exist for certifying hardware systems, methods of rigorously certifying software systems are still being actively researched. Possible certification methods for embedded software systems range from formal verification to statistical testing. These methods have different strengths and weaknesses and can be used to complement each other. One potentially useful approach is to decompose the specification into distinct aspects that can be independently certified using the method that is most effective for it. Even though substantial-research has been carried out to reduce the complexity of the software system through decomposition, one major hurdle is the need to certify the overall system on the basis of the aspect properties. One way to address this issue is to focus on architectures in which the aspects are relatively independent of each other. However, complex embedded systems are typically comprised of multiple architectures. We present an alternative approach based on the use of application-oriented-frameworks for implementing embedded systems. We show that it is possible to design such frameworks for embedded applications and derive expressions for determining the system reliability from the reliabilities of the framework and the aspects. The method is illustrated using a distributed multimedia collaboration system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274042","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274042","","Embedded software;Embedded computing;Software systems;Embedded system;Communications technology;Application software;Distributed computing;Logic;Hardware;Certification","embedded systems;formal verification;formal specification;software reliability;software metrics;statistical testing;software architecture;object-oriented methods","software reliability analysis;application-specific embedded software framework;software systems;hardware systems;formal verification;statistical testing;software complexity;software architectures;distributed multimedia collaboration system;software composition","","3","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal transfer trees and distinguishing trees for testing observable nondeterministic finite-state machines","Fan Zhang; To-yat Cheung","Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China; NA","IEEE Transactions on Software Engineering","","2003","29","1","1","14","The fault-state detection approach for blackbox testing consists of two phases. The first is to bring the system under test (SUT) from its initial state to a targeted state t and the second is to check various specified properties of the SUT at t. This paper investigates the first phase for testing systems specified as observable nondeterministic finite-state machines with probabilistic and weighted transitions. This phase involves two steps. The first step transfers the SUT to some state t' and the second step identifies whether t' is indeed the targeted state t or not. State transfer is achieved by moving the SUT along one of the paths of a transfer tree (TT) and state identification is realized by using diagnosis trees (DT). A theoretical foundation for the existence and characterization of TT and DT with minimum weighted height or minimum average weight is presented. Algorithms for their computation are proposed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166585","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166585","","System testing;Software testing;Fault detection;Software systems;Computer Society;Design methodology;Automata;Nonhomogeneous media","finite state machines;program testing;optimisation;trees (mathematics)","optimal transfer trees;distinguishing trees;observable nondeterministic finite-state machine testing;fault-state detection;blackbox testing;probabilistic transitions;weighted transitions;TT;DT;diagnosis trees","","22","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Validation of Three Software Metrics Suites to Predict Fault-Proneness of Object-Oriented Classes Developed Using Highly Iterative or Agile Software Development Processes","H. M. Olague; L. H. Etzkorn; S. Gholston; S. Quattlebaum","US Army Space and Missile Defense Command, SMDC-RDTI-S, Huntsville, AL; Computer Science Department, University of Alabama in Huntsville, Huntsville, AL; Industrial and Systems Engineering and Engineering Management Department, University of Alabama in Huntsville, Huntsville, AL; Dragonfly Athletics, Hartselle, AL","IEEE Transactions on Software Engineering","","2007","33","6","402","419","Empirical validation of software metrics suites to predict fault proneness in object-oriented (OO) components is essential to ensure their practical use in industrial settings. In this paper, we empirically validate three OO metrics suites for their ability to predict software quality in terms of fault-proneness: the Chidamber and Kemerer (CK) metrics, Abreu's Metrics for Object-Oriented Design (MOOD), and Bansiya and Davis' Quality Metrics for Object-Oriented Design (QMOOD). Some CK class metrics have previously been shown to be good predictors of initial OO software quality. However, the other two suites have not been heavily validated except by their original proposers. Here, we explore the ability of these three metrics suites to predict fault-prone classes using defect data for six versions of Rhino, an open-source implementation of JavaScript written in Java. We conclude that the CK and QMOOD suites contain similar components and produce statistical models that are effective in detecting error-prone classes. We also conclude that the class components in the MOOD metrics suite are not good class fault-proneness predictors. Analyzing multivariate binary logistic regression models across six Rhino versions indicates these models may be useful in assessing quality in OO classes produced using modern highly iterative or agile software development processes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1015","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181709","Object-oriented software metrics;object-oriented metrics;software quality metrics;software maintenance programming;software reuse.","Software metrics;Programming;Software quality;Object oriented modeling;Mood;Java;Software maintenance;Computer industry;Open source software;Logistics","Java;object-oriented programming;regression analysis;software metrics;software quality","software metrics;fault-prone prediction;object-oriented class;iterative software development process;agile software development process;software quality;object-oriented design;JavaScript;statistical analysis;multivariate binary logistic regression model","","130","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Exception handling in the spreadsheet paradigm","M. Burnett; A. Agrawal; P. van Zee","Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","10","923","942","Exception handling is widely regarded as a necessity in programming languages today and almost every programming language currently used for professional software development supports some form of it. However, spreadsheet systems, which may be the most widely used type of ""programming language"" today in terms of number of users using it to create ""programs"" (spreadsheets), have traditionally had only extremely limited support for exception handling. Spreadsheet system users range from end users to professional programmers and this wide range suggests that an approach to exception handling for spreadsheet systems needs to be compatible with the equational reasoning model of spreadsheet formulas, yet feature expressive power comparable to that found in other programming languages. We present an approach to exception handling for spreadsheet system users that is aimed at this goal. Some of the features of the approach are new; others are not new, but their effects on the programming language properties of spreadsheet systems have not been discussed before in the literature. We explore these properties, offer our solutions to problems that arise with these properties, and compare the functionality of the approach with that of exception handling approaches in other languages.","0098-5589;1939-3520;2326-3881","","10.1109/32.879817","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879817","","Programming profession;Computer languages;Equations;User interfaces;Software engineering;Power system modeling;Robustness;Logic;Control systems;Marketing and sales","exception handling;spreadsheet programs;software engineering","exception handling;programming language;professional software development;spreadsheet systems;end user programming;professional programmers;equational reasoning model","","4","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Computer-mediated group support, anonymity, and the software inspection process: an empirical investigation","P. Vitharana; K. Ramamurthy","Sch. of Manage., Syracuse Univ., NY, USA; NA","IEEE Transactions on Software Engineering","","2003","29","2","167","180","In software inspection, a key principle endorsed by Fagan (1986) is openness. However, scholars have recently questioned the efficacy of openness. For example, some argue that ego-involvement and personality conflicts that become more transparent due to openness might impede inspection. Still others point out that familiarity and (preexisting) relationships among inspection team members negatively affect the comprehensiveness in detection of defects. This brings up concerns if the openness as originally envisioned by Fagan may in fact lead to suboptimal outcomes. As the trend towards computer-based inspection continues, we believe that anonymity could play a positive role in overcoming some of the drawbacks noted in team-based inspection. Drawing upon the literature on software inspection and group support systems, this research proposes possible influences of group member anonymity on the outcome of computer-mediated software inspection and empirically examines the validity of the posited relationships in a set of controlled laboratory experiments. Two different inspection tasks with varying levels of software code complexity are employed. While both the control groups (i.e., teams without anonymity) and treatment groups (i.e., teams with support for anonymity) consume more or less the same time in performing the inspection tasks, the treatment groups are more effective in identifying the seeded errors in the more complex task. Treatment groups also express a more positive attitude toward both code inspection tasks. The findings of the study suggest a number of directions for future research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178054","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178054","","Collaborative software;Inspection;Software quality;Programming;Control systems;Impedance;Laboratories;Error correction;Software systems;Software tools","groupware;software quality;software development management;program testing;software process improvement;human factors","computer-mediated group support;anonymity;software inspection process;ego-involvement;personality conflicts;inspection team members;suboptimal outcomes;computer-based inspection;team-based inspection;software inspection;group support systems;group member anonymity;software code complexity;seeded errors;software quality assurance","","17","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Effects of Developer Experience on Learning and Applying Unit Test-Driven Development","R. Latorre","Universidad Autónoma de Madrid, Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","4","381","395","Unit test-driven development (UTDD) is a software development practice where unit test cases are specified iteratively and incrementally before production code. In the last years, researchers have conducted several studies within academia and industry on the effectiveness of this software development practice. They have investigated its utility as compared to other development techniques, focusing mainly on code quality and productivity. This quasi-experiment analyzes the influence of the developers' experience level on the ability to learn and apply UTDD. The ability to apply UTDD is measured in terms of process conformance and development time. From the research point of view, our goal is to evaluate how difficult is learning UTDD by professionals without any prior experience in this technique. From the industrial point of view, the goal is to evaluate the possibility of using this software development practice as an effective solution to take into account in real projects. Our results suggest that skilled developers are able to quickly learn the UTDD concepts and, after practicing them for a short while, become as effective in performing small programming tasks as compared to more traditional test-last development techniques. Junior programmers differ only in their ability to discover the best design, and this translates into a performance penalty since they need to revise their design choices more frequently than senior programmers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.2295827","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6690135","Test-driven development;test-first design;software engineering process;software quality/SQA;software construction;process conformance;programmer productivity","Software;Testing;Training;Programming profession;Context;Production","learning (artificial intelligence);program testing;software quality","learning;unit test-driven development;UTDD;software development;code quality;productivity","","8","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Fluid Rewards for a Stochastic Process Algebra","M. Tribastone; J. Ding; S. Gilmore; J. Hillston","Ludwig-Maximilians-Universität, München; Yangzhou University, Yangzhou; Edinburgh University, Edinburgh; Edinburgh University, Edinburgh","IEEE Transactions on Software Engineering","","2012","38","4","861","874","Reasoning about the performance of models of software systems typically entails the derivation of metrics such as throughput, utilization, and response time. If the model is a Markov chain, these are expressed as real functions of the chain, called reward models. The computational complexity of reward-based metrics is of the same order as the solution of the Markov chain, making the analysis infeasible when evaluating large-scale systems. In the context of the stochastic process algebra PEPA, the underlying continuous-time Markov chain has been shown to admit a deterministic (fluid) approximation as a solution of an ordinary differential equation, which effectively circumvents state-space explosion. This paper is concerned with approximating Markovian reward models for PEPA with fluid rewards, i.e., functions of the solution of the differential equation problem. It shows that (1) the Markovian reward models for typical metrics of performance enjoy asymptotic convergence to their fluid analogues, and that (2) via numerical tests, the approximation yields satisfactory accuracy in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.81","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975178","Modeling and prediction;ordinary differential equations;Markov processes","Convergence;Markov processes;Approximation methods;Computational modeling;Mathematical model;Servers","computational complexity;Markov processes;mathematics computing;process algebra;stochastic processes","fluid rewards;stochastic process algebra;software systems;metric derivation;Markov chain;computational complexity;ordinary differential equation;state-space explosion","","15","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Using Genetic Search for Reverse Engineering of Parametric Behavior Models for Performance Prediction","K. Krogmann; M. Kuperberg; R. Reussner","Karlsruhe Institute of Technology (KIT), Karlsruhe; Karlsruhe Institute of Technology (KIT), Karlsruhe; Karlsruhe Institute of Technology (KIT), Karlsruhe","IEEE Transactions on Software Engineering","","2010","36","6","865","877","In component-based software engineering, existing components are often reused in new applications. Correspondingly, the response time of an entire component-based application can be predicted from the execution durations of individual component services. These execution durations depend on the runtime behavior of a component which itself is influenced by three factors: the execution platform, the usage profile, and the component wiring. To cover all relevant combinations of these influencing factors, conventional prediction of response times requires repeated deployment and measurements of component services for all such combinations, incurring a substantial effort. This paper presents a novel comprehensive approach for reverse engineering and performance prediction of components. In it, genetic programming is utilized for reconstructing a behavior model from monitoring data, runtime bytecode counts, and static bytecode analysis. The resulting behavior model is parameterized over all three performance-influencing factors, which are specified separately. This results in significantly fewer measurements: The behavior model is reconstructed only once per component service, and one application-independent bytecode benchmark run is sufficient to characterize an execution platform. To predict the execution durations for a concrete platform, our approach combines the behavior model with platform-specific benchmarking results. We validate our approach by predicting the performance of a file sharing application.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5530323","Genetic search;genetic programming;reverse engineering;performance prediction;bytecode benchmarking.","Reverse engineering;Predictive models;Application software;Delay;Runtime;Software engineering;Wiring;Genetic programming;Monitoring;Concrete","genetic algorithms;object-oriented programming;reverse engineering;search problems;software performance evaluation","genetic search;reverse engineering;parametric behavior model;component based software engineering;genetic programming;runtime bytecode count;static bytecode analysis;application independent bytecode benchmark","","21","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Using Traceability Links to Recommend Adaptive Changes for Documentation Evolution","B. Dagenais; M. P. Robillard","Resulto Inc., Montreal, QC, Canada; School of Computer Science, McGill University, 3480 University Street, McConnell Engineering Building, Office 114N, Montreal, QC, Canada","IEEE Transactions on Software Engineering","","2014","40","11","1126","1146","Developer documentation helps developers learn frameworks and libraries, yet developing and maintaining accurate documentation requires considerable effort and resources. Contributors who work on developer documentation often need to manually track all changes in the code, determine which changes are significant enough to document, and then, adapt the documentation. We propose AdDoc, a technique that automatically discovers documentation patterns, i.e., coherent sets of code elements that are documented together, and that reports violations of these patterns as the code and the documentation evolves. We evaluated our approach in a retrospective analysis of four Java open source projects and found that at least 50 percent of all the changes in the documentation were related to existing documentation patterns. Our technique allows contributors to quickly adapt existing documentation, so that they can focus their documentation effort on the new features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2347969","NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6878435","Documentation;maintainability;frameworks","Documentation;Java;Manuals;Libraries;Sections;Joining processes;Concrete","data mining;Java;program diagnostics;public domain software;system documentation","traceability links;adaptive changes;documentation evolution;developer documentation;AdDoc;automatic documentation pattern discovery;code elements;Java open source projects","","5","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Trends in the Quality of Human-Centric Software Engineering Experiments--A Quasi-Experiment","B. Kitchenham; D. I. K. Sjøberg; T. Dybå; O. P. Brereton; D. Budgen; M. Höst; P. Runeson","Keele University, Keele; University of Oslo, Oslo; University of Oslo, Oslo and SINTEF, Trondheim; Keele University, Keele; Durham University, Durham; Lund University, Lund; Lund University, Lund","IEEE Transactions on Software Engineering","","2013","39","7","1002","1017","Context: Several text books and papers published between 2000 and 2002 have attempted to introduce experimental design and statistical methods to software engineers undertaking empirical studies. Objective: This paper investigates whether there has been an increase in the quality of human-centric experimental and quasi-experimental journal papers over the time period 1993 to 2010. Method: Seventy experimental and quasi-experimental papers published in four general software engineering journals in the years 1992-2002 and 2006-2010 were each assessed for quality by three empirical software engineering researchers using two quality assessment methods (a questionnaire-based method and a subjective overall assessment). Regression analysis was used to assess the relationship between paper quality and the year of publication, publication date group (before 2003 and after 2005), source journal, average coauthor experience, citation of statistical text books and papers, and paper length. The results were validated both by removing papers for which the quality score appeared unreliable and using an alternative quality measure. Results: Paper quality was significantly associated with year, citing general statistical texts, and paper length (p <; 0.05). Paper length did not reach significance when quality was measured using an overall subjective assessment. Conclusions: The quality of experimental and quasi-experimental software engineering papers appears to have improved gradually since 1993.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6374196","Quality evaluation;empirical studies;human-centric experiments;experimentation;software engineering","Software engineering;Guidelines;Correlation;Manuals;Educational institutions;Humans;Materials","citation analysis;design of experiments;publishing;regression analysis;software quality;text analysis","human-centric software engineering experiments;experimental design;statistical methods;human-centric experimental journal papers;quasi-experimental journal papers;quality assessment methods;questionnaire-based method;subjective overall assessment;regression analysis;paper quality;publication year;publication date group;source journal;average coauthor experience;statistical text books citation;statistical papers citation;paper length","","5","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Improving Fault Detection Capability by Selectively Retaining Test Cases during Test Suite Reduction","D. Jeffrey; N. Gupta","NA; NA","IEEE Transactions on Software Engineering","","2007","33","2","108","123","Software testing is a critical part of software development. As new test cases are generated over time due to software modifications, test suite sizes may grow significantly. Because of time and resource constraints for testing, test suite minimization techniques are needed to remove those test cases from a suite that, due to code modifications over time, have become redundant with respect to the coverage of testing requirements for which they were generated. Prior work has shown that test suite minimization with respect to a given testing criterion can significantly diminish the fault detection effectiveness (FDE) of suites. We present a new approach for test suite reduction that attempts to use additional coverage information of test cases to selectively keep some additional test cases in the reduced suites that are redundant with respect to the testing criteria used for suite minimization, with the goal of improving the FDE retention of the reduced suites. We implemented our approach by modifying an existing heuristic for test suite minimization. Our experiments show that our approach can significantly improve the FDE of reduced test suites without severely affecting the extent of suite size reduction","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052586","Software testing;testing criteria;test suite minimization;test suite reduction;fault detection effectiveness.","Fault detection;Software testing;Programming;Time factors;Life testing;Resource management;Software development management;Polynomials","program testing","test suite reduction;software testing;software development;test cases;fault detection effectiveness","","72","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing staffing needs for a software maintenance project through queuing simulation","G. Antoniol; A. Cimitile; G. A. Di Lucca; M. Di Penta","Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy","IEEE Transactions on Software Engineering","","2004","30","1","43","58","We present an approach based on queuing theory and stochastic simulation to help planning, managing, and controlling the project staffing and the resulting service level in distributed multiphase maintenance processes. Data from a Y2K massive maintenance intervention on a large COBOL/JCL financial software system were used to simulate and study different service center configurations for a geographically distributed software maintenance project. In particular, a monolithic configuration corresponding to the customer's point-of-view and more fine-grained configurations, accounting for different process phases as well as for rework, were studied. The queuing theory and stochastic simulation provided a means to assess staffing, evaluate service level, and assess the likelihood to meet the project deadline while executing the project. It turned out to be an effective staffing tool for managers, provided that it is complemented with other project-management tools, in order to prioritize activities, avoid conflicts, and check the availability of resources.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265735","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265735","","Software maintenance;Queueing analysis;Computational modeling;Stochastic processes;Project management;Costs;Computer simulation;Software systems;Counting circuits;Computer Society","software maintenance;queueing theory;discrete event simulation;personnel;project management;stochastic processes;program testing","queuing theory;stochastic simulation;distributed multiphase maintenance process;Y2K massive maintenance;financial software system;distributed software maintenance project;project-management tool;software maintenance staffing;discrete-event simulation;process simulation;schedule estimation","","43","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Managing Technical Debt in Enterprise Software Packages","N. Ramasubbu; C. F. Kemerer","Joseph M. Katz Graduate School of Business, University of Pittsburgh, Pittsburgh, PA; Joseph M. Katz Graduate School of Business, University of Pittsburgh, Pittsburgh, PA","IEEE Transactions on Software Engineering","","2014","40","8","758","772","We develop an evolutionary model and theory of software technical debt accumulation to facilitate a rigorous and balanced analysis of its benefits and costs in the context of a large commercial enterprise software package. Our theory focuses on the optimization problem involved in managing technical debt, and illustrates the different tradeoff patterns between software quality and customer satisfaction under early and late adopter scenarios at different lifecycle stages of the software package. We empirically verify our theory utilizing a ten year longitudinal data set drawn from 69 customer installations of the software package. We then utilize the empirical results to develop actionable policies for managing technical debt in enterprise software product adoption.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2327027","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6824267","Technical debt;enterprise software;software platforms;customer satisfaction;software quality;technology adoption;COTS;software evolution;software maintenance;software management;longitudinal data","Software packages;Business;Software quality;Measurement;Maintenance engineering;Context","cost-benefit analysis;evolutionary computation;software development management;software maintenance;software packages;software quality","technical debt management;evolutionary model;software technical debt theory;commercial enterprise software package;optimization problem;software quality;customer satisfaction;early adopter scenario;late adopter scenario;software package lifecycle stage;enterprise software product adoption","","10","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Researcher Bias: The Use of Machine Learning in Software Defect Prediction","M. Shepperd; D. Bowes; T. Hall","Brunel University, Uxbridge, Middlesex, United Kingdom; Science and Technology Research Institute, University of Hertfordshire, Hatfield, Hertfordshire, United Kingdom; Brunel University, Uxbridge, Middlesex, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","6","603","616","Background. The ability to predict defect-prone software components would be valuable. Consequently, there have been many empirical studies to evaluate the performance of different techniques endeavouring to accomplish this effectively. However no one technique dominates and so designing a reliable defect prediction model remains problematic. Objective. We seek to make sense of the many conflicting experimental results and understand which factors have the largest effect on predictive performance. Method. We conduct a meta-analysis of all relevant, high quality primary studies of defect prediction to determine what factors influence predictive performance. This is based on 42 primary studies that satisfy our inclusion criteria that collectively report 600 sets of empirical prediction results. By reverse engineering a common response variable we build a random effects ANOVA model to examine the relative contribution of four model building factors (classifier, data set, input metrics and researcher group) to model prediction performance. Results. Surprisingly we find that the choice of classifier has little impact upon performance (1.3 percent) and in contrast the major (31 percent) explanatory factor is the researcher group. It matters more who does the work than what is done. Conclusion. To overcome this high level of researcher bias, defect prediction researchers should (i) conduct blind analysis, (ii) improve reporting protocols and (iii) conduct more intergroup studies in order to alleviate expertise issues. Lastly, research is required to determine whether this bias is prevalent in other applications domains.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2322358","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6824804","Software defect prediction;meta-analysis;researcher bias","Software;Predictive models;Correlation;Data models;Buildings;Software engineering;Measurement","learning (artificial intelligence);object-oriented programming;reverse engineering;software metrics;software performance evaluation;statistical analysis","machine learning;software defect prediction;defect-prone software component prediction;performance evaluation;reverse engineering;common response variable;random effects ANOVA model;model building factors;classifier factor;data set factor;input metrics factor;researcher group factor;researcher bias;blind analysis;reporting protocol improvement;intergroup studies","","69","","53","","","","","","IEEE","IEEE Journals & Magazines"
"On inspection and verification of software with timing requirements","J. Xu","Dept. of Comput. Sci., York Univ., North York, Ont., Canada","IEEE Transactions on Software Engineering","","2003","29","8","705","720","Software with hard timing requirements should be designed using a systematic approach to make its timing properties easier to inspect and verify; otherwise, it may be practically impossible to determine whether the software satisfies the timing requirements. Pre-runtime scheduling provides such an approach by placing restrictions on software structures to reduce complexity. A major benefit of using a pre-runtime scheduling approach is that it makes it easier to systematically inspect and verify the timing properties of the actual software code, not just various high-level abstractions of the code.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223645","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223645","","Inspection;Timing;Job shop scheduling;Software tools;Real time systems;Algorithm design and analysis;Protocols;Processor scheduling;Transportation;Finance","program verification;timing;inspection;real-time systems;scheduling;formal specification;program control structures;software reliability;software metrics","software verification;timing requirement;preruntime scheduling;software structure;software inspection;timing property;software code;real-time software;software complexity","","19","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Semantic Slicing of Software Version Histories","Y. Li; C. Zhu; J. Rubin; M. Chechik","Department of Computer Science, University of Toronto, Toronto, ON, Canada; Department of Computer Science, University of Toronto, Toronto, ON, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Computer Science, University of Toronto, Toronto, ON, Canada","IEEE Transactions on Software Engineering","","2018","44","2","182","201","Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of the change history, “inheriting” additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and propose techniques to minimize the produced slice. We then instantiate the overall approach, CSlicer, in a specific implementation for Java projects managed in Git and evaluate its correctness and effectiveness on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2664824","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7843626","Software changes;version control;dependency;program analysis","History;Semantics;Software;Minimization;Context;Computer bugs;Java","configuration management;Java;program slicing;public domain software","CSlicer;configuration management system;software developers;software version histories;open-source software repositories;semantic slicing problem;semantically-related commits","","3","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Self-Supervising BPEL Processes","L. Baresi; S. Guinea","Politecnico di Milano, Milano, Italy; Politecnico di Milano, Milano, Italy","IEEE Transactions on Software Engineering","","2011","37","2","247","263","Service compositions suffer changes in their partner services. Even if the composition does not change, its behavior may evolve over time and become incorrect. Such changes cannot be fully foreseen through prerelease validation, but impose a shift in the quality assessment activities. Provided functionality and quality of service must be continuously probed while the application executes, and the application itself must be able to take corrective actions to preserve its dependability and robustness. We propose the idea of self-supervising BPEL processes, that is, special-purpose compositions that assess their behavior and react through user-defined rules. Supervision consists of monitoring and recovery. The former checks the system's execution to see whether everything is proceeding as planned, while the latter attempts to fix any anomalies. The paper introduces two languages for defining monitoring and recovery and explains how to use them to enrich BPEL processes with self-supervision capabilities. Supervision is treated as a cross-cutting concern that is only blended at runtime, allowing different stakeholders to adopt different strategies with no impact on the actual business logic. The paper also presents a supervision-aware runtime framework for executing the enriched processes, and briefly discusses the results of in-lab experiments and of a first evaluation with industrial partners.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5432226","Software engineering;software/program verification;assertion checkers;assertion languages;performance;design tools and techniques;distributed/Internet-based software engineering tools and techniques.","Runtime;Monitoring;Robustness;Software engineering;Application software;Quality assessment;Quality of service;Logic;Software performance;Software tools","business process re-engineering;program verification;service-oriented architecture;Web services","self supervising BPEL process;quality assessment;stakeholder;business logic;supervision aware runtime framework;industrial partner;business process execution language","","57","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Test Oracle Strategies for Model-Based Testing","N. Li; J. Offutt","Research and Development Division, Medidata Solutions, New York, NY; George Mason University, Fairfax, VA","IEEE Transactions on Software Engineering","","2017","43","4","372","395","Testers use model-based testing to design abstract tests from models of the system's behavior. Testers instantiate the abstract tests into concrete tests with test input values and test oracles that check the results. Given the same test inputs, more elaborate test oracles have the potential to reveal more failures, but may also be more costly. This research investigates the ability for test oracles to reveal failures. We define ten new test oracle strategies that vary in amount and frequency of program state checked. We empirically compared them with two baseline test oracle strategies. The paper presents several main findings. (1) Test oracles must check more than runtime exceptions because checking exceptions alone is not effective at revealing failures. (2) Test oracles do not need to check the entire output state because checking partial states reveals nearly as many failures as checking entire states. (3) Test oracles do not need to check program states multiple times because checking states less frequently is as effective as checking states more frequently. In general, when state machine diagrams are used to generate tests, checking state invariants is a reasonably effective low cost approach to creating test oracles.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2597136","George Mason University; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7529115","Test oracle;RIPR model;test oracle strategy;test automation;subsumption;model-based testing","Unified modeling language;Software;Context;Concrete;System testing;Observability","data flow analysis;diagrams;finite state machines;program testing;software fault tolerance","test oracle strategy;model-based testing;abstract test design;failure revelation;runtime exception;partial state checking;state machine diagram","","6","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Model-based performance risk analysis","V. Cortellessa; K. Goseva-Popstojanova; Kalaivani Appukkutty; A. R. Guedem; A. Hassan; R. Elnaggar; W. Abdelmoez; H. H. Ammar","Dept. of Comput. Sci., L'Aquila Univ., Italy; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","1","3","20","Performance is a nonfunctional software attribute that plays a crucial role in wide application domains spreading from safety-critical systems to e-commerce applications. Software risk can be quantified as a combination of the probability that a software system may fail and the severity of the damages caused by the failure. In this paper, we devise a methodology for estimation of performance-based risk factor, which originates from violations, of performance requirements, (namely, performance failures). The methodology elaborates annotated UML diagrams to estimate the performance failure probability and combines it with the failure severity estimate which is obtained using the functional failure analysis. We are thus able to determine risky scenarios as well as risky software components, and the analysis feedback can be used to improve the software design. We illustrate the methodology on an e-commerce case study using step-by step approach, and then provide a brief description of a case study based on large real system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392717","Index Terms- Nonfunctional requirements;software risk;software performance;UML;performance failure;Functional Failure Analysis.","Risk analysis;Software systems;Unified modeling language;Software performance;Software safety;Application software;Failure analysis;Computer Society;Feedback;Software design","software performance evaluation;risk analysis;electronic commerce;software fault tolerance;Unified Modeling Language;safety-critical software;formal specification","nonfunctional software;safety-critical systems;e-commerce applications;annotated UML diagrams;functional failure analysis;risky software components;model-based performance risk analysis","","30","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Locating features in source code","T. Eisenbarth; R. Koschke; D. Simon","Inst. of Comput. Sci., Stuttgart Univ., Germany; Inst. of Comput. Sci., Stuttgart Univ., Germany; Inst. of Comput. Sci., Stuttgart Univ., Germany","IEEE Transactions on Software Engineering","","2003","29","3","210","224","Understanding the implementation of a certain feature of a system requires identification of the computational units of the system that contribute to this feature. In many cases, the mapping of features to the source code is poorly documented. In this paper, we present a semiautomatic technique that reconstructs the mapping for features that are triggered by the user and exhibit an observable behavior. The mapping is in general not injective; that is, a computational unit may contribute to several features. Our technique allows for the distinction between general and specific computational units with respect to a given set of features. For a set of features, it also identifies jointly and distinctly required computational units. The presented technique combines dynamic and static analyses to rapidly focus on the system's parts that relate to a specific set of features. Dynamic information is gathered based on a set of scenarios invoking the features. Rather than assuming a one-to-one correspondence between features and scenarios as in earlier work, we can now handle scenarios that invoke many features. Furthermore, we show how our method allows incremental exploration of features while preserving the ""mental map"" the analyst has gained through the analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183929","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183929","","Computer Society;Software architecture;Instruction sets;Documentation;Degradation;Reverse engineering;Terminology","reverse engineering;program diagnostics;software architecture","formal concept analysis;feature location;program analysis;software architecture recovery;identification;program comprehension","","270","","46","","","","","","IEEE","IEEE Journals & Magazines"
"The class blueprint: visually supporting the understanding of glasses","S. Ducasse; M. Lanza","Software Composition Group, Bern Univ., Switzerland; NA","IEEE Transactions on Software Engineering","","2005","31","1","75","90","Understanding source code is an important task in the maintenance of software systems. Legacy systems are not only limited to procedural languages, but are also written in object-oriented languages. In such a context, understanding classes is a key activity as they are the cornerstone of the object-oriented paradigm and the primary abstraction from which applications are built. Such an understanding is however difficult to obtain because of reasons such as the presence of late binding and inheritance. A first level of class understanding consists of the understanding of its overall structure, the control flow among its methods, and the accesses on its attributes. We propose a novel visualization of classes called class blueprint that is based on a semantically enriched visualization of the internal structure of classes. This visualization allows a software engineer to build a first mental model of a class that he validates via opportunistic code-reading. Furthermore, we have identified visual patterns that represent recurrent situations and as such convey additional, information to the viewer. The contributions of this article are the class blueprint, a novel visualization of the internal structure of classes, the identification of visual patterns, and the definition of a vocabulary based on these visual patterns. We have performed several case studies of which one is presented in depth, and validated the usefulness of the approach in a controlled experiment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392721","Index Terms- Object-oriented programming;software visualization;reverse engineering;visual patterns;smalltalk.","Visualization;Software maintenance;Software systems;Application software;Cognitive science;Vocabulary;Programming profession;Reverse engineering;Phase measurement;Software measurement","object-oriented programming;Smalltalk;reverse engineering;program visualisation;software maintenance;inheritance;vocabulary;formal verification","software system maintenance;legacy systems;procedural languages;object-oriented languages;late binding;class understanding;class blueprint;opportunistic code-reading;software visualization;reverse engineering;visual patterns;Smalltalk","","43","","44","","","","","","IEEE","IEEE Journals & Magazines"
"WASP: Protecting Web Applications Using Positive Tainting and Syntax-Aware Evaluation","W. Halfond; A. Orso; P. Manolios","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","65","81","Many software systems have evolved to include a Web-based component that makes them available to the public via the Internet and can expose them to a variety of Web-based attacks. One of these attacks is SQL injection, which can give attackers unrestricted access to the databases that underlie Web applications and has become increasingly frequent and serious. This paper presents a new highly automated approach for protecting Web applications against SQL injection that has both conceptual and practical advantages over most existing techniques. From a conceptual standpoint, the approach is based on the novel idea of positive tainting and on the concept of syntax-aware evaluation. From a practical standpoint, our technique is precise and efficient, has minimal deployment requirements, and incurs a negligible performance overhead in most cases. We have implemented our techniques in the Web application SQL-injection preventer (WASP) tool, which we used to perform an empirical evaluation on a wide range of Web applications that we subjected to a large and varied set of attacks and legitimate accesses. WASP was able to stop all of the otherwise successful attacks and did not generate any false positives.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70748","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359474","Protection mechanisms;Security and Protection;Protection mechanisms;Security and Protection","Protection;Databases;Application software;Computer Society;Internet;Runtime;Data security;Information security;Software systems;Performance evaluation","Internet;security of data;SQL","WASP;positive tainting;syntax-aware evaluation;software systems;Internet;Web-based attacks;Web application SQL-injection preventer","","59","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Are Slice-Based Cohesion Metrics Actually Useful in Effort-Aware Post-Release Fault-Proneness Prediction? An Empirical Study","Y. Yang; Y. Zhou; H. Lu; L. Chen; Z. Chen; B. Xu; H. Leung; Z. Zhang","State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China; State Key Laboratory for Novel Software Technology, School of Software, Nanjing, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing, China; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong, China; State Key Laboratory of Computer Science, Institute of Software, Beijing, China","IEEE Transactions on Software Engineering","","2015","41","4","331","357","Background. Slice-based cohesion metrics leverage program slices with respect to the output variables of a module to quantify the strength of functional relatedness of the elements within the module. Although slice-based cohesion metrics have been proposed for many years, few empirical studies have been conducted to examine their actual usefulness in predicting fault-proneness. Objective. We aim to provide an in-depth understanding of the ability of slice-based cohesion metrics in effort-aware post-release fault-proneness prediction, i.e. their effectiveness in helping practitioners find post-release faults when taking into account the effort needed to test or inspect the code. Method. We use the most commonly used code and process metrics, including size, structural complexity, Halstead's software science, and code churn metrics, as the baseline metrics. First, we employ principal component analysis to analyze the relationships between slice-based cohesion metrics and the baseline metrics. Then, we use univariate prediction models to investigate the correlations between slice-based cohesion metrics and post-release fault-proneness. Finally, we build multivariate prediction models to examine the effectiveness of slice-based cohesion metrics in effort-aware post-release fault-proneness prediction when used alone or used together with the baseline code and process metrics. Results. Based on open-source software systems, our results show that: 1) slice-based cohesion metrics are not redundant with respect to the baseline code and process metrics; 2) most slice-based cohesion metrics are significantly negatively related to post-release fault-proneness; 3) slice-based cohesion metrics in general do not outperform the baseline metrics when predicting post-release fault-proneness; and 4) when used with the baseline metrics together, however, slice-based cohesion metrics can produce a statistically significant and practically important improvement of the effectiveness in effort-aware post-release fault-proneness prediction. Conclusion. Slice-based cohesion metrics are complementary to the most commonly used code and process metrics and are of practical value in the context of effort-aware post-release fault-proneness prediction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2370048","National Key Basic Research and Development Program of China; National Natural Science Foundation of China; National Natural Science Foundation of Jiangsu Province; National Science and Technology Major Project of China; Hong Kong Competitive Earmarked Research; PolyU; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6954519","Cohesion;metrics;slice-based;fault-proneness;prediction;effort-aware","Measurement;Software;Predictive models;Context;Complexity theory;Correlation;Laboratories","principal component analysis;public domain software;software metrics","effort aware post-release fault proneness prediction;slice-based cohesion metrics leverage program slices;structural complexity;Halstead's software science;code churn metrics;baseline metrics;principal component analysis;univariate prediction models;multivariate prediction models;baseline code;process metrics;open source software systems","","16","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Static Specification Mining Using Automata-Based Abstractions","S. Shoham; E. Yahav; S. J. Fink; M. Pistoia","Technion, Haifa; IBM Research, Hawthorne; IBM Research, Hawthorne; IBM Research, Hawthorne","IEEE Transactions on Software Engineering","","2008","34","5","651","666","We present a novel approach to client-side mining of temporal API specifications based on static analysis. Specifically, we present an interprocedural analysis over a combined domain that abstracts both aliasing and event sequences for individual objects. The analysis uses a new family of automata-based abstractions to represent unbounded event sequences, designed to disambiguate distinct usage patterns and merge similar usage patterns. Additionally, our approach includes an algorithm that summarizes abstract traces based on automata clusters, and effectively rules out spurious behaviors. We show experimental results mining specifications from a number of Java clients and APIs. The results indicate that effective static analysis for client-side mining requires fairly precise treatment of aliasing and abstract event sequences. Based on the results, we conclude that static client-side specification mining shows promise as a complement or alternative to dynamic approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.63","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4589215","Specification;Formal methods;Validation;Software/Program Verification;Specifying and Verifying and Reasoning about Programs;Specification;Formal methods;Validation;Software/Program Verification;Specifying and Verifying and Reasoning about Programs","Pattern analysis;Libraries;Abstracts;Clustering algorithms;Automata;Java;Software engineering;Formal specifications;Internet;Search engines","application program interfaces;data mining;finite automata;formal specification;Java;program diagnostics","static specification mining;automata-based abstractions;temporal API specifications;static analysis;distinct usage patterns;Java clients;client-side mining;abstract event sequences","","26","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Monitor-Based Instant Software Refactoring","H. Liu; X. Guo; W. Shao","Beijing Institute of Technology, Beijing; Beijing Institute of Technology, Beijing; Peking University, Beijing","IEEE Transactions on Software Engineering","","2013","39","8","1112","1126","Software refactoring is an effective method for improvement of software quality while software external behavior remains unchanged. To facilitate software refactoring, a number of tools have been proposed for code smell detection and/or for automatic or semi-automatic refactoring. However, these tools are passive and human driven, thus making software refactoring dependent on developers' spontaneity. As a result, software engineers with little experience in software refactoring might miss a number of potential refactorings or may conduct refactorings later than expected. Few refactorings might result in poor software quality, and delayed refactorings may incur higher refactoring cost. To this end, we propose a monitor-based instant refactoring framework to drive inexperienced software engineers to conduct more refactorings promptly. Changes in the source code are instantly analyzed by a monitor running in the background. If these changes have the potential to introduce code smells, i.e., signs of potential problems in the code that might require refactorings, the monitor invokes corresponding smell detection tools and warns developers to resolve detected smells promptly. Feedback from developers, i.e., whether detected smells have been acknowledged and resolved, is consequently used to optimize smell detection algorithms. The proposed framework has been implemented, evaluated, and compared with the traditional human-driven refactoring tools. Evaluation results suggest that the proposed framework could drive inexperienced engineers to resolve more code smells (by an increase of 140 percent) promptly. The average lifespan of resolved smells was reduced by 92 percent. Results also suggest that the proposed framework could help developers to avoid similar code smells through timely warnings at the early stages of software development, thus reducing the total number of code smells by 51 percent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6409360","Software refactoring;code smell detection;monitor;instant refactoring","Software;Monitoring;Detection algorithms;Cloning;Detectors;Algorithm design and analysis;Inspection","software maintenance;software quality","monitor-based instant software refactoring;software quality;software external behavior;code smell detection;smell detection algorithm","","19","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Perceived influences on implementing data warehousing","R. G. Little; M. L. Gibson","Dept. of Inf. Syst./Decision Sci., Auburn Univ., Montgomery, AL, USA; NA","IEEE Transactions on Software Engineering","","2003","29","4","290","296","This study surveyed data warehousing implementation project participants to determine what aspects they perceived should contribute to the implementation process. The respondents included: functional managers/staff, IS managers/staff, and consultants. The study identified eight significant factors that participants perceived should impact data warehouse implementation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1191794","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1191794","","Warehousing;Data warehouses;Decision support systems;Databases;Data mining;Decision making;Performance analysis;Instruments;Vehicles","data warehouses;decision support systems;data mining","data warehousing;data mining;decision support systems;online analytical processing;expert interviews;survey instrument;factor analysis;Cronbach's coefficient alpha;latent factor names","","16","","18","","","","","","IEEE","IEEE Journals & Magazines"
"The Link between Dependency and Cochange: Empirical Evidence","M. M. Geipel; F. Schweitzer","ETH Zurich, Zurich; ETH Zurich, Zurich","IEEE Transactions on Software Engineering","","2012","38","6","1432","1444","We investigate the relationship between class dependency and change propagation (cochange) in software written in Java. On the one hand, we find a strong correlation between dependency and cochange. Furthermore, we provide empirical evidence for the propagation of change along paths of dependency. These findings support the often alleged role of dependencies as propagators of change. On the other hand, we find that approximately half of all dependencies are never involved in cochanges and that the vast majority of cochanges pertain to only a small percentage of dependencies. This means that inferring the cochange characteristics of a software architecture solely from its dependency structure results in a severely distorted approximation of cochange characteristics. Any metric which uses dependencies alone to pass judgment on the evolvability of a piece of Java software is thus unreliable. As a consequence, we suggest to always take both the change characteristics and the dependency structure into account when evaluating software architecture.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363462","Modularity;class dependency;open source","Software development;Java;Open source software","Java;software architecture","class dependency;change propagation;Java software;cochange characteristic;software architecture","","10","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing software prediction techniques using simulation","M. Shepperd; G. Kadoda","Sch. of Design, Eng. & Comput., Bournemouth Univ., Poole, UK; NA","IEEE Transactions on Software Engineering","","2001","27","11","1014","1022","The need for accurate software prediction systems increases as software becomes much larger and more complex. We believe that the underlying characteristics: size, number of features, type of distribution, etc., of the data set influence the choice of the prediction system to be used. For this reason, we would like to control the characteristics of such data sets in order to systematically explore the relationship between accuracy, choice of prediction system, and data set characteristic. It would also be useful to have a large validation data set. Our solution is to simulate data allowing both control and the possibility of large (1000) validation cases. The authors compare four prediction techniques: regression, rule induction, nearest neighbor (a form of case-based reasoning), and neural nets. The results suggest that there are significant differences depending upon the characteristics of the data set. Consequently, researchers should consider prediction context when evaluating competing prediction systems. We observed that the more ""messy"" the data and the more complex the relationship with the dependent variable, the more variability in the results. In the more complex cases, we observed significantly different results depending upon the particular training set that has been sampled from the underlying data set. However, our most important result is that it is more fruitful to ask which is the best prediction system in a particular context rather than which is the ""best"" prediction system.","0098-5589;1939-3520;2326-3881","","10.1109/32.965341","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=965341","","Predictive models;Software systems;Machine learning;Uncertainty;Control systems;Accuracy;Nearest neighbor searches;Neural networks;Helium;Data engineering","software metrics;learning (artificial intelligence);case-based reasoning;neural nets;virtual machines","software prediction technique comparison;simulation;software prediction systems;prediction problem;data set characteristics;regression;rule induction;nearest neighbor;case-based reasoning;neural nets;small data sets;training set;machine learning","","128","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Adaptation of Service Protocols Using Process Algebra and On-the-Fly Reduction Techniques","R. Mateescu; P. Poizat; G. Salaün","Inria Grenoble-Rhône-Alpes/CONVECS, Montbonnot Saint-Martin; Université d'Evry Val d'Essonne, Paris and LRI UMR CNRS, Université Paris Sud, Orsay; Grenoble INP and Inria Grenoble-Rhône-Alpes/CONVECS, Montbonnot Saint-Martin","IEEE Transactions on Software Engineering","","2012","38","4","755","777","Reuse and composition are increasingly advocated and put into practice in modern software engineering. However, the software entities that are to be reused to build an application, e.g., services, have seldom been developed to integrate and to cope with the application requirements. As a consequence, they present mismatch, which directly hampers their reusability and the possibility of composing them. Software Adaptation has become a hot topic as a nonintrusive solution to work mismatch out using corrective pieces named adaptors. However, adaptation is a complex issue, especially when behavioral interfaces, or conversations, are taken into account. In this paper, we present state-of-the-art techniques to generate adaptors given the description of reused entities' conversations and an abstract specification of the way mismatch can be solved. We use a process algebra to encode the adaptation problem, and propose on-the-fly exploration and reduction techniques to compute adaptor protocols. Our approach follows the model-driven engineering paradigm, applied to service-oriented computing as a representative field of composition-based software engineering. We take service description languages as inputs of the adaptation process and we implement adaptors as centralized service compositions, i.e., orchestrations. Our approach is completely tool supported.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.62","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928357","Service composition;software adaptation;interfaces;protocols;mismatch;adaptation contracts;process algebra;on--the-fly generation;verification;tools","Adaptation model;Protocols;Contracts;Algebra;Semantics;Encoding;Computational modeling","formal specification;process algebra;protocols;service-oriented architecture;software reusability;specification languages","service protocol adaptation;process algebra;on-the-fly reduction techniques;composition;software entities;reusability;software adaptation;adaptors;reused entity conversations;abstract specification;on-the-fly exploration;model-driven engineering paradigm;service-oriented computing;composition-based software engineering;service description languages;centralized service compositions","","23","","65","","","","","","IEEE","IEEE Journals & Magazines"
"A formal specification and verification framework for Time Warp-based parallel simulation","P. Frey; R. Radhakrishnan; H. W. Carter; P. A. Wilsey; P. Alexander","Cadence Design Syst. Inc., San Jose, CA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","1","58","78","The paper describes a formal framework developed using the Prototype Verification System (PVS) to model and verify distributed simulation kernels based on the Time Warp paradigm. The intent is to provide a common formal base from which domain specific simulators can be modeled, verified, and developed. PVS constructs are developed to represent basic Time Warp constructs. Correctness conditions for Time Warp simulation are identified, describing causal ordering of event processing and correct rollback processing. The PVS theorem prover and type-check condition system are then used to verify all correctness conditions. In addition, the paper discusses the framework's reusability and extensibility properties in support of specification and verification of Time Warp extensions and optimizations.","0098-5589;1939-3520;2326-3881","","10.1109/32.979989","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979989","","Formal specifications;Time warp simulation","time warp simulation;formal specification;program verification;theorem proving;parallel programming","formal specification;formal verification framework;Time Warp-based parallel simulation;Prototype Verification System;distributed simulation kernels;Time Warp paradigm;common formal base;domain specific simulators;correctness conditions;causal ordering;event processing;correct rollback processing;PVS theorem prover;type-check condition system;parallel discrete event simulation","","5","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional Control of IP Media","P. Zave; E. Cheung","AT&T Laboratories-Research, Florham Park; AT&T Laboratories-Research, Florham Park","IEEE Transactions on Software Engineering","","2009","35","1","46","66","In many IP media services, the media channels are point-to-point, dynamic, and set up with the participation of one or more application servers, even thou the media packets themselves travel directly between media endpoints. The application servers must be programmed so that media behavior is globally correct, even though the servers may attempt to manipulate the same media channels concurrently and without knowledge of each other. Our proposed solution to this problem of compositional media control includes an architecture-independent descriptive model, a set of high-level programming primitives, a formal specification of their compositional semantics, a signaling protocol, an implementation, and partial verification of correctness. The paper includes performance analysis, comparison to related work, and principles for making other networked applications more compositional.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4569853","distributed applications;domain-specific architectures;protocol verification;protocol design;software/program verification;networks;streaming media;multimedia services;telecommunications;feature interaction;distributed applications;domain-specific architectures;protocol verification;protocol design;software/program verification;networks;streaming media;multimedia services;telecommunications;feature interaction","Application software;Network servers;Streaming media;Protocols;Internet telephony;Web server;Performance analysis;Computer architecture;Computer networks;Home computing","formal specification;IP networks;multimedia communication;signalling protocols","compositional control;IP media services;media channels;media packets;media endpoints;architecture-independent descriptive model;high-level programming primitives;formal specification;signaling protocol","","5","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Identification of Move Method Refactoring Opportunities","N. Tsantalis; A. Chatzigeorgiou","University of Macedonia, Thessaloniki; University of Macedonia, Thessaloniki","IEEE Transactions on Software Engineering","","2009","35","3","347","367","Placement of attributes/methods within classes in an object-oriented system is usually guided by conceptual criteria and aided by appropriate metrics. Moving state and behavior between classes can help reduce coupling and increase cohesion, but it is nontrivial to identify where such refactorings should be applied. In this paper, we propose a methodology for the identification of Move Method refactoring opportunities that constitute a way for solving many common feature envy bad smells. An algorithm that employs the notion of distance between system entities (attributes/methods) and classes extracts a list of behavior-preserving refactorings based on the examination of a set of preconditions. In practice, a software system may exhibit such problems in many different places. Therefore, our approach measures the effect of all refactoring suggestions based on a novel entity placement metric that quantifies how well entities have been placed in system classes. The proposed methodology can be regarded as a semi-automatic approach since the designer will eventually decide whether a suggested refactoring should be applied or not based on conceptual or other design quality criteria. The evaluation of the proposed approach has been performed considering qualitative, metric, conceptual, and efficiency aspects of the suggested refactorings in a number of open-source projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4752842","Move Method refactoring;Feature Envy;object-oriented design;Jaccard distance;design quality.","Software systems;Performance evaluation;Open source software;Runtime;Productivity;Data mining","object-oriented programming;software maintenance;software metrics","Move Method refactoring opportunity identification;object-oriented system;conceptual criteria;software metrics;feature envy","","139","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Semantics-Preserving Design of Embedded Control Software from Synchronous Models","L. Mangeruca; M. Baleani; A. Ferrari; A. Sangiovanni-Vincentelli","PARADES GEIE, via San Pantaleo 66, 00186 Rome, Italy; PARADES GEIE, via San Pantaleo 66, 00186 Rome, Italy; PARADES GEIE, via San Pantaleo 66, 00186 Rome, Italy; Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, CA 94720","IEEE Transactions on Software Engineering","","2007","33","8","497","509","The design of embedded controllers is experiencing a growth in complexity as embedded systems increase their functionality while they become ubiquitous in electronic appliances, cars, airplanes, etc. As requirements become more challenging, mathematical models gain importance for mastering complexity. Among the different computational models proposed, synchronous models have proved to be the most widely used for control dominated applications. While synchronous models simplify the way of dealing with concurrency by decoupling functional and timing aspects, their software implementation on multitasking and multiprocessor platforms is far from straightforward, because of the asynchronous nature of most industrial software platforms. Known solutions in the literature either restrict the solution space or focus on special cases. We present a method for preserving the synchronous semantics through buffer-based intertask communication mechanisms, grounded on an abstraction of the target platform. This allows us to deal with any task set and, most importantly, being independent of the implementation, to explore the design space effectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70718","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267022","Software design methodologies;protection mechanisms;embedded software design.","Embedded software;Space exploration;Control systems;Embedded system;Home appliances;Airplanes;Mathematical model;Pervasive computing;Computational modeling;Application software","embedded systems;multiprocessing programs;software engineering;systems analysis","semantics-preserving design;embedded control software;embedded systems;software implementation;multitasking;multiprocessor","","4","","43","","","","","","IEEE","IEEE Journals & Magazines"
"The Work Life of Developers: Activities, Switches and Perceived Productivity","A. N. Meyer; L. E. Barton; G. C. Murphy; T. Zimmermann; T. Fritz","University of Zurich, Zürich, Switzerland; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; Microsoft Research, Redmond, WA; University of Zurich, Zürich, Switzerland","IEEE Transactions on Software Engineering","","2017","43","12","1178","1193","Many software development organizations strive to enhance the productivity of their developers. All too often, efforts aimed at improving developer productivity are undertaken without knowledge about how developers spend their time at work and how it influences their own perception of productivity. To fill in this gap, we deployed a monitoring application at 20 computers of professional software developers from four companies for an average of 11 full work day in situ. Corroborating earlier findings, we found that developers spend their time on a wide variety of activities and switch regularly between them, resulting in highly fragmented work. Our findings extend beyond existing research in that we correlate developers' work habits with perceived productivity and also show productivity is a personal matter. Although productivity is personal, developers can be roughly grouped into morning, low-at-lunch and afternoon people. A stepwise linear regression per participant revealed that more user input is most often associated with a positive, and emails, planned meetings and work unrelated websites with a negative perception of productivity. We discuss opportunities of our findings, the potential to predict high and low productivity and suggest design approaches to create better tool support for planning developers' work day and improving their personal productivity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2656886","ABB; SNF; NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7829407","Productivity;developer activity;work fragmentation;interruptions;human factors;user studies","Productivity;Software development;Encoding;Human factors;Monitoring","regression analysis;software development management;software engineering","developer productivity;software development organizations;personal productivity;work unrelated websites;perceived productivity;highly fragmented work;professional software developers","","4","","64","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Modular Pluggable Analyses for Data Structure Consistency","V. Kuncak; P. Lam; K. Zee; M. C. Rinard","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","12","988","1005","Hob is a program analysis system that enables the focused application of multiple analyses to different modules in the same program. In our approach, each module encapsulates one or more data structures and uses membership in abstract sets to characterize how objects participate in data structures. Each analysis verifies that the implementation of the module 1) preserves important internal data structure consistency properties and 2) correctly implements a set algebra interface that characterizes the effects of operations on the data structure. Collectively, the analyses use the set algebra to 1) characterize how objects participate in multiple data structures and to 2) enable the interanalysis communication required to verify properties that depend on multiple modules analyzed by different analyses. We implemented our system and deployed several pluggable analyses, including a flag analysis plug-in for modules in which abstract set membership is determined by a flag field in each object, a PALE shape analysis plug-in, and a theorem proving plug-in for analyzing arbitrarily complicated data structures. Our experience shows that our system can effectively 1) verify the consistency of data structures encapsulated within a single module and 2) combine analysis results from different analysis plug-ins to verify properties involving objects shared by multiple modules analyzed by different analyses","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.125","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016574","Typestate;data structure;invariant;program analysis;program verification;shape analysis;formal methods;programming language design.","Data analysis;Data structures;Algorithm design and analysis;Shape;Algebra;Computer languages;Design methodology;Computer crashes;Scalability;Failure analysis","data structures;formal specification;program diagnostics;program verification","modular pluggable analyses;data structure consistency;program analysis system;set algebra interface;flag analysis plug-in;abstract set membership;PALE shape analysis plug-in;theorem proving plug-in;Hob analysis approach;program verification","","18","","80","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling Product Line Software Assets Using Domain-Specific Kits","N. I. Altintas; S. Cetin; A. H. Dogru; H. Oguztuzun","Cybersoft Information Technologies, Istanbul; Cybersoft Information Technologies, Istanbul; Middle East Technical University, Ankara; Middle East Technical University, Ankara","IEEE Transactions on Software Engineering","","2012","38","6","1376","1402","Software Product Line Engineering (SPLE) is a prominent paradigm for the assembly of a family of products using product line core assets. The modeling of software assets that together form the actual products is critical for achieving the strategic benefits of Software Product Lines (SPLs). We propose a feature-based approach to software asset modeling based on abstractions provided by Domain-Specific Kits (DSKs). This approach involves a software Asset Metamodel (AMM) used to derive Asset Modeling Languages (AMLs) that define reusable software assets in domain-specific terms. The approach also prescribes a roadmap for modeling these software assets in conjunction with the product line reference architecture. Asset capabilities can be modeled using feature diagrams as the external views of the software assets. Internal views can be expressed in terms of Domain-Specific Artifacts (DSAs) with Variability Points (VPs), where the domain-specific artifacts are created using Domain-Specific Kits. This approach produces loosely coupled and highly cohesive software assets that are reusable for multiple product lines. The approach is validated by assessing software asset reuse in two different product lines in the finance domain. We also evaluated the productivity gains in large-scale complex projects, and found that the approach yielded a significant reduction in the total project effort.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.109","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6065739","Asset modeling;domain-specific kits;feature models;reuse;software asset;software product lines","Software reliability;Computer architecture;Productivity;Programming;Complexity theory;Systematics","financial data processing;product development;project management;simulation languages;software reusability","product line software asset modelling;domain-specific kits;software product line engineering;SPLE;product line core assets;feature-based approach;DSK;software asset metamodel;AMM;asset modeling languages;AML;software asset reusability;domain-specific terms;product line reference architecture;feature diagrams;internal views;domain-specific artifacts;DSA;variability points;VP;finance domain;productivity gains;large-scale complex projects","","5","","86","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying Code of Individual Features in Client-Side Web Applications","J. Maras; M. Stula; J. Carlson; I. Crnkovic","University of Split, Split; University of Split, Split; Mälardalen University, Västerås; Mälardalen University, Västerås","IEEE Transactions on Software Engineering","","2013","39","12","1680","1697","Web applications are one of today's fastest growing software systems. Structurally, they are composed of two parts: the server side, used for data access and business logic, and the client side, used as a user interface. In recent years, with developers building complex interfaces, the client side is playing an increasingly important role. Unfortunately, the techniques and tools used to support development are not as advanced as in other disciplines. From the user's perspective, the client side offers a number of features that are relatively easy to distinguish. However, the same cannot be said for their implementation details. This makes the understanding, maintenance, and reuse of code difficult. The goal of the work presented in this paper is to improve reusability, maintainability, and performance of client-side web applications by identifying the code that implements a particular feature. We have evaluated the approach based on three different experiments: extracting features, extracting library functionalities, and page optimization. The evaluation shows that the method is able to identify the implementation details of individual features, and that by extracting the identified code, a considerable reduction in code size and increase in performance can be achieved.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6576749","Web applications;feature location;program slicing;code extraction","Feature extraction;HTML;Cascading style sheets;Browsers;Web and internet services;Optimization;Codes","feature extraction;Internet;user interfaces","individual features code identification;client-side Web application reusability;software systems;server side;data access;business logic;user interface;client-side Web application maintainability;client-side Web application performance;feature extraction;library functionality extraction;page optimization","","5","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Context-aware middleware for resource management in the wireless Internet","P. Bellavista; A. Corradi; R. Montanari; C. Stefanelli","Dept. of Electron., Comput. Sci., & Syst. (DEIS), Bologna Univ., Italy; Dept. of Electron., Comput. Sci., & Syst. (DEIS), Bologna Univ., Italy; Dept. of Electron., Comput. Sci., & Syst. (DEIS), Bologna Univ., Italy; NA","IEEE Transactions on Software Engineering","","2003","29","12","1086","1099","The provisioning of Web services over the wireless Internet introduces novel challenging issues for service design and implementation: from user/terminal mobility during service execution, to wide heterogeneity of portable access devices and unpredictable modifications in accessible resources. In this scenario, there are frequent provision-time changes in the context, defined as the logical set of accessible resources depending on client location, access terminal capabilities, and system/service management policies. The development of context-dependent services requires novel middlewares with full context visibility. We propose a middleware for context-aware resource management, called CARMEN, capable of supporting the automatic reconfiguration of wireless Internet services in response to context changes without any intervention on the service logic. CARMEN determines the context on the basis of metadata, which include declarative management policies and profiles for user preferences, terminal capabilities, and resource characteristics. In addition, CARMEN exploits the mobile agent technology to implement mobile middleware components that follow the provision-time movement of clients to support locally their customized service access. The proposed middleware shows how metadata and mobile agents can favor component reusability and automatic service reconfiguration, by reducing the development/ deployment complexity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265523","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265523","","Middleware;Resource management;Web and internet services;Context-aware services;Personal digital assistants;Mobile computing;Mobile agents;Computer Society;Web services;Automatic logic units","Internet;middleware;mobile computing;mobile radio;mobile agents","context-aware middleware;resource management;wireless Internet;Web services;user/terminal mobility;portable access devices;provision-time changes;CARMEN;automatic services reconfiguration;metadata;declarative management policies;user preferences;terminal capabilities;resource characteristics;mobile agent technology;mobile middleware components","","101","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Modular operational test plans for inferences on software reliability based on a Markov model","J. Rajgopal; M. Mazumdar","Dept. of Ind. Eng., Pittsburgh Univ., PA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","4","358","363","This paper considers the problem of assessing the reliability of a software system that can be decomposed into a finite number of modules. It uses a Markovian model for the transfer of control between modules in order to develop the system reliability expression in terms of the module reliabilities. An operational test procedure is considered in which only the individual modules are tested and the system is considered acceptable if, and only if, no failures are observed. The minimum number of tests required of each module is determined such that the probability of accepting a system whose reliability falls below a specified value R/sub 0/ is less than a specified small fraction /spl beta/. This sample size determination problem is formulated as a two-stage mathematical program and an algorithm is developed for solving this problem. Two examples from the literature are considered to demonstrate the procedure.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995424","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995424","","Software testing;Software reliability","software reliability;program testing;Markov processes;probability","modular operational test plans;software reliability;Markov model;system reliability expression;probability;software testing;sample size determination problem;two-stage mathematical programming","","16","","23","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Literature Review on Fault Prediction Performance in Software Engineering","T. Hall; S. Beecham; D. Bowes; D. Gray; S. Counsell","Brunel University, Uxbridge; University of Limerick, Limerick; University of Hertfordshire, Hatfield; University of Hertfordshire, Hatfield; Brunel University, Uxbridge","IEEE Transactions on Software Engineering","","2012","38","6","1276","1304","Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.103","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6035727","Systematic literature review;software fault prediction","Predictive models;Context modeling;Software testing;Data models;Systematics;Analytical models;Fault diagnosis","Bayes methods;regression analysis;software fault tolerance;software quality","systematic literature review;fault prediction performance;software engineering;cost reduction;software quality;independent variables;fault prediction models;contextual information;methodological information;simple modeling techniques;naive Bayes;logistic regression;feature selection;predictive performance;fault prediction study;reliable methodology","","259","","241","","","","","","IEEE","IEEE Journals & Magazines"
"SLA-Driven Clustering of QoS-Aware Application Servers","G. Lodi; F. Panzieri; D. Rossi; E. Turrini","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","3","186","197","In this paper, we discuss the design, implementation, and experimental evaluation of a middleware architecture for enabling service level agreement (SLA)-driven clustering of QoS-aware application servers. Our middleware architecture supports application server technologies with dynamic resource management: application servers can dynamically change the amount of clustered resources assigned to hosted applications on-demand so as to meet application-level quality of service (QoS) requirements. These requirements can include timeliness, availability, and high throughput and are specified in SLAs. A prototype of our architecture has been implemented using the open-source J2EE application server JBoss. The evaluation of this prototype shows that our approach makes possible JBoss' resource usage optimization and allows JBoss to effectively meet the QoS requirements of the applications it hosts, i.e., to honor the SLAs of those applications","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084136","Service Level Agreement;Quality of Service;QoS-aware application server;QoS-aware cluster;dynamic cluster configuration;monitoring;load balancing.","Quality of service;Load management;Middleware;Resource management;Availability;Monitoring;Runtime;Application software;Prototypes;Scalability","Java;middleware;quality of service;resource allocation;system monitoring","service level agreement-driven clustering;QoS-aware application server technology;middleware architecture;dynamic resource management;JBoss open-source J2EE application server;JBoss resource usage optimization;dynamic cluster configuration;runtime monitoring;load balancing","","45","","50","","","","","","IEEE","IEEE Journals & Magazines"
"A Second Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems","T. Galinac Grbac; P. Runeson; D. Huljenić","Univesity of Rijeka, Rijeka; Lund University, Lund; Ericsson Nikola Tesla, Zagreb","IEEE Transactions on Software Engineering","","2013","39","4","462","476","Background: Software engineering is searching for general principles that apply across contexts, for example, to help guide software quality assurance. Fenton and Ohlsson presented such observations on fault distributions, which have been replicated once. Objectives: We aimed to replicate their study again to assess the robustness of the findings in a new environment, five years later. Method: We conducted a literal replication, collecting defect data from five consecutive releases of a large software system in the telecommunications domain, and conducted the same analysis as in the original study. Results: The replication confirms results on unevenly distributed faults over modules, and that fault proneness distributions persist over test phases. Size measures are not useful as predictors of fault proneness, while fault densities are of the same order of magnitude across releases and contexts. Conclusions: This replication confirms that the uneven distribution of defects motivates uneven distribution of quality assurance efforts, although predictors for such distribution of efforts are not sufficiently precise.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6235962","Software fault distributions;software metrics;empirical research;replication","Testing;Measurement;Context;Software;Software engineering;Complexity theory;Telecommunications","software fault tolerance;software quality","second replicated quantitative analysis;fault distribution;complex software system;software engineering;literal replication;telecommunications domain;data analysis;data collection;fault proneness distribution;size measure;fault density;quality assurance effort","","14","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Coordination Breakdowns and Their Impact on Development Productivity and Software Failures","M. Cataldo; J. D. Herbsleb","Robert Bosch LLC; Carnegie Mellon University, Pittsburgh","IEEE Transactions on Software Engineering","","2013","39","3","343","360","The success of software development projects depends on carefully coordinating the effort of many individuals across the multiple stages of the development process. In software engineering, modularization is the traditional technique intended to reduce the interdependencies among modules that constitute a system. Reducing technical dependencies, the theory argues, results in a reduction of work dependencies between teams developing interdependent modules. Although that research stream has been quite influential, it considers a static view of the problem of coordination in engineering activities. Building on a dynamic view of coordination, we studied the relationship between socio-technical congruence and software quality and development productivity. In order to investigate the generality of our findings, our analyses were performed on two large-scale projects from two companies with distinct characteristics in terms of product and process maturity. Our results revealed that the gaps between coordination requirements and the actual coordination activities carried out by the developers significantly increased software failures. Our analyses also showed that higher levels of congruence are associated with improved development productivity. Finally, our results showed the congruence between dependencies and coordinative actions is critical both in mature development settings as well as in novel and dynamic development contexts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6205767","Metrics/measurement;productivity;organizational management and coordination;quality analysis and evaluation","Productivity;Programming;Software quality;Context;Complexity theory;Organizations","software metrics;software quality;software reliability","coordination breakdowns;development productivity;software failures;software development projects;development process;software engineering;modularization;technical dependency;work dependency;interdependent modules;engineering activity;socio-technical congruence;software quality;large-scale projects;product maturity;process maturity;coordination requirements;coordination activity;dynamic development contexts","","40","","78","","","","","","IEEE","IEEE Journals & Magazines"
"A Dissection of the Test-Driven Development Process: Does It Really Matter to Test-First or to Test-Last?","D. Fucci; H. Erdogmus; B. Turhan; M. Oivo; N. Juristo","Department of Information Processing Science, University of Oulu, Oulu, Finland; Silicon Valley Campus, Carnegie Mellon University, Pittsburgh, PA; Department of Information Processing Science, University of Oulu, Oulu, Finland; Department of Information Processing Science, University of Oulu, Oulu, Finland; Department of Information Processing Science, University of Oulu, Oulu, Finland","IEEE Transactions on Software Engineering","","2017","43","7","597","614","Background: Test-driven development (TDD) is a technique that repeats short coding cycles interleaved with testing. The developer first writes a unit test for the desired functionality, followed by the necessary production code, and refactors the code. Many empirical studies neglect unique process characteristics related to TDD iterative nature. Aim: We formulate four process characteristic: sequencing, granularity, uniformity, and refactoring effort. We investigate how these characteristics impact quality and productivity in TDD and related variations. Method: We analyzed 82 data points collected from 39 professionals, each capturing the process used while performing a specific development task. We built regression models to assess the impact of process characteristics on quality and productivity. Quality was measured by functional correctness. Result: Quality and productivity improvements were primarily positively associated with the granularity and uniformity. Sequencing, the order in which test and production code are written, had no important influence. Refactoring effort was negatively associated with both outcomes. We explain the unexpected negative correlation with quality by possible prevalence of mixed refactoring. Conclusion: The claimed benefits of TDD may not be due to its distinctive test-first dynamic, but rather due to the fact that TDD-like processes encourage fine-grained, steady steps that improve focus and flow.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2616877","Academy of Finland; TEKES; FiDiPro; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7592412","Test-driven development;empirical investigation;process dimensions;external quality;productivity","Testing;Productivity;Context;Companies;Sequential analysis;Conferences","program testing;regression analysis;software engineering","test-driven development process;TDD technique;TDD iterative nature;sequencing characteristic;granularity characteristic;uniformity characteristic;refactoring effort characteristic;regression model","","2","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Measuring the Discriminative Power of Object-Oriented Class Cohesion Metrics","J. Al Dallal","Kuwait University, Kuwait","IEEE Transactions on Software Engineering","","2011","37","6","788","804","Several object-oriented cohesion metrics have been proposed in the literature. These metrics aim to measure the relationship between class members, namely, methods and attributes. Different metrics use different models to represent the connectivity pattern of cohesive interactions (CPCI) between class members. Most of these metrics are normalized to allow for easy comparison of the cohesion of different classes. However, in some cases, these metrics obtain the same cohesion values for different classes that have the same number of methods and attributes but different CPCIs. This leads to incorrectly considering the classes to be the same in terms of cohesion, even though their CPCIs clearly indicate that the degrees of cohesion are different. We refer to this as a lack of discrimination anomaly (LDA) problem. In this paper, we list and discuss cases in which the LDA problem exists, as expressed through the use of 16 cohesion metrics. In addition, we empirically study the frequent occurrence of the LDA problem when the considered metrics are applied to classes in five open source Java systems. Finally, we propose a metric and a simulation-based methodology to measure the discriminative power of cohesion metrics. The discrimination metric measures the probability that a cohesion metric will produce distinct cohesion values for classes with the same number of attributes and methods but different CPCIs. A highly discriminating cohesion metric is more desirable because it exhibits a lower chance of incorrectly considering classes to be cohesively equal when they have different CPCIs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.97","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5639020","Cohesive interactions;connectivity pattern;discrimination metric;discriminative power;lack of discrimination anomaly;object-oriented class cohesion.","Power measurement;Object oriented modeling;Software measurement;Phase measurement","Java;laser Doppler anemometry;object-oriented methods;public domain software","discriminative power measurement;object oriented class cohesion metrics;cohesive interactions;discrimination anomaly;open source Java systems;simulation based methodology","","23","","41","","","","","","IEEE","IEEE Journals & Magazines"
"A Model of Data Warehousing Process Maturity","A. Sen; K. Ramamurthy; A. P. Sinha","Texas A&M University, College Station; University of Wisconsin-Milwaukee, Milwaukee; University of Wisconsin-Milwaukee, Milwaukee","IEEE Transactions on Software Engineering","","2012","38","2","336","353","Even though data warehousing (DW) requires huge investments, the data warehouse market is experiencing incredible growth. However, a large number of DW initiatives end up as failures. In this paper, we argue that the maturity of a data warehousing process (DWP) could significantly mitigate such large-scale failures and ensure the delivery of consistent, high quality, “single-version of truth” data in a timely manner. However, unlike software development, the assessment of DWP maturity has not yet been tackled in a systematic way. In light of the critical importance of data as a corporate resource, we believe that the need for a maturity model for DWP could not be greater. In this paper, we describe the design and development of a five-level DWP maturity model (DWP-M) over a period of three years. A unique aspect of this model is that it covers processes in both data warehouse development and operations. Over 20 key DW executives from 13 different corporations were involved in the model development process. The final model was evaluated by a panel of experts; the results strongly validate the functionality, productivity, and usability of the model. We present the initial and final DWP-M model versions, along with illustrations of several key process areas at different levels of maturity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.2","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680911","Data warehousing process;design-science research;model validation;software maturity models.","Data warehouses;Business;Warehousing;Software;Programming;Data mining;Standards organizations","data warehouses","data warehousing process maturity;data warehouse market;large-scale failures;DWP-M model","","19","","75","","","","","","IEEE","IEEE Journals & Magazines"
"The effectiveness of software development technical reviews: a behaviorally motivated program of research","C. Sauer; D. R. Jeffery; L. Land; P. Yetton","Templeton Coll., Oxford Univ., UK; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","1","1","14","Software engineers use a number of different types of software development technical review (SDTR) for the purpose of detecting defects in software products. This paper applies the behavioral theory of group performance to explain the outcomes of software reviews. A program of empirical research is developed, including propositions to both explain review performance and identify ways of improving review performance based on the specific strengths of individuals and groups. Its contributions are to clarify our understanding of what drives defect detection performance in SDTRs and to set an agenda for future research. In identifying individuals' task expertise as the primary driver of review performance, the research program suggests specific points of leverage for substantially improving review performance. It points to the importance of understanding software reading expertise and implies the need for a reconsideration of existing approaches to managing reviews.","0098-5589;1939-3520;2326-3881","","10.1109/32.825763","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=825763","","Programming;Software quality;Inspection;Management training;Engineering management;Software safety;Costs;Humans;Technology management;Australia","software quality;program testing;program debugging","software development technical reviews;software defect detection;behavioral theory;group performance;research;software reading;software quality","","100","","88","","","","","","IEEE","IEEE Journals & Magazines"
"Management of performance requirements for information systems","B. A. Nixon","Dept. of Comput. Sci., Toronto Univ., Ont., Canada","IEEE Transactions on Software Engineering","","2000","26","12","1122","1146","The management of performance requirements is a major challenge for information systems as well as other software systems. This is because performance requirements can have a global impact on the target system. In addition, there are interactions and trade-offs among performance requirements, other nonfunctional requirements (NFRs), and the numerous alternatives for the target system. To provide a systematic approach to managing performance requirements, this paper presents a performance requirements framework (PeRF). It integrates and catalogues a variety of kinds of knowledge of information systems and performance. These include: performance concepts, software performance engineering principles for building performance into systems, and information systems development knowledge. In addition, layered structures organize performance knowledge and the development process. All this knowledge is represented using an existing goal-oriented approach, the ""NFR framework"", which offers a developer-directed graphical treatment for stating NFRs, analyzing and interrelating them, and determining the impact of decisions upon NFRs. This approach allows customized solutions to be built, taking into account the characteristics of the particular domain. The use of PeRF in managing performance requirements is illustrated in a study of performance requirements and other NFRs for a university student record system. This paper concludes with a summary of other studies of information systems, tool support and directions for future work.","0098-5589;1939-3520;2326-3881","","10.1109/32.888627","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888627","","Management information systems;Knowledge engineering;Delay;Software quality;Software performance;Information systems;Software systems;Design engineering;Data engineering;Knowledge management","information systems;software performance evaluation;systems analysis;educational administrative data processing;software quality;software management","performance requirements management;information systems;nonfunctional requirements;performance requirements framework;PeRF;performance concepts;software performance engineering principles;systems development knowledge;layered structures;goal-oriented approach;NFR framework;developer-directed graphical treatment;decision impact;customized solutions;university student record system;tool support;requirements engineering;semantic data models;catalogues;Year-2000 compliance","","21","","65","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study on the Relationship Between Software Design Quality, Development Effort and Governance in Open Source Projects","E. Capra; C. Francalanci; F. Merlo","Politecnico di Milano, Milano; Politecnico di Milano, Milan; Politecnico di Milano, Milan","IEEE Transactions on Software Engineering","","2008","34","6","765","782","The relationship among software design quality, development effort, and governance practices is a traditional research problem. However, the extent to which consolidated results on this relationship remain valid for open source (OS) projects is an open research problem. An emerging body of literature contrasts the view of open source as an alternative to proprietary software and explains that there exists a continuum between closed and open source projects. This paper hypothesizes that as projects approach the OS end of the continuum, governance becomes less formal. In turn a less formal governance is hypothesized to require a higher-quality code as a means to facilitate coordination among developers by making the structure of code explicit and facilitate quality by removing the pressure of deadlines from contributors. However, a less formal governance is also hypothesized to increase development effort due to a more cumbersome coordination overhead. The verification of research hypotheses is based on empirical data from a sample of 75 major OS projects. Empirical evidence supports our hypotheses and suggests that software quality, mainly measured as coupling and inheritance, does not increase development effort, but represents an important managerial variable to implement the more open governance approach that characterizes OS projects which, in turn, increases development effort.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4599582","Cost estimation;Qualitative process analysis;Organizational management and coordination;Management;Complexity measures;Process metrics;Metrics/Measurement;Software Engineering;Software/Software Engineering;Cost estimation;Qualitative process analysis;Organizational management and coordination;Management;Complexity measures;Process metrics;Metrics/Measurement;Software Engineering;Software/Software Engineering","Software design;Open source software;Software quality;Costs;Project management;Software measurement;Quality management;Companies;Software development management;Equations","project management;public domain software;software development management;software quality","software design quality;open source projects;qualitative process analysis;structural equation modeling;software cost estimation;software development effort","","41","","111","","","","","","IEEE","IEEE Journals & Magazines"
"Finding Clones with Dup: Analysis of an Experiment","B. S. Baker","140 North Road, Berkeley Heights, NJ 07922","IEEE Transactions on Software Engineering","","2007","33","9","608","621","An experiment was carried out by a group of scientists to compare different tools and techniques for detecting duplicated or near-duplicated source code. The overall comparative results are presented elsewhere. This paper takes a closer look at the results for one tool, Dup, which finds code sections that are textually the same or the same except for systematic substitution of parameters such as identifiers and constants. Various factors that influenced the results are identified and their impact on the results is assessed via rerunning Dup with changed options and modifications. These improve the performance of Dup with regard to the experiment and could be incorporated into a postprocessor to be used with other tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70720","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288194","Redundant code;duplicated code;softwareclones","Cloning;Java;Computer bugs;Plagiarism;Data structures;Design methodology;Software tools;Tree graphs;Particle measurements","software reusability;software tools","Dup;source code;code sections;systematic substitution;postprocessor","","31","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Studying the fault-detection effectiveness of GUI test cases for rapidly evolving software","A. M. Memon; Q. Xie","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","2005","31","10","884","896","Software is increasingly being developed/maintained by multiple, often geographically distributed developers working concurrently. Consequently, rapid-feedback-based quality assurance mechanisms such as daily builds and smoke regression tests, which help to detect and eliminate defects early during software development and maintenance, have become important. This paper addresses a major weakness of current smoke regression testing techniques, i.e., their inability to automatically (re)test graphical user interfaces (GUIs). Several contributions are made to the area of GUI smoke testing. First, the requirements for GUI smoke testing are identified and a GUI smoke test is formally defined as a specialized sequence of events. Second, a GUI smoke regression testing process called daily automated regression tester (DART) that automates GUI smoke testing is presented. Third, the interplay between several characteristics of GUI smoke test suites including their size, fault detection ability, and test oracles is empirically studied. The results show that: 1) the entire smoke testing process is feasible in terms of execution time, storage space, and manual effort, 2) smoke tests cannot cover certain parts of the application code, 3) having comprehensive test oracles may make up for not having long smoke test cases, and 4) using certain oracles can make up for not having large smoke test suites.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.117","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542069","Index Terms- Smoke testing;GUI testing;test oracles;empirical studies;regression testing.","Graphical user interfaces;Software testing;Computer aided software engineering;Automatic testing;Application software;Software maintenance;Fault detection;Quality assurance;Smoke detectors;System testing","software prototyping;program testing;fault diagnosis;software maintenance;software quality;software fault tolerance;graphical user interfaces;formal specification;formal verification","fault-detection;GUI test cases;graphical user interfaces;rapidly evolving software;quality assurance mechanism;software development;software maintenance;smoke regression testing technique;daily automated regression tester;test oracles","","124","","38","","","","","","IEEE","IEEE Journals & Magazines"
"A reflective implementation of Java multi-methods","R. Forax; E. Duris; G. Roussel","Inst. Gaspard-Monge, Univ. de Marne-la-Vallee, France; Inst. Gaspard-Monge, Univ. de Marne-la-Vallee, France; Inst. Gaspard-Monge, Univ. de Marne-la-Vallee, France","IEEE Transactions on Software Engineering","","2004","30","12","1055","1071","In Java, method implementations are chosen at runtime by late-binding with respect to the runtime class of just the receiver argument. However, in order to simplify many programming designs, late-binding with respect to the dynamic type of all arguments is sometimes desirable. This behavior, usually provided by multimethods, is known as multipolymorphism. This work presents a new multimethod implementation based on the standard Java reflection mechanism. Provided as a package, it does not require any language extension or any virtual machine modification. The design issues of this reflective implementation are presented together with a new and simple multimethod dispatch algorithm that efficiently supports class loading at runtime. This implementation provides a practicable and fully portable multimethod solution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377197","Index Terms- Java;polymorphism;reflection;dynamic class loading.","Java;Runtime;Reflection;Dynamic programming;Algorithm design and analysis;Packaging machines;Virtual machining;Software maintenance;Libraries","Java;object-oriented programming","Java multimethods;late-binding;multipolymorphism;Java reflection mechanism;dynamic class loading","","5","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A learning agent that assists the browsing of software libraries","C. G. Drummond; D. Ionescu; R. C. Holte","Sch. of Inf. Technol. & Eng., Ottawa Univ., Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2000","26","12","1179","1196","Locating software items is difficult, even for knowledgeable software designers, when searching in large, complex and continuously growing libraries. This paper describes a technique we term ""active browsing"". An active browser suggests to the designer items it estimates to be close to the target of the search. The novel aspect of active browsing is that it is entirely unobtrusive: it infers its similarity measure from the designer's normal browsing actions, without any special input. Experiments are presented in which the active browsing system succeeds 40% of the time in identifying the target before the designer has found it. An additional experiment indicates that this approach does, indeed, speed up searches.","0098-5589;1939-3520;2326-3881","","10.1109/32.888631","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888631","","Software libraries;Software design;Programming;Humans;Documentation;Costs;Information retrieval;Power system reliability;Organizing","software libraries;online front-ends;learning (artificial intelligence);software agents;software reusability;user modelling;utility programs","learning agent;software library browsing assistant;software items location;active browsing;similarity measure;search speed;software reuse;software library searching;simulated human users","","17","","47","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of open-source and closed-source software products","J. W. Paulson; G. Succi; A. Eberlein","Gen. Dynamics Canada Ltd., Calgary, Alta., Canada; NA; NA","IEEE Transactions on Software Engineering","","2004","30","4","246","256","We describe an empirical study of open-source and closed-source software projects. The motivation for this research is to quantitatively investigate common perceptions about open-source projects, and to validate these perceptions through an empirical study. We investigate the hypothesis that open-source software grows more quickly, but does not find evidence to support this. The project growth is similar for all the projects in the analysis, indicating that other factors may limit growth. The hypothesis that creativity is more prevalent in open-source software is also examined, and evidence to support this hypothesis is found using the metric of functions added over time. The concept of open-source projects succeeding because of their simplicity is not supported by the analysis, nor is the hypothesis of open-source projects being more modular. However, the belief that defects are found and fixed more rapidly in open-source projects is supported by an analysis of the functions modified. We find support for two of the five common beliefs and conclude that, when implementing or switching to the open-source development model, practitioners should ensure that an appropriate metrics collection strategy is in place to verify the perceived benefits.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274044","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274044","","Open source software;Computer Society;Programming;Software metrics;Computer industry;Scalability;Data security;Costs;Software systems","public domain software;software metrics;formal verification","open-source software project;closed-source software projects;open-source development model;software metrics;software growth models","","106","","14","","","","","","IEEE","IEEE Journals & Magazines"
"A formal architectural model for logical agent mobility","Dianxiang Xu; Jianwen Yin; Yi Deng; Junhua Ding","Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","1","31","45","The process of agent migration is the major difference between logical code mobility of software agents and physical mobility of mobile nodes in ad hoc networks. Without considering agent transfer, it would make little sense to mention the modeling of strong code mobility, which aims to make a migrated agent restarted exactly from the state when it was stopped before migration. From the perspective of system's architecture, this paper proposes a two-layer approach for the formal modeling of logical agent mobility (LAM) using predicate/transition (PrT) nets. We view a mobile agent system as a set of agent spaces and agents could migrate from one space to another. Each agent space is explicitly abstracted to be a component, consisting of an environmental part and an internal connector dynamically binding agents with their environment. We use a system net, agent nets, and a connector net to model the environment, agents, and the connector, respectively. In particular, agent nets are packed up as parts of tokens in system nets, so that agent transfer and location change are naturally captured by transition firing (token game) in Petri nets. Agent nets themselves are active only at specific places and disabled at all the other places in a system net. The semantics of such a two-layer LAM model is defined by transforming it into a PrT net. This facilitates the analysis of several properties about location, state, and connection. In addition, this paper also presents a case study of modeling and analyzing an information retrieval system with mobile agents.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166587","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166587","","Mobile agents;Connectors;Software agents;Ad hoc networks;Computer Society;Mobile computing;Petri nets;Information analysis;Information retrieval;Software architecture","mobile agents;Petri nets;reachability analysis;software architecture;ad hoc networks","agent migration;logical code mobility;software agents;physical mobility;ad hoc networks;strong code mobility;two-layer approach;logical agent mobility;Petri nets;information retrieval system;software architecture","","36","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Using Stochastic State Classes in Quantitative Evaluation of Dense-Time Reactive Systems","E. Vicario; L. Sassoli; L. Carnevali","Universit&#x0E0; di Firenze, Firenze; Universit&#x0E0; di Firenze, Firenze; Universit&#x0E0; di Firenze, Firenze","IEEE Transactions on Software Engineering","","2009","35","5","703","719","In the verification of reactive systems with nondeterministic densely valued temporal parameters, the state-space can be covered through equivalence classes, each composed of a discrete logical location and a dense variety of clock valuations encoded as a difference bounds matrix (DBM). The reachability relation among such classes enables qualitative verification of properties pertaining events ordering and stimulus/response deadlines, but it does not provide any measure of probability for feasible behaviors. We extend DBM equivalence classes with a density-function which provides a measure for the probability of individual states. To this end, we extend time Petri nets by associating a probability density-function to the static firing interval of each nondeterministic transition. We then explain how this stochastic information induces a probability distribution for the states contained within a DBM class and how this probability evolves in the enumeration of the reachability relation among classes. This enables the construction of a stochastic transition system which supports correctness verification based on the theory of TPNs, provides a measure of probability for each feasible run, enables steady-state analysis based on Markov renewal theory. In so doing, we provide a means to identify feasible behaviors and to associate them with a measure of probability in models with multiple concurrent generally distributed nondeterministic timers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5010440","Correctness verification;performance and dependability evaluation;stochastic Time Petri nets;non-Markovian Stochastic Petri Nets;dense-time state-space analysis;Difference Bounds Matrix;Markov Renewal Theory.","Stochastic systems;Stochastic processes;Petri nets;Delay;Clocks;Timing;Density functional theory;Concurrent computing;Computer Society;Cost accounting","equivalence classes;Markov processes;matrix algebra;Petri nets;program verification;reachability analysis;statistical distributions","stochastic state-space class analysis;dense-time reactive system verification;quantitative evaluation;nondeterministic densely-valued temporal parameter;equivalence class;discrete logical location;difference bound matrix;DBM;reachability relation;qualitative correctness verification;event ordering;stimulus/response deadline;stochastic timed Petri net;probability density distribution function measure;static firing interval;nondeterministic transition;stochastic transition system;STPN theory;Markov renewal theory;nondeterministic timer","","27","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic coupling measurement for object-oriented software","E. Arisholm; L. C. Briand; A. Foyen","Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; NA; NA","IEEE Transactions on Software Engineering","","2004","30","8","491","506","The relationships between coupling and external quality factors of object-oriented software have been studied extensively for the past few years. For example, several studies have identified clear empirical relationships between class-level coupling and class fault-proneness. A common way to define and measure coupling is through structural properties and static code analysis. However, because of polymorphism, dynamic binding, and the common presence of unused (""dead"") code in commercial software, the resulting coupling measures are imprecise as they do not perfectly reflect the actual coupling taking place among classes at runtime. For example, when using static analysis to measure coupling, it is difficult and sometimes impossible to determine what actual methods can be invoked from a client class if those methods are overridden in the subclasses of the server classes. Coupling measurement has traditionally been performed using static code analysis, because most of the existing work was done on nonobject oriented code and because dynamic code analysis is more expensive and complex to perform. For modern software systems, however, this focus on static analysis can be problematic because although dynamic binding existed before the advent of object-orientation, its usage has increased significantly in the last decade. We describe how coupling can be defined and precisely measured based on dynamic analysis of systems. We refer to this type of coupling as dynamic coupling. An empirical evaluation of the proposed dynamic coupling measures is reported in which we study the relationship of these measures with the change proneness of classes. Data from maintenance releases of a large Java system are used for this purpose. Preliminary results suggest that some dynamic coupling measures are significant indicators of change proneness and that they complement existing coupling measures based on static analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316867","Index Terms- Coupling measurement;change predictions;quality modeling;maintenance.","Software measurement;Object oriented modeling;Software quality;Predictive models;Performance evaluation;Performance analysis;Q factor;Fault diagnosis;Runtime;Software systems","object-oriented programming;program diagnostics;software quality;software maintenance;inheritance","dynamic coupling;object-oriented software;class-level coupling;class fault-proneness;static code analysis;polymorphism;dynamic binding;software quality modeling;software maintenance","","142","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Are Fix-Inducing Changes a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Prediction","S. McIntosh; Y. Kamei","Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Principles of Software Languages Group (POSL), Kyushu University, Fukuoka, Japan","IEEE Transactions on Software Engineering","","2018","44","5","412","428","Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2693980","Natural Sciences and Engineering Research Council of Canada (NSERC); JSPS KAKENHI; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7898457","Just-In-Time prediction;defect prediction;mining software repositories","Predictive models;Data models;Software;Complexity theory;Market research;Context modeling;Calibration","data mining;just-in-time;learning (artificial intelligence);public domain software;software management;software quality;source code (software)","Just-In-Time models;fix-inducing code changes;code change properties;target moving;just-in-time defect prediction;fix-inducing changes;JIT models;OpenStack systems;Qt systems;mining software repositories","","2","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Trace Analysis of Discrete-Event System Models","P. Kemper; C. Tepper","College of William and Mary, Williamsburg; ITGAIN Consulting, Hanover","IEEE Transactions on Software Engineering","","2009","35","2","195","208","In this paper, we describe a novel technique that helps a modeler gain insight into the dynamic behavior of a complex stochastic discrete event simulation model based on trace analysis. We propose algorithms to distinguish progressive from repetitive behavior in a trace and to extract a minimal progressive fragment of a trace. The implied combinatorial optimization problem for trace reduction is solved in linear time with dynamic programming. We present and compare several approximate and one exact solution method. Information on the reduction operation as well as the reduced trace itself helps a modeler to recognize the presence of certain errors and to identify their cause. We track down a subtle modeling error in a dependability model of a multi-class server system to illustrate the effectiveness of our approach in revealing the cause of an observed effect. The proposed technique has been implemented and integrated in Traviando, a trace analyzer to debug stochastic simulation models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.75","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4620122","Testing and Debugging;Simulation;Trace analysis;Cycle reduction;Testing and Debugging;Simulation;Trace analysis;Cycle reduction","Discrete event systems;Discrete event simulation;Context modeling;Communication system software;Software performance;Automatic control;Control systems;Stochastic systems;Debugging;Data mining","combinatorial mathematics;discrete event simulation;dynamic programming;program diagnostics","automated trace analysis;discrete event system models;dynamic behavior;complex stochastic discrete event simulation model;minimal progressive fragment;combinatorial optimization problem;trace reduction;linear time;dynamic programming;reduction operation;modeling error;dependability model;multiclass server system;Traviando;trace analyzer;stochastic simulation model","","13","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic detection and masking of nonatomic exception handling","C. Fetzer; P. Felber; K. Hogstedt","Dept. of Comput. Sci., Dresden Univ. of Technol., Germany; NA; NA","IEEE Transactions on Software Engineering","","2004","30","8","547","560","The development of robust software is a difficult undertaking and is becoming increasingly more important as applications grow larger and more complex. Although modern programming languages such as C++ and Java provide sophisticated exception handling mechanisms to detect and correct runtime error conditions, exception handling code must still be programmed with care to preserve application consistency. In particular, exception handling is only effective if the premature termination of a method due to an exception does not leave an object in an inconsistent state. We address this issue by introducing the notion of failure atomicity in the context of exceptions. We propose practical techniques to automatically detect and mask the nonatomic exception handling situations encountered during program execution. These techniques can be applied to applications written in various programming languages that support exceptions. We perform experimental evaluation on both C++ and Java applications to demonstrate the effectiveness of our techniques and measure the overhead that they introduce.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316871","Index Terms- Software engineering;software/program verification;reliability;testing and debugging;error handling and recovery;reliability;testing tools.","Robustness;Application software;Computer languages;Programming profession;Java;Runtime;Software testing;Error correction codes;Performance evaluation;Reliability engineering","software reliability;exception handling;program testing;program debugging;program verification;C++ language;Java","nonatomic exception handling;robust software development;C++;Java;software verification;program verification;software reliability;program testing;program debugging;error handling;error recovery","","13","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Estimating Computational Requirements in Multi-Threaded Applications","J. F. Pérez; G. Casale; S. Pacheco-Sanchez","Department of Computing, Imperial College London, United Kingdom; Department of Computing, Imperial College London, United Kingdom; SAP HANA Cloud Computing, Systems Engineering, Belfast, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","3","264","278","Performance models provide effective support for managing quality-of-service (QoS) and costs of enterprise applications. However, expensive high-resolution monitoring would be needed to obtain key model parameters, such as the CPU consumption of individual requests, which are thus more commonly estimated from other measures. However, current estimators are often inaccurate in accounting for scheduling in multi-threaded application servers. To cope with this problem, we propose novel linear regression and maximum likelihood estimators. Our algorithms take as inputs response time and resource queue measurements and return estimates of CPU consumption for individual request types. Results on simulated and real application datasets indicate that our algorithms provide accurate estimates and can scale effectively with the threading levels.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2363472","Seventh Framework Programme; InvestNI/SAP VIRTEX; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6926798","Demand estimation;Multi-threaded application servers;Application performance management;Demand estimation;multi-threaded application servers;application performance management","Time factors;Servers;Instruction sets;Maximum likelihood estimation;Computational modeling;Time measurement","maximum likelihood estimation;multi-threading;quality of service;queueing theory;regression analysis;software performance evaluation;systems analysis","computational requirement estimation;expensive high-resolution monitoring;quality-of-service;QoS management;cost management;performance models;CPU consumption;multithreaded application server scheduling;linear regression;maximum likelihood estimators;input response time;resource queue measurements;enterprise applications","","12","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Software Reliability Modeling with Software Metrics Data via Gaussian Processes","N. Torrado; M. P. Wiper; R. E. Lillo","Universidad Carlos III de Madrid, Madrid; Universidad Carlos III de Madrid, Madrid; Universidad Carlos III de Madrid, Madrid","IEEE Transactions on Software Engineering","","2013","39","8","1179","1186","In this paper, we describe statistical inference and prediction for software reliability models in the presence of covariate information. Specifically, we develop a semiparametric, Bayesian model using Gaussian processes to estimate the numbers of software failures over various time periods when it is assumed that the software is changed after each time period and that software metrics information is available after each update. Model comparison is also carried out using the deviance information criterion, and predictive inferences on future failures are shown. Real-life examples are presented to illustrate the approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6392172","Software metrics;software failures;reliability;statistical methods;Markov chain Monte Carlo method","Software;Gaussian processes;Software reliability;Software metrics;Predictive models;Bayesian methods","Bayes methods;Gaussian processes;inference mechanisms;software metrics;software reliability;statistical analysis;system recovery","software reliability modeling;statistical inference;covariate information;semiparametric Bayesian model;Gaussian process;software failure;software metrics information;deviance information criterion;predictive inference;future failure","","5","","30","","","","","","IEEE","IEEE Journals & Magazines"
"COVERT: Compositional Analysis of Android Inter-App Permission Leakage","H. Bagheri; A. Sadeghi; J. Garcia; S. Malek","Department of Computer Science, George Mason University, Fairfax, VA; Department of Computer Science, George Mason University, Fairfax, VA 22030; Computer Science and Artificial Intelligence Laboratory at MIT, Cambridge, MA; Computer Science and Artificial Intelligence Laboratory at MIT, Cambridge, MA","IEEE Transactions on Software Engineering","","2015","41","9","866","886","Android is the most popular platform for mobile devices. It facilitates sharing of data and services among applications using a rich inter-app communication system. While access to resources can be controlled by the Android permission system, enforcing permissions is not sufficient to prevent security violations, as permissions may be mismanaged, intentionally or unintentionally. Android's enforcement of the permissions is at the level of individual apps, allowing multiple malicious apps to collude and combine their permissions or to trick vulnerable apps to perform actions on their behalf that are beyond their individual privileges. In this paper, we present COVERT, a tool for compositional analysis of Android inter-app vulnerabilities. COVERT's analysis is modular to enable incremental analysis of applications as they are installed, updated, and removed. It statically analyzes the reverse engineered source code of each individual app, and extracts relevant security specifications in a format suitable for formal verification. Given a collection of specifications extracted in this way, a formal analysis engine (e.g., model checker) is then used to verify whether it is safe for a combination of applications-holding certain permissions and potentially interacting with each other-to be installed together. Our experience with using COVERT to examine over 500 real-world apps corroborates its ability to find inter-app vulnerabilities in bundles of some of the most popular apps on the market.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2419611","US Defense Advanced Research Projects Agency; US National Security Agency; US Department of Homeland Security; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7079508","Formal Verification;Static Analysis;Android;Inter-App Vulnerabilities;Formal verification;static analysis;Android;Inter-App vulnerabilities","Smart phones;Androids;Humanoid robots;Security;Analytical models;Data mining;Metals","Android (operating system);formal specification;formal verification;mobile computing;program diagnostics;security of data","COVERT tool;compositional analysis;formal analysis engine;security specification;formal verification;incremental analysis;Android inter-app vulnerabilities analysis;security violation;Android permission system;inter-app communication system;mobile devices;Android inter-app permission leakage","","34","","60","","","","","","IEEE","IEEE Journals & Magazines"
"WAM—The Weighted Average Method for Predicting the Performance of Systems with Bursts of Customer Sessions","D. Krishnamurthy; J. Rolia; M. Xu","University of Calgary, Calgary; Hewlett Packard Labs, Bristol; University of Calgary, Calgary","IEEE Transactions on Software Engineering","","2011","37","5","718","735","Predictive performance models are important tools that support system sizing, capacity planning, and systems management exercises. We introduce the Weighted Average Method (WAM) to improve the accuracy of analytic predictive performance models for systems with bursts of concurrent customers. WAM considers the customer population distribution at a system to reflect the impact of bursts. The WAM approach is robust with respect to distribution functions, including heavy-tail-like distributions, for workload parameters. We demonstrate the effectiveness of WAM using a case study involving a multitier TPC-W benchmark system. To demonstrate the utility of WAM with multiple performance modeling approaches, we developed both Queuing Network Models and Layered Queuing Models for the system. Results indicate that WAM improves prediction accuracy for bursty workloads for QNMs and LQMs by 10 and 12 percent, respectively, with respect to a Markov Chain approach reported in the literature.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.65","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5953602","Performance of systems;modeling techniques;queuing theory;operational analysis.","Markov processes;Analytical models;Predictive models;Accuracy;Queueing analysis;Time factors;Software","Markov processes;queueing theory;software performance evaluation;systems analysis","weighted average method;system performance prediction;customer session bursts;system sizing;capacity planning;systems management exercises;analytic predictive performance models;heavy tail like distributions;multitier TPC-W benchmark system;queuing network models;layered queuing models;bursty workloads;Markov chain approach","","9","","41","","","","","","IEEE","IEEE Journals & Magazines"
"MAHAKIL: Diversity Based Oversampling Approach to Alleviate the Class Imbalance Issue in Software Defect Prediction","K. E. Bennin; J. Keung; P. Phannachitta; A. Monden; S. Mensah","Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; College of Arts, Media and Technology, Chiang Mai University, Chiang Mai, Thailand; Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong","IEEE Transactions on Software Engineering","","2018","44","6","534","550","Highly imbalanced data typically make accurate predictions difficult. Unfortunately, software defect datasets tend to have fewer defective modules than non-defective modules. Synthetic oversampling approaches address this concern by creating new minority defective modules to balance the class distribution before a model is trained. Notwithstanding the successes achieved by these approaches, they mostly result in over-generalization (high rates of false alarms) and generate near-duplicated data instances (less diverse data). In this study, we introduce MAHAKIL, a novel and efficient synthetic oversampling approach for software defect datasets that is based on the chromosomal theory of inheritance. Exploiting this theory, MAHAKIL interprets two distinct sub-classes as parents and generates a new instance that inherits different traits from each parent and contributes to the diversity within the data distribution. We extensively compare MAHAKIL with SMOTE, Borderline-SMOTE, ADASYN, Random Oversampling and the No sampling approach using 20 releases of defect datasets from the PROMISE repository and five prediction models. Our experiments indicate that MAHAKIL improves the prediction performance for all the models and achieves better and more significant pf values than the other oversampling approaches, based on Brunner's statistical significance test and Cliff's effect sizes. Therefore, MAHAKIL is strongly recommended as an efficient alternative for defect prediction models built on highly imbalanced datasets.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2731766","General Research Fund of the Research Grants Council of Hong Kong; City University of Hong Kong; JSPS KAKENHI; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7990590","Software defect prediction;class imbalance learning;synthetic sample generation;data sampling methods;classification problems","Biological cells;Software;Predictive models;Animals;Electronic mail;Sampling methods","learning (artificial intelligence);pattern classification;sampling methods;software quality","MAHAKIL;software defect prediction;software defect datasets;nondefective modules;minority defective modules;class distribution;data instances;less diverse data;distinct sub-classes;data distribution;Random Oversampling;No sampling approach;prediction performance;defect prediction models;highly imbalanced datasets;defective modules;class imbalance issue;synthetic oversampling approaches;synthetic oversampling approach","","3","","80","","","","","","IEEE","IEEE Journals & Magazines"
"A Study of Causes and Consequences of Client-Side JavaScript Bugs","F. S. Ocariza; K. Bajaj; K. Pattabiraman; A. Mesbah","Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Software Engineering","","2017","43","2","128","144","Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2586066","Natural Sciences and Engineering Research Council of Canada (NSERC); Intel Corporation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7501855","Faults;JavaScript;Document Object Model (DOM);bug reports;empirical study","Computer bugs;Servers;Market research;HTML;Data mining;Reliability;Cascading style sheets","client-server systems;Internet;Java;program debugging;software fault tolerance","client-side JavaScript bugs;user-interactivity;client-server communications;JavaScript programmers;bug repositories;information extraction;DOM-related JavaScript faults;JavaScript code;document object model;Web application components;static analysis tools","","8","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Revisiting the Performance Evaluation of Automated Approaches for the Retrieval of Duplicate Issue Reports","M. S. Rakha; C. Bezemer; A. E. Hassan","Queen's University, Kingston, ON, Canada; Queen's University, Kingston, ON, Canada; Queen's University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2018","44","12","1245","1268","Issue tracking systems (ITSs), such as Bugzilla, are commonly used to track reported bugs, improvements and change requests for a software project. To avoid wasting developer resources on previously-reported (i.e., duplicate) issues, it is necessary to identify such duplicates as soon as they are reported. Several automated approaches have been proposed for retrieving duplicate reports, i.e., identifying the duplicate of a new issue report in a list of<inline-formula><tex-math notation=""LaTeX"">$n$</tex-math><alternatives><inline-graphic xlink:href=""rakha-ieq1-2755005.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>candidates. These approaches rely on leveraging the textual, categorical, and contextual information in previously-reported issues to decide whether a newly-reported issue has previously been reported. In general, these approaches are evaluated using data that spans a relatively short period of time (i.e., the classical evaluation). However, in this paper, we show that the classical evaluation tends to overestimate the performance of automated approaches for retrieving duplicate issue reports. Instead, we propose a realistic evaluation using all the reports that are available in the ITS of a software project. We conduct experiments in which we evaluate two popular approaches for retrieving duplicate issues (BM25F and REP) using the classical and realistic evaluations. We find that for the issue tracking data of the Mozilla foundation, the Eclipse foundation and OpenOffice, the realistic evaluation shows that previously proposed approaches perform considerably lower than previously reported using the classical evaluation. As a result, we conclude that the reported performance of approaches for retrieving duplicate issue reports is significantly overestimated in literature. In order to improve the performance of the automated retrieval of duplicate issue reports, we propose to leverage the resolution field of issue reports. Our experiments show that a relative improvement in the performance of a median of 7-21.5 percent and a maximum of 19-60 percent can be achieved by leveraging the resolution field of issue reports for the automated retrieval of duplicates.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2755005","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8048025","Text analysis;software engineering;performance evaluation","Text analysis;Computer bugs;Frequency measurement;Performance evaluation;Manuals;Ports (Computers)","","","","2","","36","","","","","","IEEE","IEEE Journals & Magazines"
"A Model-Driven Methodology for Developing Secure Data-Management Applications","D. Basin; M. Clavel; M. Egea; M. A. G. de Dios; C. Dania","ETH Zürich, Zürich, Switzerland; IMDEA Software, Campus de Montegancedo, s/n, Pozuelo de Alarcon, Madrid, Spain; ATOS Research & Innovation, Madrid, Spain; IMDEA Software, Campus de Montegancedo, s/n, Pozuelo de Alarcon, Madrid, Spain; IMDEA Software, Campus de Montegancedo, s/n, Pozuelo de Alarcon, Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","4","324","337","We present a novel model-driven methodology for developing secure data-management applications. System developers proceed by modeling three different views of the desired application: its data model, security model, and GUI model. These models formalize respectively the application's data domain, authorization policy, and its graphical interface together with the application's behavior. Afterwards a model-transformation function lifts the policy specified by the security model to the GUI model. This allows a separation of concerns where behavior and security are specified separately, and subsequently combined to generate a security-aware GUI model. Finally, a code generator generates a multi-tier application, along with all support for access control, from the security-aware GUI model. We report on applications built using our approach and the associated tool.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.2297116","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6698396","Model-driven development;model-driven security;access control;GUI models;model transformation","Data models;Graphical user interfaces;Unified modeling language;Authorization;Syntactics","authorisation;graphical user interfaces;software engineering","model-driven methodology;secure data-management applications;data model;security model;graphical user intefaces;authorization policy;model-transformation function;security-aware GUI model;code generator;multitier application;access control","","6","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Improving Automated Bug Triaging with Specialized Topic Model","X. Xia; D. Lo; Y. Ding; J. M. Al-Kofahi; T. N. Nguyen; X. Wang","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Information Systems, Singapore Management University, Singapore, Singapore; School of Information Systems, Singapore Management University, Singapore, Singapore; Electrical and Computer Engineering Department, Iowa State University, Ames, IA, USA; Electrical and Computer Engineering Department, Iowa State University, Ames, IA, USA; College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Software Engineering","","2017","43","3","272","297","Bug triaging refers to the process of assigning a bug to the most appropriate developer to fix. It becomes more and more difficult and complicated as the size of software and the number of developers increase. In this paper, we propose a new framework for bug triaging, which maps the words in the bug reports (i.e., the term space) to their corresponding topics (i.e., the topic space). We propose a specialized topic modeling algorithm named<italic>multi-feature topic model (MTM)</italic>which extends Latent Dirichlet Allocation (LDA) for bug triaging.<italic>MTM</italic>considers product and component information of bug reports to map the term space to the topic space. Finally, we propose an incremental learning method named<italic>TopicMiner</italic>which considers the topic distribution of a new bug report to assign an appropriate fixer based on the affinity of the fixer to the topics. We pair<italic>TopicMiner</italic>with MTM (<italic>TopicMiner<inline-formula><tex-math notation=""LaTeX"">$^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=""xia-ieq1-2576454.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula></italic>). We have evaluated our solution on 5 large bug report datasets including GCC, OpenOffice, Mozilla, Netbeans, and Eclipse containing a total of 227,278 bug reports. We show that<italic>TopicMiner<inline-formula><tex-math notation=""LaTeX"">$^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=""xia-ieq2-2576454.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula></italic>can achieve top-1 and top-5 prediction accuracies of 0.4831-0.6868, and 0.7686-0.9084, respectively. We also compare<italic>TopicMiner<inline-formula><tex-math notation=""LaTeX"">$^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=""xia-ieq3-2576454.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula></italic>with Bugzie, LDA-KL, SVM-LDA, LDA-Activity, and Yang et al.'s approach. The results show that<italic>TopicMiner<inline-formula><tex-math notation=""LaTeX"">$^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=""xia-ieq4-2576454.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula></italic>on average improves top-1 and top-5 prediction accuracies of Bugzie by 128.48 and 53.22 percent, LDA-KL by 262.91 and 105.97 percent, SVM-LDA by 205.89 and 110.48 percent, LDA-Activity by 377.60 and 176.32 percent, and Yang et al.'s approach by 59.88 and 13.70 percent, respectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2576454","National Basic Research Program of China (the 973 Program); NSFC; National Key Technology R&D Program; Ministry of Science and Technology of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7484672","Developer;bug triaging;feature information;topic model","Software;Resource management;Software algorithms;Support vector machines;Learning systems;Indexes;Computer bugs","","","","11","","46","","","","","","IEEE","IEEE Journals & Magazines"
"GreenDroid: Automated Diagnosis of Energy Inefficiency for Smartphone Applications","Y. Liu; C. Xu; S. C. Cheung; J. Lü","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; State Key Laboratory for Novel Software Technology and Department of Computer Science and Technology, Nanjing University, 163 Xianlin Avenue, Nanjing, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; State Key Laboratory for Novel Software Technology and Department of Computer Science and Technology, Nanjing University, 163 Xianlin Avenue, Nanjing, China","IEEE Transactions on Software Engineering","","2014","40","9","911","940","Smartphone applications' energy efficiency is vital, but many Android applications suffer from serious energy inefficiency problems. Locating these problems is labor-intensive and automated diagnosis is highly desirable. However, a key challenge is the lack of a decidable criterion that facilitates automated judgment of such energy problems. Our work aims to address this challenge. We conducted an in-depth study of 173 open-source and 229 commercial Android applications, and observed two common causes of energy problems: missing deactivation of sensors or wake locks, and cost-ineffective use of sensory data. With these findings, wepropose an automated approach to diagnosing energy problems in Android applications. Our approach explores an application's state space by systematically executing the application using Java PathFinder (JPF). It monitors sensor and wake lock operations to detect missing deactivation of sensors and wake locks. It also tracks the transformation and usage of sensory data and judges whether they are effectively utilized by the application using our state-sensitive data utilization metric. In this way, our approach can generate detailed reports with actionable information to assist developers in validating detected energy problems. We built our approach as a tool, GreenDroid, on top of JPF. Technically, we addressed the challenges of generating user interaction events and scheduling event handlers in extending JPF for analyzing Android applications. We evaluated GreenDroid using 13 real-world popular Android applications. GreenDroid completed energy efficiency diagnosis for these applications in a few minutes. It successfully located real energy problems in these applications, and additionally found new unreported energy problems that were later confirmed by developers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2323982","Research Grants Council; National High-Tech Research & Development Program; National Natural Science Foundation; New Century Excellent Talents in University; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6815752","Smartphone application;energy inefficiency;automated diagnosis;sensory data utilization;green computing","Androids;Humanoid robots;Computer bugs;Sensors;Open source software;Green products;Google","Android (operating system);Java;power aware computing;program diagnostics;public domain software;smart phones","GreenDroid;automated energy inefficiency diagnosis;smartphone applications;labor-intensive diagnosis;automated diagnosis;open-source Android applications;commercial Android applications;Java PathFinder;JPF;wake lock operations;state-sensitive data utilization metric;user interaction events;scheduling event handlers","","25","","72","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Analysis of Business Process Execution Language Usage","M. Hertis; M. B. Juric","Laboratory for Integration of Information Systems, Faculty of Computer and Information Science, University of Ljubljana, Trzaska cesta 25, Ljubljana, Slovenia, and Seltron d.o.o., Trzaska cesta 85 a, Maribor; Faculty, of Computer and Information Science, Ljubljana, Slovenia","IEEE Transactions on Software Engineering","","2014","40","8","738","757","The current state of executable business process languages allows for and demands optimization of design practices and specifications. In this paper, we present the first empirical study that analyses Web Services Business Process Execution Language (WS-BPEL or BPEL) usage and characteristics of real world executable business processes. We have analysed 1,145 BPEL processes by measuring activity usage and process complexity. In addition, we investigated the occurrence of activity usage patterns. The results revealed that the usage frequency of BPEL activities varies and that some activities have a strong co-occurrence. BPEL activities often appear in activity patterns that are repeated in multiple processes. Furthermore, the current process complexity metrics have proved to be inadequate for measuring BPEL process complexity. The empirical results provide fundamental knowledge on how BPEL specification and process design practices can be improved. We propose BPEL design guidelines and BPEL language improvements for the design of more understandable and less complex processes. The results are of interest to business process language designers, business process tool developers, business process designers and developers, and software engineering researchers, and contribute to the general understanding of BPEL and service-oriented architecture.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2322618","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6812231","WS-BPEL Analysis;complexity measure;service composition;process patterns;process complexity;process comprehension;empirical study","Complexity theory;Business;Measurement;Semantics;XML;Syntactics;Guidelines","service-oriented architecture;Web Services Business Process Execution Language","empirical analysis;design practices;design specifications;Web services business process execution language;WS-BPEL;executable business processes;activity usage;process complexity;BPEL activities;BPEL design guidelines;BPEL language improvements;service-oriented architecture","","8","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Toward the Reverse Engineering of UML Sequence Diagrams for Distributed Java Software","L. C. Briand; Y. Labiche; J. Leduc","Software Quality Engineering Laboratory, Department of Systems and Computer Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, ON K1S5B6, Canada; Software Quality Engineering Laboratory, Department of Systems and Computer Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, ON K1S5B6, Canada; Siemens Corporate Research Inc., 755 College Road East, Princeton, NJ 08540","IEEE Transactions on Software Engineering","","2006","32","9","642","663","This paper proposes a methodology and instrumentation infrastructure toward the reverse engineering of UML (Unified Modeling Language) sequence diagrams from dynamic analysis. One motivation is, of course, to help people understand the behavior of systems with no (complete) documentation. However, such reverse-engineered dynamic models can also be used for quality assurance purposes. They can, for example, be compared with design sequence diagrams and the conformance of the implementation to the design can thus be verified. Furthermore, discrepancies can also suggest failures in meeting the specifications. Due to size constraints, this paper focuses on the distribution aspects of the methodology we propose. We formally define our approach using metamodels and consistency rules. The instrumentation is based on aspect-oriented programming in order to alleviate the effort overhead usually associated with source code instrumentation. A case study is discussed to demonstrate the applicability of the approach on a concrete example","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.96","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707665","UML;sequence diagram;reverse engineering;distribution;RMI;AspectJ;OCL.","Reverse engineering;Unified modeling language;Java;Object oriented modeling;Instruments;Testing;Runtime;Quality assurance;Information retrieval;Documentation","formal specification;formal verification;Java;object-oriented programming;program diagnostics;reverse engineering;software quality;Unified Modeling Language","reverse engineering;UML sequence diagram;distributed Java software;Unified Modeling Language;dynamic program analysis;quality assurance;formal verification;formal specification;metamodel;consistency rule;aspect-oriented programming;source code instrumentation","","94","","41","","","","","","IEEE","IEEE Journals & Magazines"
"A Controlled Experiment for Program Comprehension through Trace Visualization","B. Cornelissen; A. Zaidman; A. van Deursen","Software Improvement Group, Amsterdam; Delft University of Technology, Delft; Delft University of Technology, Delft","IEEE Transactions on Software Engineering","","2011","37","3","341","355","Software maintenance activities require a sufficient level of understanding of the software at hand that unfortunately is not always readily available. Execution trace visualization is a common approach in gaining this understanding, and among our own efforts in this context is Extravis, a tool for the visualization of large traces. While many such tools have been evaluated through case studies, there have been no quantitative evaluations to the present day. This paper reports on the first controlled experiment to quantitatively measure the added value of trace visualization for program comprehension. We designed eight typical tasks aimed at gaining an understanding of a representative subject system, and measured how a control group (using the Eclipse IDE) and an experimental group (using both Eclipse and Extravis) performed these tasks in terms of time spent and solution correctness. The results are statistically significant in both regards, showing a 22 percent decrease in time requirements and a 43 percent increase in correctness for the group using trace visualization.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5441291","Program comprehension;dynamic analysis;controlled experiment.","Visualization;Computer Society;Time measurement;Programming;Documentation;Scalability;Software maintenance;Gain measurement;Control systems;Performance evaluation","data visualisation;software maintenance","program comprehension;software maintenance;execution trace visualization","","50","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Complete and Interpretable Conformance Checking of Business Processes","L. García-Bañuelos; N. R. T. P. van Beest; M. Dumas; M. L. Rosa; W. Mertens","University of Tartu, Tartu, Estonia; Data61, CSIRO, Brisbane, QLD, Australia; University of Tartu, Tartu, Estonia; Queensland University of Technology, Brisbane, QLD, Australia; Queensland University of Technology, Brisbane, QLD, Australia","IEEE Transactions on Software Engineering","","2018","44","3","262","290","This article presents a method for checking the conformance between an event log capturing the actual execution of a business process, and a model capturing its expected or normative execution. Given a process model and an event log, the method returns a set of statements in natural language describing the behavior allowed by the model but not observed in the log and vice versa. The method relies on a unified representation of process models and event logs based on a well-known model of concurrency, namely event structures. Specifically, the problem of conformance checking is approached by converting the event log into an event structure, converting the process model into another event structure, and aligning the two event structures via an error-correcting synchronized product. Each difference detected in the synchronized product is then verbalized as a natural language statement. An empirical evaluation shows that the proposed method can handle real datasets and produces more concise and higher-level difference descriptions than state-of-the-art conformance checking methods. In a survey designed according to the technology acceptance model, practitioners showed a preference towards the proposed method with respect to a state-of-the-art baseline.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2668418","Australian Research Council Discovery; Estonian Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7852436","Process mining;conformance checking;process model;event log;event structure","Business;Synchronization;Computational modeling;Data mining;Natural languages;Software systems;Context modeling","business data processing;data mining;natural languages","event log;process model;event structure;conformance checking;technology acceptance model;business process expected execution;business process normative execution;concurrency model;error-correcting synchronized product;natural language statement","","1","","39","","","","","","IEEE","IEEE Journals & Magazines"
"How Effectively Does Metamorphic Testing Alleviate the Oracle Problem?","H. Liu; F. Kuo; D. Towey; T. Y. Chen","Australia-India Centre for Automation Software Engineering, RMIT University, Melbourne 3001 VIC, Australia; Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn 3122 VIC, Australia; Division of Computer Science, The University of Nottingham, Ningbo, China; Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn 3122 VIC, Australia","IEEE Transactions on Software Engineering","","2014","40","1","4","22","In software testing, something which can verify the correctness of test case execution results is called an oracle. The oracle problem occurs when either an oracle does not exist, or exists but is too expensive to be used. Metamorphic testing is a testing approach which uses metamorphic relations, properties of the software under test represented in the form of relations among inputs and outputs of multiple executions, to help verify the correctness of a program. This paper presents new empirical evidence to support this approach, which has been used to alleviate the oracle problem in various applications and to enhance several software analysis and testing techniques. It has been observed that identification of a sufficient number of appropriate metamorphic relations for testing, even by inexperienced testers, was possible with a very small amount of training. Furthermore, the cost-effectiveness of the approach could be enhanced through the use of more diverse metamorphic relations. The empirical studies presented in this paper clearly show that a small number of diverse metamorphic relations, even those identified in an ad hoc manner, had a similar fault-detection capability to a test oracle, and could thus effectively help alleviate the oracle problem.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6613484","Software testing;test oracle;oracle problem;metamorphic testing;metamorphic relation","Computer crashes;Software;Educational institutions;Software testing;Training;Benchmark testing","program diagnostics;program testing;software fault tolerance;software quality","metamorphic testing;oracle problem;software testing;metamorphic relations;software properties;program correctness;software analysis;fault-detection capability","","52","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient online schedulability tests for real-time systems","Tei-Wei Kuo; Li-Pin Chang; Yu-Hua Liu; Kwei-Jay Lin","Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan; NA; NA","IEEE Transactions on Software Engineering","","2003","29","8","734","751","Many computer systems, such as those for open system environments or multimedia services, need an efficient schedulability test for online admission control of new jobs. Although various polynomial time schedulability tests have been proposed, they often fail to decide the schedulability of the system precisely when the system is heavily loaded. On the other hand, most precise schedulability tests proposed to date have a high complexity and may not be suitable for online tests. We present new efficient online schedulability tests for both the periodic process model [C. L. Liu et al., (1973)] and the multiframe process model [A. K. Mok et al., (1997)] in uniprocessor environments. The schedulability tests are shown to be more precise and efficient than any existing polynomial-time schedulability tests. Moreover, the tests can be done incrementally as each new task arrives at the system. Our proposed tests can also be used for the multiframe model where a task may have different computation times in different periods. We show the performance of the proposed schedulability tests in several simulation experiments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223647","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223647","","System testing;Real time systems;Processor scheduling;Admission control;Open systems;Polynomials;Multimedia systems;Switches;Timing","scheduling;resource allocation;real-time systems;online operation;open systems;computational complexity;set theory;directed graphs","online schedulability test;real-time system;open system environment;online job admission control;polynomial time schedulability test;periodic process model;multiframe process model;uniprocessor environment;MPEG stream;division graph;directed graph;reduced set;time complexity","","16","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Loupe: Verifying Publish-Subscribe Architectures with a Magnifying Lens","L. Baresi; C. Ghezzi; L. Mottola","Politecnico di Milano, Milano; Politecnico di Milano, Milano; Politecnico di Milano, Milano","IEEE Transactions on Software Engineering","","2011","37","2","228","246","The Publish-Subscribe (P/S) communication paradigm fosters high decoupling among distributed components. This facilitates the design of dynamic applications, but also impacts negatively on their verification, making it difficult to reason on the overall federation of components. In addition, existing P/S infrastructures offer radically different features to the applications, e.g., in terms of message reliability. This further complicates the verification as its outcome depends on the specific guarantees provided by the underlying P/S system. Although model checking has been proposed as a tool for the verification of P/S architectures, existing solutions overlook many characteristics of the underlying communication infrastructure to avoid state explosion problems. To overcome these limitations, the Loupe domain-specific model checker adopts a different approach. The P/S infrastructure is not modeled on top of a general-purpose model checker. Instead, it is embedded within the checking engine, and the traditional P/S operations become part of the modeling language. In this paper, we describe Loupe's design and the dedicated state abstractions that enable accurate verification without incurring state explosion problems. We also illustrate our use of state-of-the-art software verification tools to assess some key functionality in Loupe's current implementation. A complete case study shows how Loupe eases the verification of P/S architectures. Finally, we quantitatively compare Loupe's performance against alternative approaches. The results indicate that Loupe is effective and efficient in enabling accurate verification of P/S architectures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5432228","Publish-subscribe;verification;model-checking.","Publish-subscribe;Lenses;Application software;Explosions;Computer architecture;Context;Engines;Software tools;Software systems;Business communication","formal verification;message passing;middleware","publish-subscribe architectures;lens magnification;publish-subscribe communication paradigm;distributed component;P-S infrastructures;message reliability;P-S architectures verification;communication infrastructure;state explosion problem;Loupe domain-specific model checker;general purpose model checker;P-S operation;modeling language;Loupe design;dedicated state abstraction;state-of-the-art software verification tool","","7","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical analysis of safety-critical anomalies during operations","R. R. Lutz; I. C. Mikulski","Dept. of Comput. Sci., Iowa State Univ., Ames, IA, USA; NA","IEEE Transactions on Software Engineering","","2004","30","3","172","180","Analysis of anomalies that occur during operations is an important means of improving the quality of current and future software. Although the benefits of anomaly analysis of operational software are widely recognized, there has been relatively little research on anomaly analysis of safety-critical systems. In particular, patterns of software anomaly data for operational, safety-critical systems are not well understood. We present the results of a pilot study using orthogonal defect classification (ODC) to analyze nearly two hundred such anomalies on seven spacecraft systems. These data show several unexpected classification patterns such as the causal role of difficulties accessing or delivering data, of hardware degradation, and of rare events. The anomalies often revealed latent software requirements that were essential for robust, correct operation of the system. The anomalies also caused changes to documentation and to operational procedures to prevent the same anomalous situations from recurring. Feedback from operational anomaly reports helped measure the accuracy of assumptions about operational profiles, identified unexpected dependencies among embedded software and their systems and environment, and indicated needed improvements to the software, the development process, and the operational procedures. The results indicate that, for long-lived, critical systems, analysis of the most severe anomalies can be a useful mechanism both for maintaining safer, deployed systems and for building safer, similar systems in the future.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271171","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271171","","Software safety;Embedded software;Software quality;Space vehicles;Hardware;Degradation;Robustness;Documentation;Feedback;Software measurement","program diagnostics;software maintenance;software quality;safety-critical software;software metrics;formal specification;formal verification","anomaly analysis;safety-critical system;software pattern;software requirement;program diagnostics;software maintenance","","40","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Robustness testing of Java server applications","C. Fu; A. Milanova; B. G. Ryder; D. G. Wonnacott","Dept. of Comput. Sci., Rutgers Univ., Piscataway, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","4","292","311","This paper presents a new compile-time analysis that enables a testing methodology for white-box coverage testing of error recovery code (i.e., exception handlers) of server applications written in Java, using compiler-directed fault injection. The analysis allows compiler-generated instrumentation to guide the fault injection and to record the recovery code exercised. (An injected fault is experienced as a Java exception.) The analysis 1) identifies the exception-flow ""def-uses"" to be tested in this manner, 2) determines the kind of fault to be requested at a program point, and 3) finds appropriate locations for code instrumentation. The analysis incorporates refinements that establish sufficient context sensitivity to ensure relatively precise def-use links and to eliminate some spurious def-uses due to demonstrably infeasible control flow. A runtime test harness calculates test coverage of these links using an exception def-catch metric. Experiments with the methodology demonstrate the utility of the increased precision in obtaining good test coverage on a set of moderately sized server benchmarks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435351","Index Terms- Reliability;def-use testing;Java;exceptions;test coverage metrics.","Robustness;Java;Application software;Vehicle crash testing;System testing;Computer Society;Computer crashes;Instruments;Web server;Telephony","Java;program testing;error handling;program compilers;software fault tolerance;data flow analysis;file servers;program control structures","Java server applications testing;compile-time analysis;white-box coverage testing;error recovery code;exception handlers;compiler-directed fault injection;compiler-generated instrumentation;software reliability","","26","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Visualizing Design Patterns in Their Applications and Compositions","J. Dong; S. Yang; K. Zhang","Department of Computer Science, University of Texas at Dallas, 2601 North Floyd Road, Richardson, TX 75083; Department of Computer Science, University of Texas at Dallas, 2601 North Floyd Road, Richardson, TX 75083; Department of Computer Science, University of Texas at Dallas, 2601 North Floyd Road, Richardson, TX 75083","IEEE Transactions on Software Engineering","","2007","33","7","433","453","Design patterns are generic design solutions that can be applied and composed in different applications where pattern-related information is generally implicit in the Unified Modeling Language (UML) diagrams of the applications. It is unclear in which pattern instances each modeling element, such as class, attribute, and operation, participates. It is hard for a designer to find the design patterns used in an application design. Consequently, the benefits of design patterns are compromised because designers cannot communicate with each other in terms of the design patterns they used and their design decisions and trade-offs. In this paper, we present a UML profile that defines new stereotypes, tagged values, and constraints for tracing design patterns in UML diagrams. These new stereotypes and tagged values are attached to a modeling element to explicitly represent the role the modeling element plays in a design pattern so that the user can identify the pattern in a UML diagram. Based on this profile, we also develop a Web service (tool) for explicitly visualizing design patterns in UML diagrams. With this service, users are able to visualize design patterns in their applications and compositions because pattern-related information can be dynamically displayed. A real-world case study and a comparative experiment with existing approaches are conducted to evaluate our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1012","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4227827","Design pattern;UML;model-driven architecture;Web service;visual tool.","Visualization;Unified modeling language;Object oriented modeling;Software systems;Application software;Software design;Web services;Service oriented architecture;Natural languages;Production facilities","software engineering;Unified Modeling Language;Web services","design patterns;Unified Modeling Language diagrams;UML profile;Web service","","34","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Program Characterization Using Runtime Values and Its Application to Software Plagiarism Detection","Y. Jhi; X. Jia; X. Wang; S. Zhu; P. Liu; D. Wu","Samsung SDS R&D Center, Seoul, Korea; State Key Laboratory of Information Security, Institute of Information Engineering, Beijing, Haidian District, China; Shape Security, Mountain View, CA; Department of Computer Science and Engineering, Pennsylvania State University, University Park, PA; College of Information Sciences and Technology, Pennsylvania State University, University Park, PA; College of Information Sciences and Technology, Pennsylvania State University, University Park, PA","IEEE Transactions on Software Engineering","","2015","41","9","925","943","Illegal code reuse has become a serious threat to the software community. Identifying similar or identical code fragments becomes much more challenging in code theft cases where plagiarizers can use various automated code transformation or obfuscation techniques to hide stolen code from being detected. Previous works in this field are largely limited in that (i) most of them cannot handle advanced obfuscation techniques, and (ii) the methods based on source code analysis are not practical since the source code of suspicious programs typically cannot be obtained until strong evidences have been collected. Based on the observation that some critical runtime values of a program are hard to be replaced or eliminated by semantics-preserving transformation techniques, we introduce a novel approach to dynamic characterization of executable programs. Leveraging such invariant values, our technique is resilient to various control and data obfuscation techniques. We show how the values can be extracted and refined to expose the critical values and how we can apply this runtime property to help solve problems in software plagiarism detection. We have implemented a prototype with a dynamic taint analyzer atop a generic processor emulator. Our value-based plagiarism detection method (VaPD) uses the longest common subsequence based similarity measuring algorithms to check whether two code fragments belong to the same lineage. We evaluate our proposed method through a set of real-world automated obfuscators. Our experimental results show that the value-based method successfully discriminates 34 plagiarisms obfuscated by SandMark, plagiarisms heavily obfuscated by KlassMaster, programs obfuscated by Thicket, and executables obfuscated by Loco/Diablo.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2418777","US National Science Foundation (NSF); AFRL; National Natural Science Foundation of China (NSFC); National High-tech R&D Program of China; Strategic Priority Research Program of the Chinese Academy of Sciences; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7076635","Software plagiarism detection;dynamic code identification.;Software plagiarism detection;dynamic code identification","Plagiarism;Runtime;Optimization;Program processors;Semantics;Java","security of data;software engineering;source code (software)","program characterization;runtime values;software plagiarism detection;illegal code reuse;software community;identical code fragments;obfuscation techniques;source code;semantics-preserving transformation techniques;generic processor emulator;value-based plagiarism detection method;VaPD;SandMark;Thicket;Loco/Diablo","","10","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Adding roles to CORBA objects","C. Canal; L. Fuentes; E. Pimentel; J. M. Troya; A. Vallecillo","Dept. de Lenguajes y Ciencias de la Comput., Malaga Univ., Spain; Dept. de Lenguajes y Ciencias de la Comput., Malaga Univ., Spain; Dept. de Lenguajes y Ciencias de la Comput., Malaga Univ., Spain; Dept. de Lenguajes y Ciencias de la Comput., Malaga Univ., Spain; Dept. de Lenguajes y Ciencias de la Comput., Malaga Univ., Spain","IEEE Transactions on Software Engineering","","2003","29","3","242","260","Traditional IDLs were defined for describing the services that objects offer, but not those services they require from other objects, nor the relative order in which they expect their methods to be called. Some of the existing proposals try to add protocol information to object interfaces, but most of them fail to do so in a modular way. In this paper we propose an extension of the CORBA IDL that uses a sugared subset of the polyadic /spl pi/-calculus for describing object service protocols, based on the concept of roles. Roles allow the modular specification of the observable behavior of CORBA objects, reducing the complexity of the compatibility tests. Our main aim is the automated checking of protocol interoperability between CORBA objects in open component-based environments, using similar techniques to those used in software architecture description and analysis. In addition, our proposal permits the study of substitutability between CORBA objects, as well as the realization of dynamic compatibility tests during their runtime execution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183935","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183935","","Application software;Computer architecture;Protocols;Software architecture;Programming;Proposals;Testing;Software reusability;Irrigation;Runtime","distributed object management;object-oriented methods","CORBA objects;IDLs;protocol interoperability;automated checking;protocols;component-based software development;software components","","28","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Using Local Clocks to Reproduce Concurrency Bugs","Z. Wang; C. Wu; X. Yuan; Z. Wang; J. Li; P. Yew; J. Huang; X. Feng; Y. Lan; Y. Chen; Y. Lai; Y. Guan","Institute of Computing Technology, University of Chinese Academy of Sciences, Huairou, Beijing, P.R. China; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Huairou, Beijing, P.R. China; Huawei Technologies, Huairou, Beijing, P.R. China; Huawei Technologies, Huairou, Beijing, P.R. China; Horizon Robotics, Inc. Huairou, Beijing, P.R. China; Department of Computer Science and Engineering, University of Minnesota at Twin-Cities, Minnesota, MN; Department of Computer Science and Engineering, Texas A&M University, College Station, TX; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Huairou, Beijing, P.R. China; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Huairou, Beijing, P.R. China; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Huairou, Beijing, P.R. China; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Huairou, Beijing, P.R. China; Capital Normal University, Huairou, Beijing, P.R. China","IEEE Transactions on Software Engineering","","2018","44","11","1112","1128","Multi-threaded programs play an increasingly important role in current multi-core environments. Exposing concurrency bugs and debugging such multi-threaded programs are quite challenging due to their inherent non-determinism. In order to mitigate such non-determinism, many approaches such as record-and-replay have been proposed. However, those approaches often suffer significant performance degradation because they require a large amount of recorded information and/or long analysis and replay time. In this paper, we propose an efficient and effective approach, ReCBuLC (reproducing concurrency bugs using local clocks), to take advantage of the hardware clocks available on modern processors. The key idea is to reduce the recording overhead and the time to analyze events’ global order by recording timestamps in each thread. These timestamps are used to determine the global order of shared accesses. To avoid the large overhead in accessing system-wide global clock, we opt to use local per-core clocks that incur much less access overhead. We then propose techniques to resolve skews among local clocks and obtain an accurate global event order. By using per-core clocks, state-of-the-art bug reproducing systems such as PRES and CLAP can reduce their recording overheads by up to 85 percent, and the analysis time up to 84.66%<inline-formula><tex-math notation=""LaTeX"">$\sim$</tex-math><alternatives><inline-graphic xlink:href=""wang-ieq1-2752158.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>99.99%, respectively.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2752158","National High Technology Research and Development Program of China; National Natural Science Foundation of China (NSFC); Innovation Research Group of NSFC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8038023","Concurrency;bug reproducing;local clock","Clocks;Program processors;Computer bugs;Concurrent computing;Hardware;Debugging;Computer architecture","","","","","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Software Development Estimation Biases: The Role of Interdependence","M. Jorgensen; S. Grimstad","University of Oslo, Lysaker; University of Oslo, Lysaker","IEEE Transactions on Software Engineering","","2012","38","3","677","693","Software development effort estimates are frequently too low, which may lead to poor project plans and project failures. One reason for this bias seems to be that the effort estimates produced by software developers are affected by information that has no relevance for the actual use of effort. We attempted to acquire a better understanding of the underlying mechanisms and the robustness of this type of estimation bias. For this purpose, we hired 374 software developers working in outsourcing companies to participate in a set of three experiments. The experiments examined the connection between estimation bias and developer dimensions: self-construal (how one sees oneself), thinking style, nationality, experience, skill, education, sex, and organizational role. We found that estimation bias was present along most of the studied dimensions. The most interesting finding may be that the estimation bias increased significantly with higher levels of interdependence, i.e., with stronger emphasis on connectedness, social context, and relationships. We propose that this connection may be enabled by an activation of one's self-construal when engaging in effort estimation, and a connection between a more interdependent self-construal and increased search for indirect messages, lower ability to ignore irrelevant context, and a stronger emphasis on socially desirable responses.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6193066","Effort estimation;estimation bias;cultural differences;software engineering.","Estimation;Software;Companies;Context;Programming;Outsourcing;Instruments","estimation theory;software management","software development estimation;interdependence role;project failures;project plans;outsourcing companies","","6","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting with sparse data","M. Shepperd; M. Cartwright","Sch. of Design, Eng. & Comput., Bournemouth Univ., Poole, UK; NA","IEEE Transactions on Software Engineering","","2001","27","11","987","998","It is well-known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. The authors describe our sparse data method (SDM) based upon a pairwise comparison technique and T.L. Saaty's (1980) Analytic Hierarchy Process (AHP). Our minimum data requirement is a single known point. The technique is supported by a software tool known as DataSalvage. We show, for data from two companies, how our approach, based upon expert judgement, adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that our approach is robust to pairwise comparison errors. We then describe the results of a small usability trial with a practicing project manager. From this empirical work, we conclude that the technique is promising and may help overcome some of the present barriers to effective project prediction.","0098-5589;1939-3520;2326-3881","","10.1109/32.965339","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=965339","","Costs;Programming;Software engineering;Machine learning;Calibration;Software tools;Sensitivity analysis;Robustness;Usability;Project management","software cost estimation;software reliability;data analysis;software tools","sparse data;project cost related factor prediction;software engineering;systematic historic data;sparse data method;SDM;pairwise comparison technique;Analytic Hierarchy Process;AHP;minimum data requirement;single known point;software tool;DataSalvage;expert judgement;sensitivity analysis;pairwise comparison errors;usability trial;practicing project manager;project prediction;software project effort","","49","","32","","","","","","IEEE","IEEE Journals & Magazines"
"A tool for analyzing and fine tuning the real-time properties of an embedded system","D. B. Stewart; G. Arora","EVP/CTO of Embedded Res. Solutions, Annapolis, MD, USA; NA","IEEE Transactions on Software Engineering","","2003","29","4","311","326","This paper describes a computer-aided software engineering (CASE) tool that helps designers analyze and fine-tune the timing properties of their embedded real-time software. Existing CASE tools focus on the software specification and design of embedded systems. However, they provide little, if any, support after the software has been implemented. Even if the developer used a CASE tool to design the system, their system most likely does not meet the specifications on the first try. This paper includes guidelines for implementing analyzable code, profiling a real-time system, filtering and extracting measured data, analyzing the data, and interactively predicting the effect of changes to the real-time system. The tool is a necessary first step towards automating the debugging and fine tuning of an embedded system's temporal properties.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1191796","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1191796","","Real time systems;Embedded system;Computer aided software engineering;Embedded software;Software tools;Timing;Software design;Guidelines;Filtering;Data mining","computer aided software engineering;real-time systems;embedded systems;interactive programming","embedded system real-time properties analysis tool;embedded system real-time properties fine tuning tool;computer-aided software engineering tool;CASE tool;embedded real-time software;analyzable code;real-time system profiling;data filtering;data extraction;data analysis;debugging","","14","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Apportioning: a technique for efficient reachability analysis of concurrent object-oriented programs","S. Iyer; S. Ramesh","Sch. of Inf. Technol., Indian Inst. of Technol., Mumbai, India; NA","IEEE Transactions on Software Engineering","","2001","27","11","1037","1056","The object-oriented paradigm in software engineering provides support for the construction of modular and reusable program components and is attractive for the design of large and complex distributed systems. Reachability analysis is an important and well-known tool for static analysis of critical properties in concurrent programs, such as deadlock freedom. It involves the systematic enumeration of all possible global states of program execution and provides the same level of assurance for properties of the synchronization structure in concurrent programs, such as formal verification. However, direct application of traditional reachability analysis to concurrent object-oriented programs has many problems, such as incomplete analysis for reusable classes (not safe) and increased computational complexity (not efficient). We propose a novel technique called apportioning, for safe and efficient reachability analysis of concurrent object-oriented programs, that is based upon a simple but powerful idea of classification of program analysis points as local (having influence within a class) and global (having possible influence outside a class). We have developed a number of apportioning-based algorithms, having different degrees of safety and efficiency. We present the details of one of these algorithms, formally show its safety for an appropriate class of programs, and present experimental results to demonstrate its efficiency for various examples.","0098-5589;1939-3520;2326-3881","","10.1109/32.965343","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=965343","","Reachability analysis;Modular construction;System recovery;Formal verification;Safety;Libraries;Software engineering;Computational complexity;Algorithm design and analysis;Object oriented programming","object-oriented programming;parallel programming;reachability analysis;program diagnostics;software cost estimation","apportioning technique;reachability analysis;concurrent object-oriented programs;object-oriented paradigm;software engineering;modular reusable program components;complex distributed systems;static analysis;concurrent programs;deadlock freedom;global states;synchronization structure;formal verification;direct application;reusable classes;computational complexity;program analysis points;apportioning-based algorithms;experimental results","","5","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Palantir: Early Detection of Development Conflicts Arising from Parallel Code Changes","A. Sarma; D. F. Redmiles; A. van der Hoek","University of Nebraska- Lincoln, Lincoln; University of California, Irvine, Irvine; University of California, Irvine, Irvine","IEEE Transactions on Software Engineering","","2012","38","4","889","908","The earlier a conflict is detected, the easier it is to resolve-this is the main precept of workspace awareness. Workspace awareness seeks to provide users with information of relevant ongoing parallel changes occurring in private workspaces, thereby enabling the early detection and resolution of potential conflicts. The key approach is to unobtrusively inform developers of potential conflicts arising because of concurrent changes to the same file and dependency violations in ongoing parallel work. This paper describes our research goals, approach, and implementation of workspace awareness through Palantír and includes a comprehensive evaluation involving two laboratory experiments. We present both quantitative and qualitative results from the experiments, which demonstrate that the use of Palantír, as compared to not using Palantír 1) leads to both earlier detection and earlier resolution of a larger number of conflicts, 2) leaves fewer conflicts unresolved in the code base that was ultimately checked in, and 3) involves reasonable overhead. Furthermore, we report on interesting changes in users' behavior, especially how conflict resolution strategies changed among Palantír users.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928359","Software engineering;computer-supported collaborative work;programmer workbench;configuration management","Monitoring;Measurement;Instant messaging;Computer architecture;Databases;Context;Laboratories","configuration management;groupware;parallel processing;software management","Palantír;development conflict early detection;parallel code changes;workspace awareness;dependency violations;laboratory experiments;conflict resolution strategies;software configuration management system;computer-supported collaborative work;software engineering","","25","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Discovering Documentation for Java Container Classes","J. Henkel; C. Reichenbach; A. Diwan","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","8","526","543","Modern programs make extensive use of reusable software libraries. For example, we found that 17 percent to 30 percent of the classes in a number of large Java applications use the container classes from the java.util package. Given this extensive code reuse in Java programs, it is important for the reusable interfaces to have clear and unambiguous documentation. Unfortunately, most documentation is expressed in English and, therefore, does not always satisfy these requirements. Worse yet, there is no way of checking that the documentation is consistent with the associated code. Formal specifications present an alternative that does not suffer from these problems; however, formal specifications are notoriously hard to write. To alleviate this difficulty, we have implemented a tool that automatically derives documentation in the form of formal specifications. Our tool probes Java classes by invoking them on dynamically generated tests and captures the information observed during their execution as algebraic axioms. Although the tool is not complete or correct from a formal perspective, we demonstrate that it discovers many useful axioms when applied to container classes. These axioms then form an initial formal documentation of the class they describe.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70705","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267024","","Documentation;Java;Containers;Formal specifications;Libraries;Packaging;Probes;Testing;Data structures;Natural languages","algebraic specification;Java;software libraries;system documentation","Java container class;software library;formal specification;documentation discovery;algebraic axiom","","19","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing the Refactorability of Software Clones","N. Tsantalis; D. Mazinanian; G. P. Krishnan","Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Quebec, Canada","IEEE Transactions on Software Engineering","","2015","41","11","1055","1090","The presence of duplicated code in software systems is significant and several studies have shown that clones can be potentially harmful with respect to the maintainability and evolution of the source code. Despite the significance of the problem, there is still limited support for eliminating software clones through refactoring, because the unification and merging of duplicated code is a very challenging problem, especially when software clones have gone through several modifications after their initial introduction. In this work, we propose an approach for automatically assessing whether a pair of clones can be safely refactored without changing the behavior of the program. In particular, our approach examines if the differences present between the clones can be safely parameterized without causing any side-effects. The evaluation results have shown that the clones assessed as refactorable by our approach can be indeed refactored without causing any compile errors or test failures. Additionally, the computational cost of the proposed approach is negligible (less than a second) in the vast majority of the examined cases. Finally, we perform a large-scale empirical study on over a million clone pairs detected by four different clone detection tools in nine open-source projects to investigate how refactorability is affected by different clone properties and tool configuration options. Among the highlights of our conclusions, we found that (a) clones in production code tend to be more refactorable than clones in test code, (b) clones with a close relative location (i.e., same method, type, or file) tend to be more refactorable than clones in distant locations (i.e., same hierarchy, or unrelated types), (c) Type-1 clones tend to be more refactorable than the other clone types, and (d) clones with a small size tend to be more refactorable than clones with a larger size.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2448531","European Union (European Social Fund—ESF); Greek national funds through the Operational Program; National Strategic Reference Framework (NSRF); Thalis—Athens University of Economics; Business—Software Engineering Research Platform; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7130676","Code duplication;Software clone management;Clone refactoring;Refactorability assessment;Empirical study;Code duplication;software clone management;clone refactoring;refactorability assessment;empirical study","Cloning;Arrays;Java;Software systems;Production;Space exploration","software maintenance;software management;source code (software)","software clone refactorability assessment;duplicated code;software systems;source code evolution;source code maintainability;duplicated code merging;duplicated code unification;compile errors;test failures;computational cost;large-scale empirical;clone pairs;clone detection tools;open-source projects;clone properties;tool configuration;test code;relative location;distant locations;type-1 clones","","17","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Exploiting Dynamic Information in IDEs Improves Speed and Correctness of Software Maintenance Tasks","D. Rothlisberger; M. Harry; W. Binder; P. Moret; D. Ansaloni; A. Villazon; O. Nierstrasz","Universit&#x0E1;t Bern, Bern; Universit&#x0E1;t Bern, Bern; University of Lugano (USI), Lugano; University of Lugano (USI), Lugano; University of Lugano (USI), Lugano; Universidad Privada Boliviana (UPB), Cochabamba; Universit&#x0E1;t Bern, Bern","IEEE Transactions on Software Engineering","","2012","38","3","579","591","Modern IDEs such as Eclipse offer static views of the source code, but such views ignore information about the runtime behavior of software systems. Since typical object-oriented systems make heavy use of polymorphism and dynamic binding, static views will miss key information about the runtime architecture. In this paper, we present an approach to gather and integrate dynamic information in the Eclipse IDE with the goal of better supporting typical software maintenance activities. By means of a controlled experiment with 30 professional developers, we show that for typical software maintenance tasks, integrating dynamic information into the Eclipse IDE yields a significant 17.5 percent decrease of time spent while significantly increasing the correctness of the solutions by 33.5 percent. We also provide a comprehensive performance evaluation of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6178187","Object-oriented programming;integrated environments;restructuring;reverse engineering;reengineering;complexity measures;performance measures.","Runtime;Measurement;Java;Context;Software maintenance;Concrete;Weaving","dynamic programming;object-oriented programming;program compilers;software maintenance","exploiting dynamic information;IDE;software maintenance tasks;Eclipse;source code;runtime behavior;software systems;object-oriented systems;dynamic binding;runtime architecture;dynamic information","","13","","35","","","","","","IEEE","IEEE Journals & Magazines"
"A Survey of Recent Trends in Testing Concurrent Software Systems","F. A. Bianchi; A. Margara; M. Pezzè","Faculty of Informatics, Univeristà della Svizzera italiana, USI Lugano, Lugano, Switzerland; Faculty of Informatics, Univeristà della Svizzera italiana, USI Lugano, Lugano, Switzerland; Faculty of Informatics, Univeristà della Svizzera italiana, USI Lugano, Lugano, Switzerland","IEEE Transactions on Software Engineering","","2018","44","8","747","783","Many modern software systems are composed of multiple execution flows that run simultaneously, spanning from applications designed to exploit the power of modern multi-core architectures to distributed systems consisting of multiple components deployed on different physical nodes. We collectively refer to such systems as concurrent systems. Concurrent systems are difficult to test, since the faults that derive from their concurrent nature depend on the interleavings of the actions performed by the individual execution flows. Testing techniques that target these faults must take into account the concurrency aspects of the systems. The increasingly rapid spread of parallel and distributed architectures led to a deluge of concurrent software systems, and the explosion of testing techniques for such systems in the last decade. The current lack of a comprehensive classification, analysis and comparison of the many testing techniques for concurrent systems limits the understanding of the strengths and weaknesses of each approach and hampers the future advancements in the field. This survey provides a framework to capture the key features of the available techniques to test concurrent software systems, identifies a set of classification criteria to review and compare the available techniques, and discusses in details their strengths and weaknesses, leading to a thorough assessment of the field and paving the road for future progresses.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2707089","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7932530","Survey;classification;testing;concurrent systems;parallel systems;distributed systems","Testing;Software systems;Message passing;History;Concurrent computing;Computer architecture;Synchronization","formal specification;multiprocessing systems;object-oriented programming;parallel architectures;program testing","distributed systems;testing techniques;concurrency aspects;modern software systems;modern multicore architectures;concurrent software system testing;multiple execution flows;parallel architecture;distributed architecture;classification criteria","","","","216","","","","","","IEEE","IEEE Journals & Magazines"
"A testing framework for mobile computing software","I. Satoh","Nat. Inst. of Informatics, Tokyo, Japan","IEEE Transactions on Software Engineering","","2003","29","12","1112","1121","We present a framework for testing applications for mobile computing devices. When a device is moved into and attached to a new network, the proper functioning of applications running on the device often depends on the resources and services provided locally in the current network. This framework provides an application-level emulator for mobile computing devices to solve this problem. Since the emulator is constructed as a mobile agent, it can carry applications across networks on behalf of its target device and allow the applications to connect to local servers in its current network in the same way as if they had been moved with and executed on the device itself. This paper also demonstrates the utility of this framework by describing the development of typical network-dependent applications in mobile and ubiquitous computing settings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265525","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265525","","Software testing;Mobile computing;Computer networks;Application software;Mobile agents;Computer applications;Network servers;Pervasive computing;Floors;Wireless networks","mobile computing;mobile agents;program testing;wireless LAN;cellular radio","testing framework;mobile computing software;mobile computing devices;application-level emulator;mobile agent;network-dependent applications;ubiquitous computing","","52","","21","","","","","","IEEE","IEEE Journals & Magazines"
"An automated verification method for distributed systems software based on model extraction","G. J. Holzmann; M. H. Smith","Lucent Technol. Bell Labs., Murray Hill, NJ, USA; NA","IEEE Transactions on Software Engineering","","2002","28","4","364","377","Software verification methods are used only sparingly in industrial software development today. The most successful methods are based on the use of model checking. There are, however, many hurdles to overcome before the use of model checking tools can truly become mainstream. To use a model checker, the user must first define a formal model of the application, and to do so requires specialized knowledge of both the application and of model checking techniques. For larger applications, the effort to manually construct a formal model can take a considerable investment of time and expertise, which can rarely be afforded. Worse, it is hard to secure that a manually constructed model can keep pace with the typical software application, as it evolves from the concept stage to the product stage. We describe a verification method that requires far less specialized knowledge in model construction. It allows us to extract models mechanically from source code. The model construction process now becomes easily repeatable, as the application itself continues to evolve. Once the model is constructed, existing model checking techniques allow us to perform all checks in a mechanical fashion, achieving nearly complete automation. The level of thoroughness that can be achieved with this new type of software testing is significantly greater than for conventional techniques. We report on the application of this method in the verification of the call processing software for a new telephone switch that was developed at Lucent Technologies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995426","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995426","","System software","program verification;distributed processing;program testing;telecommunication computing","automated verification method;distributed systems software;model extraction;software verification methods;industrial software development;formal model;source code;software testing;formal methods;call processing software;telephone switch;Lucent Technologies;reactive systems;case studies","","39","","29","","","","","","IEEE","IEEE Journals & Magazines"
"A Dynamic Slicing Technique for UML Architectural Models","J. T. Lallchandani; R. Mall","Indian Institute of Technology Kharagpur, WB INDIA; Indian Institute of Technology Kharagpur, WB INDIA","IEEE Transactions on Software Engineering","","2011","37","6","737","771","This paper proposes a technique for dynamic slicing of UML architectural models. The presence of related information in diverse model parts (or fragments) makes dynamic slicing of Unified Modeling Language (UML) models a complex problem. We first extract all relevant information from a UML model specifying a software architecture into an intermediate representation, which we call a Model Dependency Graph (MDG). For a given slicing criterion, our slicing algorithm traverses the constructed MDG to identify the relevant model parts that are directly or indirectly affected during the execution of a specified scenario. One novelty of our approach is computation of dynamic slice based on the structural and behavioral (interactions only) UML models as against independently processing separate UML models, and determining the implicit interdependencies among different model elements distributed across model views. We also briefly discuss a prototype tool named Archlice, which we have developed to implement our algorithm.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.112","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680909","Software architecture;UML;architectural metamodel;dynamic slicing;impact analysis.","Unified modeling language;Computational modeling;Heuristic algorithms;Computer architecture;Analytical models;Software architecture;Software algorithms","program slicing;software architecture;software prototyping;Unified Modeling Language","dynamic slicing technique;UML architectural models;unified modeling language models;model dependency graph;software architecture;prototype tool;Archlice","","15","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt","E. d. S. Maldonado; E. Shihab; N. Tsantalis","Department of Computer Science and Software Engineering, Data-Driven Analysis of Software (DAS) Lab, Concordia University, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Data-Driven Analysis of Software (DAS) Lab, Concordia University, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada","IEEE Transactions on Software Engineering","","2017","43","11","1044","1062","The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-admitted technical debt), and that the most common types of self-admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-admitted technical debt, significantly outperforming the current state-of-the-art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2654244","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7820211","Technical debt;source code comments;natural language processing;empirical study","Software;Natural language processing;Manuals;Entropy;Unified modeling language;Java;Structured Query Language","computer crime;Java;natural language processing;project management;public domain software;software maintenance;software management;software quality;SQL","Natural Language Processing;requirement debt;self-admitted technical debt detection;open source projects;source code quality;source code comment classification;NLP;design debt","","7","","58","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Putting Preemptive Time Petri Nets to Work in a V-Model SW Life Cycle","L. Carnevali; L. Ridi; E. Vicario","Universit&#x0E0; di Firenze, Firenze; Universit&#x0E0; di Firenze, Firenze; Universit&#x0E0; di Firenze, Firenze","IEEE Transactions on Software Engineering","","2011","37","6","826","844","Preemptive Time Petri Nets (pTPNs) support modeling and analysis of concurrent timed SW components running under fixed priority preemptive scheduling. The model is supported by a well-established theory based on symbolic state space analysis through Difference Bounds Matrix (DBM) zones, with specific contributions on compositional modularization, trace analysis, and efficient overapproximation and cleanup in the management of suspension deriving from preemptive behavior. In this paper, we devise and implement a framework that brings the theory to application. To this end, we cast the theory into an organic tailoring of design, coding, and testing activities within a V-Model SW life cycle in respect of the principles of regulatory standards applied to the construction of safety-critical SW components. To implement the toolchain subtended by the overall approach into a Model Driven Development (MDD) framework, we complement the theory of state space analysis with methods and techniques supporting semiformal specification and automated compilation into pTPN models and real-time code, measurement-based Execution Time estimation, test case selection and execution, coverage evaluation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680913","Real-time systems;safety-critical SW components;SW life cycle;V-Model;preemptive Time Petri Nets;symbolic state space analysis;model driven development;automated model transformation;automated code generation;Execution Time estimation;real-time testing;test case selection and execution;coverage analysis.","Real time systems;Analytical models;Unified modeling language;Petri nets;Mathematical model;Computer architecture","formal specification;Petri nets;program diagnostics;program testing;safety-critical software;scheduling","preemptive time Petri nets;V-model SW life cycle;concurrent timed SW components;fixed priority preemptive scheduling;symbolic state space analysis;difference bounds matrix zones;compositional modularization;trace analysis;overapproximation;safety-critical SW components;model driven development framework;semiformal specification;automated compilation;pTPN models;real-time code;measurement-based execution time estimation;test case selection;test case execution;coverage evaluation","","8","","71","","","","","","IEEE","IEEE Journals & Magazines"
"A General Testability Theory: Classes, Properties, Complexity, and Testing Reductions","I. Rodríguez; L. Llana; P. Rabanal","Department of Sistemas Informáticos y Computación, Universidad Complutense de Madrid, Madrid, Spain; Department of Sistemas Informáticos y Computación, Universidad Complutense de Madrid, Madrid, Spain; Department of Sistemas Informáticos y Computación, Universidad Complutense de Madrid, Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","9","862","894","In this paper we develop a general framework to reason about testing. The difficulty of testing is assessed in terms of the amount of tests that must be applied to determine whether the system is correct or not. Based on this criterion, five testability classes are presented and related. We also explore conditions that enable and disable finite testability, and their relation to testing hypotheses is studied. We measure how far incomplete test suites are from being complete, which allows us to compare and select better incomplete test suites. The complexity of finding that measure, as well as the complexity of finding minimum complete test suites, is identified. Furthermore, we address the reduction of testing problems to each other, that is, we study how the problem of finding test suites to test systems of some kind can be reduced to the problem of finding test suites for another kind of systems. This enables to export testing methods. In order to illustrate how general notions are applied to specific cases, many typical examples from the formal testing techniques domain are presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2331690","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6839051","Formal testing techniques;general testing frameworks","Testing;Complexity theory;Proposals;Computational modeling;Probabilistic logic;Abstracts;Computer languages","formal languages;formal verification","general testability theory;complexity;testing reductions;testability classes;finite testability;incomplete test suites;formal testing techniques domain","","14","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating the effect of a delegated versus centralized control style on the maintainability of object-oriented software","E. Arisholm; D. I. K. Sjoberg","Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway; Dept. of Software Eng., Simula Res. Lab., Lysaker, Norway","IEEE Transactions on Software Engineering","","2004","30","8","521","534","A fundamental question in object-oriented design is how to design maintainable software. According to expert opinion, a delegated control style, typically a result of responsibility-driven design, represents object-oriented design at its best, whereas a centralized control style is reminiscent of a procedural solution, or a ""bad"" object-oriented design. We present a controlled experiment that investigates these claims empirically. A total of 99 junior, intermediate, and senior professional consultants from several international consultancy companies were hired for one day to participate in the experiment. To compare differences between (categories of) professionals and students, 59 students also participated. The subjects used professional Java tools to perform several change tasks on two alternative Java designs that had a centralized and delegated control style, respectively. The results show that the most skilled developers, in particular, the senior consultants, require less time to maintain software with a delegated control style than with a centralized control style. However, more novice developers, in particular, the undergraduate students and junior consultants, have serious problems understanding a delegated control style, and perform far better with a centralized control style. Thus, the maintainability of object-oriented software depends, to a large extent, on the skill of the developers who are going to maintain it. These results may have serious implications for object-oriented development in an industrial context: having senior consultants design object-oriented systems may eventually pose difficulties unless they make an effort to keep the designs simple, as the cognitive complexity of ""expert"" designs might be unmanageable for less skilled maintainers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316869","Index Terms- Design principles;responsibility delegation;control styles;object-oriented design;object-oriented programming;software maintainability;controlled experiment.","Software maintenance;Centralized control;Software design;Java;Design methodology;Unified modeling language;Business communication;Logic;Electrical equipment industry;Object oriented programming","software maintenance;object-oriented programming;Java;software development management","object-oriented software maintainability;object-oriented design;professional Java tool;Java design;centralized control style;delegated control style;object-oriented development;object-oriented programming","","75","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying extensions required by RUP (rational unified process) to comply with CMM (capability maturity model) levels 2 and 3","L. V. Manzoni; R. T. Price","Univ. Fed. do Rio Grande do Sul, Porto Alegre, Brazil; Univ. Fed. do Rio Grande do Sul, Porto Alegre, Brazil","IEEE Transactions on Software Engineering","","2003","29","2","181","192","This paper describes an assessment of the rational unified process (RUP) based on the capability maturity model (CMM). For each key practice (KP) identified in each key process area (KPA) of CMM levels 2 and 3, the Rational Unified Process was assessed to determine whether it satisfied the KP or not. For each KPA, the percentage of the key practices supported was calculated, and the results were tabulated. The report includes considerations about the coverage of each key process area, describing the highlights of the RUP regarding its support for CMM levels 2 and 3, and suggests where an organization using it will need to complement it to conform to CMM. The assessment resulted in the elaboration of proposals to enhance the RUP in order to satisfy the key process areas of CMM. Some of these are briefly described in this article.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178058","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178058","","Coordinate measuring machines;Capability maturity model;Software engineering;Software quality;Unified modeling language;Software tools;Proposals;Software standards;Systems engineering and theory;Production","software process improvement","rational unified process;capability maturity model;RUP;CMM;key practice;key process area;KP;KPA","","19","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Methodbook: Recommending Move Method Refactorings via Relational Topic Models","G. Bavota; R. Oliveto; M. Gethers; D. Poshyvanyk; A. De Lucia","University of Sannio, Benevento, Italy; University of Molise, Pesche (IS), Italy; Information Systems Department , University of Maryland, Baltimore County, 1000 Hilltop Circle, Baltimore; College of William and Mary, McGlothlin-Street Hall 006, Williamsburg; University of Salerno, Fisciano (SA), Italy","IEEE Transactions on Software Engineering","","2014","40","7","671","694","During software maintenance and evolution the internal structure of the software system undergoes continuous changes. These modifications drift the source code away from its original design, thus deteriorating its quality, including cohesion and coupling of classes. Several refactoring methods have been proposed to overcome this problem. In this paper we propose a novel technique to identify Move Method refactoring opportunities and remove the Feature Envy bad smell from source code. Our approach, coined as Methodbook, is based on relational topic models (RTM), a probabilistic technique for representing and modeling topics, documents (in our case methods) and known relationships among these. Methodbook uses RTM to analyze both structural and textual information gleaned from software to better support move method refactoring. We evaluated Methodbook in two case studies. The first study has been executed on six software systems to analyze if the move method operations suggested by Methodbook help to improve the design quality of the systems as captured by quality metrics. The second study has been conducted with eighty developers that evaluated the refactoring recommendations produced by Methodbook. The achieved results indicate that Methodbook provides accurate and meaningful recommendations for move method refactoring operations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6684534","Refactoring;relational topic models;empirical studies","Software systems;Couplings;Measurement;Object oriented modeling;Educational institutions;Electronic mail","software maintenance;software metrics;source code (software)","Methodbook;recommending move method refactorings;relational topic model;software maintenance;source code;modifications drift;quality metrics;software development","","53","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Atomicity Analysis of Service Composition across Organizations","C. Ye; S. C. Cheung; W. K. Chan; C. Xu","The Hong Kong University of Science and Technology, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong; City University of Hong Kong, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong","IEEE Transactions on Software Engineering","","2009","35","1","2","28","Atomicity is a highly desirable property for achieving application consistency in service compositions. To achieve atomicity, a service composition should satisfy the atomicity sphere, a structural criterion for the backend processes of involved services. Existing analysis techniques for atomicity sphere generally assume complete knowledge of all involved backend processes. Such an assumption is invalid when some service providers do not release all details of their backend processes to service consumers outside the organizations. To address this problem, we propose a process algebraic framework to publish atomicity-equivalent public views from the backend processes. These public views extract relevant task properties and reveal only partial process details that service providers need to expose. Our framework enables the analysis of atomicity sphere for service compositions using these public views instead of their backend processes. This allows service consumers to choose suitable services such that their composition satisfies the atomicity sphere without disclosing the details of their backend processes. Based on the theoretical result, we present algorithms to construct atomicity-equivalent public views and to analyze the atomicity sphere for a service composition. Two case studies from supply chain and insurance domains are given to evaluate our proposal and demonstrate the applicability of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4641941","Analysis;Specification;Software and System Safety;System integration and implementation;Formal methods;Model checking;Validation;Analysis;Specification;Software and System Safety;System integration and implementation;Formal methods;Model checking;Validation","Web services;Privacy;System recovery;Supply chains;Protection;Application software;Algorithm design and analysis;Insurance;Algebra;Internet","process algebra;Web services","atomicity sphere analysis;Web service composition;backend process;process algebraic framework;atomicity-equivalent public view;supply chain;insurance","","19","","76","","","","","","IEEE","IEEE Journals & Magazines"
"A formal specification framework for object-oriented distributed systems","D. Buchs; N. Guelfi","Software Eng. Lab., Swiss Federal Inst. of Technol., Lausanne, Switzerland; NA","IEEE Transactions on Software Engineering","","2000","26","7","635","652","In this paper, we present the Concurrent Object-Oriented Petri Nets (CO-OPN/2) formalism devised to support the specification of large distributed systems. Our approach is based on two underlying formalisms: order-sorted algebra and algebraic Petri nets. With respect to the lack of structuring capabilities of Petri nets, CO-OPN/2 has adopted the object-oriented paradigm. In this hybrid approach (model- and property-oriented), classes of objects are described by means of algebraic Petri nets, while data structures are expressed by order-sorted algebraic specifications. An original feature is the sophisticated synchronization mechanism. This mechanism allows to involve many partners in a synchronization and to describe the synchronization policy. A typical example of distributed systems, namely the Transit Node, is used throughout this paper to introduce our formalism and the concrete specification language associated with it. By successive refinements of the components of the example, we present, informally, most of the notions of CO-OPN/2. We also give some insights about the coordination layer, Context and Objects Interface Language (COIL), which is built on top of CO-OPN/2. This coordination layer is used for the description of the concrete distributed architecture of the system. Together, CO-OPN/2 and COIL provide a complete formal framework for the specification of distributed systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.859532","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=859532","","Formal specifications;Petri nets;Concrete;Data structures;Concurrent computing;Distributed processing;Communication systems;Algebra;Object oriented modeling;Specification languages","algebraic specification;Petri nets;object-oriented methods","formal specification;object-oriented distributed systems;Concurrent Object-Oriented Petri Nets;Petri nets;order-sorted algebra;data structures;classes of objects;distributed systems","","21","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Quantifying the Effect of Code Smells on Maintenance Effort","D. I. K. Sjøberg; A. Yamashita; B. C. D. Anda; A. Mockus; T. Dybå","University of Oslo, Oslo; University of Oslo, Oslo; University of Oslo, Oslo; Avaya Labs Research, Basking Ridge; University of Oslo, Oslo and SINTEF, Trondheim","IEEE Transactions on Software Engineering","","2013","39","8","1144","1156","Context: Code smells are assumed to indicate bad design that leads to less maintainable code. However, this assumption has not been investigated in controlled studies with professional software developers. Aim: This paper investigates the relationship between code smells and maintenance effort. Method: Six developers were hired to perform three maintenance tasks each on four functionally equivalent Java systems originally implemented by different companies. Each developer spent three to four weeks. In total, they modified 298 Java files in the four systems. An Eclipse IDE plug-in measured the exact amount of time a developer spent maintaining each file. Regression analysis was used to explain the effort using file properties, including the number of smells. Result: None of the 12 investigated smells was significantly associated with increased effort after we adjusted for file size and the number of changes; Refused Bequest was significantly associated with decreased effort. File size and the number of changes explained almost all of the modeled variation in effort. Conclusion: The effects of the 12 smells on maintenance effort were limited. To reduce maintenance effort, a focus on reducing code size and the work practices that limit the number of changes may be more beneficial than refactoring code smells.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6392174","Maintainability;object-oriented design;product metrics;code churn","Maintenance engineering;Java;Software;Surgery;Time measurement;Context;Electronic mail","Java;regression analysis;software maintenance","code smell effect quantification;maintenance effort;maintainable code;maintenance tasks;Java systems;Java files;Eclipse IDE plug-in;regression analysis;file properties;refused bequest;file size;code size reduction;code smell refactoring","","99","","46","","","","","","IEEE","IEEE Journals & Magazines"
"An authentication logic with formal semantics supporting synchronization, revocation, and recency","S. G. Stubblebine; R. N. Wright","Stubblebine Consulting, Madison, NJ, USA; NA","IEEE Transactions on Software Engineering","","2002","28","3","256","285","Distributed systems inherently involve dynamic changes to the value of security-relevant attributes such as the goodness of encryption keys, trustworthiness of participants, and synchronization between principals. Since concurrent knowledge is usually infeasible or impractical, it is often necessary for the participants of distributed protocols to determine and act on beliefs that may not be supported by the current state of the system. Policies for determining beliefs in such situations can range from extremely conservative, such as only believing statements if they are very recent, to extremely optimistic, such as believing all statements that are not yet known to be revoked. Such security policies often are heavily dependent on timing of received messages and on synchronization between principals. We present a logic for analyzing cryptographic protocols that has the capability to specify time and synchronization details. This capability considerably advances the scope of known techniques both for expressing practical authentication policies of protocol participants as constraints and for reasoning about protocol goals subject to these constraints.","0098-5589;1939-3520;2326-3881","","10.1109/32.991320","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=991320","","Authentication;Logic","formal logic;synchronisation;distributed processing;protocols;cryptography;message authentication","formal methods;authentication logic;security policies;secure authentication;revocation;temporal reasoning;clock synchronization;distributed systems security;cryptographic protocols;public key infrastructures;private key infrastructures","","10","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Global analysis and transformations in preprocessed languages","D. Spinellis","Dept. of Manage. Sci. & Technol., Athens Univ. of Econ. & Bus., Greece","IEEE Transactions on Software Engineering","","2003","29","11","1019","1030","Tool support for refactoring code written in mainstream languages such as C and C++ is currently lacking due to the complexity introduced by the mandatory preprocessing phase that forms part of the C/C++ compilation cycle. The definition and use of macros complicates the notions of scope and of identifier boundaries. The concept of token equivalence classes can be used to bridge the gap between the language proper semantic analysis and the non-preprocessed source code. The CScout toolchest uses the developed theory to analyze large interdependent program families. A Web-based interactive front end allows the precise realization of rename and remove refactorings on the original C source code. In addition, CScout can convert programs into a portable obfuscated format or store a complete and accurate representation of the code and its identifiers in a relational database.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245303","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245303","","Humans;Bridges;Relational databases;Reverse engineering;Design methodology;Performance analysis;Tagging;Programming profession;Encapsulation;Automation","equivalence classes;software engineering;reverse engineering;C++ language;relational databases","global analysis;preprocessed languages;tool support;refactoring code;macros;token equivalence classes;semantic analysis;nonpreprocessed source code;CScout toolchest;Web-based interactive front end;relational database;preprocessor;reverse engineering","","21","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling software measurement data","B. A. Kitchenham; R. T. Hughes; S. G. Linkman","Dept. of Comput. Sci., Keele Univ., UK; NA; NA","IEEE Transactions on Software Engineering","","2001","27","9","788","804","This paper proposes a method for specifying models of software data sets in order to capture the definitions and relationships among software measures. We believe a method of defining software data sets is necessary to ensure that software data are trustworthy. Software companies introducing a measurement program need to establish procedures to collect and store trustworthy measurement data. Without appropriate definitions it is difficult to ensure data values are repeatable and comparable. Software metrics researchers need to maintain collections of software data sets. Such collections allow researchers to assess the generality of software engineering phenomena. Without appropriate safeguards, it is difficult to ensure that data from different sources are analyzed correctly. These issues imply the need for a standard method of specifying software data sets so they are fully documented and can be exchanged with confidence. We suggest our method of defining data sets can be used as such a standard. We present our proposed method in terms of a conceptual entity-relationship data model that allows complex software data sets to be modeled and their data values stored. The standard can, therefore, contribute both to the definition of a company measurement program and to the exchange of data sets among researchers.","0098-5589;1939-3520;2326-3881","","10.1109/32.950316","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=950316","","Software measurement;Memory;Particle measurements;Data analysis;Software metrics;Software maintenance;Computer Society;Software engineering;Software standards;Data models","software metrics;software engineering;software development management","software measurement data;software data sets;definitions;relationships;software measures;software companies;measurement program;trustworthy measurement data;software metrics;software engineering phenomena;generality;conceptual entity-relationship data model;data values;company measurement program","","64","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Spatial complexity metrics: an investigation of utility","N. E. Gold; A. M. Mohan; P. J. Layzell","Dept. of Comput. Sci., King's Coll., London, UK; NA; NA","IEEE Transactions on Software Engineering","","2005","31","3","203","212","Software comprehension is one of the largest costs in the software lifecycle. In an attempt to control the cost of comprehension, various complexity metrics have been proposed to characterize the difficulty of understanding a program and, thus, allow accurate estimation of the cost of a change. Such metrics are not always evaluated. This paper evaluates a group of metrics recently proposed to assess the ""spatial complexity"" of a program (spatial complexity is informally defined as the distance a maintainer must move within source code to build a mental model of that code). The evaluation takes the form of a large-scale empirical study of evolving source code drawn from a commercial organization. The results of this investigation show that most of the spatial complexity metrics evaluated offer no substantially better information about program complexity than the number of lines of code. However, one metric shows more promise and is thus deemed to be a candidate for further use and investigation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423992","Index Terms- Maintenance measurement;complexity measures;maintainability;software psychology.","Costs;Software maintenance;Cognitive science;Large-scale systems;Software measurement;Psychology;Gold;Computer Society;Monitoring;Preventive maintenance","software maintenance;software metrics;software cost estimation","software comprehension;software lifecycle;spatial complexity metrics;program complexity;software maintenance;software psychology","","8","","36","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study of the Complex Relationships between Requirements Engineering Processes and Other Processes that Lead to Payoffs in Productivity, Quality, and Risk Management","D. Damian; J. Chisan","NA; NA","IEEE Transactions on Software Engineering","","2006","32","7","433","453","Requirements engineering is an important component of effective software engineering, yet more research is needed to demonstrate the benefits to development organizations. While the existing literature suggests that effective requirements engineering can lead to improved productivity, quality, and risk management, there is little evidence to support this. We present empirical evidence showing how requirements engineering practice relates to these claims. This evidence was collected over the course of a 30-month case study of a large software development project undergoing requirements process improvement. Our findings add to the scarce evidence on RE payoffs and, more importantly, represent an in-depth explanation of the role of requirements engineering processes in contributing to these benefits. In particular, the results of our case study show that an effective requirements process at the beginning of the project had positive outcomes throughout the project lifecycle, improving the efficacy of other project processes, ultimately leading to improvements in project negotiation, project planning, and managing feature creep, testing, defects, rework, and product quality. Finally, we consider the role collaboration had in producing the effects we observed and the implications of this work to both research and practice","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.61","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677531","Requirements engineering;process improvement;process interactions;empirical investigation.","Productivity;Risk management;Software engineering;Programming;Process planning;Quality management;Project management;Creep;Life testing;Collaborative work","formal specification;project management;risk management;software development management;software process improvement;software quality","requirements engineering;software engineering;risk management;software development project;software process improvement;project lifecycle;software quality","","85","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Toward an architectural knowledge base for wireless service engineering","E. Niemela; J. Kalaoja; P. Lago","VTT Tech. Res. Centre of Finland, Oulu, Finland; VTT Tech. Res. Centre of Finland, Oulu, Finland; NA","IEEE Transactions on Software Engineering","","2005","31","5","361","379","Wireless services are software-based services that exploit distribution infrastructure embedded in our everyday life as various communication and computing technologies. Service architecture defines concepts and principles to develop and maintain services to obtain the quality issues with minimum cost and faster time-to-market. In order to boost the development of wireless services, more effective means of using existing architectural know-how and artifacts are required. Our contribution is the architectural knowledge base that introduces three cornerstones: the service taxonomy, reference service architecture, and basic services that alt together provide an efficient means of creating added value with wireless services. The service taxonomy assists in identifying the required functional and quality properties of services and the constraints of the underlying technology platforms. The reference architecture realizes the required properties, based on a selected set of architectural styles and patterns, and provides a skeleton upon which a new end-user service can be developed faster and more easily by using partially ready-made solutions, and furthermore, to keep the architectural knowledge base evolving at the same time. The architectural knowledge base has been validated in several research projects with industrial companies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1438373","Index Terms- Service architecture;reference architecture;quality attribute;service category;wireless service.","Knowledge engineering;Application software;Computer architecture;Middleware;Open source software;Pervasive computing;Maintenance engineering;Time to market;Taxonomy;Web and internet services","software architecture;knowledge based systems;software quality;mobile computing;time to market;middleware","architectural knowledge base;wireless service engineering;software-based service;quality issue;time-to-market;service taxonomy;reference service architecture;quality attribute;service category","","15","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Design synthesis from interaction and state-based specifications","J. Sun; J. S. Dong","Dept. of Comput. Sci., Nat. Univ. of Singapore, Singapore; Dept. of Comput. Sci., Nat. Univ. of Singapore, Singapore","IEEE Transactions on Software Engineering","","2006","32","6","349","364","Interaction-based and state-based modeling are two complementary approaches of behavior modeling. The former focuses on global interactions between system components. The latter concentrates on the internal states of individual components. Both approaches have been proven useful in practice. One challenging and important research objective is to combine the modeling power of both effectively and then use the combination as the basis for automatic design synthesis. We present a combination of interaction-based and state-based modeling, namely, live sequence charts and Z, for system specification. We then propose a way of generating distributed design from the combinations. Our approach handles systems with intensive interactive behaviors as well as complex state structures","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650212","Z language;live sequence charts;specification;synthesis.","Power system modeling;Automata;Sun;Distributed power generation;System testing;Large-scale systems;Data structures;State-space methods;Explosions;Open systems","charts;formal specification;object-oriented programming;specification languages","automatic design synthesis;interaction-based modeling;state-based modeling;behavior modeling;live sequence charts;system specification;Z language","","32","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental Maintenance of Software Artifacts","S. P. Reiss","NA","IEEE Transactions on Software Engineering","","2006","32","9","682","697","Software is multidimensional, but the tools that support it are not. This lack of tool support causes the software artifacts representing different dimensions to evolve independently and to become inconsistent over time. In order to properly support the evolution of software, one must ensure that the different dimensions evolve concurrently. We have built a software development tool, CLIME that uses constraints implemented as database queries to ensure just this. Our approach makes the tool responsible for detecting inconsistencies between software design, specifications, documentation, source code, test cases, and other artifacts without requiring any of these to be a primary representation. The tool works incrementally as the software evolves, without imposing a particular methodology or process. It includes a front end that lets the user explore and fix current inconsistencies. This paper describes the basis for CLIME, the techniques underlying the tool, the interface provided to the programmer, the incremental maintenance of constraints between these artifacts, and our experiences","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707667","Software maintenance;evolution;programming tools.","Software maintenance;Documentation;Programming profession;Software systems;System testing;Multidimensional systems;Software tools;Software testing;Databases;Software design","software maintenance;software tools","incremental software maintenance;software artifact;software evolution;software development tool;database query","","19","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Tracking Load-Time Configuration Options","M. Lillack; C. Kästner; E. Bodden","University of Leipzig, Leipzig, Germany; Carnegie Mellon University, Pittsburgh, PA; Paderborn University & Fraunhofer IEM, Paderborn, Germany","IEEE Transactions on Software Engineering","","2018","44","12","1269","1291","Many software systems are highly configurable, despite the fact that configuration options and their interactions make those systems significantly harder to understand and maintain. In this work, we consider load-time configuration options, such as parameters from the command-line or from configuration files. They are particularly hard to reason about: tracking configuration options from the point at which they are loaded to the point at which they influence control-flow decisions is tedious and error-prone, if done manually. We design and implement Lotrack, an extended static taint analysis to track configuration options automatically. Lotrack derives a configuration map that explains for each code fragment under which configurations it may be executed. An evaluation on Android apps and Java applications from different domains shows that Lotrack yields high accuracy with reasonable performance. We use Lotrack to empirically characterize how much of the implementation of Android apps depends on the platform's configuration options or interactions of these options.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2756048","German Federal Ministry of Education and Research; US National Science Foundation; Science of Security Lablet; AFRL and DARPA; German Research Foundation (DFG); Heinz Nixdorf Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8049300","Variability mining;configuration options;static analysis","Androids;Humanoid robots;Java;Static analysis;Bluetooth;Data mining","Java;mobile computing","configuration map;Lotrack;tracking load-time configuration options;configuration files;tracking configuration options","","1","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing and improving state-based class testing: a series of experiments","L. C. Briand; M. Di Penta; Y. Labiche","Dept. of syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2004","30","11","770","783","This work describes an empirical investigation of the cost effectiveness of well-known state-based testing techniques for classes or clusters of classes that exhibit a state-dependent behavior. This is practically relevant as many object-oriented methodologies recommend modeling such components with statecharts which can then be used as a basis for testing. Our results, based on a series of three experiments, show that in most cases state-based techniques are not likely to be sufficient by themselves to catch most of the faults present in the code. Though useful, they need to be complemented with black-box, functional testing. We focus here on a particular technique, Category Partition, as this is the most commonly used and referenced black-box, functional testing technique. Two different oracle strategies have been applied for checking the success of test cases. One is a very precise oracle checking the concrete state of objects whereas the other one is based on the notion of state invariant (abstract states). Results show that there is a significant difference between them, both in terms of fault detection and cost. This is therefore an important choice to make that should be driven by the characteristics of the component to be tested, such as its criticality, complexity, and test budget.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359770","Index Terms- State-based testing;testing experimentation;UML statecharts;category partition.","Object oriented modeling;System testing;Costs;Fault detection;Unified modeling language;Computer Society;Concrete;Phase detection;Guidelines;Performance evaluation","Unified Modeling Language;object-oriented methods;formal verification;program debugging;charts;program testing","state-based class testing;state-dependent behavior;object-oriented methodology;functional testing;Category Partition;fault detection;UML statechart","","67","","44","","","","","","IEEE","IEEE Journals & Magazines"
"The SEXTANT Software Exploration Tool","T. Schafer; M. Eichberg; M. Haupt; M. Mezini","Software Technology Group, Darmstadt University of Technology, Hochschultsr. 10, 64289 Darmstadt, Germany; Software Technology Group, Darmstadt University of Technology, Hochschultsr. 10, 64289 Darmstadt, Germany; Software Technology Group, Darmstadt University of Technology, Hochschultsr. 10, 64289 Darmstadt, Germany; Software Technology Group, Darmstadt University of Technology, Hochschultsr. 10, 64289 Darmstadt, Germany","IEEE Transactions on Software Engineering","","2006","32","9","753","768","In this paper, we discuss a set of functional requirements for software exploration tools and provide initial evidence that various combinations of these features are needed to effectively assist developers in understanding software. We observe that current tools for software exploration only partly support these features. This has motivated the development of SEXTANT, a software exploration tool tightly integrated into the Eclipse IDE that has been developed to fill this gap. By means of case studies, we demonstrate how the requirements fulfilled by SEXTANT are conducive to an understanding needed to perform a maintenance task","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.94","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707671","Software exploration;program comprehension;reverse engineering;software maintenance;software visualization.","Software tools;Visualization;Navigation;Software maintenance;Cognition;Computer architecture;Reverse engineering","formal specification;program visualisation;programming environments;reverse engineering;software maintenance","functional requirement;SEXTANT software exploration tool;software understanding;Eclipse IDE;software maintenance;program comprehension;reverse engineering;software visualization","","9","","60","","","","","","IEEE","IEEE Journals & Magazines"
"FlowTalk: Language Support for Long-Latency Operations in Embedded Devices","A. Bergel; W. Harrison; V. Cahill; S. Clarke","University of Chile, Santiago; Software Structure Group; Lero and Trinity College Dublin, Ireland; Lero and Trinity College Dublin, Ireland","IEEE Transactions on Software Engineering","","2011","37","4","526","543","Wireless sensor networks necessitate a programming model different from those used to develop desktop applications. Typically, resources in terms of power and memory are constrained. C is the most common programming language used to develop applications on very small embedded sensor devices. We claim that C does not provide efficient mechanisms to address the implicit asynchronous nature of sensor sampling. C applications for these devices suffer from a disruption in their control flow. In this paper, we present FlowTalk, a new object-oriented programming language aimed at making software development for wireless embedded sensor devices easier. FlowTalk is an object-oriented programming language in which dynamicity (e.g., object creation) has been traded for a reduction in memory consumption. The event model that traditionally comes from using sensors is adapted in FlowTalk with controlled disruption, a light-weight continuation mechanism. The essence of our model is to turn asynchronous long-latency operations into synchronous and blocking method calls. FlowTalk is built for TinyOS and can be used to develop applications that can fit in 4 KB of memory for a large number of wireless sensor devices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5492692","Embedded systems;object-based programming.","Sampling methods;Object oriented modeling;Wireless sensor networks;Application software;Computer languages;Object oriented programming;Java;Biosensors;Automotive engineering;Embedded software","C language;embedded systems;intelligent sensors;object-oriented languages;object-oriented programming;software engineering;wireless sensor networks","FlowTalk;language support;wireless sensor networks;programming language;C language;embedded sensor devices;sensor sampling;object-oriented programming language;memory consumption;light-weight continuation mechanism;asynchronous long-latency operations;TinyOS;memory size 4 KByte","","","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Synthesis of Mediators to Support Component Interoperability","A. Bennaceur; V. Issarny","Department of Computing, The Open University, United Kingdom; MiMove team, Inria, France","IEEE Transactions on Software Engineering","","2015","41","3","221","240","Interoperability is a major concern for the software engineering field, given the increasing need to compose components dynamically and seamlessly. This dynamic composition is often hampered by differences in the interfaces and behaviours of independently-developed components. To address these differences without changing the components, mediators that systematically enforce interoperability between functionally-compatible components by mapping their interfaces and coordinating their behaviours are required. Existing approaches to mediator synthesis assume that an interface mapping is provided which specifies the correspondence between the operations and data of the components at hand. In this paper, we present an approach based on ontology reasoning and constraint programming in order to infer mappings between components' interfaces automatically. These mappings guarantee semantic compatibility between the operations and data of the interfaces. Then, we analyse the behaviours of components in order to synthesise, if possible, a mediator that coordinates the computed mappings so as to make the components interact properly. Our approach is formally-grounded to ensure the correctness of the synthesised mediator. We demonstrate the validity of our approach by implementing the MICS (Mediator synthesis to Connect Components) tool and experimenting it with various real-world case studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2364844","ERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6936339","Interoperability;Constraint Programming;Automated Synthesis;Mediators;Protocols;Interoperability;constraint programming;automated synthesis;mediators;protocols","Ontologies;Semantics;Google;Interoperability;Cognition;Programming;Protocols","object-oriented programming;ontologies (artificial intelligence);open systems","automated mediator synthesis;component interoperability;dynamic composition;functionally compatible component;interface mapping;ontology reasoning;constraint programming;component interface;semantic compatibility;MICS;mediator synthesis to connect components","","14","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Using a concept lattice of decomposition slices for program understanding and impact analysis","P. Tonella","Centro per la Ricerca Sci. e Tecnologica, Trento, Italy","IEEE Transactions on Software Engineering","","2003","29","6","495","509","The decomposition slice graph and concept lattice are two program representations used to abstract the details of code into a higher-level view of the program. The decomposition slice graph partitions the program into computations performed on different variables and shows the dependence relation between computations, holding when a computation needs another computation as a building block. The concept lattice groups program entities which share common attributes and organizes such groupings into a hierarchy of concepts, which are related through generalizations/specializations. This paper investigates the relationship existing between these two program representations. The main result of this paper is a novel program representation, called concept lattice of decomposition slices, which is shown to be an extension of the decomposition slice graph, and is obtained by means of concept analysis, with additional nodes associated with weak interferences between computations, i.e., shared statements which are not decomposition slices. The concept lattice of decomposition slices can be used to support software maintenance by providing relevant information about the computations performed by a program and the related dependences/interferences, as well as by representing a natural data structure on which to conduct impact analysis. Preliminary results on small to medium size code support the applicability of this method at the intraprocedural level or when investigating the dependences among small groups of procedures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205178","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205178","","Lattices;Interference;Software maintenance;Data structures;Performance analysis;Information analysis;Application software","reverse engineering;program slicing;software maintenance","program representations;program understanding;impact analysis;concept lattice;decomposition slice graph;dependence relation;common attributes;generalizations;specializations;weak interferences;shared statements;software maintenance;data structure;intraprocedural level","","89","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Software process representation and analysis for framework instantiation","T. C. Oliveira; I. M. Filho; C. J. P. de Lucena; P. S. C. Alencar; D. D. Cowan","Dept. de Inf., Pontificia Univ. Catolica do Rio de Janeiro, Brazil; Dept. de Inf., Pontificia Univ. Catolica do Rio de Janeiro, Brazil; Dept. de Inf., Pontificia Univ. Catolica do Rio de Janeiro, Brazil; NA; NA","IEEE Transactions on Software Engineering","","2004","30","3","145","159","Object-oriented frameworks are currently regarded as a promising technology for reusing designs and implementations. However, developers find there is still a steep learning curve when extracting the design rationale and understanding the framework documentation during framework instantiation. Thus, instantiation is a costly process in terms of time, people, and other resources. These problems raise a number of questions including: ""How can we raise the level of abstraction in which the framework instantiation is expressed, reasoned about and implemented?"" ""How can the same high-level design abstractions that were used to develop the framework be used during framework instantiation instead of using source code as is done currently?"" ""How can we define extended design abstractions that can allow framework instantiation to be explicitly represented and validated?"" We present an approach to framework instantiation based on software processes that addresses these issues. Our main goal is to represent the framework design models in an explicit and declarative way, and support changes to this design based on explicit instantiation tasks based on software processes while maintaining system integrity, invariants, and general constraints. In this way, the framework instantiation can be performed in a valid and controlled way.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271169","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271169","","Documentation;Productivity;Object oriented modeling;Software maintenance;Software design;Programming;Design methodology;Natural languages;Unified modeling language","software reusability;software process improvement;formal specification;object-oriented methods","software process representation;software analysis;framework instantiation;object-oriented framework;system integrity;software design;formal specification;lightweight analysis;design analysis","","12","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Characterizing Communication Channel Deadlocks in Sequence Diagrams","B. Mitchell","University of Surrey, Guilford","IEEE Transactions on Software Engineering","","2008","34","3","305","320","UML sequence diagrams (SDs) are a mainstay of requirements specifications for communication protocols. Mauw and Reniers' algebraic (MRA) semantics formally specifies a behavior for these SDs that guarantees deadlock-free processes. Practitioners commonly use communication semantics that differ from MRA, which may result in deadlocks, for example, FIFO, token ring, etc. We define a process algebra that is an extension of the MRA semantics for regular SDs. Our algebra can describe several commonly used communication semantics. Regular SDs are constructed from concurrent message flows via iteration, branching, and sequential composition. Their behavior is defined in terms of a set of partial orders on the events in the SD. Such partial orders are known as causal orders. We define partial order theoretic properties of a causal order that are particular kinds of race condition. We prove that any of the common communication semantics that we list either guarantees deadlock-free SDs or can result in a deadlock if and only if a causal order of an SD contains one of these types of race condition. This describes a complete classification of deadlocks as specific types of race condition.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4509440","Requirements Analysis;Formal methods;Distributed programming;Distributed networks;Protocol verification;Requirements Analysis;Formal methods;Distributed programming;Distributed networks;Protocol verification","Communication channels;System recovery;Token networks;Telecommunication standards;Protocols;Algebra;Automotive engineering;Unified modeling language;Manufacturing industries;Vehicle dynamics","formal specification;process algebra;system recovery;Unified Modeling Language","communication channel deadlocks;sequence diagrams;UML;communication protocols;algebraic semantics;deadlock-free processes;iteration;branching;sequential composition","","12","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Heterogeneous Defect Prediction","J. Nam; W. Fu; S. Kim; T. Menzies; L. Tan","Handong Global University, Pohang, Korea; Department of Computer Science, North Carolina State University, Raleigh, NC; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science, North Carolina State University, Raleigh, NC; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Software Engineering","","2018","44","9","874","896","Many recent studies have documented the success of cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. However, most studies share the same limitations: it requires homogeneous data; i.e., different projects must describe themselves using the same metrics. This paper presents methods for heterogeneous defect prediction (HDP) that matches up different metrics in different projects. Metric matching for HDP requires a “large enough” sample of distributions in the source and target projects-which raises the question on how large is “large enough” for effective heterogeneous defect prediction. This paper shows that empirically and theoretically, “large enough” may be very small indeed. For example, using a mathematical model of defect prediction, we identify categories of data sets were as few as 50 instances are enough to build a defect prediction model. Our conclusion for this work is that, even when projects use different metric sets, it is possible to quickly transfer lessons learned about defect prediction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2720603","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7959597","Defect prediction;quality assurance;heterogeneous metrics;transfer learning","Predictive models;Software metrics;Quality assurance;Training","data handling;pattern matching","metric matching;cross-project defect prediction;defect data;homogeneous data;heterogeneous defect prediction;metric sets","","5","","103","","","","","","IEEE","IEEE Journals & Magazines"
"Tool support for testing concurrent Java components","B. Long; D. Hoffman; P. Strooper","Sch. of Inf. Technol. & Electr. Eng., Univ. of Queensland, Brisbane, Qld., Australia; NA; NA","IEEE Transactions on Software Engineering","","2003","29","6","555","566","Concurrent programs are hard to test due to the inherent nondeterminism. This paper presents a method and tool support for testing concurrent Java components. Tool support is offered through ConAn (Concurrency Analyser), a tool for generating drivers for unit testing Java classes that are used in a multithreaded context. To obtain adequate controllability over the interactions between Java threads, the generated driver contains threads that are synchronized by a clock. The driver automatically executes the calls in the test sequence in the prescribed order and compares the outputs against the expected outputs specified in the test sequence. The method and tool are illustrated in detail on an asymmetric producer-consumer monitor. Their application to testing over 20 concurrent components, a number of which are sourced from industry and were found to contain faults, is presented and discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205182","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205182","","Java;Monitoring;Automatic testing;Software testing;Concurrent computing;Synchronization;Clocks;System testing;Sequential analysis","Java;object-oriented programming;program testing;software tools;multi-threading;concurrency control","concurrent Java component testing;tool support;concurrent programs;nondeterminism;ConAn tool;Concurrency Analyser;drivers;unit testing;Java classes;multithreaded context;clock;synchronization;test sequence;asymmetric producer-consumer monitor;faults","","29","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Static analysis of XML transformations in Java","C. Kirkegaard; A. Moller; M. I. Schwartzbach","Dept. of Comput. Sci., Aarhus Univ., Denmark; Dept. of Comput. Sci., Aarhus Univ., Denmark; Dept. of Comput. Sci., Aarhus Univ., Denmark","IEEE Transactions on Software Engineering","","2004","30","3","181","192","XML documents generated dynamically by programs are typically represented as text strings or DOM trees. This is a low-level approach for several reasons: 1) traversing and modifying such structures can be tedious and error prone, 2) although schema languages, e.g., DTD, allow classes of XML documents to be defined, there are generally no automatic mechanisms for statically checking that a program transforms from one class to another as intended. We introduce XACT, a high-level approach for Java using XML templates as a first-class data type with operations for manipulating XML values based on XPath. In addition to an efficient runtime representation, the data type permits static type checking using DTD schemas as types. By specifying schemes for the input and output of a program, our analysis algorithm will statically verify that valid input data is always transformed into valid output data and that the operations are used consistently.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271173","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271173","","XML;Java;Web services;Markup languages;Tree data structures;Humans;Computer languages;Runtime;Algorithm design and analysis;Vocabulary","formal specification;formal verification;Java;program diagnostics;XML","static analysis;XML transformation;Java;XML document;static type checking;language constructs;markup language;requirement specification","","25","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Software assurance by bounded exhaustive testing","D. Coppit; Jinlin Yang; S. Khurshid; Wei Le; K. Sullivan","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","4","328","339","Bounded exhaustive testing (BET) is a verification technique in which software is automatically tested for all valid inputs up to specified size bounds. A particularly interesting case of BET arises in the context of systems that take structurally complex inputs. Early research suggests that the BET approach can reveal faults in small systems with inputs of low structural complexity, but its potential utility for larger systems with more complex input structures remains unclear. We set out to test its utility on one such system. We used Alloy and TestEra to generate inputs to test the Galileo dynamic fault tree analysis tool, for which we already had both a formal specification of the input space and a test oracle. An initial attempt to generate inputs using a straightforward translation of our specification to Alloy did not work well. The generator failed to generate inputs to meaningful bounds. We developed an approach in which we factored the specification, used TestEra to generate abstract inputs based on one factor, and passed the results through a postprocessor that reincorporated information from the second factor. Using this technique, we were able to generate test inputs to meaningful bounds, and the inputs revealed nontrivial faults in the Galileo implementation, our specification, and our oracle. Our results suggest that BET, combined with specification abstraction and factoring techniques, could become a valuable addition to our verification toolkit and that further investigation is warranted.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435353","Index Terms- Formal methods;program verification;testing and debugging.","Software testing;Automatic testing;System testing;Fault trees;Formal specifications;Debugging;Data structures;Hardware;Fault tolerant systems;Aircraft","program verification;program testing;fault trees;software tools;formal specification;automatic programming;program debugging;software fault tolerance","software assurance;bounded exhaustive testing;BET;software verification technique;Alloy;TestEra;Galileo dynamic fault tree analysis tool;formal specification;formal methods;program verification;program debugging","","23","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Using the Conceptual Cohesion of Classes for Fault Prediction in Object-Oriented Systems","A. Marcus; D. Poshyvanyk; R. Ferenc","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","287","300","High cohesion is a desirable property of software as it positively impacts understanding, reuse, and maintenance. Currently proposed measures for cohesion in Object-Oriented (OO) software reflect particular interpretations of cohesion and capture different aspects of it. Existing approaches are largely based on using the structural information from the source code, such as attribute references, in methods to measure cohesion. This paper proposes a new measure for the cohesion of classes in OO software systems based on the analysis of the unstructured information embedded in the source code, such as comments and identifiers. The measure, named the Conceptual Cohesion of Classes (C3), is inspired by the mechanisms used to measure textual coherence in cognitive psychology and computational linguistics. This paper presents the principles and the technology that stand behind the C3 measure. A large case study on three open source software systems is presented which compares the new measure with an extensive set of existing metrics and uses them to construct models that predict software faults. The case study shows that the novel measure captures different aspects of class cohesion compared to any of the existing cohesion measures. In addition, combining C3 with existing structural cohesion metrics proves to be a better predictor of faulty classes when compared to different combinations of structural cohesion metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70768","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4384505","Maintainability;Metrics/Measurement;Quality analysis and evaluation;Restructuring;reverse engineering;and reengineering;Code documentation;Document analysis;Document indexing;Maintainability;Metrics/Measurement;Quality analysis and evaluation;Restructuring;reverse engineering;and reengineering;Code documentation;Document analysis;Document indexing","Software measurement;Open source software;Software maintenance;Current measurement;Particle measurements;Software systems;Information analysis;Coherence;Psychology;Computational linguistics","object-oriented programming;software fault tolerance;software metrics","conceptual class cohesion;object-oriented software system;open source software system;software metrics;software fault prediction","","132","","78","","","","","","IEEE","IEEE Journals & Magazines"
"Revisiting Java Bytecode Compression for Embedded and Mobile Computing Environments","D. Saougkos; G. Manis; K. Blekas; A. V. Zarras","Computer Science Department, University of Ioannina, PO Box 1186, GR 45110, Greece; Computer Science Department, University of Ioannina, PO Box 1186, GR 45110, Greece; Computer Science Department, University of Ioannina, PO Box 1186, GR 45110, Greece; Computer Science Department, University of Ioannina, PO Box 1186, GR 45110, Greece","IEEE Transactions on Software Engineering","","2007","33","7","478","495","Pattern-based Java bytecode compression techniques rely on the identification of identical instruction sequences that occur more than once. Each occurrence of such a sequence is substituted by a single instruction. The sequence defines a pattern that is used for extending the standard bytecode instruction set with the instruction that substitutes the pattern occurrences in the original bytecode. Alternatively, the pattern may be stored in a dictionary that serves for the bytecode decompression. In this case, the instruction that substitutes the pattern in the original bytecode serves as an index to the dictionary. In this paper, we investigate a bytecode compression technique that considers a more general case of patterns. Specifically, we employ the use of an advanced pattern discovery technique that allows locating patterns of an arbitrary length, which may contain a variable number of wildcards in place of certain instruction opcodes or operands. We evaluate the benefits and the limitations of this technique in various scenarios that aim at compressing the reference implementation of MIDP, a standard Java environment for the development of applications for mobile devices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1021","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4227829","Java;compression (coding).","Java;Mobile computing;Embedded computing;Dictionaries;Computer aided instruction;Standards development;Page description languages;Code standards;Virtual machining","distributed programming;Java;mobile computing","embedded environments;mobile computing environments;pattern-based Java bytecode compression techniques;standard bytecode instruction;bytecode decompression;pattern discovery technique;mobile devices","","","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Metrics for Measuring the Quality of Modularization of Large-Scale Object-Oriented Software","S. Sarkar; A. C. Kak; G. M. Rama","Infosys Technologies Ltd, Bangalore; Purdue University, West Lafayette; Infosys Technologies Ltd, Bangalore","IEEE Transactions on Software Engineering","","2008","34","5","700","720","The metrics formulated to date for characterizing the modularization quality of object-oriented software have considered module and class to be synonymous concepts. But a typical class in object oriented programming exists at too low a level of granularity in large object-oriented software consisting of millions of lines of code. A typical module (sometimes referred to as a superpackage) in a large object-oriented software system will typically consist of a large number of classes. Even when the access discipline encoded in each class makes for ""clean"" class-level partitioning of the code, the intermodule dependencies created by associational, inheritance-based, and method invocations may still make it difficult to maintain and extend the software. The goal of this paper is to provide a set of metrics that characterize large object-oriented software systems with regard to such dependencies. Our metrics characterize the quality of modularization with respect to the APIs of the modules, on the one hand, and, on the other, with respect to such object-oriented inter-module dependencies as caused by inheritance, associational relationships, state access violations, fragile base-class design, etc. Using a two-pronged approach, we validate the metrics by applying them to popular open-source software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4623684","Metrics/Measurement;Maintainability;Modules and interfaces;Object-oriented programming;Coupling;Maintenance and Enhancement;Large-Scale Software;Metrics/Measurement;Maintainability;Modules and interfaces;Object-oriented programming;Coupling;Maintenance and Enhancement;Large-Scale Software","Software measurement;Large-scale systems;Software quality;Software maintenance;Software systems;Application software;Object oriented programming;Software metrics;Aging;Computer architecture","application program interfaces;inheritance;object-oriented programming;software metrics;software quality","modularization quality;large-scale object-oriented software;synonymous concepts;granularity;API;object-oriented intermodule dependencies;inheritance;open-source software systems","","36","","55","","","","","","IEEE","IEEE Journals & Magazines"
"GUI Interaction Testing: Incorporating Event Context","X. Yuan; M. B. Cohen; A. M. Memon","Google Kirkland; University of Nebraska-Lincoln, Lincoln; University of Maryland, College Park","IEEE Transactions on Software Engineering","","2011","37","4","559","574","Graphical user interfaces (GUIs), due to their event-driven nature, present an enormous and potentially unbounded way for users to interact with software. During testing, it is important to “adequately cover” this interaction space. In this paper, we develop a new family of coverage criteria for GUI testing grounded in combinatorial interaction testing. The key motivation of using combinatorial techniques is that they enable us to incorporate “context” into the criteria in terms of event combinations, sequence length, and by including all possible positions for each event. Our new criteria range in both efficiency (measured by the size of the test suite) and effectiveness (the ability of the test suites to detect faults). In a case study on eight applications, we automatically generate test cases and systematically explore the impact of context, as captured by our new criteria. Our study shows that by increasing the event combinations tested and by controlling the relative positions of events defined by the new criteria, we can detect a large number of faults that were undetectable by earlier techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5444885","GUI testing;automated testing;model-based testing;combinatorial interaction testing;GUITAR testing system.","Graphical user interfaces;System testing;Software testing;Automatic testing;Fault detection;Context modeling;Computer science;Software performance;Logic testing;User interfaces","automatic test pattern generation;graphical user interfaces;program testing","GUI interaction testing;graphical user interface;event driven nature;combinatorial interaction testing;automatic test case generation","","72","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Directed Explicit State-Space Search in the Generation of Counterexamples for Stochastic Model Checking","H. Aljazzar; S. Leue","University of Konstanz, Konstanz; University of Konstanz, Konstanz","IEEE Transactions on Software Engineering","","2010","36","1","37","60","Current stochastic model checkers do not make counterexamples for property violations readily available. In this paper, we apply directed explicit state-space search to discrete and continuous-time Markov chains in order to compute counterexamples for the violation of PCTL or CSL properties. Directed explicit state-space search algorithms explore the state space on-the-fly, which makes our method very efficient and highly scalable. They can also be guided using heuristics which usually improve the performance of the method. Counterexamples provided by our method have two important properties. First, they include those traces which contribute the greatest amount of probability to the property violation. Hence, they show the most probable offending execution scenarios of the system. Second, the obtained counterexamples tend to be small. Hence, they can be effectively analyzed by a human user. Both properties make the counterexamples obtained by our method very useful for debugging purposes. We implemented our method based on the stochastic model checker PRISM and applied it to a number of case studies in order to illustrate its applicability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5262946","Directed explicit state-space search;heuristic search;counterexamples;stochastic model checking.","Stochastic processes;Debugging;Probabilistic logic;Safety;Performance analysis;Space exploration;State-space methods;Humans;Shape;Sampling methods","formal verification;Markov processes;program debugging;tree searching","directed explicit state-space search;stochastic model checking;discrete-time Markov chains;continuous-time Markov chains;PCTL properties;CSL properties;heuristic search;debugging","","19","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Correctness verification and performance analysis of real-time systems using stochastic preemptive time Petri nets","G. Bucci; L. Sassoli; E. Vicario","Dipt. Sistemi e Informatica, Firenze Univ., Italy; Dipt. Sistemi e Informatica, Firenze Univ., Italy; Dipt. Sistemi e Informatica, Firenze Univ., Italy","IEEE Transactions on Software Engineering","","2005","31","11","913","927","Time Petri nets describe the state of a timed system through a marking and a set of clocks. If clocks take values in a dense domain, state space analysis must rely on equivalence classes. These support verification of logical sequencing and quantitative timing of events, but they are hard to be enriched with a stochastic characterization of nondeterminism necessary for performance and dependability evaluation. Casting clocks into a discrete domain overcomes the limitation, but raises a number of problems deriving from the intertwined effects of concurrency and timing. We present a discrete-time variant of time Petri nets, called stochastic preemptive time Petri nets, which provides a unified solution for the above problems through the adoption of a maximal step semantics in which the logical location evolves through the concurrent firing of transition sets. We propose an analysis technique, which integrates the enumeration of a succession relation among sets of timed states with the calculus of their probability distribution. This enables a joint approach to the evaluation of performance and dependability indexes as well as to the verification of sequencing and timeliness correctness. Expressive and analysis capabilities of the model are demonstrated with reference to a real-time digital control system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.122","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556551","Index Terms- Real-time reactive systems;preemptive scheduling;correctness verification;performance and dependability evaluation;discrete time;maximal step semantics;confusion;well definedness;stochastic preemptive Time Petri nets.","Performance analysis;Real time systems;Stochastic systems;Petri nets;Clocks;Timing;Stochastic processes;State-space methods;Casting;Concurrent computing","software performance evaluation;program verification;real-time systems;stochastic processes;probability;Petri nets;equivalence classes","state space analysis;equivalence classes;logical sequencing verification;quantitative timing;dependability evaluation;discrete-time variant;stochastic preemptive time Petri nets;maximal step semantics;probability distribution;real-time digital control system;correctness verification;performance analysis;real-time system","","24","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Estimation of Defects Based on Defect Decay Model: ED^{3}M","S. W. Haider; J. W. Cangussu; K. M. L. Cooper; R. Dantu; S. Haider","The University of Texas at Dallas, Dallas; The University of Texas at Dallas, Dallas; The University of Texas at Dallas, Dallas; University of North Texas, Denton; The University of Texas at Dallas, Dallas","IEEE Transactions on Software Engineering","","2008","34","3","336","356","An accurate prediction of the number of defects in a software product during system testing contributes not only to the management of the system testing process but also to the estimation of the product's required maintenance. Here, a new approach called ED<sup>3</sup>M is presented that computes an estimate of the total number of defects in an ongoing testing process. ED<sup>3</sup>M is based on estimation theory. Unlike many existing approaches the technique presented here does not depend on historical data from previous projects or any assumptions about the requirements and/or testers' productivity. It is a completely automated approach that relies only on the data collected during an ongoing testing process. This is a key advantage of the ED<sup>3</sup>M approach, as it makes it widely applicable in different testing environments. Here, the ED<sup>3</sup>M approach has been evaluated using five data sets from large industrial projects and two data sets from the literature. In addition, a performance analysis has been conducted using simulated data sets to explore its behavior using different models for the input data. The results are very promising; they indicate the ED<sup>3</sup>M approach provides accurate estimates with as fast or better convergence time in comparison to well-known alternative techniques, while only using defect data as the input.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4492790","Statistical methods;Testing and Debugging;Metrics/Measurement;Defect prediction;system testing;estimation theory;Statistical methods;Testing and Debugging;Metrics/Measurement;Defect prediction;system testing;estimation theory","System testing;Estimation theory;Software testing;Costs;Programming;Phase estimation;Inspection;Software maintenance;Software systems;Productivity","program testing;software metrics","defect decay model;software product;defect estimation;system testing process;estimation theory;software metrics","","6","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Conservative Reasoning about the Probability of Failure on Demand of a 1-out-of-2 Software-Based System in Which One Channel Is ""Possibly Perfect""","B. Littlewood; A. Povyakalo","City University London, London; City University London, London","IEEE Transactions on Software Engineering","","2013","39","11","1521","1530","In earlier work, [11] (henceforth LR), an analysis was presented of a 1-out-of-2 software-based system in which one channel was “possibly perfect”. It was shown that, at the aleatory level, the system pfd (probability of failure on demand) could be bounded above by the product of the pfd of channel A and the pnp (probability of nonperfection) of channel B. This result was presented as a way of avoiding the well-known difficulty that for two certainly-fallible channels, failures of the two will be dependent, i.e., the system pfd cannot be expressed simply as a product of the channel pfds. A price paid in this new approach for avoiding the issue of failure dependence is that the result is conservative. Furthermore, a complete analysis requires that account be taken of epistemic uncertainty-here concerning the numeric values of the two parameters pfd<sub>A</sub>and pnp<sub>B</sub>. Unfortunately this introduces a different difficult problem of dependence: estimating the dependence between an assessor's beliefs about the parameters. The work reported here avoids this problem by obtaining results that require only an assessor's marginal beliefs about the individual channels, i.e., they do not require knowledge of the dependence between these beliefs. The price paid is further conservatism in the results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6574864","Software reliability;fault tolerance;software perfection;probability of failure;epistemic uncertainty;software diversity;multiversion software","Phase frequency detector;Uncertainty;Cognition;Software reliability;Software;Safety","belief networks;failure analysis;probability;software reliability;uncertainty handling","conservative reasoning;probability of failure on demand;1-out-of-2 software-based system;PFD;PNP;probability of nonperfection;certainly fallible channel;assessor marginal belief;epistemic uncertainty;software perfection","","4","","15","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study of RefactoringChallenges and Benefits at Microsoft","M. Kim; T. Zimmermann; N. Nagappan","Department of Electrical and Computer Engineering, University of Texas, Austin; Microsoft Research at Redmond; Microsoft Research at Redmond","IEEE Transactions on Software Engineering","","2014","40","7","633","649","It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring benefits or investigate developers' perception towards these benefits. This paper presents a field study of refactoring benefits and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey finds that the refactoring definition in practice is not confined to a rigorous definition of semantics-preserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized effort on refactoring Windows. The quantitative analysis of Windows 7 version history finds the top 5 percent of preferentially refactored modules experience higher reduction in the number of inter-module dependencies and several complexity measures but increase size more than the bottom 95 percent. This indicates that measuring the impact of refactoring requires multi-dimensional assessment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2318734","National Science Foundation; Microsoft SEIF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6802406","Refactoring;empirical study;software evolution;component dependencies;defects;churn","Computer bugs;Software;Interviews;History;Complexity theory;Size measurement;Software metrics","data analysis;software maintenance;software metrics;software quality","refactoring challenges;refactoring benefits;Microsoft;software quality improvement;survey;semi-structured interviews;quantitative Windows 7 version history data analysis;intermodule dependencies;complexity measures;software evolution","","21","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Delta Execution for Efficient State-Space Exploration of Object-Oriented Programs","M. d'Amorim; S. Lauterburg; D. Marinov","University of Illinois at Urbana-Champaign, Urbana; University of Illinois at Urbana-Champaign, Urbana; University of Illinois at Urbana-Champaign, Urbana","IEEE Transactions on Software Engineering","","2008","34","5","597","613","We present Delta execution, a technique that speeds up state-space exploration of object-oriented programs. State-space exploration is the essence of model checking and an increasingly popular approach for automating test generation. A key issue in exploration of object-oriented programs is handling the program state, in particular the heap. We exploit the fact that many execution paths in state-space exploration partially overlap. Delta execution simultaneously operates on several states/heaps and shares the common parts across the executions, separately executing only the ""deltas"" where the executions differ. We implemented Delta execution in two model checkers: JPF, a popular general-purpose model checker for Java programs, and BOX, a specialized model checker that we developed for efficient exploration of sequential Java programs. The results for bounded-exhaustive exploration of ten basic subject programs and one larger case study show that Delta execution reduces exploration time from 1.06x to 126.80x (with median 5.60x) in JPF and from 0.58x to 4.16x (with median 2.23x) in BOX. The results for a non-exhaustive exploration in JPF show that Delta execution reduces exploration time from 0.92x to 6.28x (with median 4.52x).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4528965","Software/Program Verification;Model checking;Testing and Debugging;Software/Program Verification;Model checking;Testing and Debugging","Object oriented modeling;Boolean functions;Sliding mode control;Data structures;Automatic testing;Java;Software testing;Software reliability;Shape;Debugging","object-oriented programming;program debugging;program testing;program verification","Delta execution;state-space exploration;object-oriented programs;model checking;automating test generation;Java programs;sequential Java programs","","5","","51","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Study of Failure Proximity","C. Liu; X. Zhang; J. Han","Microsoft Corporation, Redmond; Purdue University, West Lafayette; University of Illinois at Urbana-Champaign, Urbana","IEEE Transactions on Software Engineering","","2008","34","6","826","843","Software end-users are the best testers, who keep revealing bugs in software that has undergone rigorous in-house testing. In order to leverage their testing efforts, failure reporting components have been widely deployed in released software. Many utilities of the collected failure data depend on an effective failure indexing technique, which, at the optimal case, would index all failures due to the same bug together. Unfortunately, the problem of failure proximity, which underpins the effectiveness of an indexing technique, has not been systematically studied. This article presents the first systematic study of failure proximity. A failure proximity consists of two components: a fingerprinting function that extracts signatures from failures, and a distance function that calculates the likelihood of two failures being due to the same bug. By considering different instantiations of the two functions, we study an array of six failure proximities (two of them are new) in this article. These proximities range from the simplest approach that checks failure points to the most sophisticated approach that utilizes fault localization algorithms to extract failure signatures. Besides presenting technical details of each proximity, we also study the properties of each proximity and tradeoffs between proximities. These altogether deliver a systematic view of failure proximity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4589219","Debugging aids;Dumps;Debugging aids;Dumps","Indexing;Failure analysis;Software maintenance;Software testing;Computer bugs;Software quality;Feedback;Debugging;Chaos;System testing","program debugging;program testing;software fault tolerance;software maintenance","failure proximity;software end-users;in-house testing;failure indexing technique;signature extraction;debugging aids;software maintenance","","16","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Reviewing software diagrams: a cognitive study","B. C. Hungerford; A. R. Hevner; R. W. Collins","Dept. of Manage. Inf. Syst., Wisconsin Univ., Oshkosh, WI, USA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","2","82","96","Reviews and inspections of software artifacts throughout the development life cycle are effective techniques for identifying defects and improving software quality. While review methods for text-based artifacts (e.g., code) are well understood, very little guidance is available for performing reviews of software diagrams, which are rapidly becoming the dominant form of software specification and design. Drawing upon human cognitive theory, we study how 12 experienced software developers perform individual reviews on a software design containing two types of diagrams: entity-relationship diagrams and data flow diagrams. Verbal protocol methods are employed to describe and analyze defect search patterns among the software artifacts, both text and diagrams, within the design. Results indicate that search patterns that rapidly switch between the two design diagrams are the most effective. These findings support the cognitive theory thesis that how an individual processes information impacts processing success. We conclude with specific recommendations for improving the practice of reviewing software diagrams.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265814","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265814","","Software quality;Software performance;Software design;Humans;Programming;Inspection;Protocols;Switches;Guidelines;Unified modeling language","software reviews;software quality;entity-relationship modelling;data flow analysis;specification languages;object-oriented programming","software review;software diagram;software quality;human cognitive theory;entity relationship diagram;data flow diagram;verbal protocol method","","39","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Crosscutting Concerns through Random Walks","C. Zhang; H. Jacobsen","The Hong Kong University of Science and Technology, Hong Kong; University of Toronto, Toronto","IEEE Transactions on Software Engineering","","2012","38","5","1123","1137","Inspired by our past manual aspect mining experiences, this paper describes a probabilistic random walk model to approximate the process of discovering crosscutting concerns (CCs) in the absence of the domain knowledge about the investigated application. The random walks are performed on the concept graphs extracted from the program sources to calculate metrics of “utilization” and “aggregation” for each of the program elements. We rank all the program elements based on these metrics and use a threshold to produce a set of candidates that represent crosscutting concerns. We implemented the algorithm as the Prism CC miner (PCM) and evaluated PCM on Java applications ranging from a small-scale drawing application to a medium-sized middleware application and to a large-scale enterprise application server. Our quantification shows that PCM is able to produce comparable results (95 percent accuracy for the top 125 candidates) with respect to the manual mining effort. PCM is also significantly more effective as compared to the conventional approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.83","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5989837","Aspect mining;mining crosscutting concerns","Phase change materials;Radiation detectors;Data mining;Manuals;Mathematical model;Computational modeling;Algorithm design and analysis","aspect-oriented programming;data mining;graph theory;Java;middleware;probability;small-to-medium enterprises","crosscutting concerns mining;probabilistic random walk model;concept graphs;utilization metric;aggregation metric;program sources;program elements;Prism CC miner;Java applications;small-scale drawing application;medium-sized middleware application;large-scale enterprise application server;aspect mining","","5","","37","","","","","","IEEE","IEEE Journals & Magazines"
"A Framework for Programming Robust Context-Aware Applications","D. Kulkarni; A. Tripathi","University of Minnesota, Minneapolis; University of Minnesota, Minneapolis","IEEE Transactions on Software Engineering","","2010","36","2","184","197","In this paper, we present a forward recovery model for programming robust context-aware applications. The mechanisms devised as part of this model fall into two categories: asynchronous event handling and synchronous exception handling. These mechanisms enable designing recovery actions to handle different kinds of failure conditions arising in context-aware applications. These include service discovery failures, service binding failures, exceptions raised by a service, and context invalidations. This model is integrated in the high-level programming framework that we have designed for building context-aware collaborative (CSCW) applications. In this paper, we demonstrate the capabilities of this model for programming various kinds of recovery patterns in context-aware applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5396345","Exception handling;context-aware applications;robustness;fault tolerance;design methodology.","Robustness;Context-aware services;Context modeling;Access control;Application software;Collaboration;Information systems;Buildings;Fault tolerance;Design methodology","exception handling;groupware;object-oriented programming;software fault tolerance;system recovery;ubiquitous computing","robust context aware application programming;forward recovery model;asynchronous event handling;synchronous exception handling;recovery patterns;service discovery failures;service binding failures;context invalidations;high level programming framework;context aware collaborative applications;CSCW","","33","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing Critical Decision-Based Processes","C. Damas; B. Lambeau; A. van Lamsweerde","Department of Computing, Icteam Institute, Université Catholique de Louvain, Louvain-La-Neuve, Belgium; Department of Computing, Icteam Institute, Université Catholique de Louvain, Louvain-La-Neuve, Belgium; Department of Computing, Icteam Institute, Université Catholique de Louvain, Louvain-La-Neuve, Belgium","IEEE Transactions on Software Engineering","","2014","40","4","338","365","Decision-based processes are composed of tasks whose application may depend on explicit decisions relying on the state of the process environment. In specific domains such as healthcare, decision-based processes are often complex and critical in terms of timing and resources. The paper presents a variety of tool-supported techniques for analyzing models of such processes. The analyses allow a variety of errors to be detected early and incrementally on partial models, notably: inadequate decisions resulting from inaccurate or outdated information about the environment state; incomplete decisions; non-deterministic task selections; unreachable tasks along process paths; and violations of non-functional process requirements involving time, resources or costs. The proposed techniques are based on different instantiations of the same generic algorithm that propagates decorations iteratively through the process model. This algorithm in particular allows event-based models to be automatically decorated with state-based invariants. A formal language supporting both event-based and state-based specifications is introduced as a process modeling language to enable such analyses. This language mimics the informal flowcharts commonly used by process stakeholders. It extends High-Level Message Sequence Charts with guards on task-related and environment-related variables. The language provides constructs for specifying task compositions, task refinements, decision trees, multi-agent communication scenarios, and time and resource constraints. The proposed techniques are demonstrated on the incremental building and analysis of a complex model of a real protocol for cancer therapy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2312954","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6776491","Process modeling;process analysis;model verification;decision errors;safety-critical workflows;non-functional requirements;domain-specific languages;formal specification","Analytical models;Unified modeling language;Algorithm design and analysis;Semantics;Blood;Flowcharts;Medical treatment","cancer;decision trees;formal languages;formal specification;multi-agent systems;patient treatment","critical decision-based process analysis;tool-supported techniques;partial models;environment state;incomplete decisions;nondeterministic task selections;unreachable tasks;process paths;nonfunctional process requirement violation;event-based models;state-based invariants;formal language;event-based specification;state-based specification;process modeling language;informal flowcharts;high-level message sequence charts;task-related variables;environment-related variables;task compositions;task refinements;decision trees;multiagent communication scenarios;time constraints;resource constraints;cancer therapy","","2","","80","","","","","","IEEE","IEEE Journals & Magazines"
"What Makes a Good Bug Report?","T. Zimmermann; R. Premraj; N. Bettenburg; S. Just; A. Schroter; C. Weiss","Microsoft Research, Redmond; Vrije Universiteit Amsterdam, Amsterdam; Queen's University, Kingston; Saarland University, Saarbruecken; University of Victoria, Victoria; University of Zurich, Zürich","IEEE Transactions on Software Engineering","","2010","36","5","618","643","In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.63","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5487527","Testing and debugging;distribution;maintenance;and enhancement;human factors;management;measurement.","Computer bugs;Programming;Prototypes;Software engineering;Information analysis;Software testing;Debugging;Software maintenance;Human factors;Engineering management","program debugging;program testing;software quality","software development;APACHE;ECLIPSE;MOZILLA;CUEZILLA prototype;bug tracking tools","","82","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Software Architecture Visualization: An Evaluation Framework and Its Application","K. Gallagher; A. Hatch; M. Munro","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","260","270","In order to characterize and improve software architecture visualization practice, the paper derives and constructs a qualitative framework, with seven key areas and 31 features, for the assessment of software architecture visualization tools. The framework is derived by the application of the Goal Question Metric paradigm to information obtained from a literature survey and addresses a number of stakeholder issues. The evaluation is performed from multiple stakeholder perspectives and in various architectural contexts. Stakeholders can apply the framework to determine if a particular software architecture visualization tool is appropriate to a given task. The framework is applied in the evaluation of a collection of six software architecture visualization tools. The framework may also be used as a design template for a comprehensive software architecture visualization tool.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70757","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4378398","Software Architectures;Visualization techniques and methodologies;Software Architectures;Visualization techniques and methodologies","Software architecture;Visualization;Application software;Computer architecture;Guidelines;Software systems;Computer Society;Performance evaluation;Navigation;Computer science","software architecture","software architecture visualization tools;multiple stakeholder perspectives;goal question metric paradigm application","","10","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Architecture-based performance analysis applied to a telecommunication system","D. Petriu; C. Shousha; A. Jalnapurkar","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2000","26","11","1049","1065","Software architecture plays an important role in determining software quality characteristics, such as maintainability, reliability, reusability, and performance. Performance effects of architectural decisions can be evaluated at an early stage by constructing and analyzing quantitative performance models, which capture the interactions between the main components of the system as well as the performance attributes of the components themselves. The paper proposes a systematic approach to building layered queueing network (LQN) performance models from a UML description of the high-level architecture of a system and more exactly from the architectural patterns used for the system. The performance model structure retains a clear relationship with the system architecture, which simplifies the task of converting performance analysis results into conclusions and recommendations related to the software architecture. The proposed approach is applied to a telecommunication product for which an LQN model is built and analyzed. The analysis shows how the performance bottleneck is moving from component to component (hardware or software) under different loads and configurations and exposes some weaknesses in the original software architecture, which prevent the system from using the available processing power at full capacity due to excessive serialization.","0098-5589;1939-3520;2326-3881","","10.1109/32.881717","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=881717","","Performance analysis;Software architecture;Computer architecture;Software quality;Maintenance;Buildings;Unified modeling language;Power system modeling;Hardware;Software performance","software performance evaluation;software architecture;software quality;queueing theory;telecommunication computing;specification languages;object-oriented programming","architecture based performance analysis;telecommunication system;software architecture;software quality characteristics;maintainability;reliability;reusability;performance effects;architectural decisions;quantitative performance models;performance attributes;systematic approach;layered queueing network;LQN performance models;UML description;high-level architecture;architectural patterns;performance model structure;system architecture;performance analysis results;telecommunication product;LQN model;performance bottleneck;processing power;serialization","","34","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Evaluation of the Impact of Object-Oriented Code Refactoring on Quality Attributes: A Systematic Literature Review","J. Al Dallal; A. Abdin","Department of Information Science, Kuwait University, Safat, Kuwait; Department of Information Science, Kuwait University, Safat, Kuwait","IEEE Transactions on Software Engineering","","2018","44","1","44","69","Software refactoring is a maintenance task that addresses code restructuring to improve its quality. Many studies have addressed the impact of different refactoring scenarios on software quality. This study presents a systematic literature review that aggregates, summarizes, and discusses the results of 76 relevant primary studies (PSs) concerning the impact of refactoring on several internal and external quality attributes. The included PSs were selected using inclusion and exclusion criteria applied to relevant articles published before the end of 2015. We analyzed the PSs based on a set of classification criteria, including software quality attributes and measures, refactoring scenarios, evaluation approaches, datasets, and impact results. We followed the vote-counting approach to determine the level of consistency among the PS reported results concerning the relationship between refactoring and software quality. The results indicated that different refactoring scenarios sometimes have opposite impacts on different quality attributes. Therefore, it is false that refactoring always improves all software quality aspects. The vote-counting study provided a clear view of the impacts of some individual refactoring scenarios on some internal quality attributes such as cohesion, coupling, complexity, inheritance, and size, but failed to identify their impacts on external and other internal quality attributes due to insufficient findings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2658573","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7833023","quality attribute;quality measure;refactoring scenario;systematic literature review","Software quality;Systematics;Unified modeling language;Bibliographies;Libraries;Object oriented modeling","software maintenance;software metrics;software quality","76 relevant primary studies;PSs;internal quality;external quality;classification criteria;software quality attributes;evaluation approaches;vote-counting approach;different refactoring scenarios;opposite impacts;different quality attributes;software quality aspects;vote-counting study;individual refactoring scenarios;systematic literature review;software refactoring;maintenance task;code restructuring","","2","","99","","","","","","IEEE","IEEE Journals & Magazines"
"Software Architecture Optimization Methods: A Systematic Literature Review","A. Aleti; B. Buhnova; L. Grunske; A. Koziolek; I. Meedeniya","Monash University, Australia; Masaryk University, Brno; University of Kaiserslautern, Kaiserslautern; University of Zurich, Zurich; Swinburne University of Technology, Hawthorn","IEEE Transactions on Software Engineering","","2013","39","5","658","683","Due to significant industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture design has become an important development activity and the research domain is rapidly evolving. In the last decades, software architecture optimization methods, which aim to automate the search for an optimal architecture design with respect to a (set of) quality attribute(s), have proliferated. However, the reported results are fragmented over different research communities, multiple system domains, and multiple quality attributes. To integrate the existing research results, we have performed a systematic literature review and analyzed the results of 188 research papers from the different research communities. Based on this survey, a taxonomy has been created which is used to classify the existing research. Furthermore, the systematic analysis of the research literature provided in this review aims to help the research community in consolidating the existing research efforts and deriving a research agenda for future developments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311410","Software architecture optimization;systematic literature review;optimization methods;problem overview","Taxonomy;Computer architecture;Software;Software architecture;Systematics;Optimization methods","software architecture;software quality","software architecture optimization method;software system;software architecture design;software quality attribute","","102","","245","","","","","","IEEE","IEEE Journals & Magazines"
"CHARMY: A Framework for Designing and Verifying Architectural Specifications","P. Pelliccione; P. Inverardi; H. Muccini","Universit&#x0E0; dell' Aquila, L'Aquila; Universit&#x0E0; dell' Aquila, L'Aquila; Universit&#x0E0; dell' Aquila, L'Aquila","IEEE Transactions on Software Engineering","","2009","35","3","325","346","Introduced in the early stages of software development, the Charmy framework assists the software architect in making and evaluating architectural choices. Rarely, the software architecture of a system can be established once and forever. Most likely poorly defined and understood architectural constraints and requirements force the software architect to accept ambiguities and move forward to the construction of a suboptimal software architecture. Charmy aims to provide an easy and practical tool for supporting the iterative modeling and evaluation of software architectures. From an UML-based architectural design, an executable prototype is automatically created. Charmy simulation and model checking features help in understanding the functioning of the system and discovering potential inconsistencies of the design. When a satisfactory and stable software architecture is reached, Java code conforming to structural software architecture constraints is automatically generated through suitable transformations. The overall approach is tool supported.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.104","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4711062","Software architectures;model checking.;Software Architectures;Model checking;Design notations and documentation;State diagrams;Rapid prototyping","Software architecture;Computer architecture;Software prototyping;Software systems;Prototypes;Connectors;Unified modeling language;Programming;Java;Topology","Java;program verification;software architecture;Unified Modeling Language","software development;Charmy framework;UML-based architectural design;model checking;Java code;structural software architecture constraints","","32","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Aspectizing Java Access Control","R. Toledo; A. Nunez; E. Tanter; J. Noye","University of Chile, Santiago; &#x0C9;cole des Mines de Nantes-INRIA, LINA, Nantes; University of Chile, Santiago; &#x0C9;cole des Mines de Nantes-INRIA, LINA, Nantes","IEEE Transactions on Software Engineering","","2012","38","1","101","117","It is inevitable that some concerns crosscut a sizeable application, resulting in code scattering and tangling. This issue is particularly severe for security-related concerns: It is difficult to be confident about the security of an application when the implementation of its security-related concerns is scattered all over the code and tangled with other concerns, making global reasoning about security precarious. In this study, we consider the case of access control in Java, which turns out to be a crosscutting concern with a nonmodular implementation based on runtime stack inspection. We describe the process of modularizing access control in Java by means of Aspect-Oriented Programming (AOP). We first show a solution based on AspectJ, the most popular aspect-oriented extension to Java, that must rely on a separate automata infrastructure. We then put forward a novel solution via dynamic deployment of aspects and scoping strategies. Both solutions, apart from providing a modular specification of access control, make it possible to easily express other useful policies such as the Chinese wall policy. However, relying on expressive scope control results in a compact implementation, which, at the same time, permits the straightforward expression of even more interesting policies. These new modular implementations allowed by AOP alleviate maintenance and evolution issues produced by the crosscutting nature of access control.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680915","Programming languages;security;aspect-oriented programming;access control.","Aspect-oriented programming;Access control;Computer architecture;Java;Programming;Computer security","aspect-oriented programming;authorisation;automata theory;Java","Java access control aspectization;code scattering;code tangling;security-related concerns;runtime stack inspection;aspect-oriented programming;AspectJ;automata infrastructure;aspects strategies;scoping strategies;Chinese wall policy","","5","","45","","","","","","IEEE","IEEE Journals & Magazines"
"A Component Model for Model Transformations","J. S. Cuadrado; E. Guerra; J. de Lara","Department of Computer Science, Universidad Autónoma de Madrid, Spain; Department of Computer Science, Universidad Autónoma de Madrid, Spain; Department of Computer Science, Universidad Autónoma de Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","11","1042","1060","Model-driven engineering promotes an active use of models to conduct the software development process. In this way, models are used to specify, simulate, verify, test and generate code for the final systems. Model transformations are key enablers for this approach, being used to manipulate instance models of a certain modelling language. However, while other development paradigms make available techniques to increase productivity through reutilization, there are few proposals for the reuse of model transformations across different modelling languages. As a result, transformations have to be developed from scratch even if other similar ones exist. In this paper, we propose a technique for the flexible reutilization of model transformations. Our proposal is based on generic programming for the definition and instantiation of transformation templates, and on component-based development for the encapsulation and composition of transformations. We have designed a component model for model transformations, supported by an implementation currently targeting the Atlas Transformation Language (ATL). To evaluate its reusability potential, we report on a generic transformation component to analyse workflow models through their transformation into Petri nets, which we have reused for eight workflow languages, including UML Activity Diagrams, YAWL and two versions of BPMN.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339852","Spanish Ministry of Economy and Competitivity with project Go-Lite; EU commission with project MONDO; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6858077","Model-driven engineering;model transformation;reusability;genericity;component-based development","Unified modeling language;Adaptation models;Petri nets;Analytical models;Logic gates;Software;Proposals","object-oriented programming;Petri nets;software reusability","BPMN;YAWL;UML activity diagrams;workflow languages;Petri nets;workflow models;generic transformation component;ATL;Atlas transformation language;component-based development;generic programming;modelling language instance models;software development process;model-driven engineering;model transformations;component model","","14","","69","","","","","","IEEE","IEEE Journals & Magazines"
"A classification and comparison framework for software architecture description languages","N. Medvidovic; R. N. Taylor","Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","1","70","93","Software architectures shift the focus of developers from lines-of-code to coarser-grained architectural elements and their overall interconnection structure. Architecture description languages (ADLs) have been proposed as modeling notations to support architecture-based development. There is, however, little consensus in the research community on what is an ADL, what aspects of an architecture should be modeled in an ADL, and which of several possible ADLs is best suited for a particular problem. Furthermore, the distinction is rarely made between ADLs on one hand and formal specification, module interconnection, simulation and programming languages on the other. This paper attempts to provide an answer to these questions. It motivates and presents a definition and a classification framework for ADLs. The utility of the definition is demonstrated by using it to differentiate ADLs from other modeling notations. The framework is used to classify and compare several existing ADLs, enabling us, in the process, to identify key properties of ADLs. The comparison highlights areas where existing ADLs provide extensive support and those in which they are deficient, suggesting a research agenda for the future.","0098-5589;1939-3520;2326-3881","","10.1109/32.825767","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=825767","","Software architecture;Computer architecture;LAN interconnection;Architecture description languages;Application software;Formal specifications;Computer languages;Connectors;Computer science;Computer Society","software architecture;formal specification;specification languages","software architecture description languages;interconnection structure;modeling notations;architecture-based development;formal specification;module interconnection;simulation;programming languages;classification framework","","801","","74","","","","","","IEEE","IEEE Journals & Magazines"
"An Eye-Tracking Study of Java Programmers and Application to Source Code Summarization","P. Rodeghero; C. Liu; P. W. McBurney; C. McMillan","Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN","IEEE Transactions on Software Engineering","","2015","41","11","1038","1054","Source Code Summarization is an emerging technology for automatically generating brief descriptions of code. Current summarization techniques work by selecting a subset of the statements and keywords from the code, and then including information from those statements and keywords in the summary. The quality of the summary depends heavily on the process of selecting the subset: a high-quality selection would contain the same statements and keywords that a programmer would choose. Unfortunately, little evidence exists about the statements and keywords that programmers view as important when they summarize source code. In this paper, we present an eye-tracking study of 10 professional Java programmers in which the programmers read Java methods and wrote English summaries of those methods. We apply the findings to build a novel summarization tool. Then, we evaluate this tool. Finally, we further analyze the programmers' method summaries to explore specific keyword usage and provide evidence to support the development of source code summarization systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2442238","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7118751","Source code summaries;program comprehension;Source code summaries;program comprehension","Java;Software;Documentation;Navigation;XML;Software engineering","Java;program compilers;source code (software)","eye-tracking study;Java programmer;source code summarization;code generation","","7","","75","","","","","","IEEE","IEEE Journals & Magazines"
"EARMO: An Energy-Aware Refactoring Approach for Mobile Apps","R. Morales; R. Saborido; F. Khomh; F. Chicano; G. Antoniol","Polytechynique Montéal, Montreal, QC, Canada; Polytechynique Montéal, Montreal, QC, Canada; Polytechynique Montéal, Montreal, QC, Canada; University of Málaga, Málaga, Spain; Polytechynique Montéal, Montreal, QC, Canada","IEEE Transactions on Software Engineering","","2018","44","12","1176","1206","The energy consumption of mobile apps is a trending topic and researchers are actively investigating the role of coding practices on energy consumption. Recent studies suggest that design choices can conflict with energy consumption. Therefore, it is important to take into account energy consumption when evolving the design of a mobile app. In this paper, we analyze the impact of eight type of anti-patterns on a testbed of 20 android apps extracted from F-Droid. We propose EARMO, a novel anti-pattern correction approach that accounts for energy consumption when refactoring mobile anti-patterns. We evaluate EARMO using three multiobjective search-based algorithms. The obtained results show that EARMO can generate refactoring recommendations in less than a minute, and remove a median of 84 percent of anti-patterns. Moreover, EARMO extended the battery life of a mobile phone by up to 29 minutes when running in isolation a refactored multimedia app with default settings (no Wi-Fi, no location services, and minimum screen brightness). Finally, we conducted a qualitative study with developers of our studied apps, to assess the refactoring recommendations made by EARMO. Developers found 68 percent of refactorings suggested by EARMO to be very relevant.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2757486","Natural Sciences and Engineering Research Council of Canada (NSERC); Consejo Nacional de Ciencia y Tecnología, México; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8052533","Software maintenance;refactoring;anti-patterns;mobile apps;energy consumption;search-based software engineering","Mobile communication;Energy consumption;Software;Androids;Humanoid robots;Energy measurement;Software maintenance","Android (operating system);mobile computing;power aware computing;search problems;smart phones;software maintenance","EARMO;energy-aware refactoring approach;refactoring recommendations;mobile phone;energy consumption;mobile apps;Android apps;antipattern correction approach;mobile antipatterns;F-Droid;multiobjective search-based algorithms","","3","","96","","","","","","IEEE","IEEE Journals & Magazines"
"Dependence Guided Symbolic Execution","H. Wang; T. Liu; X. Guan; C. Shen; Q. Zheng; Z. Yang","Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Computer Science, Western Michigan University, Kalamazoo, MI","IEEE Transactions on Software Engineering","","2017","43","3","252","271","Symbolic execution is a powerful technique for systematically exploring the paths of a program and generating the corresponding test inputs. However, its practical usage is often limited by the<italic>path explosion</italic>problem, that is, the number of explored paths usually grows exponentially with the increase of program size. In this paper, we argue that for the purpose of fault detection it is not necessary to systematically explore the paths, and propose a new symbolic execution approach to mitigate the path explosion problem by predicting and eliminating the redundant paths based on symbolic value. Our approach can achieve the equivalent fault detection capability as traditional symbolic execution without exhaustive path exploration. In addition, we develop a practical implementation called Dependence Guided Symbolic Execution (DGSE) to soundly approximate our approach. Through exploiting program dependence, DGSE can predict and eliminate the redundant paths at a reasonable computational cost. Our empirical study shows that the redundant paths are abundant and widespread in a program. Compared with traditional symbolic execution, DGSE only explores 6.96 to 96.57 percent of the paths and achieves a speedup of 1.02<inline-formula><tex-math notation=""LaTeX"">$\times$</tex-math><alternatives><inline-graphic xlink:href=""liu-ieq1-2584063.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>to 49.56<inline-formula><tex-math notation=""LaTeX"">$\times$</tex-math><alternatives><inline-graphic xlink:href=""liu-ieq2-2584063.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>. We have released our tool and the benchmarks used to evaluate DGSE<inline-formula><tex-math notation=""LaTeX"">$^\ast$</tex-math><alternatives><inline-graphic xlink:href=""liu-ieq3-2584063.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2584063","National Natural Science Foundation of China; Fok Ying-Tong Education Foundation; National Research Program of China; Ministry of Education Innovation Research Team; Fundamental Research Funds for the Central Universities; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7497518","Symbolic execution;path coverage;program dependence","Fault detection;Explosions;Benchmark testing;Electronic mail;Computational efficiency;Input variables","","","","9","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Engineering of Framework-Specific Modeling Languages","M. Antkiewicz; K. Czarnecki; M. Stephan","University of Waterloo, Waterloo; University of Waterloo, Waterloo; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2009","35","6","795","824","Framework-specific modeling languages (FSMLs) help developers build applications based on object-oriented frameworks. FSMLs model abstractions and rules of application programming interfaces (APIs) exposed by frameworks and can express models of how applications use APIs. Such models aid developers in understanding, creating, and evolving application code. We present four exemplar FSMLs and a method for engineering new FSMLs. The method was created postmortem by generalizing the experience of building the exemplars and by specializing existing approaches to domain analysis, software development, and quality evaluation of models and languages. The method is driven by the use cases that the FSML under development should support and the evaluation of the constructed FSML is guided by two existing quality frameworks. The method description provides concrete examples for the engineering steps, outcomes, and challenges. It also provides strategies for making engineering decisions. Our work offers a concrete example of software language engineering and its benefits. FSMLs capture existing domain knowledge in language form and support application code understanding through reverse engineering, application code creation through forward engineering, and application code evolution through round-trip engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4907004","Framework-specific modeling language;domain-specific language;object-oriented framework;application programming interface (API);feature model;framework-specific model;forward engineering;reverse engineering;round-trip engineering;evolution;code pattern;mapping.","Object oriented modeling;Application software;Documentation;Writing;Concrete;Reverse engineering;Knowledge engineering;Java;Scattering;Buildings","application program interfaces;object-oriented programming;software engineering","framework-specific modeling languages;object-oriented frameworks;application programming interfaces;software development;software language engineering;reverse engineering;application code creation through forward engineering;application code evolution through round-trip engineering","","24","","102","","","","","","IEEE","IEEE Journals & Magazines"
"CoMoM: Efficient Class-Oriented Evaluation of Multiclass Performance Models","G. Casale","College of William and Mary, Williamsburg","IEEE Transactions on Software Engineering","","2009","35","2","162","177","We introduce the class-oriented method of moments (CoMoM), a new exact algorithm to compute performance indexes in closed multiclass queuing networks. Closed models are important for performance evaluation of multitier applications, but when the number of service classes is large, they become too expensive to solve with exact methods such as mean value analysis (MVA). CoMoM addresses this limitation by a new recursion that scales efficiently with the number of classes. Compared to the MVA algorithm, which recursively computes mean queue lengths, CoMoM also carries on in the recursion information on higher-order moments of queue lengths. We show that this additional information greatly reduces the number of operations needed to solve the model and makes CoMoM the best-available algorithm for networks with several classes. We conclude the paper by generalizing CoMoM to the efficient computation of marginal queue-length probabilities, which finds application in the evaluation of state-dependent attributes such as quality-of-service metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4641939","Performance of Systems;Modeling techniques;Queuing theory;Performance of Systems;Modeling techniques;Queuing theory","Performance analysis;Moment methods;Quality of service;Queueing analysis;Network servers;Computer networks;Capacity planning;Web server;Transaction databases;Algorithm design and analysis","method of moments;probability;quality of service;queueing theory","class-oriented method of moment;closed multiclass queueing network;marginal queue-length probability;performance evaluation;multitier application;mean value analysis;higher-order moment;J2EE application;state-dependent index evaluation;energy consumption;quality-of-service metrics","","4","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Equality to Equals and Unequals: A Revisit of the Equivalence and Nonequivalence Criteria in Class-Level Testing of Object-Oriented Software","H. Y. Chen; T. H. Tse","Jinan University, Guangzhou; The University of Hong Kong, Hong Kong","IEEE Transactions on Software Engineering","","2013","39","11","1549","1563","Algebraic specifications have been used in the testing of object-oriented programs and received much attention since the 1990s. It is generally believed that class-level testing based on algebraic specifications involves two independent aspects: the testing of equivalent and nonequivalent ground terms. Researchers have cited intuitive examples to illustrate the philosophy that even if an implementation satisfies all the requirements specified by the equivalence of ground terms, it may still fail to satisfy some of the requirements specified by the nonequivalence of ground terms. Thus, both the testing of equivalent ground terms and the testing of nonequivalent ground terms have been considered as significant and cannot replace each other. In this paper, we present an innovative finding that, given any canonical specification of a class with proper imports, a complete implementation satisfies all the observationally equivalent ground terms if and only if it satisfies all the observationally nonequivalent ground terms. As a result, these two aspects of software testing cover each other and can therefore replace each other. These findings provide a deeper understanding of software testing based on algebraic specifications, rendering the theory more elegant and complete. We also highlight a couple of important practical implications of our theoretical results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6570471","Software testing;equivalence criterion;nonequivalence criterion;algebraic specification;object-oriented software","Software;Software testing;Observers;Context;Semantics;Computer science","algebraic specification;object-oriented programming;program testing","nonequivalence criteria;class-level testing;object-oriented software;algebraic specifications;object-oriented programs;equivalent ground terms;canonical specification;software testing","","5","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Testing from Partial Finite State Machines without Harmonised Traces","R. M. Hierons","Department of Computer Science, Brunel University London, Uxbridge, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","11","1033","1043","This paper concerns the problem of testing from a partial, possibly non-deterministic, finite state machine (FSM) S. Two notions of correctness (quasi-reduction and quasi-equivalence) have previously been defined for partial FSMs but these, and the corresponding test generation techniques, only apply to FSMs that have harmonised traces. We show how quasi-reduction and quasi-equivalence can be generalised to all partial FSMs. We also consider the problem of generating an m-complete test suite from a partial FSM S: a test suite that is guaranteed to determine correctness as long as the system under test has no more than m states. We prove that we can complete S to form a completely-specified non-deterministic FSM S' such that any m-complete test suite generated from S' can be converted into an m-complete test suite for S. We also show that there is a correspondence between test suites that are reduced for S and S' and also that are minimal for S and S'.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2652457","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7815407","Software engineering/software/program verification;software engineering/testing and debugging;systems and software;checking experiment;partial finite state machine","Testing;Fault detection;Redundancy;Automata;Indexes;Software;Debugging","finite state machines;formal specification;program testing","finite state machine;test generation techniques;nondeterministic FSM;partial finite state machines;m-complete test suite;harmonised traces;partial FSMs","","","","30","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Extracting Development Tasks to Navigate Software Documentation","C. Treude; M. P. Robillard; B. Dagenais","Departamento de Informática e Matemática Aplicada, Universidade Federal do Rio Grande do Norte, Natal, RN, Brazil; School of Computer Science, McGill University, Montréal, QC, Canada; Resulto, Montréal, QC, Canada","IEEE Transactions on Software Engineering","","2015","41","6","565","581","Knowledge management plays a central role in many software development organizations. While much of the important technical knowledge can be captured in documentation, there often exists a gap between the information needs of software developers and the documentation structure. To help developers navigate documentation, we developed a technique for automatically extracting tasks from software documentation by conceptualizing tasks as specific programming actions that have been described in the documentation. More than 70 percent of the tasks we extracted from the documentation of two projects were judged meaningful by at least one of two developers. We present TaskNavigator, a user interface for search queries that suggests tasks extracted with our technique in an auto-complete list along with concepts, code elements, and section headers. We conducted a field study in which six professional developers used TaskNavigator for two weeks as part of their ongoing work. We found search results identified through extracted tasks to be more helpful to developers than those found through concepts, code elements, and section headers. The results indicate that task descriptions can be effectively extracted from software documentation, and that they help bridge the gap between documentation structure and the information needs of software developers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2387172","NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7000568","Software Documentation;Development Tasks;Navigation;Auto-Complete;Natural Language Processing;Software documentation;development tasks;navigation;auto-complete;natural language processing","Documentation;Software;Navigation;Data mining;Programming;Natural language processing;Subscriptions","knowledge management;software engineering;user interfaces","software documentation navigation;knowledge management;software developers;information needs;programming action;TaskNavigator user interface;documentation structure","","14","","59","","","","","","IEEE","IEEE Journals & Magazines"
"X-FEDERATE: a policy engineering framework for federated access management","R. Bhatti; E. Bertino; A. Ghafoor","Dept. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA; Dept. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA; Dept. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA","IEEE Transactions on Software Engineering","","2006","32","5","330","346","Policy-based management (PBM) has been considered as a promising approach for design and enforcement of access management policies for distributed systems. The increasing shift toward federated information sharing in the organizational landscape, however, calls for revisiting current PBM approaches to satisfy the unique security requirements of the federated paradigm. This presents a twofold challenge for the design of a PBM approach, where, on the one hand, the policy must incorporate the access management needs of the individual systems, while, on the other hand, the policies across multiple systems must be designed in such a manner that they can be uniformly developed, deployed, and integrated within the federated system. In this paper, we analyze the impact of security management challenges on policy design and formulate a policy engineering methodology based on principles of software engineering to develop a PBM solution for federated systems. We present X-FEDERATE, a policy engineering framework for federated access management using an extension of the well-known role-based access control (RBAC) model. Our framework consists of an XML-based policy specification language, its UML-based meta-model, and an enforcement architecture. We provide a comparison of our framework with related approaches and highlight its significance for federated access management. The paper also presents a federation protocol and discusses a prototype of our framework that implements the protocol in a federated digital library environment","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642680","Federated systems;software engineering;security management;role-based access control.","Engineering management;Access protocols;Information security;Software development management;Design engineering;Software engineering;Access control;Specification languages;Computer architecture;Prototypes","authorisation;data models;digital libraries;software architecture;Unified Modeling Language;XML","X-FEDERATE;policy engineering framework;federated access management;policy-based management;distributed systems;federated information sharing;security management;role-based access control;XML-based policy specification language;UML-based meta-model;enforcement architecture;federated digital library environment","","24","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A Survey on Metamorphic Testing","S. Segura; G. Fraser; A. B. Sanchez; A. Ruiz-Cortés","Department of Computer Languages and Systems, Universidad de Sevilla, Spain; Department of Computer Science, University of Sheffield, Sheffield, United Kingdom; Department of Computer Languages and Systems, Universidad de Sevilla, Spain; Department of Computer Languages and Systems, Universidad de Sevilla, Spain","IEEE Transactions on Software Engineering","","2016","42","9","805","824","A test oracle determines whether a test execution reveals a fault, often by comparing the observed program output to the expected output. This is not always practical, for example when a program's input-output relation is complex and difficult to capture formally. Metamorphic testing provides an alternative, where correctness is not determined by checking an individual concrete output, but by applying a transformation to a test input and observing how the program output “morphs” into a different one as a result. Since the introduction of such metamorphic relations in 1998, many contributions on metamorphic testing have been made, and the technique has seen successful applications in a variety of domains, ranging from web services to computer graphics. This article provides a comprehensive survey on metamorphic testing: It summarises the research results and application areas, and analyses common practice in empirical studies of metamorphic testing as well as the main open challenges.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2532875","European Commission (FEDER); Spanish Government; CICYT projects TAPAS; BELI; Andalusian Government projects THEOS; COPAS; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7422146","Metamorphic testing;oracle problem;survey","Testing;Search engines;Google;Libraries;Concrete;Distance measurement;Web services","program testing","metamorphic testing;test oracle;test execution;metamorphic relations","","45","","148","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical validation of object-oriented metrics in two different iterative software processes","M. Alshayeb; Wei Li","Dept. of Comput. Sci., Alabama Univ., Huntsville, AL, USA; Dept. of Comput. Sci., Alabama Univ., Huntsville, AL, USA","IEEE Transactions on Software Engineering","","2003","29","11","1043","1049","Object-oriented (OO) metrics are used mainly to predict software engineering activities/efforts such as maintenance effort, error proneness, and error rate. There have been discussions about the effectiveness of metrics in different contexts. In this paper, we present an empirical study of OO metrics in two iterative processes: the short-cycled agile process and the long-cycled framework evolution process. We find that OO metrics are effective in predicting design efforts and source lines of code added, changed, and deleted in the short-cycled agile process and ineffective in predicting the same aspects in the long-cycled framework process. This leads us to believe that OO metrics' predictive capability is limited to the design and implementation changes during the development iterations, not the long-term evolution of an established system in different releases.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245305","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245305","","Predictive models;Size measurement;Software maintenance;Object oriented modeling;Software measurement;Software systems;Testing;Software engineering;Error analysis;Lead","object-oriented programming;software metrics;software maintenance","object-oriented metrics;iterative software processes;software maintenance;short-cycled agile process;long-cycled framework evolution process","","65","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Test Case Prioritization Using Lexicographical Ordering","S. Eghbali; L. Tahvildari","Department of Electrical and Computer Engineering, 200 University Ave West, University of Waterloo, Waterloo, Ontario; Department of Electrical and Computer Engineering, 200 University Ave West, University of Waterloo, Waterloo, Ontario","IEEE Transactions on Software Engineering","","2016","42","12","1178","1195","Test case prioritization aims at ordering test cases to increase the rate of fault detection, which quantifies how fast faults are detected during the testing phase. A common approach for test case prioritization is to use the information of previously executed test cases, such as coverage information, resulting in an iterative (greedy) prioritization algorithm. Current research in this area validates the fact that using coverage information can improve the rate of fault detection in prioritization algorithms. The performance of such iterative prioritization schemes degrade as the number of ties encountered in prioritization steps increases. In this paper, using the notion of lexicographical ordering, we propose a new heuristic for breaking ties in coverage based techniques. Performance of the proposed technique in terms of the rate of fault detection is empirically evaluated using a wide range of programs. Results indicate that the proposed technique can resolve ties and in turn noticeably increases the rate of fault detection.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2550441","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7456343","Regression testing;test case prioritization;lexicographical ordering","Software testing;Fault detection;Feature extraction;Regression analysis;Fault diagnosis","fault diagnosis;greedy algorithms;iterative methods;program testing;regression analysis","regression testing;coverage information;iterative greedy prioritization algorithm;fault detection;lexicographical ordering;test case prioritization","","8","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Abstracting runtime heaps for program understanding","M. Marron; C. Sanchez; Z. Su; M. Fahndrich","Imdea Software Institute, Boadilla del Monte; Imdea Software Institute, Boadilla del Monte; University of California, Davis, Davis; Microsoft Research","IEEE Transactions on Software Engineering","","2013","39","6","774","786","Modern programming environments provide extensive support for inspecting, analyzing, and testing programs based on the algorithmic structure of a program. Unfortunately, support for inspecting and understanding runtime data structures during execution is typically much more limited. This paper provides a general purpose technique for abstracting and summarizing entire runtime heaps. We describe the abstract heap model and the associated algorithms for transforming a concrete heap dump into the corresponding abstract model as well as algorithms for merging, comparing, and computing changes between abstract models. The abstract model is designed to emphasize high-level concepts about heap-based data structures, such as shape and size, as well as relationships between heap structures, such as sharing and connectivity. We demonstrate the utility and computational tractability of the abstract heap model by building a memory profiler. We use this tool to identify, pinpoint, and correct sources of memory bloat for programs from DaCapo.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6331492","Heap structure;runtime analysis;memory profiling;program understanding","Abstracts;Concrete;Shape;Runtime;Arrays;Computational modeling","data structures;merging;program diagnostics;program testing","program testing;program analysis;program inspection;program algorithmic structure;runtime data structure relationships;runtime heap abstracting;runtime heap summarization;concrete heap dump;program merging;program comparison;program computing;high-level concepts;heap structure sharing;heap structure connectivity;abstract heap model utility;abstract heap model computational tractability;memory profiler;memory bloat;DaCapo","","7","","37","","","","","","IEEE","IEEE Journals & Magazines"
"The JEDI event-based infrastructure and its application to the development of the OPSS WFMS","G. Cugola; E. Di Nitto; A. Fuggetta","Dept. of Electron. & Inf., Politecnico di Milano, Italy; NA; NA","IEEE Transactions on Software Engineering","","2001","27","9","827","850","The development of complex distributed systems demands the creation of suitable architectural styles (or paradigms) and related runtime infrastructures. An emerging style that is receiving increasing attention is based on the notion of event. In an event-based architecture, distributed software components interact by generating and consuming events. An event is the occurrence of some state change in a component of a software system, made visible to the external world. The occurrence of an event in a component is asynchronously notified to any other component that has declared some interest in it. This paradigm (usually called ""publish/subscribe"", from the names of the two basic operations that regulate the communication) holds the promise of supporting a flexible and effective interaction among highly reconfigurable, distributed software components. In the past two years, we have developed an object-oriented infrastructure called JEDI (Java event-based distributed infrastructure). JEDI supports the development and operation of event-based systems and has been used to implement a significant example of distributed system, namely, the OPSS workflow management system (WFMS). The paper illustrates the main features of JEDI and how we have used them to implement OPSS. Moreover, the paper provides an initial evaluation of our experiences in using the event-based architectural style and a classification of some of the event-based infrastructures presented in the literature.","0098-5589;1939-3520;2326-3881","","10.1109/32.950318","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=950318","","Middleware;Runtime;Computer architecture;Software systems;Java;Workflow management software;Software architecture;Broadcasting;Computer networks;Distributed computing","reviews;software architecture;workflow management software;distributed object management;Java","JEDI event-based infrastructure;OPSS WFMS;complex distributed systems;architectural styles;runtime infrastructures;event-based architecture;distributed software components;publish/subscribe;object-oriented infrastructure;Java event-based distributed infrastructure;workflow management system;software architectures;business processes;middleware","","263","","61","","","","","","IEEE","IEEE Journals & Magazines"
"On the effectiveness of the test-first approach to programming","H. Erdogmus; M. Morisio; M. Torchiano","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2005","31","3","226","237","Test-driven development (TDD) is based on formalizing a piece of functionality as a test, implementing the functionality such that the test passes, and iterating the process. This paper describes a controlled experiment for evaluating an important aspect of TDD: in TDD, programmers write functional tests before the corresponding implementation code. The experiment was conducted with undergraduate students. While the experiment group applied a test-first strategy, the control group applied a more conventional development technique, writing tests after the implementation. Both groups followed an incremental process, adding new features one at a time and regression testing them. We found that test-first students on average wrote more tests and, in turn, students who wrote more tests tended to be more productive. We also observed that the minimum quality increased linearly with the number of programmer tests, independent of the development strategy employed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423994","Index Terms- General programming techniques;coding tools and techniques;testing and debugging;testing strategies;productivity;Software Quality/SQA;software engineering process;programming paradigms.","Programming profession;Productivity;Quality assurance;Computer Society;Software testing;Feedback;Functional programming;Writing;Debugging;Software quality","program testing;program debugging;software quality","test-driven development;general programming technique;program testing;program debugging;software quality;software engineering process;formal verification","","116","","25","","","","","","IEEE","IEEE Journals & Magazines"
"InterPlay: Horizontal Scale-Up and Transition to Design in Scenario-Based Programming","D. Barak; D. Harel; R. Marelly","Weizmann Institute of Science, Rehovot, Israel; Weizmann Institute of Science, Rehovot, Israel; Weizmann Institute of Science, Rehovot, Israel","IEEE Transactions on Software Engineering","","2006","32","7","467","485","We describe InterPlay, a simulation engine coordinator that supports cooperation and interaction of multiple simulation and execution tools, thus helping to scale up the design and development cycle of reactive systems. InterPlay involves a number of related ideas. In the first, we concentrate on the interobject design approach involving live sequence charts (LSCs) and its support tool, the play-engine, enabling multiple play-engines to run in cooperation. This makes possible the distributed design of large-scale systems by different teams, as well as the refinement of parts of a system using different play-engines. The second idea concerns combining the interobject approach with the more conventional intraobject approach, involving, for example, statecharts and Rhapsody. InterPlay makes it possible to run the play-engine in cooperation with Rhapsody, and is very useful when some system objects have clear and distinct internal behavior, or in an iterative development process where the design is implementation-oriented and the ultimate goal is to end up with an intraobject implementation. Finally, we have expanded the play-engine's ability to delegate some of the system's functionality to complex GUIs. This enables beneficial interaction with ""smart"" GUIs that have built-in behavior of their own, and which are more naturally implemented in code","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677533","Modeling methodologies;scenario-based programming;InterPlay;play-engine;LSCs;intraobject;interobject;transition to design.","Engines;Operating systems;Large-scale systems;Process design;Design methodology;Graphical user interfaces;Distributed computing;Joining processes;Java;Voice mail","digital simulation;graphical user interfaces;object-oriented programming;software tools","simulation engine coordinator;reactive system;interobject design approach;live sequence chart;distributed design;large-scale system;iterative development process;intraobject design approach;GUI;graphical user interface;scenario-based programming;InterPlay;play-engine","","6","","21","","","","","","IEEE","IEEE Journals & Magazines"
"How Programmers Debug, Revisited: An Information Foraging Theory Perspective","J. Lawrance; C. Bogart; M. Burnett; R. Bellamy; K. Rector; S. D. Fleming","Wentworth Institute of Technology, Boston; Oregon State University, Corvallis; Oregon State University, Corvallis; IBM TJ Watson Research Center, Hawthorne; Oregon State University, Corvallis; Oregon State University, Corvallis","IEEE Transactions on Software Engineering","","2013","39","2","197","215","Many theories of human debugging rely on complex mental constructs that offer little practical advice to builders of software engineering tools. Although hypotheses are important in debugging, a theory of navigation adds more practical value to our understanding of how programmers debug. Therefore, in this paper, we reconsider how people go about debugging in large collections of source code using a modern programming environment. We present an information foraging theory of debugging that treats programmer navigation during debugging as being analogous to a predator following scent to find prey in the wild. The theory proposes that constructs of scent and topology provide enough information to describe and predict programmer navigation during debugging, without reference to mental states such as hypotheses. We investigate the scope of our theory through an empirical study of 10 professional programmers debugging a real-world open source program. We found that the programmers' verbalizations far more often concerned scent-following than hypotheses. To evaluate the predictiveness of our theory, we created an executable model that predicted programmer navigation behavior more accurately than comparable models that did not consider information scent. Finally, we discuss the implications of our results for enhancing software engineering tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.111","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5674060","Information foraging theory;debugging;software maintenance;programmer navigation;information scent;empirical software engineering","Debugging;Navigation;Topology;Programming environments;Predictive models;Approximation methods","cognition;program debugging;public domain software;software maintenance;topology","information foraging theory;human debugging theories;complex mental constructs;navigation theory;programming environment;topology constructs;information scent constructs;open source code program debugging;programmer verbalizations;programmer navigation behavior prediction;software engineering tool enhancement","","38","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Scenario-based assessment of nonfunctional requirements","A. Gregoriades; A. Sutcliffe","Surrey Defence Technol. Centre, Surrey Univ., Guildford, UK; NA","IEEE Transactions on Software Engineering","","2005","31","5","392","409","This paper describes a method and a tool for validating nonfunctional requirements in complex socio-technical systems. The system requirements analyzer (SRA) tool validates system reliability and operational performance requirements using scenario-based testing. Scenarios are transformed into sequences of task steps and the reliability of human agents performing tasks with computerized technology is assessed using Bayesian belief network (BN) models. The tool tests system performance within an envelope of environmental variations and reports the number of tests that pass a benchmark threshold. The tool diagnoses problematic areas in scenarios representing pathways through system models, assists in the identification of their causes, and supports comparison of alternative requirements specifications and system designs. It is suitable for testing socio-technical systems where operational scenarios are sequential and deterministic, in domains where designs are incrementally modified so set up costs of the BNs can be defrayed over multiple tests.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1438375","Index Terms- Nonfunctional requirements validation;scenario-based testing;Bayesian belief networks;systems engineering.","System testing;Sociotechnical systems;Sequential analysis;Performance analysis;Computer network reliability;Humans;Computer networks;Bayesian methods;System performance;Benchmark testing","formal specification;formal verification;belief networks;systems analysis;software agents;program testing;software reliability","nonfunctional requirement;complex socio-technical system;system requirements analyzer tool validation;system reliability;operational performance requirement;scenario-based testing;human agent;computerized technology;Bayesian belief network model;system performance;benchmark threshold;tool diagnosis;system requirement specification;system design","","25","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Reachability testing of concurrent programs","Y. Lei; R. H. Carver","Dept. of Comput. Sci., Texas Univ., Arlington, TX, USA; NA","IEEE Transactions on Software Engineering","","2006","32","6","382","403","One approach to testing concurrent programs, called reachability testing, generates synchronization sequences automatically and on-the-fly, without constructing any static models. In this paper, we present a general execution model for concurrent programs that allows reachability testing to be applied to several commonly used synchronization constructs. We also present a new method for performing reachability testing. This new method guarantees that every partially ordered synchronization sequence will be exercised exactly once without having to save any sequences that have already been exercised. We describe a prototype reachability testing tool called RichTest and report some empirical results, including a comparison between RichTest and a partial order reduction-based tool called VeriSoft. RichTest performed significantly better for the programs in our study","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650214","Software testing;reachability testing;concurrent programming.","Automatic testing;Interleaved codes;Performance evaluation;Prototypes;Multithreading;Computational efficiency;Web server;Java;Libraries","concurrency control;multi-threading;program testing;reachability analysis;synchronisation","reachability testing;concurrent programs;static model;synchronization sequence;RichTest tool;VeriSoft partial order reduction-based tool","","51","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Evolutionary Optimization of Software Quality Modeling with Multiple Repositories","Y. Liu; T. M. Khoshgoftaar; N. Seliya","Georgia College &amp; State University, Milledgeville, GA; Florida Atlantic University, Boca Raton, FL; University of Michigan-Dearborn, Dearborn, MI","IEEE Transactions on Software Engineering","","2010","36","6","852","864","A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5467094","Genetic programming;optimization;software quality;defects;machine learning;software measurement.","Software quality;Software measurement;Predictive models;Software metrics;Electronic mail;Genetic programming;Robustness;Costs;Machine learning;Software engineering","genetic algorithms;software management;software metrics;software quality","evolutionary optimization;software quality modeling;multiple software project repository;genetic programming;baseline classifier;validation classifier;validation-and-voting classifier;robust software quality model;software metrics;machine learner;software data set;search-based software quality model;software measurement data repository","","51","","48","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Study on Explicit-State Non-Zenoness Checking for Timed Automata","T. Wang; J. Sun; X. Wang; Y. Liu; Y. Si; J. S. Dong; X. Yang; X. Li","College of Computer Science, Zhejiang University, P.R., China; ISTD, Singapore University of Technology and Design, Singapore; College of Computer Science, Zhejiang University, P.R., China; School of Computer Engineering, Nanyang Technological University, Singapore; College of Computer Science, Zhejiang University, P.R., China; School of Computing, National University of Singapore, Singapore; College of Computer Science, Zhejiang University, P.R., China; School of Computer Science and Technology, Tianjin University, P.R., China","IEEE Transactions on Software Engineering","","2015","41","1","3","18","Zeno runs, where infinitely many actions occur within finite time, may arise in Timed Automata models. Zeno runs are not feasible in reality and must be pruned during system verification. Thus it is necessary to check whether a run is Zeno or not so as to avoid presenting Zeno runs as counterexamples during model checking. Existing approaches on non-Zenoness checking include either introducing an additional clock in the Timed Automata models or additional accepting states in the zone graphs. In addition, there are approaches proposed for alternative timed modeling languages, which could be generalized to Timed Automata. In this work, we investigate the problem of non-Zenoness checking in the context of model checking LTL properties, not only evaluating and comparing existing approaches but also proposing a new method. To have a systematic evaluation, we develop a software toolkit to support multiple non-Zenoness checking algorithms. The experimental results show the effectiveness of our newly proposed algorithm, and demonstrate the strengths and weaknesses of different approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2359893","National Natural Science Foundation Program; National Key Technology Support Program; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6908008","Timed automata;non-Zenoness;model checking;verification tool","Automata;Clocks;Safety;Educational institutions;Systematics;Cost accounting;Model checking","automata theory;formal verification;graph theory;real-time systems","explicit-state nonzenoness checking;timed automata models;zeno runs;system verification;clocks;zone graphs;timed modeling languages;model checking LTL properties;systematic evaluation;software toolkit","","2","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying Failure Causes in Java Programs: An Application of Change Impact Analysis","Xiaoxia Ren; O. C. Chesley; B. G. Ryder","IEEE Computer Society; NA; NA","IEEE Transactions on Software Engineering","","2006","32","9","718","732","During program maintenance, a programmer may make changes that enhance program functionality or fix bugs in code. Then, the programmer usually will run unit/regression tests to prevent invalidation of previously tested functionality. If a test fails unexpectedly, the programmer needs to explore the edit to find the failure-inducing changes for that test. Crisp uses results from Chianti, a tool that performs semantic change impact analysis, to allow the programmer to examine those parts of the edit that affect the failing test. Crisp then builds a compilable intermediate version of the program by adding a programmer-selected partial edit to the original code, augmenting the selection as necessary to ensure compilation. The programmer can reexecute the test on the intermediate version in order to locate the exact reasons for the failure by concentrating on the specific changes that were applied. In nine initial case studies on pairs of versions from two real Java programs, Daikon and Eclipse jdt compiler, we were able to use Crisp to identify the failure-inducing changes for all but 1 of 68 failing tests. On average, 33 changes were found to affect each failing test (of the 67), but only 1-4 of these changes were found to be actually failure-inducing","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707669","Fault localization;semantic change impact analysis;edit change dependence;regression testing;intermediate versions of programs.","Java;Failure analysis;Testing;Programming profession;Application software;Performance evaluation;Performance analysis;Prototypes;Computer Society;Computer bugs","Java;program compilers;program debugging;program diagnostics;program testing;software maintenance","program maintenance;bug fixing;semantic change impact analysis;compilable intermediate program version;Java program;program compiler;failure-inducing change;fault localization;edit change dependence;unit testing;regression testing","","32","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A Comparison of Program Comprehension Strategies by Blind and Sighted Programmers","A. Armaly; P. Rodeghero; C. McMillan","Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN","IEEE Transactions on Software Engineering","","2018","44","8","712","724","Programmers who are blind use a screen reader to speak source code one word at a time, as though the code were text. This process of reading is in stark contrast to sighted programmers, who skim source code rapidly with their eyes. At present, it is not known whether the difference in these processes has effects on the program comprehension gained from reading code. These effects are important because they could reduce both the usefulness of accessibility tools and the generalizability of software engineering studies to persons with low vision. In this paper, we present an empirical study comparing the program comprehension of blind and sighted programmers. We found that both blind and sighted programmers prioritize reading method signatures over other areas of code. Both groups obtained an equal and high degree of comprehension, despite the different reading processes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2729548","National Science Foundation Graduate Research Fellowship Program; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7987041","Program comprehension;accessibility technology;blindness","Tools;Software;Blindness;Navigation;Programming profession;Software engineering","programming environments;public domain software;software prototyping;source code (software)","source code;sighted programmers;program comprehension strategies;reading processes;stark;program comprehension;reading method signature","","1","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing the Cost Effectiveness of Fault Prediction in Acceptance Testing","A. Monden; T. Hayashi; S. Shinoda; K. Shirai; J. Yoshida; M. Barker; K. Matsumoto","Nara Institute of Science and Technology, Ikoma; NTT West Corporation, Osaka; NTT West Corporation, Osaka; NTT West Corporation, Osaka; NTT West Corporation, Osaka; Nara Institute of Science and Technology, Ikoma; Nara Institute of Science and Technology, Ikoma","IEEE Transactions on Software Engineering","","2013","39","10","1345","1357","Until now, various techniques for predicting fault-prone modules have been proposed and evaluated in terms of their prediction performance; however, their actual contribution to business objectives such as quality improvement and cost reduction has rarely been assessed. This paper proposes using a simulation model of software testing to assess the cost effectiveness of test effort allocation strategies based on fault prediction results. The simulation model estimates the number of discoverable faults with respect to the given test resources, the resource allocation strategy, a set of modules to be tested, and the fault prediction results. In a case study applying fault prediction of a small system to acceptance testing in the telecommunication industry, results from our simulation model showed that the best strategy was to let the test effort be proportional to ""the number of expected faults in a module × log(module size)."" By using this strategy with our best fault prediction model, the test effort could be reduced by 25 percent while still detecting as many faults as were normally discovered in testing, although the company required about 6 percent of the test effort for metrics collection, data cleansing, and modeling. The simulation results also indicate that the lower bound of acceptable prediction accuracy is around 0.78 in terms of an effort-aware measure, Norm(P<sub>opt</sub>). The results indicate that reduction of the test effort can be achieved by fault prediction only if the appropriate test strategy is employed with high enough fault prediction accuracy. Based on these preliminary results, we expect further research to assess their general validity with larger systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6497441","Complexity measures;fault prediction;quality assurance;resource allocation;simulation","Testing;Predictive models;Measurement;Software;Resource management;Companies;Accuracy","program testing;resource allocation;software cost estimation;software fault tolerance;software metrics","cost effectiveness assessment;fault prediction;acceptance testing;quality improvement;cost reduction;software testing;test effort allocation strategies;fault discovery;resource allocation strategy;test resources;telecommunication industry;metrics collection;data cleansing;data modeling;effort-aware measure","","17","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting Consistency-Maintenance Requirement of Code Clonesat Copy-and-Paste Time","X. Wang; Y. Dang; L. Zhang; D. Zhang; E. Lan; H. Mei","Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, and with the Department of Computer Science, University of Texas, San Antonio; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Corporation, One Microsoft Way, Redmond, WA; Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, and with the Department of Computer Science, University of Texas, San Antonio","IEEE Transactions on Software Engineering","","2014","40","8","773","794","Code clones have always been a double edged sword in software development. On one hand, it is a very convenient way to reuse existing code, and to save coding effort. On the other hand, since developers may need to ensure consistency among cloned code segments, code clones can lead to extra maintenance effort and even bugs. Recently studies on the evolution of code clones show that only some of the code clones experience consistent changes during their evolution history. Therefore, if we can accurately predict whether a code clone will experience consistent changes, we will be able to provide useful recommendations to developers onleveraging the convenience of some code cloning operations, while avoiding other code cloning operations to reduce future consistency maintenance effort. In this paper, we define a code cloning operation as consistency-maintenance-required if its generated code clones experience consistent changes in the software evolution history, and we propose a novel approach that automatically predicts whether a code cloning operation requires consistency maintenance at the time point of performing copy-and-paste operations. Our insight is that whether a code cloning operation requires consistency maintenance may relate to the characteristics of the code to be cloned and the characteristics of its context. Based on a number of attributes extracted from the cloned code and the context of the code cloning operation, we use Bayesian Networks, a machine-learning technique, to predict whether an intended code cloning operation requires consistency maintenance. We evaluated our approach on four subjects-two large-scale Microsoft software projects, and two popular open-source software projects-under two usage scenarios: 1) recommend developers to perform only the cloning operations predicted to be very likely to be consistency-maintenance-free, and 2) recommend developers to perform all cloning operations unless they are predicted very likely to be consistency-maintenance-required. In the first scenario, our approach is able to recommend developers to perform more than 50 percent cloning operations with a precision of at least 94 percent in the four subjects. In the second scenario, our approach is able to avoid 37 to 72 percent consistency-maintenance-required code clones by warning developers on only 13 to 40 percent code clones, in the four subjects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2323972","National 863 Program; National 973 Program; Science Fund for Creative Research Groups; Natural Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6815760","Code cloning;consistency maintenance;programming aid","Cloning;Software;Maintenance engineering;Bayes methods;History;Training;Educational institutions","belief networks;learning (artificial intelligence);public domain software;software maintenance","consistency-maintenance requirement;code clones;copy-and-paste time;software development;maintenance effort;code cloning operations;consistency maintenance effort;Bayesian networks;machine-learning technique;Microsoft software projects;open-source software projects","","7","","46","","","","","","IEEE","IEEE Journals & Magazines"
"A cognitive-based mechanism for constructing software inspection teams","J. Miller; Zhichao Yin","Dept. of Electr. & Comput. Eng., Alberta Univ., Edmonton, Alta., Canada; Dept. of Electr. & Comput. Eng., Alberta Univ., Edmonton, Alta., Canada","IEEE Transactions on Software Engineering","","2004","30","11","811","825","Software inspection is well-known as an effective means of defect detection. Nevertheless, recent research has suggested that the technique requires further development to optimize the inspection process. As the process is inherently group-based, one approach to improving performance is to attempt to minimize the commonality within the process and the group. This work proposes an approach to add diversity into the process by using a cognitively-based team selection mechanism. The paper argues that a team with diverse information processing strategies, as defined by the selection mechanism, maximize the number of different defects discovered.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359772","Index Terms- Planning for SQA and V&amp;V;code inspections and walkthroughs;programming teams;software psychology.","Inspection;Costs;NIST;Information processing;Psychology;Computer bugs;Economic indicators;Investments;Testing;Personnel","program testing;program verification;cognition","cognitive-based mechanism;software inspection team;defect detection;code inspection;code walkthrough;programming team;software psychology","","23","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Magiclock: Scalable Detection of Potential Deadlocks in Large-Scale Multithreaded Programs","Y. Cai; W. K. Chan","Department of Computer Science, City University of Hong Kong, Tat Chee Avenue; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue","IEEE Transactions on Software Engineering","","2014","40","3","266","281","We present Magiclock, a novel potential deadlock detection technique by analyzing execution traces (containing no deadlock occurrence) of large-scale multithreaded programs. Magiclock iteratively eliminates removable lock dependencies before potential deadlock localization. It divides lock dependencies into thread specific partitions, consolidates equivalent lock dependencies, and searches over the set of lock dependency chains without the need to examine any duplicated permutations of the same lock dependency chains. We validate Magiclock through a suite of real-world, large-scale multithreaded programs. The experimental results show that Magiclock is significantly more scalable and efficient than existing dynamic detectors in analyzing and detecting potential deadlocks in execution traces of large-scale multithreaded programs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2301725","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6718069","Deadlock detection;multithreaded programs;concurrency;lock order graph;scalability","System recovery;Message systems;Classification algorithms;Instruction sets;Image edge detection;Monitoring;Multicore processing","concurrency control;multi-threading;operating systems (computers);system recovery","Magiclock;potential deadlocks scalable detection;large-scale multithreaded programs;potential deadlock localization;lock order graph;scalability","","19","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Polymetric views - a lightweight visual approach to reverse engineering","M. Lanza; S. Ducasse","Inst. fur Informatik & angewandte Math., Bern Univ., Switzerland; Inst. fur Informatik & angewandte Math., Bern Univ., Switzerland","IEEE Transactions on Software Engineering","","2003","29","9","782","795","Reverse engineering software systems has become a major concern in software industry because of their sheer size and complexity. This problem needs to be tackled since the systems in question are of considerable worth to their owners and maintainers. In this article, we present the concept of a polymetric view, a lightweight software visualization technique enriched with software metrics information. Polymetric views help to understand the structure and detect problems of a software system in the initial phases of a reverse engineering process. We discuss the benefits and limits of several predefined polymetric views we have implemented in our tool CodeCrawler. Moreover, based on clusters of different polymetric views, we have developed a methodology which supports and guides a software engineer in the first phases of a reverse engineering of a large software system. We have refined this methodology by repeatedly applying it on industrial systems and illustrate it by applying a selection of polymetric views to a case study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232284","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232284","","Reverse engineering;Software systems;Visualization;Software metrics;Systems engineering and theory;Investments;Costs;Humans;Maintenance engineering;Computer industry","reverse engineering;object-oriented programming;software metrics;program visualisation;bibliographies","reverse engineering;software industry;polymetric view;lightweight software visualization technique;software metrics information;CodeCrawler;software engineer;large software system;object-oriented programming;software visualization","","164","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting Project Velocity in XP Using a Learning Dynamic Bayesian Network Model","P. Hearty; N. Fenton; D. Marquez; M. Neil","Queen Mary University of London-Computer, London; Queen Mary University of London-Computer, London; Queen Mary University of London-Computer, London; Queen Mary University of London-Computer, London","IEEE Transactions on Software Engineering","","2009","35","1","124","137","Bayesian networks, which can combine sparse data, prior assumptions and expert judgment into a single causal model, have already been used to build software effort prediction models. We present such a model of an extreme programming environment and show how it can learn from project data in order to make quantitative effort predictions and risk assessments without requiring any additional metrics collection program. The model's predictions are validated against a real world industrial project, with which they are in good agreement.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4624275","extreme programming;Bayesian networks;causal models;risk assessment;extreme programming;Bayesian networks;causal models;risk assessment","Bayesian methods;Predictive models;Project management;Risk management;Programming environments;Testing;Large-scale systems;Size measurement;Calendars;Uncertainty","belief networks;project management;risk management;software metrics","project velocity;XP;extreme programming;learning dynamic Bayesian network model;software effort prediction models;quantitative effort predictions;risk assessments;metrics collection program;software development","","25","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Pointcut Rejuvenation: Recovering Pointcut Expressions in Evolving Aspect-Oriented Software","R. Khatchadourian; P. Greenwood; A. Rashid; G. Xu","Ohio State University, Columbus; Lancaster University, Lancaster; Lancaster University, Lancaster; Ohio State University, Columbus","IEEE Transactions on Software Engineering","","2012","38","3","642","657","Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base code can lead to join points incorrectly falling in or out of the scope of pointcuts. In this paper, we present an automated approach that limits fragility problems by providing mechanical assistance in pointcut maintenance. The approach is based on harnessing arbitrarily deep structural commonalities between program elements corresponding to join points selected by a pointcut. The extracted patterns are then applied to later versions to offer suggestions of new join points that may require inclusion. To illustrate that the motivation behind our proposal is well founded, we first empirically establish that join points captured by a single pointcut typically portray a significant amount of unique structural commonality by analyzing patterns extracted from 23 AspectJ programs. Then, we demonstrate the usefulness of our technique by rejuvenating pointcuts in multiple versions of three of these programs. The results show that our parameterized heuristic algorithm was able to accurately and automatically infer the majority of new join points in subsequent software versions that were not captured by the original pointcuts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5710951","Software development environments;software maintenance;software tools.","Software;Fuels;Software engineering;Observers;Robustness;Programming;Proposals","aspect-oriented programming","pointcut rejuvenation;pointcut expression recovery;aspect-oriented software;pointcut fragility;aspect-oriented programming;pointcut maintenance;deep structural commonalities harnessing;program elements;join points;pattern analysis;AspectJ programs;parameterized heuristic algorithm","","2","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Enhancing the Description-to-Behavior Fidelity in Android Apps with Privacy Policy","L. Yu; X. Luo; C. Qian; S. Wang; H. K. N. Leung","Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong","IEEE Transactions on Software Engineering","","2018","44","9","834","854","Since more than 96 percent of mobile malware targets the Android platform, various techniques based on static code analysis or dynamic behavior analysis have been proposed to detect malicious apps. As malware is becoming more complicated and stealthy, recent research proposed a promising detection approach that looks for the inconsistency between an app's permissions and its description. In this paper, we first revisit this approach and reveal that using description and permission will lead to many false positives because descriptions often fail to declare all sensitive operations. Then, we propose exploiting an app's privacy policy and its bytecode to enhance the malware detection based on description and permissions. It is non-trivial to automatically analyze privacy policy and perform the cross-verification among these four kinds of software artifacts including, privacy policy, bytecode, description, and permissions. To address these challenging issues, we first propose a novel data flow model for analyzing privacy policy, and then develop a new system, named TAPVerifier, for carrying out investigation of individual software artifacts and conducting the cross-verification. The experimental results show that TAPVerifier can analyze privacy policy with a high accuracy and recall rate. More importantly, integrating privacy policy and bytecode level information can remove up to 59.4 percent false alerts of the state-of-the-art systems, such as AutoCog, CHABADA, etc.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2730198","Hong Kong GRF; Shenzhen City Science and Technology R&D Fund; Hong Kong RGC Project; HKPolyU Research; National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7987793","Mobile applications;privacy policy","Privacy;Data privacy;Semantics;Malware;Permission;Google;Androids","Android (operating system);data privacy;invasive software;mobile computing;program diagnostics","privacy policy;permission;description-to-behavior fidelity;Android apps;static code analysis;dynamic behavior analysis;malicious apps;promising detection approach;malware detection;mobile malware;TAPVerifier","","4","","91","","","","","","IEEE","IEEE Journals & Magazines"
"Probabilistic Model Checking of Regenerative Concurrent Systems","M. Paolieri; A. Horváth; E. Vicario","Department of Information Engineering, Università di Firenze, Firenze, Italy; Department of Computer Science, Università di Torino, Torino, Italy; Department of Information Engineering, Università di Firenze, Firenze, Italy","IEEE Transactions on Software Engineering","","2016","42","2","153","169","We consider the problem of verifying quantitative reachability properties in stochastic models of concurrent activities with generally distributed durations. Models are specified as stochastic time Petri nets and checked against Boolean combinations of interval until operators imposing bounds on the probability that the marking process will satisfy a goal condition at some time in the interval [α, β] after an execution that never violates a safety property. The proposed solution is based on the analysis of regeneration points in model executions: a regeneration is encountered after a discrete event if the future evolution depends only on the current marking and not on its previous history, thus satisfying the Markov property. We analyze systems in which multiple generally distributed timers can be started or stopped independently, but regeneration points are always encountered with probability 1 after a bounded number of discrete events. Leveraging the properties of regeneration points in probability spaces of execution paths, we show that the problem can be reduced to a set of Volterra integral equations, and we provide algorithms to compute their parameters through the enumeration of finite sequences of stochastic state classes encoding the joint probability density function (PDF) of generally distributed timers after each discrete event. The computation of symbolic PDFs is limited to discrete events before the first regeneration, and the repetitive structure of the stochastic process is exploited also before the lower bound α, providing crucial benefits for large time bounds. A case study is presented through the probabilistic formulation of Fischer's mutual exclusion protocol, a well-known real-time verification benchmark.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2468717","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7202875","Probabilistic Model Checking;Reachability;Stochastic Petri Net;Markov Regenerative Process;Markov Renewal Theory;Probabilistic model checking;reachability;stochastic Petri net;Markov regenerative process;Markov renewal theory","Probabilistic logic;Probability density function;Markov processes;Computational modeling;Numerical models;Petri nets","concurrency control;Markov processes;Petri nets;program verification;Volterra equations","probabilistic model checking;regenerative concurrent systems;quantitative reachability properties;stochastic models;concurrent activities;stochastic time Petri nets;Boolean combinations;regeneration point analysis;model executions;discrete event;Markov property;distributed timers;Volterra integral equations;stochastic state classes;joint probability density function;symbolic PDFs;stochastic process;Fischer's mutual exclusion protocol;real-time verification benchmark","","12","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing, Comparing, and Combining State Machine-Based Testing and Structural Testing: A Series of Experiments","S. Mouchawrab; L. C. Briand; Y. Labiche; M. Di Penta","Carleton University, Ottawa, Canada; Simula Research Laboratory, Lysaker, Norway; Carleton University, Ottawa, Canada; University of Sannio, Benevento, Italy","IEEE Transactions on Software Engineering","","2011","37","2","161","187","A large number of research works have addressed the importance of models in software engineering. However, the adoption of model-based techniques in software organizations is limited since these models are perceived to be expensive and not necessarily cost-effective. Focusing on model-based testing, this paper reports on a series of controlled experiments. It investigates the impact of state machine testing on fault detection in class clusters and its cost when compared with structural testing. Based on previous work showing this is a good compromise in terms of cost and effectiveness, this paper focuses on a specific state-based technique: the round-trip paths coverage criterion. Round-trip paths testing is compared to structural testing, and it is investigated whether they are complementary. Results show that even when a state machine models the behavior of the cluster under test as accurately as possible, no significant difference between the fault detection effectiveness of the two test strategies is observed, while the two test strategies are significantly more effective when combined by augmenting state machine testing with structural testing. A qualitative analysis also investigates the reasons why test techniques do not detect certain faults and how the cost of state machine testing can be brought down.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5416729","State-based software testing;structural testing;controlled experiments;state machines.","Object oriented modeling;Costs;Fault detection;Unified modeling language;System testing;Software testing;Automatic testing;Software engineering;Software design;Logic testing","fault tolerant computing;finite state machines;program testing;software engineering","state machine based testing;software engineering;model based techniques;software organizations;fault detection;round trip paths testing;structural testing","","33","","69","","","","","","IEEE","IEEE Journals & Magazines"
"An Investigation into the Functional Form of the Size-Defect Relationship for Software Modules","A. G. Koru; D. Zhang; K. El Emam; H. Liu","University of Maryland Baltimore County, Baltimore; University of Maryland Baltimore County, Baltimore; University of Ottawa, Ottawa; Georgetown University, Washington","IEEE Transactions on Software Engineering","","2009","35","2","293","304","The importance of the relationship between size and defect proneness of software modules is well recognized. Understanding the nature of that relationship can facilitate various development decisions related to prioritization of quality assurance activities. Overall, the previous research only drew a general conclusion that there was a monotonically increasing relationship between module size and defect proneness. In this study, we analyzed class-level size and defect data in order to increase our understanding of this crucial relationship. In order to obtain validated and more generalizable results, we studied four large-scale object-oriented products, Mozilla, Cn3d, JBoss, and Eclipse. Our results consistently revealed a significant effect of size on defect proneness; however, contrary to common intuition, the size-defect relationship took a logarithmic form, indicating that smaller classes were proportionally more problematic than larger classes. Therefore, practitioners should consider giving higher priority to smaller modules when planning focused quality assurance activities with limited resources. For example, in Mozilla and Eclipse, an inspection strategy investing 80% of available resources on 100-LOC classes and the rest on 1,000-LOC classes would be more than twice as cost effective as the opposite strategy. These results should be immediately useful to guide focused quality assurance activities in large-scale software projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4693715","Software science;Product metrics;Planning for SQA and Measurement applied to SQA and Software Quality/SQA;Software Engineering;Software/Software Engin;Open-source software;Software science;Product metrics;Planning for SQA and Measurement applied to SQA and Software Quality/SQA;Software Engineering;Software/Software Engin;Open-source software","Open source software;Software quality;Size measurement;Inspection;Object oriented modeling;Predictive models;Quality assurance;Large-scale systems;Software measurement;Density measurement","object-oriented methods;project management;software management;software metrics;software quality","software modules;size-defect relationship;quality assurance activities;object-oriented products;Mozilla;Cn3d;JBoss;Eclipse;software projects;product metrics","","65","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Timed Wp-method: testing real-time systems","A. En-Nouaary; R. Dssouli; F. Khendek","Dept. of Electr. & Comput. Eng., Concordia Univ., Montreal, Que., Canada; Dept. of Electr. & Comput. Eng., Concordia Univ., Montreal, Que., Canada; Dept. of Electr. & Comput. Eng., Concordia Univ., Montreal, Que., Canada","IEEE Transactions on Software Engineering","","2002","28","11","1023","1038","Real-time systems interact with their environment using time constrained input/output signals. Examples of real-time systems include patient monitoring systems, air traffic control systems, and telecommunication systems. For such systems, a functional misbehavior or a deviation from the specified time constraints may have catastrophic consequences. Therefore, ensuring the correctness of real-time systems becomes necessary. Two different techniques are usually used to cope with the correctness of a software system prior to its deployment, namely, verification and testing. In this paper, we address the issue of testing real-time software systems specified as a timed input output automaton (TIOA). TIOA is a variant of timed automaton. We introduce the syntax and semantics of TIOA. We present the potential faults that can be encountered in a timed system implementation. We study these different faults based on TIOA model and look at their effects on the execution of the system using the region graph. We present a method for generating timed test cases. This method is based on a state characterization technique and consists of the following three steps: First, we sample the region graph using a suitable granularity, in order to construct a subautomaton easily testable, called grid automaton. Then, we transform the grid automaton into a nondeterministic timed finite state machine (NTFSM). Finally, we adapt the generalized Wp-method to generate timed test cases from NTFSM. We assess the fault coverage of our test cases generation method and prove its ability to detect all the possible faults. Throughout the paper, we use examples to illustrate the various concepts and techniques used in our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049402","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049402","","System testing;Real time systems;Automata;Time factors;Software systems;Software testing;Automatic testing;Patient monitoring;Air traffic control;Fault detection","program testing;real-time systems;formal specification;automata theory","real-time systems testing;constrained input/output signals;correctness;software system;timed input output automaton;semantics;syntax;region graph;state characterization technique;granularity;subautomaton;grid automaton;nondeterministic timed finite state machine;generalized Wp-method;timed Wp-method;fault detection","","73","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Response to ""Comments on 'Formal methods application: an empirical tale of software development""'","A. E. K. Sobel; M. R. Clarkson","NA; NA","IEEE Transactions on Software Engineering","","2003","29","6","572","575","We respond to criticism by D. Berry and W. Tichy of our paper that appeared in the March 2002 issue of IEEE Transactions on Software Engineering. Many of the supposed faults they identify in our experiment are a result of a misunderstanding on their part, while others are inherent aspects of an educational experiment. We present counterarguments that explain why our experiment is valid.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205184","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205184","","Application software;Problem-solving;Education;Fault diagnosis;Programming profession;Laboratories;Design for experiments;Software engineering;Software testing","formal verification;formal specification","formal methods application;software development","","12","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Bidirectional Symbolic Analysis for Effective Branch Testing","M. Baluda; G. Denaro; M. Pezzè","Secure Software Engineering Group, Fraunhofer SIT, Darmstadt, Germany; Department of Informatics, Systems and Communication, Università di Milano-Bicocca, Milano, Italy; Faculty of Informatics, Università della Svizzera italiana, Lugano, Switzerland","IEEE Transactions on Software Engineering","","2016","42","5","403","426","Structural coverage metrics, and in particular branch coverage, are popular approaches to measure the thoroughness of test suites. Unfortunately, the presence of elements that are not executable in the program under test and the difficulty of generating test cases for rare conditions impact on the effectiveness of the coverage obtained with current approaches. In this paper, we propose a new approach that combines symbolic execution and symbolic reachability analysis to improve the effectiveness of branch testing. Our approach embraces the ideal definition of branch coverage as the percentage of executable branches traversed with the test suite, and proposes a new bidirectional symbolic analysis for both testing rare execution conditions and eliminating infeasible branches from the set of test objectives. The approach is centered on a model of the analyzed execution space. The model identifies the frontier between symbolic execution and symbolic reachability analysis, to guide the alternation and the progress of bidirectional analysis towards the coverage targets. The experimental results presented in the paper indicate that the proposed approach can both find test inputs that exercise rare execution conditions that are not identified with state-of-the-art approaches and eliminate many infeasible branches from the coverage measurement. It can thus produce a modified branch coverage metric that indicates the amount of feasible branches covered during testing, and helps team leaders and developers in estimating the amount of not-yet-covered feasible branches. The approach proposed in this paper suffers less than the other approaches from particular cases that may trap the analysis in unbounded loops.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2490067","SNF; AVATAR; Italian PRIN; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7296670",";Structural testing;branch coverage;program analysis;symbolic execution;symbolic reachability analysis","Analytical models;Testing;Reachability analysis;Valves;Computational modeling;Measurement;Concrete","program diagnostics;program testing","bidirectional symbolic analysis;branch testing;structural coverage metrics;branch coverage;test suite thoroughness measure;test case generation;symbolic execution;symbolic reachability analysis;test objectives;coverage measurement","","5","","137","","","","","","IEEE","IEEE Journals & Magazines"
"How Software Developers Use Tagging to Support Reminding and Refinding","M. Storey; J. Ryall; J. Singer; D. Myers; L. Cheng; M. Muller","University of Victoria, Victoria; University of Victoria, Victoria; National Research Council Canada, Ottawa; University of Victoria, Victoria; IBM Research, Cambridge; IBM Research, Cambridge","IEEE Transactions on Software Engineering","","2009","35","4","470","483","Developers frequently add annotations to source code to help them remember pertinent information and mark locations of interest for future investigation. Finding and refinding these notes is a form of navigation that is integral to software maintenance. Although there is some tool support in modern development environments for authoring and navigating these comments, we have observed that these annotations often fail to remind and are sometimes difficult to find by the programmer. To address these shortcomings, we have designed a new approach for software navigation called tags for software engineering activities (TagSEA). TagSEA combines the notion of waypointing (a mechanism for marking locations in spatial navigation) with social tagging to support programmers in defining semantically rich annotations to source code comments. The tool provides support for creating, editing, navigating, and managing these annotations. We present the results from two empirical studies, where we observed and then analyzed how professional programmers used source code annotations to support their development activities over 24 months. Our findings indicate that the addition of semantic information to annotations can improve their value. We also provide suggestions on how annotation tools in general may be improved.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4782972","Annotations;software navigation;software tagging;tags;software development tools.","Tagging;Navigation;Programming profession;Software maintenance;Software tools;Software engineering;Filters;Software development management;Maintenance engineering;Computer science","authoring systems;software maintenance","source code annotations;software maintenance;modern development environments;software navigation;software engineering activities;waypointing;social tagging;software development tools","","24","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Requirements Elicitation and Specification Using the Agent Paradigm: The Case Study of an Aircraft Turnaround Simulator","T. Miller; B. Lu; L. Sterling; G. Beydoun; K. Taveter","Department of Computing and Information Systems, The University of Melbourne, Melbourne, Victoria, Autralia; Department of Computing and Information Systems, The University of Melbourne, Melbourne, Victoria, Autralia; Faculty of ICT, Swinburne University of Technology, Melbourne, Victoria, Autralia; Faculty of Informatics, University of Wollongong, Wollongong, NSW 2522, Australia; Institute of Informatics, Tallinn University of Technology, Tallinn, EU, Estonia","IEEE Transactions on Software Engineering","","2014","40","10","1007","1024","In this paper, we describe research results arising from a technology transfer exercise on agent-oriented requirements engineering with an industry partner. We introduce two improvements to the state-of-the-art in agent-oriented requirements engineering, designed to mitigate two problems experienced by ourselves and our industry partner: (1) the lack of systematic methods for agent-oriented requirements elicitation and modelling; and (2) the lack of prescribed deliverables in agent-oriented requirements engineering. We discuss the application of our new approach to an aircraft turnaround simulator built in conjunction with our industry partner, and show how agent-oriented models can be derived and used to construct a complete requirements package. We evaluate this by having three independent people design and implement prototypes of the aircraft turnaround simulator, and comparing the three prototypes. Our evaluation indicates that our approach is effective at delivering correct, complete, and consistent requirements that satisfy the stakeholders, and can be used in a repeatable manner to produce designs and implementations. We discuss lessons learnt from applying this approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339827","Australian Research Council Linkage; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6860260","Agent-oriented software engineering;agent-oriented modelling;technology transfer","Object oriented modeling;Aircraft;Atmospheric modeling;Software;Industries;Analytical models;Educational institutions","aerospace computing;aircraft;formal specification;multi-agent systems;object-oriented programming;software agents","requirements elicitation;requirements specification;agent paradigm;aircraft turnaround simulator;agent oriented requirements engineering;industry partner;systematic methods;agent-oriented requirements elicitation;agent-oriented requirement modelling;independent people design","","15","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Evidence-based guidelines for assessment of software development cost uncertainty","M. Jorgensen","Simula Res. Lab., Lysaker, Norway","IEEE Transactions on Software Engineering","","2005","31","11","942","954","Several studies suggest that uncertainty assessments of software development costs are strongly biased toward overconfidence, i.e., that software cost estimates typically are believed to be more accurate than they really are. This overconfidence may lead to poor project planning. As a means of improving cost uncertainty assessments, we provide evidence-based guidelines for how to assess software development cost uncertainty, based on results from relevant empirical studies. The general guidelines provided are: 1) Do not rely solely on unaided, intuition-based uncertainty assessment processes, 2) do not replace expert judgment with formal uncertainty assessment models, 3) apply structured and explicit judgment-based processes, 4) apply strategies based on an outside view of the project, 5) combine uncertainty assessments from different sources through group work, not through mechanical combination, 6) use motivational mechanisms with care and only if greater effort is likely to lead to improved assessments, and 7) frame the assessment problem to fit the structure of the relevant uncertainty information and the assessment process. These guidelines are preliminary and should be updated in response to new evidence.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.128","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556553","Index Terms- Cost estimation;management;software psychology;uncertainty of software development cost.","Guidelines;Programming;Uncertainty;Cost function;Humans;Software development management;Psychology;Process planning;Estimation error;Terminology","software cost estimation;formal specification;project management","evidence-based guideline;software development cost uncertainty;software psychology;software cost estimation;project planning;intuition-based uncertainty assessment;formal uncertainty assessment model;mechanical combination","","29","","75","","","","","","IEEE","IEEE Journals & Magazines"
"Proactive Self-Adaptation for Improving the Reliability of Mission-Critical, Embedded, and Mobile Software","D. Cooray; E. Kouroshfar; S. Malek; R. Roshandel","VeriSign Inc., Reston; George Mason University, Fairfax; George Mason University, Fairfax; Seattle University, Seattle","IEEE Transactions on Software Engineering","","2013","39","12","1714","1735","Embedded and mobile software systems are marked with a high degree of unpredictability and dynamism in the execution context. At the same time, such systems are often mission-critical, meaning that they need to satisfy strict reliability requirements. Most current software reliability analysis approaches are not suitable for these types of software systems, as they do not take the changes in the execution context of the system into account. We propose an approach geared to such systems which continuously furnishes refined reliability predictions at runtime by incorporating various sources of information, including the execution context of the system. The reliability predictions are leveraged to proactively place the software in the (near-)optimal configuration with respect to changing conditions. Our approach considers two representative architectural reconfiguration decisions that impact the system's reliability: reallocation of components to processes and changing the number of component replicas. We have realized the approach as part of a framework intended for mission-critical settings, called REsilient SItuated SofTware system (RESIST), and evaluated it using a mobile emergency response system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6574866","Context awareness;software architecture;self-adaptive systems;reliability;mobility","Mobile communication;Software reliability;Context awareness;Reliability engineering;Software  architecture;Computer architecture","embedded systems;mobile computing;software architecture;software reliability","proactive self-adaptation;mission-critical software;embedded software;mobile software;unpredictability degree;dynamism degree;execution context;reliability requirements;software reliability analysis approach;architectural reconfiguration decisions;component reallocation;component replicas;RESIST approach;resilient situated software system;mobile emergency response system","","7","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing real-time equational rule-based systems","Y. -. Lee; A. M. K. Cheng","Dept. of Comput. Sci., Houston Univ., TX, USA; Dept. of Comput. Sci., Houston Univ., TX, USA","IEEE Transactions on Software Engineering","","2004","30","2","112","125","Analyzing and reducing the execution-time upper bound of real-time rule-based expert systems is a very important task because of the stringent timing constraints imposed on this class of systems. We present a new runtime optimization to reduce the execution-time upper bound of real-time rule-based expert systems. In order to determine rules to be evaluated at runtime, a predicate dependency list, which consists of a predicate, its active rule set and corresponding inactive rule set, is created for each predicate in a real-time rule-based program. Based on the predicate dependency list and the current value of each variable, the new runtime optimization dynamically selects rules to be evaluated at runtime. For the timing analysis of the proposed algorithm, we introduce a predicate-based rule dependency graph, a predicate-based enable-rule graph, and their construction algorithm. We also discuss the bounded time of the equational logic rule-based program using the predicate-based rule dependency graph as well as the predicate-based enable-rule graph. The implementation and performance evaluation of the proposed algorithm using both synthetic and practical real-time rule-base programs are also presented. The performance evaluation shows that the runtime optimizer reduces the number of rule evaluations and predicate evaluations as well as the response time upper bound significantly, and the new algorithm yields better execution-time upper bound compared to other optimization methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265816","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265816","","Real time systems;Equations;Knowledge based systems;Runtime;Upper bound;Expert systems;Timing;Optimization methods;Algorithm design and analysis;Logic","knowledge based systems;logic programming;real-time systems;specification languages;optimisation","real-time rule based expert system;equational logic rule-based program;runtime optimization;predicate based rule dependency graph;EQL language","","7","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Impact of Introducing Domain-Specific Modelling in Software Maintenance: An Industrial Case Study","N. Mellegård; A. Ferwerda; K. Lind; R. Heldal; M. R. V. Chaudron","Electromobility Group at the Research Institute Viktoria Swedish ICT, Gothenburg, Sweden; Centric, Gouda, The Netherlands; Electromobility Group at the Research Institute Viktoria Swedish ICT, Gothenburg, Sweden; Software Engineering Division at the joint Department of Computer Science and Engineering, Chalmers and Gothenborg University, Gothenburg, Sweden; Software Engineering Division at the joint Department of Computer Science and Engineering, Chalmers and Gothenborg University, Gothenburg, Sweden","IEEE Transactions on Software Engineering","","2016","42","3","245","260","Domain-specific modelling (DSM) is a modern software development technology that aims at enhancing productivity. One of the claimed advantages of DSM is increased maintainability of software. However, current empirical evidence supporting this claim is lacking. In this paper, we contribute evidence from a case study conducted at a software development company. We study how the introduction of DSM affected the maintenance of a legacy system. We collected data about the maintenance phase of a system that was initially developed using manual programming, but which was gradually replaced by DSM development. We performed statistical analyses of the relation between the use of DSM and the time needed to resolve defects, the defect density, and the phase in which defects were detected. The results show that after introducing DSM the defect density is lower, that defects are found earlier, but resolving defects takes longer. Other observed benefits are that the number of developers and the number of person-hours needed for maintaining the system decreased, and the portability to new platforms increased. Our findings are useful for organizations that consider introducing DSM and would like to know which benefits can be realized in software maintenance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2479221","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7270333","Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity;Empirical investigation;software maintenance;maintenance measurement;process measurement;productivity","DSL;Maintenance engineering;Unified modeling language;Business;Software maintenance;Productivity","software maintenance;statistical analysis","domain-specific modelling;software maintenance;DSM;software development technology;software maintainability;software development company;legacy system;manual programming;statistical analysis","","8","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting Domain Analysis through Mining and Recommending Features from Online Product Listings","N. Hariri; C. Castro-Herrera; M. Mirakhorli; J. Cleland-Huang; B. Mobasher","DePaul University, Chicago; Google Inc.; DePaul University, Chicago; DePaul University, Chicago; DePaul University, Chicago","IEEE Transactions on Software Engineering","","2013","39","12","1736","1752","Domain analysis is a labor-intensive task in which related software systems are analyzed to discover their common and variable parts. Many software projects include extensive domain analysis activities, intended to jumpstart the requirements process through identifying potential features. In this paper, we present a recommender system that is designed to reduce the human effort of performing domain analysis. Our approach relies on data mining techniques to discover common features across products as well as relationships among those features. We use a novel incremental diffusive algorithm to extract features from online product descriptions, and then employ association rule mining and the (k)-nearest neighbor machine learning method to make feature recommendations during the domain analysis process. Our feature mining and feature recommendation algorithms are quantitatively evaluated and the results are presented. Also, the performance of the recommender system is illustrated and evaluated within the context of a case study for an enterprise-level collaborative software suite. The results clearly highlight the benefits and limitations of our approach, as well as the necessary preconditions for its success.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6582404","Domain analysis;recommender systems;clustering;association rule mining;k-nearest neighbor","Feature extraction;Recommender systems;Clustering algorithms;Domain analysis;Data mining;Algorithm design and analysis;Electronic mail;Nearest neighbor search;Clustering","data mining;groupware;Internet;learning (artificial intelligence);pattern classification;recommender systems;software engineering","enterprise-level collaborative software suite;feature recommendation algorithms;feature mining;k-nearest neighbor machine learning method;association rule mining;online product descriptions;feature extraction;incremental diffusive algorithm;data mining techniques;recommender system;domain analysis activity;software projects;software systems;labor-intensive task;online product listings","","36","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Asynchronous parallel simulation of parallel programs","S. Prakash; E. Deelman; R. Bagrodia","TIBCO Software Inc., Palo Alto, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","5","385","400","Parallel simulation of parallel programs for large datasets has been shown to offer significant reduction in the execution time of many discrete event models. The paper describes the design and implementation of MPI-SIM, a library for the execution driven parallel simulation of task and data parallel programs. MPI-SIM can be used to predict the performance of existing programs written using MPI for message passing, or written in UC, a data parallel language, compiled to use message passing. The simulation models can be executed sequentially or in parallel. Parallel execution of the models are synchronized using a set of asynchronous conservative protocols. The paper demonstrates how protocol performance is improved by the use of application-level, runtime analysis. The analysis targets the communication patterns of the application. We show the application-level analysis for message passing and data parallel languages. We present the validation and performance results for the simulator for a set of applications that include the NAS Parallel Benchmark suite. The application-level optimization described in the paper yielded significant performance improvements in the simulation of parallel programs, and in some cases completely eliminated the synchronizations in the parallel execution of the simulation model.","0098-5589;1939-3520;2326-3881","","10.1109/32.846297","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846297","","Discrete event simulation;Protocols;Frequency synchronization;Parallel languages;Runtime;Hardware;Program processors;Computational modeling;Libraries;Performance analysis","parallel programming;parallel languages;virtual machines;message passing;application program interfaces;synchronisation;discrete event simulation","asynchronous parallel simulation;parallel programs;large datasets;execution time;discrete event models;MPI-SIM;execution driven parallel simulation;data parallel programs;MPI;message passing;UC;data parallel language;simulation models;parallel execution;asynchronous conservative protocols;protocol performance;application-level runtime analysis;communication patterns;NAS Parallel Benchmark suite;application-level optimization;simulation model","","25","","30","","","","","","IEEE","IEEE Journals & Magazines"
"The Role of Ethnographic Studies in Empirical Software Engineering","H. Sharp; Y. Dittrich; C. R. B. de Souza","Open University, Walton Hall, Milton Keynes, UK; Software and Systems Section, IT University of Copenhagen, Rued Langgaards Vej 7, Copenhagen S, Denmark; Vale Institute of Technology and the Federal University of Pará, Tv. Boaventura da Silva, 955, Belém, PA, Brazil","IEEE Transactions on Software Engineering","","2016","42","8","786","804","Ethnography is a qualitative research method used to study people and cultures. It is largely adopted in disciplines outside software engineering, including different areas of computer science. Ethnography can provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice, i.e., it can help to uncover not only what practitioners do, but also why they do it. Despite its potential, ethnography has not been widely adopted by empirical software engineering researchers, and receives little attention in the related literature. The main goal of this paper is to explain how empirical software engineering researchers would benefit from adopting ethnography. This is achieved by explicating four roles that ethnography can play in furthering the goals of empirical software engineering: to strengthen investigations into the social and human aspects of software engineering; to inform the design of software engineering tools; to improve method and process development; and to inform research programmes. This article introduces ethnography, explains its origin, context, strengths and weaknesses, and presents a set of dimensions that position ethnography as a useful and usable approach to empirical software engineering research. Throughout the paper, relevant examples of ethnographic studies of software practice are used to illustrate the points being made.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2519887","CNPq; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7387744","Design tools and techniques;human factors in software design;software engineering process;computer-supported collaborative work","Software engineering;Software;Context;Sociology;Electronic mail;Computer science;Guidelines","cultural aspects;human factors;software process improvement","sociotechnological realities;software development practice;ethnography;empirical software engineering;human aspects;social aspects;software process development;software engineering tools","","11","","135","","","","","","IEEE","IEEE Journals & Magazines"
"Complexity of points-to analysis of Java in the presence of exceptions","R. Chatterjee; B. G. Ryder; W. A. Landi","Oracle Corp., Nashua, NH, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","6","481","512","At each program point, points-to analysis for statically typed object oriented programming languages (e.g., Java, C++) determines those objects to which a reference may refer (or a pointer may point) during execution. Points-to analysis is necessary for any semantics based software tools for object oriented systems. Our new complexity results for points-to analysis distinguish the difficulty of intraprocedural and interprocedural points-to analyses for languages with combinations of single-level types (i.e., types with data members only of primitive type), exceptions with or without subtyping, and dynamic dispatch. Our results include: 1) the first polynomial-time algorithm for points-to analysis in the presence of exceptions that handles a robust subset of Java without threads and can be applied to C++; 2) proof that the above algorithm is safe, in general, and provably precise on programs with single-level types and exceptions without subtyping, but not dynamic dispatch, thus, this case is in P; 3) proof that an interprocedural points-to analysis problem with single-level types and exceptions with subtyping, but without dynamic dispatch, is PSPACE-hard, while the intraprocedural problem is PSPACE-complete. Other complexity characterizations of points-to analysis in programs without exceptions are presented, including an algorithm with worst-case bound of O(n/sup 5/), which improves over the O(n/sup 7/) worst-case bound achievable from previous approaches of T. Reps et al. (1995) and W.A. Landi and B.G. Ryder (1991).","0098-5589;1939-3520;2326-3881","","10.1109/32.926173","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=926173","","Java;Algorithm design and analysis;Information analysis;Object oriented programming;Runtime;Software tools;Polynomials;Robustness;Object oriented modeling","Java;C++ language;program diagnostics;programming language semantics;exception handling;type theory;computational complexity","points-to analysis complexity;Java;exceptions;program point;statically typed object oriented programming languages;semantics based software tools;object oriented systems;complexity results;single-level types;data members;dynamic dispatch;polynomial-time algorithm;C++;interprocedural points-to analysis problem;PSPACE-hard;intraprocedural problem;PSPACE-complete;complexity characterizations;worst-case bound","","11","","76","","","","","","IEEE","IEEE Journals & Magazines"
"Synthesizing Modal Transition Systems from Triggered Scenarios","G. E. Sibay; V. Braberman; S. Uchitel; J. Kramer","Imperial College London, London; University of Buenos Aires, Buenos Aires; Imperial College London, London and University of Buenos Aires, Buenos Aires; Imperial College, London","IEEE Transactions on Software Engineering","","2013","39","7","975","1001","Synthesis of operational behavior models from scenario-based specifications has been extensively studied. The focus has been mainly on either existential or universal interpretations. One noteworthy exception is Live Sequence Charts (LSCs), which provides expressive constructs for conditional universal scenarios and some limited support for nonconditional existential scenarios. In this paper, we propose a scenario-based language that supports both existential and universal interpretations for conditional scenarios. Existing model synthesis techniques use traditional two-valued behavior models, such as Labeled Transition Systems. These are not sufficiently expressive to accommodate specification languages with both existential and universal scenarios. We therefore shift the target of synthesis to Modal Transition Systems (MTS), an extension of labeled Transition Systems that can distinguish between required, unknown, and proscribed behavior to capture the semantics of existential and universal scenarios. Modal Transition Systems support elaboration of behavior models through refinement, which complements an incremental elicitation process suitable for specifying behavior with scenario-based notations. The synthesis algorithm that we define constructs a Modal Transition System that uses refinement to characterize all the Labeled Transition Systems models that satisfy a mixed, conditional existential and universal scenario-based specification. We show how this combination of scenario language, synthesis, and Modal Transition Systems supports behavior model elaboration.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.62","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311408","Scenarios;MTS;synthesis;partial behavior models","Semantics;Analytical models;Online banking;Merging;Unified modeling language;Indexes;Cognition","formal verification;reasoning about programs","modal transition systems;triggered scenarios;operational behavior models;scenario-based specifications;live sequence charts;LSC;conditional universal scenarios;nonconditional existential scenarios;scenario-based language;model synthesis techniques;two-valued behavior models;specification languages;MTS;incremental elicitation process;scenario-based notations;conditional existential scenario-based specification;universal scenario-based specification;labeled transition system models;behavior model elaboration","","7","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Finding Atomicity-Violation Bugs through Unserializable Interleaving Testing","S. Lu; S. Park; Y. Zhou","University of Wisconsin-Madison, Madison; University of California, San Diego, La Jolla; University of California, San Diego, La Jolla","IEEE Transactions on Software Engineering","","2012","38","4","844","860","Multicore hardware is making concurrent programs pervasive. Unfortunately, concurrent programs are prone to bugs. Among different types of concurrency bugs, atomicity violations are common and important. How to test the interleaving space and expose atomicity-violation bugs is an open problem. This paper makes three contributions. First, it designs and evaluates a hierarchy of four interleaving coverage criteria using 105 real-world concurrency bugs. This study finds a coverage criterion (Unserializable Interleaving Coverage) that balances the complexity and the capability of exposing atomicity-violation bugs well. Second, it studies stress testing to understand why this common practice cannot effectively expose atomicity-violation bugs from the perspective of unserializable interleaving coverage. Third, it designs CTrigger following the unserializable interleaving coverage criterion. CTrigger uses trace analysis to identify feasible unserializable interleavings, and then exercises low-probability interleavings to expose atomicity-violation bugs. We evaluate CTrigger with real-world atomicity-violation bugs from seven applications. CTrigger efficiently exposes these bugs within 1-235 seconds, two to four orders of magnitude faster than stress testing. Without CTrigger, some of these bugs do not manifest even after seven days of stress testing. Furthermore, once a bug is exposed, CTrigger can reliably reproduce it, usually within 5 seconds, for diagnosis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5740930","Testing and debugging;debugging aids;diagnostics;testing strategies;test coverage of code;concurrent programming;bug characteristics","Computer bugs;Testing;Instruction sets;Concurrent computing;Stress;Complexity theory;Synchronization","formal verification;multiprocessing systems;probability;program debugging;ubiquitous computing","finding atomicity violation bugs;unserializable interleaving testing;multicore hardware;pervasive concurrent programs;concurrency bugs;unserializable interleaving coverage;trace analysis;low probability interleavings","","12","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Developing a Single Model and Test Prioritization Strategies for Event-Driven Software","R. C. Bryce; S. Sampath; A. M. Memon","Utah State University, Logan; University of Maryland, Baltimore; University of Maryland, College Park","IEEE Transactions on Software Engineering","","2011","37","1","48","64","Event-Driven Software (EDS) can change state based on incoming events; common examples are GUI and Web applications. These EDSs pose a challenge to testing because there are a large number of possible event sequences that users can invoke through a user interface. While valuable contributions have been made for testing these two subclasses of EDS, such efforts have been disjoint. This work provides the first single model that is generic enough to study GUI and Web applications together. In this paper, we use the model to define generic prioritization criteria that are applicable to both GUI and Web applications. Our ultimate goal is to evolve the model and use it to develop a unified theory of how all EDS should be tested. An empirical study reveals that the GUI and Web-based applications, when recast using the new model, show similar behavior. For example, a criterion that gives priority to all pairs of event interactions did well for GUI and Web applications; another criterion that gives priority to the smallest number of parameter value settings did poorly for both. These results reinforce our belief that these two subclasses of applications should be modeled and studied together.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401169","Combinatorial interaction testing;covering arrays;event-driven software (EDS);t-way interaction coverage;test suite prioritization;user-session testing;Web application testing;GUI testing.","Software testing;Graphical user interfaces;Application software;Computer science;User interfaces;Protocols;Embedded software;Information systems;Educational institutions;Abstracts","graphical user interfaces;Internet;program testing;service-oriented architecture","event-driven software;test prioritization strategy;EDS;GUI testing;Web application testing;graphical user interface","","63","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Forecasting Risk Impact on ERP Maintenance with Augmented Fuzzy Cognitive Maps","J. L. Salmeron; C. Lopez","University Pablo de Olavide, Seville; Pablo de Olavide University, Seville","IEEE Transactions on Software Engineering","","2012","38","2","439","452","Worldwide, firms have made great efforts to implement Enterprise Resource Planning (ERP) systems. Despite these efforts, ERP adoption success is not guaranteed. Successful adoption of an ERP system also depends on proper system maintenance. For this reason, companies should follow a maintenance strategy that drives the ERP system toward success. However, in general, ERP maintenance managers do not know what conditions they should target to successfully maintain their ERP systems. Furthermore, numerous risks threaten these projects, but they are normally dealt with intuitively. To date, there has been limited literature published regarding ERP maintenance risks or ERP maintenance success. To address this need, we have built a dynamic simulation tool that allows ERP managers to foresee the impact of risks on maintenance goals. This research would help professionals manage their ERP maintenance projects. Moreover, it covers a significant gap in the literature.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680917","ERP;fuzzy cognitive maps;risk management;simulation;software maintenance.","Decision support systems","cognition;enterprise resource planning;forecasting theory;fuzzy set theory;project management;risk analysis;software maintenance;software management","enterprise resource planning system;ERP adoption success;ERP system maintenance;ERP maintenance risks;dynamic simulation tool;ERP maintenance project management;augmented fuzzy cognitive maps;risk impact forecasting","","37","","111","","","","","","IEEE","IEEE Journals & Magazines"
"Support for managing design-time decisions","A. Egyed; D. S. Wile","Teknowledge Corp., Marina del Rey, CA, USA; Teknowledge Corp., Marina del Rey, CA, USA","IEEE Transactions on Software Engineering","","2006","32","5","299","314","The desirability of maintaining multiple stakeholders' interests during the software design process argues for leaving choices undecided as long as possible. Yet, any form of underspecification, either missing information or undecided choices, must be resolved before automated analysis tools can be used. This paper demonstrates how constraint satisfaction problem solution techniques (CSTs) can be used to automatically reduce the space of choices for ambiguities by incorporating the local effects of constraints, ultimately with more global consequences. As constraints typical of those encountered during the software design process, we use UML consistency and well-formedness rules. It is somewhat surprising that CSTs are suitable for the software modeling domain since the constraints may relate many ambiguities during their evaluation, encountering a well-known problem with CSTs called the k-consistency problem. This paper demonstrates that our CST-based approach is computationally scalable and effective-as evidenced by empirical experiments based on dozens of industrial models","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.48","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642678","UML;design choices;consistency checking;design alternatives;choice elimination.","Unified modeling language;Software design;Programming;Predictive models;Information analysis;Computer industry;Process design;Software algorithms;Application software","constraint handling;constraint theory;decision making;formal specification;software maintenance;software management;Unified Modeling Language","design-time decisions;software design process;automated analysis tools;constraint satisfaction problem solution techniques;UML;software modeling;k-consistency problem","","10","","30","","","","","","IEEE","IEEE Journals & Magazines"
"A style-aware architectural middleware for resource-constrained, distributed systems","S. Malek; M. Mikic-Rakic; N. Medvidovic","Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","3","256","272","A recent emergence of small, resource-constrained, and highly mobile computing platforms presents numerous new challenges for software developers. We refer to development in this new setting as programming-in-the-small-and-many (Prism). This paper provides a description and evaluation of Prism-MW, a middleware platform intended to support software architecture-based development in the Prism setting. Prism-MW provides efficient and scalable implementation-level support for the key aspects of Prism application architectures, including their architectural styles. Additionally, Prism-MW is extensible to support different application requirements suitable for the Prism setting. Prism-MW has been applied in a number of applications and used as an educational tool in graduate-level software architecture and embedded systems courses. Recently, Prism-MW has been successfully evaluated by a major industrial organization for use in one of their key distributed embedded systems. Our experience with the middleware indicates that the principles of architecture-based software development can be successfully, and flexibly, applied in the Prism setting.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423996","Index Terms- Software architecture;architectural style;middleware;Prism-MW.","Middleware;Computer architecture;Software architecture;Application software;Software systems;Guidelines;Mobile computing;Embedded system;Software engineering;Connectors","software architecture;middleware","mobile computing;Prism-middleware platform;software architecture;distributed embedded systems;software development;resource constraints","","51","","42","","","","","","IEEE","IEEE Journals & Magazines"
"System Test Planning of Software: An Optimization Approach","K. Chari; A. Hevner","Department of Information Systems and Decision Sciences, University of South Florida, 4202 East Fowler Avenue, CIS 1040, Tampa, FL 33620-7800; Department of Information Systems and Decision Sciences, University of South Florida, 4202 East Fowler Avenue, CIS 1040, Tampa, FL 33620-7800","IEEE Transactions on Software Engineering","","2006","32","7","503","5099","This paper extends an exponential reliability growth model to determine the optimal number of test cases to be executed for various use case scenarios during the system testing of software. An example demonstrates a practical application of the optimization model for system test planning","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677535","Nonlinear programming;reliability;test management.","Software testing;System testing;Software reliability;Software systems;Cost function;Software quality;Application software;Quality control;Software maintenance;Law","optimisation;program testing;software reliability","exponential reliability growth model;test case;use case scenario;system test planning;optimization;nonlinear programming;software testing","","7","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Work Item Tagging: Communicating Concerns in Collaborative Software Development","C. Treude; M. Storey","University of Victoria, Victoria; University of Victoria, Victoria","IEEE Transactions on Software Engineering","","2012","38","1","19","34","In collaborative software development projects, work items are used as a mechanism to coordinate tasks and track shared development work. In this paper, we explore how “tagging,” a lightweight social computing mechanism, is used to communicate matters of concern in the management of development tasks. We present the results from two empirical studies over 36 and 12 months, respectively, on how tagging has been adopted and what role it plays in the development processes of several professional development projects with more than 1,000 developers in total. Our research shows that the tagging mechanism was eagerly adopted by the teams, and that it has become a significant part of many informal processes. Different kinds of tags are used by various stakeholders to categorize and organize work items. The tags are used to support finding of tasks, articulation work, and information exchange. Implicit and explicit mechanisms have evolved to manage the tag vocabulary. Our findings indicate that lightweight informal tool support, prevalent in the social computing domain, may play an important role in improving team-based software development practices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5611552","Tagging;collaboration;software development;task management;articulation work;work items.","Tagging;Programming;Collaboration;Software engineering;Data mining","groupware;information retrieval;software development management","work item tagging;collaborative software development project;social computing mechanism;tag vocabulary;lightweight informal tool support;team-based software development practice","","23","","44","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Classifier Configuration and Classifier Combination on Bug Localization","S. W. Thomas; M. Nagappan; D. Blostein; A. E. Hassan","Queen's University, Kingston; Queen's University, Kingston; Queen's University, Kingston; Queen's University, Kingston","IEEE Transactions on Software Engineering","","2013","39","10","1427","1443","Bug localization is the task of determining which source code entities are relevant to a bug report. Manual bug localization is labor intensive since developers must consider thousands of source code entities. Current research builds bug localization classifiers, based on information retrieval models, to locate entities that are textually similar to the bug report. Current research, however, does not consider the effect of classifier configuration, i.e., all the parameter values that specify the behavior of a classifier. As such, the effect of each parameter or which parameter values lead to the best performance is unknown. In this paper, we empirically investigate the effectiveness of a large space of classifier configurations, 3,172 in total. Further, we introduce a framework for combining the results of multiple classifier configurations since classifier combination has shown promise in other domains. Through a detailed case study on over 8,000 bug reports from three large-scale projects, we make two main contributions. First, we show that the parameters of a classifier have a significant impact on its performance. Second, we show that combining multiple classifiers--whether those classifiers are hand-picked or randomly chosen relative to intelligently defined subspaces of classifiers--improves the performance of even the best individual classifiers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6520844","Software maintenance;bug localization;information retrieval;VSM;LSI;LDA;classifier combination","Large scale integration;Measurement;Vectors;Information retrieval;Matrix decomposition;Indexes;Resource management","information retrieval;pattern classification;program debugging","classifier configuration;classifier combination;source code entity determination;bug report;bug localization classifiers;information retrieval models;parameter value;classifier parameter","","31","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Input Domain Reduction through Irrelevant Variable Removal and Its Effect on Local, Global, and Hybrid Search-Based Structural Test Data Generation","P. McMinn; M. Harman; K. Lakhotia; Y. Hassoun; J. Wegener","University of Sheffield, Sheffield; University College London, London; University College London, London; King's College London, London; Berner & Mattner Systemtechnik GmbH, Berlin","IEEE Transactions on Software Engineering","","2012","38","2","453","477","Search-Based Test Data Generation reformulates testing goals as fitness functions so that test input generation can be automated by some chosen search-based optimization algorithm. The optimization algorithm searches the space of potential inputs, seeking those that are “fit for purpose,” guided by the fitness function. The search space of potential inputs can be very large, even for very small systems under test. Its size is, of course, a key determining factor affecting the performance of any search-based approach. However, despite the large volume of work on Search-Based Software Testing, the literature contains little that concerns the performance impact of search space reduction. This paper proposes a static dependence analysis derived from program slicing that can be used to support search space reduction. The paper presents both a theoretical and empirical analysis of the application of this approach to open source and industrial production code. The results provide evidence to support the claim that input domain reduction has a significant effect on the performance of local, global, and hybrid search, while a purely random search is unaffected.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5710949","Search-based software testing;evolutionary testing;automated test data generation;input domain reduction.","Input variables;Software testing;Optimization;Algorithm design and analysis;Search problems;Software algorithms","automatic test pattern generation;optimisation;program compilers;program slicing;program testing;public domain software;search problems","input domain reduction;irrelevant variable removal;hybrid search-based structural test data generation;fitness functions;test input generation;search-based optimization algorithm;key determining factor;search-based software testing;search space reduction;static dependence analysis;program slicing;open source approach;industrial production code","","26","","52","","","","","","IEEE","IEEE Journals & Magazines"
"A ranking of software engineering measures based on expert opinion","Ming Li; C. S. Smidts","Center for Reliability Eng., Maryland Univ., College Park, MD, USA; Center for Reliability Eng., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","2003","29","9","811","824","This research proposes a framework based on expert opinion elicitation, developed to select the software engineering measures which are the best software reliability indicators. The current research is based on the top 30 measures identified in an earlier study conducted by Lawrence Livermore National Laboratory. A set of ranking criteria and their levels were identified. The score of each measure for each ranking criterion was elicited through expert opinion and then aggregated into a single score using multiattribute utility theory. The basic aggregation scheme selected was a linear additive scheme. A comprehensive sensitivity analysis was carried out. The sensitivity analysis included: variation of the ranking criteria levels, variation of the weights, variation of the aggregation schemes. The top-ranked measures were identified. Use of these measures in each software development phase can lead to a more reliable quantitative prediction of software reliability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232286","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232286","","Software engineering;Software measurement;Software reliability;Analysis of variance;Sensitivity analysis;Programming;Software quality;Density measurement;Current measurement;Laboratories","software reliability;software metrics;sensitivity analysis","expert opinion elicitation;software engineering measures;best software reliability indicators;ranking criteria;multiattribute utility theory;basic aggregation scheme;linear additive scheme;comprehensive sensitivity analysis;software development phase;software reliability","","39","","41","","","","","","IEEE","IEEE Journals & Magazines"
"A Practical Approach to Size Estimation of Embedded Software Components","K. Lind; R. Heldal","Saab Automobile AB, Trollhättan; Chalmers University of Technology, Göteborg","IEEE Transactions on Software Engineering","","2012","38","5","993","1007","To estimate software code size early in the development process is important for developing cost-efficient embedded systems. We have applied the COSMIC Functional Size Measurement (FSM) method for size estimation of embedded software components in the automotive industry. Correlational studies were conducted using data from two automotive companies. The studies show strong correlation between functional size and software code size, which is important for obtaining accurate estimation results. This paper presents the characteristics and results of our work, and aims to provide a practical framework for how to use COSMIC FSM for size estimation purposes. We investigate the results from our earlier correlational studies, and conduct further studies to identify such a framework. Based on these activities, we conclude that a clear purpose of the estimation process, a well-defined domain allowing categorization of software, consistent content and quality of requirements, and historical data from implemented software are key factors for size estimation of embedded software components.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5999672","Real-time and embedded systems;software product metrics;COSMIC FSM;software components","Software;Estimation;Vehicles;Size measurement;Automotive engineering;Industries;Memory management","automotive engineering;object-oriented programming;production engineering computing;software cost estimation;software metrics","software code size estimation;embedded software components;development process;cost-efficient embedded systems;COSMIC functional size measurement method;FSM method;automotive industry;automotive companies;domain allowing categorization;consistent content;requirements quality;historical data;software product metrics","","5","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Aligning Qualitative, Real-Time, and Probabilistic Property Specification Patterns Using a Structured English Grammar","M. Autili; L. Grunske; M. Lumpe; P. Pelliccione; A. Tang","Dipartimento di Ingegneria e Scienze dell’Informazione e Matematica, Università dell’Aquila, Aquila, Italy; Institute of Software Technology, University of Stuttgart, Stuttgart, Germany; Swinburne University of Technology, Hawthorn, Australia; Dipartimento di Ingegneria e Scienze dell’Informazione e Matematica, Università dell’Aquila, Aquila, Italy; Swinburne University of Technology, Hawthorn, Australia","IEEE Transactions on Software Engineering","","2015","41","7","620","638","Formal methods offer an effective means to assert the correctness of software systems through mathematical reasoning. However, the need to formulate system properties in a purely mathematical fashion can create pragmatic barriers to the application of these techniques. For this reason, Dwyer et al. invented property specification patterns which is a system of recurring solutions to deal with the temporal intricacies that would make the construction of reactive systems very hard otherwise. Today, property specification patterns provide general rules that help practitioners to qualify order and occurrence, to quantify time bounds, and to express probabilities of events. Nevertheless, a comprehensive framework combining qualitative, real-time, and probabilistic property specification patterns has remained elusive. The benefits of such a framework are twofold. First, it would remove the distinction between qualitative and quantitative aspects of events; and second, it would provide a structure to systematically discover new property specification patterns. In this paper, we report on such a framework and present a unified catalogue that combines all known plus 40 newly identified or extended patterns. We also offer a natural language front-end to map patterns to a temporal logic of choice. To demonstrate the virtue of this new framework, we applied it to a variety of industrial requirements, and use PSPWizard, a tool specifically developed to work with our unified pattern catalogue, to automatically render concrete instances of property specification patterns to formulae of an underlying temporal logic of choice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2398877","PRESTO; European Commission; DFG; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7029714","Specification Patterns;Real-time Properties;Probabilistic Properties;Specification patterns;real-time properties;probabilistic properties","Probabilistic logic;Real-time systems;Natural languages;Software;Grammar;Electronic mail;Educational institutions","formal specification;natural language processing;probability;temporal logic","qualitative property specification pattern;real-time property specification pattern;probabilistic property specification pattern;structured English grammar;formal methods;software system correctness;mathematical reasoning;temporal intricacies;order qualification;occurrence qualification;time bound quantification;event probability;event qualitative aspect;event quantitative aspect;natural language front-end;pattern mapping;temporal logic;PSPWizard;unified pattern catalogue","","19","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Behavior protocols for software components","F. Plasil; S. Visnovsky","Inst. of Comput. Sci., Acad. of Sci. of the Czech Republic, Prague, Czech Republic; Inst. of Comput. Sci., Acad. of Sci. of the Czech Republic, Prague, Czech Republic","IEEE Transactions on Software Engineering","","2002","28","11","1056","1076","In this paper, we propose a means to enhance an architecture description language with a description of component behavior. A notation used for this purpose should be able to express the ""interplay"" on the component's interfaces and reflect step-by-step refinement of the component's specification during its design. In addition, the notation should be easy to comprehend and allow for formal reasoning about the correctness of the specification refinement and also about the correctness of an implementation in terms of whether it adheres to the specification. Targeting all these requirements together, the paper proposes employing behavior protocols which are based on a notation similar to regular expressions. As proof of the concept, the behavior protocols are used in the SOFA architecture description language at three levels: interface, frame, and architecture. Key achievements of this paper include the definitions of bounded component behavior and protocol conformance relation. Using these concepts, the designer can verify the adherence of a component's implementation to its specification at runtime, while the correctness of refining the specification can be verified at design time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049404","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049404","","Access protocols;Architecture description languages;Runtime;Software reusability;Computer Society;Software architecture;Programming profession;Object oriented programming;Object oriented modeling;Automata","object-oriented programming;formal specification;software architecture;software reusability","software component behavior;component interfaces;formal reasoning;correctness;component specification refinement;behavior protocols;regular expressions;SOFA architecture description language;architecture;frame;bounded component behavior;protocol conformance relation","","156","","33","","","","","","IEEE","IEEE Journals & Magazines"
"OBEY: Optimal Batched Refactoring Plan Execution for Class Responsibility Redistribution","H. C. Jiau; L. W. Mar; J. C. Chen","National Cheng Kung University, Tainan; National Cheng Kung University, Tainan; National Cheng Kung University, Tainan","IEEE Transactions on Software Engineering","","2013","39","9","1245","1263","The redistribution of class responsibilities is a common reengineering practice in object-oriented (OO) software evolution. During the redistribution, developers frequently construct batched refactoring plans for moving multiple methods and fields among various classes. With an objective of carefully maintaining the cohesion and coupling degree of the class design, executing a batched refactoring plan without introducing any objective-violating side effect into the refactored code is essential. However, using most refactoring engines for batched refactoring plan execution introduces coupling-increasing Middle Man bad smell in the final refactored code and therefore makes the refactoring execution suboptimal in achieving the redistribution objective. This work proposes Obey, a methodology for optimal batched refactoring plan execution. Obey analyzes a batched refactoring plan, identifies Middle Man symptoms that cause suboptimal execution, and renovates the plan for optimal execution. We have conducted an empirical study on three open-source software projects to confirm the effectiveness of Obey in a practical context.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6493333","Reengineering;class responsibility redistribution;batched refactoring execution;change impact analysis;optimization","Couplings;Engines;Software systems;Measurement;Optimization;Context","object-oriented programming;software maintenance","OBEY methodology;optimal batched refactoring plan execution;class responsibility redistribution;software reengineering practice;object-oriented software evolution;OO software evolution;refactoring engine;coupling-increasing middle man bad smell;open-source software project","","3","","74","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis of Restart Mechanisms in Software Systems","A. P. A. van Moorsel; K. Wolter","NA; NA","IEEE Transactions on Software Engineering","","2006","32","8","547","558","Restarts or retries are a common phenomenon in computing systems, for instance, in preventive maintenance, software rejuvenation, or when a failure is suspected. Typically, one sets a time-out to trigger the restart. We analyze and optimize time-out strategies for scenarios in which the expected required remaining time of a task is not always decreasing with the time invested in it. Examples of such tasks include the download of Web pages, randomized algorithms, distributed queries, and jobs subject to network or other failures. Assuming the independence of the completion time of successive tries, we derive computationally attractive expressions for the moments of the completion time, as well as for the probability that a task is able to meet a deadline. These expressions facilitate efficient algorithms to compute optimal restart strategies and are promising candidates for pragmatic online optimization of restart timers","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703386","Restart;software rejuvenation;time-out;fault-tolerant systems;performance and reliability modeling;completion time;adaptive systems;self-management.","Software systems;Internet;Failure analysis;Preventive maintenance;Software maintenance;Web pages;Software performance;Fault tolerant systems;Computer network reliability;Adaptive systems","software fault tolerance;software maintenance;system recovery","software system restart mechanism analysis;software preventive maintenance;software rejuvenation;software failure;software time-out strategy analysis;optimal restart strategy;fault-tolerant system;software performance modeling;software reliability modeling","","33","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Visualizing Co-Change Information with the Evolution Radar","M. D'Ambros; M. Lanza; M. Lungu","University of Lugano, Lugano; University of Lugano, Lugano; University of Lugano, Lugano","IEEE Transactions on Software Engineering","","2009","35","5","720","735","Software evolution analysis provides a valuable source of information that can be used both to understand a system's design and predict its future development. While for many program comprehension purposes, it is sufficient to model a single version of a system, there are types of information that can only be recovered when the history of a system is taken into account. Logical coupling, the implicit dependency between software artifacts that have been changed together, is an example of such information. Previous research has dealt with low-level couplings between files, leading to an explosion of the data to be analyzed, or has abstracted the logical couplings to the level of modules, leading to a loss of detailed information. In this paper, we present a visualization-based approach that integrates logical coupling information at different levels of abstraction. This facilitates an in-depth analysis of the logical couplings, and at the same time, leads to a characterization of a system's modules in terms of their logical coupling. The presented approach supports the retrospective analysis of a software system and maintenance activities such as restructuring and redocumentation. We illustrate retrospective analysis on two large open-source software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815274","Software evolution;software visualization;logical coupling.","Radar;Information analysis;Software systems;Information resources;System analysis and design;History;Explosions;Data analysis;Data visualization;Software maintenance","data visualisation;software engineering;systems re-engineering","evolution radar;software evolution analysis;software artifacts;visualization;logical coupling information;abstraction;system module;open source software system analysis","","47","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Using Mutation Analysis for Assessing and Comparing Testing Coverage Criteria","J. H. Andrews; L. C. Briand; Y. Labiche; A. S. Namin","Computer Science Department, University of Western Ontario, London, Ontario, Canada; Simula Research Laboratory, Department of Software Engineering, Fornebu, Lysaker, Norway; Software Quality Engineering Laboratory, Carleton University, Ottawa, Ontario, Canada; Computer Science Department, University of Western Ontario, London, Ontario, Canada","IEEE Transactions on Software Engineering","","2006","32","8","608","624","The empirical assessment of test techniques plays an important role in software testing research. One common practice is to seed faults in subject software, either manually or by using a program that generates all possible mutants based on a set of mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults, thus facilitating the statistical analysis of fault detection effectiveness of test suites; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. Focusing on four common control and data flow criteria (block, decision, C-use, and P-use), this paper investigates this important issue based on a middle size industrial program with a comprehensive pool of test cases and known faults. Based on the data available thus far, the results are very consistent across the investigated criteria as they show that the use of mutation operators is yielding trustworthy results: generated mutants can be used to predict the detection effectiveness of real faults. Applying such a mutation analysis, we then investigate the relative cost and effectiveness of the above-mentioned criteria by revisiting fundamental questions regarding the relationships between fault detection, test suite size, and control/data flow coverage. Although such questions have been partially investigated in previous studies, we can use a large number of mutants, which helps decrease the impact of random variation in our analysis and allows us to use a different analysis approach. Our results are then; compared with published studies, plausible reasons for the differences are provided, and the research leads us to suggest a way to tune the mutation analysis process to possible differences in fault detection probabilities in a specific environment","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.83","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703390","Testing and debugging;testing strategies;test coverage of code;experimental design.","Genetic mutations;Fault detection;Software testing;Size control;Failure analysis;Statistical analysis;System testing;Industrial control;Costs;Debugging","data flow analysis;design of experiments;probability;program control structures;program debugging;program testing;software fault tolerance","mutation analysis;software testing coverage criteria;empirical assessment;software fault detection probability;industrial program;program control criteria;program data flow criteria;mutation operators;program debugging;experimental design","","192","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A methodological framework for viewpoint-oriented conceptual modeling","J. Andrade; J. Ares; R. Garcia; J. Pazos; S. Rodriguez; A. Silva","Fac. de Informatica, Univ. da Coruna, Spain; Fac. de Informatica, Univ. da Coruna, Spain; Fac. de Informatica, Univ. da Coruna, Spain; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","5","282","294","To solve any nontrivial problem, it first needs to be conceptualized, taking into account the individual who has the problem. However, a problem is generally associated with more than one individual, as is usually the case in software development. Therefore, this process has to take into account different viewpoints about the problem and any discrepancies that could arise as a result. Traditionally, conceptualization in software engineering has omitted the different viewpoints that the individuals may have of the problem and has inherently enforced consistency in the event of any discrepancies, which are considered as something to be systematically rejected. The paper presents a methodological framework that explicitly drives the conceptualization of different viewpoints and manages the different types of discrepancies that arise between them, which become really important in the process. The definition of this framework is generic, and it is therefore independent of any particular software development paradigm. Its application to software engineering means that viewpoints and their possible discrepancies can be considered in the software process conceptual modeling phase. This application is illustrated by means of what is considered to be a standard problem: the IFIP case.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291832","Conceptual modeling;multiple viewpoint;discrepancies;conflicts;inconsistencies;methodological framework.","Programming;Application software;Physics;Software engineering;Drives;Humans;Diffraction;Optical refraction;Optical reflection","software engineering","nontrivial problem;software development;software engineering;IFIP case;viewpoint-oriented conceptual modeling","","25","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Shallow knowledge as an aid to deep understanding in early phase requirements engineering","P. Sawyer; P. Rayson; K. Cosh","Dept. of Comput., Lancaster Univ., UK; Dept. of Comput., Lancaster Univ., UK; NA","IEEE Transactions on Software Engineering","","2005","31","11","969","981","Requirements engineering's continuing dependence on natural language description has made it the focus of several efforts to apply language engineering techniques. The raw textual material that forms an input to early phase requirements engineering and which informs the subsequent formulation of the requirements is inevitably uncontrolled and this makes its processing very hard. Nevertheless, sufficiently robust techniques do exist that can be used to aid the requirements engineer provided that the scope of what can be achieved is understood. In this paper, we show how combinations of lexical and shallow semantic analysis techniques developed from corpus linguistics can help human analysts acquire the deep understanding needed as the first step towards the synthesis of requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.129","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556555","Index Terms- Specification;elicitation methods;tools;linguistic processing;document analysis.","Knowledge engineering;Humans;Computer Society;Frequency;Natural languages;Information analysis;Speech analysis;Raw materials;Robustness;Text analysis","formal specification;formal verification;computational linguistics","requirements engineering;natural language description;language engineering technique;lexical analysis;shallow semantic analysis;corpus linguistics;specification method;elicitation method;linguistic processing;document analysis","","47","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Does code decay? Assessing the evidence from change management data","S. G. Eick; T. L. Graves; A. F. Karr; J. S. Marron; A. Mockus","Lucent Technol. Bell Labs., Naperville, IL, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","1","1","12","A central feature of the evolution of large software systems is that change-which is necessary to add new functionality, accommodate new hardware, and repair faults-becomes increasingly difficult over time. We approach this phenomenon, which we term code decay, scientifically and statistically. We define code decay and propose a number of measurements (code decay indices) on software and on the organizations that produce it, that serve as symptoms, risk factors, and predictors of decay. Using an unusually rich data set (the fifteen-plus year change history of the millions of lines of software for a telephone switching system), we find mixed, but on the whole persuasive, statistical evidence of code decay, which is corroborated by developers of the code. Suggestive indications that perfective maintenance can retard code decay are also discussed.","0098-5589;1939-3520;2326-3881","","10.1109/32.895984","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=895984","","Software maintenance;Software systems;Hardware;Telephony;History;Statistical analysis;Computer Society;Software measurement;Switching systems;Operating systems","software maintenance;software metrics;management of change;statistical analysis","code decay;change management data;large software systems evolution;software measurements;risk factors;data set;telephone switching system;perfective maintenance;software maintenance;statistical analysis","","254","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic Testing for Deadlocks via Constraints","Y. Cai; Q. Lu","State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Technology Center of Software Engineering, Institute of Software, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Software Engineering","","2016","42","9","825","842","Existing deadlock detectors are either not scalable or may report false positives when suggesting cycles as potential deadlocks. Additionally, they may not effectively trigger deadlocks and handle false positives. We propose a technique called ConLock<sup>+</sup>, which firstly analyzes each cycle and its corresponding execution to identify a set of scheduling constraints that are necessary conditions to trigger the corresponding deadlock. The ConLock<sup>+</sup>technique then performs a second run to enforce the set of constraints, which will trigger a deadlock if the cycle is a real one. Or if not, ConLock<sup>+</sup>reports a steering failure for that cycle and also identifies other similar cycles which would also produce steering failures. For each confirmed deadlock, ConLock<sup>+</sup>performs a static analysis to identify conflicting memory access that would also contribute to the occurrence of the deadlock. This analysis is helpful to enable developers to understand and fix deadlocks. ConLock<sup>+</sup>has been validated on a suite of real-world programs with 16 real deadlocks. The results show that across all 811 cycles, ConLock<sup>+</sup>confirmed all of the 16 deadlocks with a probability of ≥80 percent. For the remaining cycles, ConLock<sup>+</sup>reported steering failures and also identified that five deadlocks also involved conflicting memory accesses.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2537335","National Basic Research (973) Program of China; National Science Foundation of China (NSFC); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7423814","Deadlock triggering;scheduling;should-happen-before relation;constraint;reliability;verification","System recovery;Instruction sets;Schedules;Testing;Synchronization;Detectors;Probabilistic logic","concurrency control;program diagnostics;program testing;scheduling","static analysis;memory access;real-world programs;steering failure;scheduling constraints;ConLock+;deadlock detectors;dynamic testing","","6","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Model Checking Probabilistic and Stochastic Extensions of the π-Calculus","G. Norman; C. Palamidessi; D. Parker; P. Wu","Oxford University, Oxford; INRIA Saclay and École Polytechnique, Paris; Oxford University, Oxford; University College London, Ipswich","IEEE Transactions on Software Engineering","","2009","35","2","209","223","We present an implementation of model checking for probabilistic and stochastic extensions of the pi-calculus, a process algebra which supports modelling of concurrency and mobility. Formal verification techniques for such extensions have clear applications in several domains, including mobile ad-hoc network protocols, probabilistic security protocols and biological pathways. Despite this, no implementation of automated verification exists. Building upon the pi-calculus model checker MMC, we first show an automated procedure for constructing the underlying semantic model of a probabilistic or stochastic pi-calculus process. This can then be verified using existing probabilistic model checkers such as PRISM. Secondly, we demonstrate how for processes of a specific structure a more efficient, compositional approach is applicable, which uses our extension of MMC on each parallel component of the system and then translates the results into a high-level modular description for the PRISM tool. The feasibility of our techniques is demonstrated through a number of case studies from the pi-calculus literature.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4626962","Model checking;Markov processes;Stochastic processes;Model checking;Markov processes;Stochastic processes","Stochastic processes;Biological system modeling;Protocols;Stochastic systems;Calculus;Algebra;Formal verification;Mobile ad hoc networks;Mobile communication;Communication system security","formal verification;pi calculus;probability;stochastic processes","model checking;probabilisty;stochastic extension;pi-calculus;process algebra;formal verification;mobile ad-hoc network protocol;probabilistic security protocol;biological pathway;semantic model;high-level modular description","","15","","43","","","","","","IEEE","IEEE Journals & Magazines"
"A Feature-Based Classification of Model Repair Approaches","N. Macedo; T. Jorge; A. Cunha","High-Assurance Software Laboratory (HASLab)INESC TEC; European Space Agency (ESA), Paris, France; High-Assurance Software Laboratory (HASLab)INESC TEC","IEEE Transactions on Software Engineering","","2017","43","7","615","640","Consistency management, the ability to detect, diagnose and handle inconsistencies, is crucial during the development process in Model-driven Engineering (MDE). As the popularity and application scenarios of MDE expanded, a variety of different techniques were proposed to address these tasks in specific contexts. Of the various stages of consistency management, this work focuses on inconsistency handling in MDE, particularly in model repair techniques. This paper proposes a feature-based classification system for model repair techniques, based on an systematic literature review of the area. We expect this work to assist developers and researchers from different disciplines in comparing their work under a unifying framework, and aid MDE practitioners in selecting suitable model repair approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2620145","North Portugal Regional Operational Programme; European Regional Development Fund (ERDF); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7605502","Model-driven engineering, consistency management, inconsistency handling, model repair","Maintenance engineering;Unified modeling language;Taxonomy;Context;Feature extraction;Software engineering;Systematics","pattern classification;software maintenance","model repair approach;consistency management;model-driven engineering;MDE;feature-based classification system","","1","","91","","","","","","IEEE","IEEE Journals & Magazines"
"An Industrial Survey of Safety Evidence Change Impact Analysis Practice","J. L. de la Vara; M. Borg; K. Wnuk; L. Moonen","Computer Science Department, Carlos III University of Madrid, Avda. de la Universidad 30, 28911 Leganes, Madrid, Spain; Software and Systems Laboratory, SICS Swedish ICT AB, Ideon Science Park, Building Beta 2, Scheelevägen 17, Lund, Sweden; Software Engineering Research Lab, Blekinge Institute of Technology, Karlskrona, Sweden; Certus Centre for Software V&V, Simula Research Laboratory, P.O. Box 134, Lysaker, Norway","IEEE Transactions on Software Engineering","","2016","42","12","1095","1117","Context. In many application domains, critical systems must comply with safety standards. This involves gathering safety evidence in the form of artefacts such as safety analyses, system specifications, and testing results. These artefacts can evolve during a system's lifecycle, creating a need for change impact analysis to guarantee that system safety and compliance are not jeopardised. Objective. We aim to provide new insights into how safety evidence change impact analysis is addressed in practice. The knowledge about this activity is limited despite the extensive research that has been conducted on change impact analysis and on safety evidence management. Method. We conducted an industrial survey on the circumstances under which safety evidence change impact analysis is addressed, the tool support used, and the challenges faced. Results. We obtained 97 valid responses representing 16 application domains, 28 countries, and 47 safety standards. The respondents had most often performed safety evidence change impact analysis during system development, from system specifications, and fully manually. No commercial change impact analysis tool was reported as used for all artefact types and insufficient tool support was the most frequent challenge. Conclusion. The results suggest that the different artefact types used as safety evidence co-evolve. In addition, the evolution of safety cases should probably be better managed, the level of automation in safety evidence change impact analysis is low, and the state of the practice can benefit from over 20 improvement areas.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2553032","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7450627","Safety-critical system;safety evidence;change impact analysis;state of the practice;survey research","Safety;Market research;Best practices;Industries;Standards;Certification","safety-critical software;software standards","industrial survey;safety evidence change impact analysis;SECIA;safety-critical system;safety standard","","16","","71","","","","","","IEEE","IEEE Journals & Magazines"
"Pert: The Application-Aware Tailoring of Java Object Persistence","P. Liu; C. Zhang","Hong Kong University of Science and Technology, Hong Kong; Hong Kong University of Science and Technology, Hong Kong","IEEE Transactions on Software Engineering","","2012","38","4","909","922","Persistence is a widely used technique which allows the objects that represent the results of lengthy computations to outlive the process that creates it in order to considerably speed up subsequent program executions. We observe that conventional persistence techniques usually do not consider the application contexts of the persistence operations, where not all of the object states need to be persisted. Leveraging this observation, we have designed and implemented a framework called Pert, which first performs static program analysis to estimate the actual usage of the persisted object, given the context of its usage in the program. The Pert runtime uses the statically computed information to efficiently make tailoring decisions to prune the redundant and unused object states during the persistence operations. Our evaluation result shows that the Pert-based optimization can speed up the conventional persistence operations by 1 to 45 times. The amount of persisted data is also dramatically reduced, as the result of the application-aware tailoring.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5963692","Object persistence;program analysis;performance optimization","Runtime;Anodes;Optimization;Context;Libraries;Java;Algorithm design and analysis","Java;optimisation","application aware tailoring;Java object persistence;lengthy computations;subsequent program executions;persistence techniques;static program analysis;Pert based optimization","","","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Metamorphic Testing of RESTful Web APIs","S. Segura; J. A. Parejo; J. Troya; A. Ruiz-Cortés","Department of Computer Languages and Systems, Universidad de Sevilla, Sevilla, Spain; Department of Computer Languages and Systems, Universidad de Sevilla, Sevilla, Spain; Department of Computer Languages and Systems, Universidad de Sevilla, Sevilla, Spain; Department of Computer Languages and Systems, Universidad de Sevilla, Sevilla, Spain","IEEE Transactions on Software Engineering","","2018","44","11","1083","1099","Web Application Programming Interfaces (APIs) allow systems to interact with each other over the network. Modern Web APIs often adhere to the REST architectural style, being referred to as RESTful Web APIs. RESTful Web APIs are decomposed into multiple resources (e.g., a video in the YouTube API) that clients can manipulate through HTTP interactions. Testing Web APIs is critical but challenging due to the difficulty to assess the correctness of API responses, i.e., the oracle problem. Metamorphic testing alleviates the oracle problem by exploiting relations (so-called metamorphic relations) among multiple executions of the program under test. In this paper, we present a metamorphic testing approach for the detection of faults in RESTful Web APIs. We first propose six abstract relations that capture the shape of many of the metamorphic relations found in RESTful Web APIs, we call these Metamorphic Relation Output Patterns (MROPs). Each MROP can then be instantiated into one or more concrete metamorphic relations. The approach was evaluated using both automatically seeded and real faults in six subject Web APIs. Among other results, we identified 60 metamorphic relations (instances of the proposed MROPs) in the Web APIs of Spotify and YouTube. Each metamorphic relation was implemented using both random and manual test data, running over 4.7K automated tests. As a result, 11 issues were detected (3 in Spotify and 8 in YouTube), 10 of them confirmed by the API developers or reproduced by other users, supporting the effectiveness of the approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2764464","European Commission (FEDER) and Spanish Government; Andalusian Government project COPAS; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8074764","Metamorphic testing;REST;RESTful Web services;web API","Testing;YouTube;Web services;Companies;Standards;Manuals;Indexes","application program interfaces;hypermedia;program testing;social networking (online);transport protocols;Web services","metamorphic relation output pattern;web application programming interfaces;RESTful web API;REST architectural style;HTTP interaction;oracle problem;abstract relation;MROP;YouTube;Spotify","","","","63","","","","","","IEEE","IEEE Journals & Magazines"
"CTK: configurable object abstractions for multiprocessors","D. M. Silva; K. Schwan; G. Eisenhauer","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","6","531","549","The Configuration Toolkit (CTK) is a library for constructing configurable object based abstractions that are part of multiprocessor programs or operating systems. The library is unique in its exploration of runtime configuration for attaining performance improvements: 1) its programming model facilitates the expression and implementation of program configuration; and 2) its efficient runtime support enables performance improvements by the configuration of program components during their execution. Program configuration is attained without compromising the encapsulation or the reuse of software abstractions. CTK programs are configured using attributes associated with object classes, object instances, state variables, operations, and object invocations. At runtime, such attributes are interpreted by policy classes, which may be varied separately from the abstractions with which they are associated. Using policies and attributes, an object's runtime behavior may be varied by: 1) changing its performance or reliability while preserving the implementation of its functional behavior, or 2) changing the implementation of its internal computational strategy. CTK's multiprocessor implementation is layered on a Cthreads-compatible programming library, which results in its portability to a wide variety of uni- and multiprocessor machines, including a Kendall Square KSR-2 Supercomputer, SGI machines, various SUN workstations, and as a native kernel on the GP1000 BBN Butterfly multiprocessor. The platforms evaluated in the paper are the KSR and SGI machines.","0098-5589;1939-3520;2326-3881","","10.1109/32.926175","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=926175","","Operating systems;Quality of service;Contracts;Runtime library;Dynamic programming;Resource management;Performance gain;Encapsulation;Supercomputers;Sun","bibliographies;configuration management;object-oriented programming;multiprocessing programs;software performance evaluation;software reusability;software libraries;multiprocessing systems","configurable object abstractions;operating systems;Configuration Toolkit;CTK library;multiprocessor programs;runtime configuration;performance improvements;programming model;program configuration;runtime support;program component configuration;software abstractions;CTK programs;object classes;object instances;state variables;object invocations;policy classes;runtime behavior;functional behavior;internal computational strategy;multiprocessor implementation;Cthreads-compatible programming library;multiprocessor machines;Kendall Square KSR-2 Supercomputer;SGI machines;SUN workstations;native kernel","","","","57","","","","","","IEEE","IEEE Journals & Magazines"
"QoS Assurance for Dynamic Reconfiguration of Component-Based Software Systems","W. Li","Central Quneensland University, Rockhampton","IEEE Transactions on Software Engineering","","2012","38","3","658","676","A major challenge of dynamic reconfiguration is Quality of Service (QoS) assurance, which is meant to reduce application disruption to the minimum for the system's transformation. However, this problem has not been well studied. This paper investigates the problem for component-based software systems from three points of view. First, the whole spectrum of QoS characteristics is defined. Second, the logical and physical requirements for QoS characteristics are analyzed and solutions to achieve them are proposed. Third, prior work is classified by QoS characteristics and then realized by abstract reconfiguration strategies. On this basis, quantitative evaluation of the QoS assurance abilities of existing work and our own approach is conducted through three steps. First, a proof-of-concept prototype called the reconfigurable component model is implemented to support the representation and testing of the reconfiguration strategies. Second, a reconfiguration benchmark is proposed to expose the whole spectrum of QoS problems. Third, each reconfiguration strategy is tested against the benchmark and the testing results are evaluated. The most important conclusion from our investigation is that the classified QoS characteristics can be fully achieved under some acceptable constraints.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5740932","Change management;componentware;dynamic reconfiguration;modeling the QoS assurance process;system evolution.","Quality of service;Encryption;Protocols;Connectors;Receivers;Benchmark testing","quality of service;software quality","QoS assurance;dynamic reconfiguration;component based software systems;Quality of Service;application disruption;physical requirements;logical requirements;abstract reconfiguration;quantitative evaluation;reconfigurable component;reconfiguration benchmark","","19","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Aspect-Oriented Refactoring of Legacy Applications: An Evaluation","M. Mortensen; S. Ghosh; J. Bieman","Google, Boulder; Colorado State University, Fort Collins; Colorado State University, Fort Collins","IEEE Transactions on Software Engineering","","2012","38","1","118","140","The primary claimed benefits of aspect-oriented programming (AOP) are that it improves the understandability and maintainability of software applications by modularizing crosscutting concerns. Before there is widespread adoption of AOP, developers need further evidence of the actual benefits as well as costs. Applying AOP techniques to refactor legacy applications is one way to evaluate costs and benefits. We replace crosscutting concerns with aspects in three industrial applications to examine the effects on qualities that affect the maintainability of the applications. We study several revisions of each application, identifying crosscutting concerns in the initial revision and also crosscutting concerns that are added in later revisions. Aspect-oriented refactoring reduced code size and improved both change locality and concern diffusion. Costs include the effort required for application refactoring and aspect creation, as well as a decrease in performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.109","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5661792","Aspect-oriented programming;crosscutting concerns;legacy systems;refactoring;maintainability.","Software measurement;Maintenance engineering;Legacy systems;Java;Programming;Aspect-oriented programming","aspect-oriented programming;software maintenance","aspect-oriented refactoring;legacy applications;primary claimed benefits;aspect-oriented programming;software understandability;software maintainability;crosscutting concerns;AOP techniques;cost evaluation;benefits evaluation;code size;change locality;concern diffusion;application refactoring;aspect creation","","13","","37","","","","","","IEEE","IEEE Journals & Magazines"
"A Rigorous Framework for Specification, Analysis and Enforcement of Access Control Policies","A. Margheri; M. Masi; R. Pugliese; F. Tiezzi","University of Southampton, Southampton, United Kingdom; Tiani “Spirit” GmbH, Wien, Austria; Dipartimento di Statistica, Università degli Studi di Firenze, Firenze, Italy; Università di Camerino, Camerino, Italy","IEEE Transactions on Software Engineering","","2019","45","1","2","33","Access control systems are widely used means for the protection of computing systems. They are defined in terms of access control policies regulating the access to system resources. In this paper, we introduce a formally-defined, fully-implemented framework for specification, analysis and enforcement of attribute-based access control policies. The framework rests on FACPL, a language with a compact, yet expressive, syntax for specification of real-world access control policies and with a rigorously defined denotational semantics. The framework enables the automated verification of properties regarding both the authorisations enforced by single policies and the relationships among multiple policies. Effectiveness and performance of the analysis rely on a semantic-preserving representation of FACPL policies in terms of SMT formulae and on the use of efficient SMT solvers. Our analysis approach explicitly addresses some crucial aspects of policy evaluation, such as missing attributes, erroneous values and obligations, which are instead overlooked in other proposals. The framework is supported by Java-based tools, among which an Eclipse-based IDE offering a tailored development and analysis environment for FACPL policies and a Java library for policy enforcement. We illustrate the framework and its formal ingredients by means of an e-Health case study, while its effectiveness is assessed by means of performance stress tests and experiments on a well-established benchmark.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2765640","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8081817","Attribute-based access control;policy languages;policy analysis;SMT","Semantics;Authorization;Tools;Syntactics;Proposals;Java","authorisation;Java;programming language semantics","access control systems;attribute-based access control policies;real-world access control policies;rigorously defined denotational semantics;FACPL policies;authorisations;semantic-preserving representation;SMT formulae;SMT solvers;Java-based tools;Eclipse-based IDE","","1","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Counterexample Generation in Probabilistic Model Checking","T. Han; J. Katoen; D. Berteun","RWTH Aachen University, Aachen and University of Twente, Enschede; RWTH Aachen University, Aachen and University of Twente, Enschede; RWTH Aachen University, Aachen and University of Twente, Enschede","IEEE Transactions on Software Engineering","","2009","35","2","241","257","Providing evidence for the refutation of a property is an essential, if not the most important, feature of model checking. This paper considers algorithms for counterexample generation for probabilistic CTL formulae in discrete-time Markov chains. Finding the strongest evidence (i.e., the most probable path) violating a (bounded) until-formula is shown to be reducible to a single-source (hop-constrained) shortest path problem. Counterexamples of smallest size that deviate most from the required probability bound can be obtained by applying (small amendments to) k-shortest (hop-constrained) paths algorithms. These results can be extended to Markov chains with rewards, to LTL model checking, and are useful for Markov decision processes. Experimental results show that typically the size of a counterexample is excessive. To obtain much more compact representations, we present a simple algorithm to generate (minimal) regular expressions that can act as counterexamples. The feasibility of our approach is illustrated by means of two communication protocols: leader election in an anonymous ring network and the Crowds protocol.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.5","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4770111","Model checking;Diagnostics","Protocols;Logic;Biological system modeling;Quantum computing;Feedback;Biology computing;Distributed computing;Computer Society;Shortest path problem;Nominations and elections","decision theory;formal verification;Markov processes;probabilistic logic;probability;temporal logic;trees (mathematics)","counterexample generation;probabilistic model checking;property refutation;discrete-time Markov chain;single-source shortest path problem;k-shortest path algorithm;Markov decision process;linear temporal logic;computation tree logic","","38","","61","","","","","","IEEE","IEEE Journals & Magazines"
"A Controlled Experiment for Evaluating the Impact of Coupling on the Maintainability of Service-Oriented Software","M. Perepletchikov; C. Ryan","RMIT University, Melbourne; RMIT University, Melbourne","IEEE Transactions on Software Engineering","","2011","37","4","449","465","One of the goals of Service-Oriented Computing (SOC) is to improve software maintainability as businesses become more agile, and thus underlying processes and rules change more frequently. This paper presents a controlled experiment examining the relationship between coupling in service-oriented designs, as measured using a recently proposed suite of SOC-specific coupling metrics and software maintainability in terms of the specific subcharacteristics of analyzability, changeability, and stability. The results indicate a statistically significant causal relationship between the investigated coupling metrics and the maintainability of service-oriented software. As such, the investigated metrics can facilitate coupling related design decisions with the aim of producing more maintainable service-oriented software products.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.61","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5482590","Services systems;design concepts;maintainability;product metrics;empirical studies.","Software maintenance;Programming;Software measurement;Logic;Software design;Stability analysis;Product design;Application software;Costs;Software metrics","service-oriented architecture;software maintenance;software metrics","software maintainability improvement;service-oriented computing;service-oriented designs;specific coupling metrics;specific subcharacteristics;statistically significant causal relationship;service-oriented software products","","21","","48","","","","","","IEEE","IEEE Journals & Magazines"
"MADMatch: Many-to-Many Approximate Diagram Matching for Design Comparison","S. Kpodjedo; F. Ricca; P. Galinier; G. Antoniol; Y. Guéhéneuc","Ecole Polytechnique de Montreal, Montreal; Università di Genova, Genova; Ecole Polytechnique de Montreal, Montreal; Ecole Polytechnique de Montreal, Montreal; Ecole Polytechnique de Montreal, Montreal","IEEE Transactions on Software Engineering","","2013","39","8","1090","1111","Matching algorithms play a fundamental role in many important but difficult software engineering activities, especially design evolution analysis and model comparison. We present MADMatch, a fast and scalable many-to-many approximate diagram matching approach based on an error-tolerant graph matching (ETGM) formulation. Diagrams are represented as graphs, costs are assigned to possible differences between two given graphs, and the goal is to retrieve the cheapest matching. We address the resulting optimization problem with a tabu search enhanced by the novel use of lexical and structural information. Through several case studies with different types of diagrams and tasks, we show that our generic approach obtains better results than dedicated state-of-the-art algorithms, such as AURA, PLTSDiff, or UMLDiff, on the exact same datasets used to introduce (and evaluate) these algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6464271","Diagram differencing;search-based software engineering;approximate graph matching;identifier splitting","Unified modeling language;Algorithm design and analysis;Software;Scalability;Software algorithms;Software engineering;Optimization","graph theory;optimisation;search problems;software engineering","MADMatch approach;many-to-many approximate diagram matching approach;error-tolerant graph matching;ETGM;software engineering;design evolution analysis;model comparison;design comparison;optimization problem;tabu search;lexical information;structural information;AURA algorithm;PLTSDiff algorithm;UMLDiff algorithm","","6","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Searching for points-to analysis","G. Bruns; S. Chandra","AT&T, Lisle, IL, USA; NA","IEEE Transactions on Software Engineering","","2003","29","10","883","897","The points-to analysis problem is to find the pointer relationships that could arise during program execution. Many points-to analysis algorithms exist, each making a particular trade off between cost of the analysis and precision of the results. In this paper, we show how points-to analysis algorithms can be defined as transformed versions of an exact algorithm. We present a set of program transformations over a general program model and use them to define some existing points-to analysis algorithms. Doing so makes explicit the approximations involved in these algorithms. We also show how the transformations can be used to define new points-to analysis algorithms. Our transformations are generic and may be useful in the design of other program analysis algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237170","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237170","","Algorithm design and analysis;Reachability analysis;Costs;Iterative algorithms;Program processors;Optimizing compilers;Data analysis;Logic programming;Approximation algorithms;Merging","reachability analysis;program diagnostics","model checking;reachability analysis;program analysis;points-to analysis;pointer relationships;program execution","","1","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Sequences of Developer Interactions in Visual Studio for Usage Smells","K. Damevski; D. C. Shepherd; J. Schneider; L. Pollock","Department of Computer Science, Virginia Commonwealth University, Richmond, VA; ABB Corporate Research, Raleigh, NC; ABB Corporate Research, Baden-Dättwill, Switzerland; Department of Computer and Information Sciences, University of Delaware, Newark, DE","IEEE Transactions on Software Engineering","","2017","43","4","359","371","In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2592905","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7516714","IDE usage data;data mining;pattern mining;usability analysis","Data mining;Visualization;Usability;Data analysis;Debugging;Software engineering;Navigation","data mining;program debugging;software engineering","developer interactions sequences mining;visual studio;usage smells extraction;large-scale dataset mining;IDE interactions;frequent IDE usage pattern mining;time-ordered sequences identifies;active file navigation;code search;software development activities","","6","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Variability in Software Systems—A Systematic Literature Review","M. Galster; D. Weyns; D. Tofan; B. Michalik; P. Avgeriou","Department of Computer Science and Software Engineering, University of Canterbury, Private Bag 4800, Christchurch, New Zealand; Department of Computer Science, Linnaeus University, Växjö, Sweden; Department of Mathematics and Computing Science, University of Groningen, Groningen 9700 AK, The Netherlands; Amartus, Poland; Department of Mathematics and Computing Science, University of Groningen, Groningen 9700 AK, The Netherlands","IEEE Transactions on Software Engineering","","2014","40","3","282","306","Context: Variability (i.e., the ability of software systems or artifacts to be adjusted for different contexts) became a key property of many systems. Objective: We analyze existing research on variability in software systems. We investigate variability handling in major software engineering phases (e.g., requirements engineering, architecting). Method: We performed a systematic literature review. A manual search covered 13 premium software engineering journals and 18 premium conferences, resulting in 15,430 papers searched and 196 papers considered for analysis. To improve reliability and to increase reproducibility, we complemented the manual search with a targeted automated search. Results: Software quality attributes have not received much attention in the context of variability. Variability is studied in all software engineering phases, but testing is underrepresented. Data to motivate the applicability of current approaches are often insufficient; research designs are vaguely described. Conclusions: Based on our findings we propose dimensions of variability in software engineering. This empirically grounded classification provides a step towards a unifying, integrated perspective of variability in software systems, spanning across disparate or loosely coupled research themes in the software engineering community. Finally, we provide recommendations to bridge the gap between research and practice and point to opportunities for future research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6682901","Variability;systematic review;software engineering","Decision support systems;Software systems;Systematics;Software engineering;Context;Manuals;Data collection","program testing;software product lines;software reliability;software reviews","software systems;variability handling;systematic literature review;manual search;software engineering journals;software reliability;reproducibility;targeted automated search;software engineering phase;software testing;software engineering community","","65","","60","","","","","","IEEE","IEEE Journals & Magazines"
"AutoSense: A Framework for Automated Sensitivity Analysis of Program Data","B. Nongpoh; R. Ray; S. Dutta; A. Banerjee","Department of Computer Science & Engineering, National Institute of Technology Meghalaya, Shillong, India; Department of Computer Science & Engineering, National Institute of Technology Meghalaya, Shillong, India; Department of Computer Science & Engineering, Jadavpur University, Kolkata, India; Advanced Computing and Microelectronics Unit, Indian Statistical Institute, Kolkata, India","IEEE Transactions on Software Engineering","","2017","43","12","1110","1124","In recent times, approximate computing is being increasingly adopted across the computing stack, from algorithms to computing hardware, to gain energy and performance efficiency by trading accuracy within acceptable limits. Approximation aware programming languages have been proposed where programmers can annotate data with type qualifiers (e.g., precise and approx) to denote its reliability. However, programmers need to judiciously annotate so that the accuracy loss remains within the desired limits. This can be non-trivial for large applications where error resilient and non-resilient program data may not be easily identifiable. Mis-annotation of even one data as error resilient/insensitive may result in an unacceptable output. In this paper, we present AutoSense, a framework to automatically classify resilient (insensitive) program data versus the sensitive ones with probabilistic reliability guarantee. AutoSense implements a combination of dynamic and static analysis methods for data sensitivity analysis. The dynamic analysis is based on statistical hypothesis testing, while the static analysis is based on classical data flow analysis. Experimental results compare our automated data classification with reported manual annotations on popular benchmarks used in approximate computing literature. AutoSense achieves promising reliability results compared to manual annotations and earlier methods, as evident from the experimental results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2654251","National Institute of Technology Meghalaya and Visvesvaraya Ph.D. Scheme; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7820185","Approximate computing;sensitivity analysis;hypothesis testing;sequential probability ratio test","Quality of service;Sensitivity analysis;Approximate computing;Probabilistic logic;Sequential analysis","data flow analysis;pattern classification;probability;program diagnostics;program verification;sensitivity analysis;statistical analysis","approximate computing literature;AutoSense;automated sensitivity analysis;probabilistic reliability;energy efficiency;manual annotations;automated data classification;classical data flow analysis;dynamic analysis;data sensitivity analysis;static analysis;resilient program data;accuracy loss;approximation aware programming languages;trading accuracy;performance efficiency;computing hardware;computing stack","","2","","31","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Dynamic Analysis for Diagnosing Integration Faults","L. Mariani; F. Pastore; M. Pezze","University of Milano Bicocca, Milan; University of Milano Bicocca, Milan; University of Milan Bicocca, Milano","IEEE Transactions on Software Engineering","","2011","37","4","486","508","Many software components are provided with incomplete specifications and little access to the source code. Reusing such gray-box components can result in integration faults that can be difficult to diagnose and locate. In this paper, we present Behavior Capture and Test (BCT), a technique that uses dynamic analysis to automatically identify the causes of failures and locate the related faults. BCT augments dynamic analysis techniques with model-based monitoring. In this way, BCT identifies a structured set of interactions and data values that are likely related to failures (failure causes), and indicates the components and the operations that are likely responsible for failures (fault locations). BCT advances scientific knowledge in several ways. It combines classic dynamic analysis with incremental finite state generation techniques to produce dynamic models that capture complementary aspects of component interactions. It uses an effective technique to filter false positives to reduce the effort of the analysis of the produced data. It defines a strategy to extract information about likely causes of failures by automatically ranking and relating the detected anomalies so that developers can focus their attention on the faults. The effectiveness of BCT depends on the quality of the dynamic models extracted from the program. BCT is particularly effective when the test cases sample the execution space well. In this paper, we present a set of case studies that illustrate the adequacy of BCT to analyze both regression testing failures and rare field failures. The results show that BCT automatically filters out most of the false alarms and provides useful information to understand the causes of failures in 69 percent of the case studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.93","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5611554","Dynamic Analysis;diagnosis;fault localization;false positive filters;regression failure analysis;field failure analysis.","Automata;Monitoring;Analytical models;Engines;Software;Testing;Inference algorithms","fault diagnosis;object-oriented programming;program testing;software fault tolerance","software components;Integration Faults;behavior capture and test technique;BCT;dynamic analysis;model-based monitoring;incremental finite state generation techniques","","33","","71","","","","","","IEEE","IEEE Journals & Magazines"
"The FreeBSD project: a replication case study of open source development","T. T. Dinh-Trong; J. M. Bieman","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA","IEEE Transactions on Software Engineering","","2005","31","6","481","494","Case studies can help to validate claims that open source software development produces higher quality software at lower cost than traditional commercial development. One problem inherent in case studies are external validity - we do not know whether or not results from one case study apply to another development project. We gain or lose confidence in case study results when similar case studies are conducted on other projects. This case study of the FreeBSD project, a long-lived open source project, provides further understanding of open source development. The paper details a method for mining repositories and querying project participants to retrieve key process information. The FreeBSD development process is fairly well-defined with proscribed methods for determining developer responsibilities, dealing with enhancements and defects, and managing releases. Compared to the Apache project, FreeBSD uses 1) a smaller set of core developers - developers who control the code base - that implement a smaller percentage of the system, 2) a larger set of top developers to implement 80 percent of the system, and 3) a more well-defined testing process. FreeBSD and Apache have a similar ratio of core developers to people involved in adapting and debugging the system and people who report problems. Both systems have similar defect densities and the developers are also users in both systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463231","Index Terms- Software engineering process;process measurement;qualitative process analysis;testing and debugging;reliability;maintenance process;maintainability;open source software;measurement;defect density;code ownership;FreeBSD.","Computer aided software engineering;Open source software;Linux;Kernel;Costs;Debugging;Density measurement;Software measurement;Maintenance;Software quality","public domain software;software development management;project management;program debugging;software reliability;software maintenance","FreeBSD project;replication case study;open source development;software engineering process;process measurement;qualitative process analysis;software testing;software debugging;software reliability;software maintenance process;open source software;defect density;code ownership;information retrievel","","56","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Workflow Models from Web Applications","M. Schur; A. Roth; A. Zeller","SAP SE; SAP SE, Germany; Saarland University–Chair for Software Engineering, Germany","IEEE Transactions on Software Engineering","","2015","41","12","1184","1201","Modern business applications predominantly rely on web technology, enabling software vendors to efficiently provide them as a service, removing some of the complexity of the traditional release and update process. While this facilitates shorter, more efficient and frequent release cycles, it requires continuous testing. Having insight into application behavior through explicit models can largely support development, testing and maintenance. Model-based testing allows efficient test creation based on a description of the states the application can be in and the transitions between these states. As specifying behavior models that are precise enough to be executable by a test automation tool is a hard task, an alternative is to extract them from running applications. However, mining such models is a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present Process Crawler (ProCrawl), a tool to mine behavior models from web applications that support multi-user workflows. ProCrawl incrementally learns a model by generating program runs and observing the application behavior through the user interface. In our evaluation on several real-world web applications, ProCrawl extracted models that concisely describe the implemented workflows and can be directly used for model-based testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2461542","German Federal Ministry of Education and Research (BMBF); European Research Council (ERC); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7169616","Specification mining;dynamic analysis;model-based testing;web system testing;Specification mining;dynamic analysis;model-based testing;web system testing","Web services;Software engineering;Data models;Data mining;Automation;Browsers","data mining;Internet;program testing;system monitoring;user interfaces","user interface;multiuser workflow;ProCrawl;Process Crawler;test automation tool;behavior model;test creation;model-based testing;continuous testing;software vendor;Web technology;Web applications;mining workflow model","","2","","47","","","","","","IEEE","IEEE Journals & Magazines"
"How We Refactor, and How We Know It","E. Murphy-Hill; C. Parnin; A. P. Black","North Carolina State University, Raleigh; Georgia Institute of Technology, Atlanta; Portland State University, Portland","IEEE Transactions on Software Engineering","","2012","38","1","5","18","Refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. However, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. In this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers. To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. Finally, we interviewed the Eclipse and Mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6112738","Refactoring;refactoring tools;floss refactoring;root-canal refactoring.","Java;Software tools;Refactoring","software maintenance;software tools","research and development;refactoring tools;randomly sampled code;Eclipse developers;Mylyn developers","","125","","22","","","","","","IEEE","IEEE Journals & Magazines"
"User interface evaluation and empirically-based evolution of a prototype experience management tool","C. B. Seaman; M. G. Mendonca; V. R. Basili; Y. M. Kim","Dept. of Inf. Syst., Maryland Univ., Baltimore, MD, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","9","838","850","Experience management refers to the capture, structuring, analysis, synthesis, and reuse of an organization's experience in the form of documents, plans, templates, processes, data, etc. The problem of managing experience effectively is not unique to software development, but the field of software engineering has had a high-level approach to this problem for some time. The Experience Factory is an organizational infrastructure whose goal is to produce, store, and reuse experiences gained in a software development organization. This paper describes The Q-Labs Experience Management System (Q-Labs EMS), which is based on the Experience Factory concept and was developed for use in a multinational software engineering consultancy. A critical aspect of the Q-Labs EMS project is its emphasis on empirical evaluation as a major driver of its development and evolution. The initial prototype requirements were grounded in the organizational needs and vision of Q-Labs, as were the goals and evaluation criteria later used to evaluate the prototype. However, the Q-Labs EMS architecture, data model, and user interface were designed to evolve, based on evolving user needs. This paper describes this approach, including the evaluation that was conducted of the initial prototype and its implications for the further development of systems to support software experience management.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232288","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232288","","User interfaces;Prototypes;Software prototyping;Software development management;Medical services;Engineering management;Programming;Software engineering;Production facilities;Computer architecture","user interfaces;software reusability;knowledge management;data models;software performance evaluation;systems re-engineering","user interface evaluation;reuse;prototype experience management tool;organization experience;software development;software engineering;Experience Factory;software experience management;Q-Labs Experience Management System;Q-Labs EMS;empirical evaluation;data model","","8","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Coping with Existing Systems in Information Systems Development","F. Zickert; R. Beck","Goethe University Frankfurt, Frankfurt; Goethe University Frankfurt, Frankfurt","IEEE Transactions on Software Engineering","","2012","38","5","1027","1039","Determining how to cope with existing systems is an important issue for information systems development (ISD). In this paper, we investigate how well different ISD patterns are suited for coping with existing systems. Empirical results, gathered from three software development projects undertaken by a financial institution, suggest propositions regarding how ISD patterns and existing systems affect the characteristics of objective ISD complexity, which in turn determine overall experienced complexity. Existing systems increase complexity due to conflicting interdependencies, but ISD patterns that reduce this complexity, such as those that employ bottom-up or concurrent consideration patterns, are best suited for coping with existing systems. In contrast, top-down and iterative focusing patterns, as classically used in new development, increase the complexity associated with conflicting interdependency, which makes them particularly unsuited for coping with existing systems in ISD.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5999674","Complexity measures;improvements;software maintenance;software engineering process","Complexity theory;Information systems;Maintenance engineering;Software maintenance;File systems;Programming;Servers","financial data processing;information systems;investment;project management;software maintenance;software metrics","information systems development;ISD patterns;software development projects;financial institution;complexity reduction;bottom-up patterns;concurrent consideration patterns;top-down focusing patterns;iterative focusing patterns;information systems portfolio;complexity measures;software maintenance","","","","73","","","","","","IEEE","IEEE Journals & Magazines"
"A Theoretical and Empirical Analysis of the Role of Test Sequence Length in Software Testing for Structural Coverage","A. Arcuri","Simula Research Laboratory, Lysaker","IEEE Transactions on Software Engineering","","2012","38","3","497","519","In the presence of an internal state, often a sequence of function calls is required to test software. In fact, to cover a particular branch of the code, a sequence of previous function calls might be required to put the internal state in the appropriate configuration. Internal states are not only present in object-oriented software, but also in procedural software (e.g., static variables in C programs). In the literature, there are many techniques to test this type of software. However, to the best of our knowledge, the properties related to the choice of the length of these sequences have received only a little attention in the literature. In this paper, we analyze the role that the length plays in software testing, in particular branch coverage. We show that, on “difficult” software testing benchmarks, longer test sequences make their testing trivial. Hence, we argue that the choice of the length of the test sequences is very important in software testing. Theoretical analyses and empirical studies on widely used benchmarks and on an industrial software are carried out to support our claims.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5750005","Evolutionary testing;object-oriented software;state problem;search-based software engineering;software testing;length;test sequence.","Software;Containers;Software testing;Algorithm design and analysis;Search problems;Software algorithms","C language;object-oriented programming;program testing","empirical analysis;theoretical analysis;test sequence length;software testing;structural coverage;internal state;function calls;object-oriented software;procedural software;C programs;static variables;test sequences","","11","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Guidelines for Eliciting Usability Functionalities","N. Juristo; A. Moreno; M. Sanchez-Segura","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","11","744","758","Like any other quality attribute, usability imposes specific constraints on software components. Features that raise the software system's usability have to be considered from the earliest development stages. But, discovering and documenting usability features is likely to be beyond the usability knowledge of most requirements engineers, developers, and users. We propose an approach based on developing specific guidelines that capitalize upon key elements recurrently intervening in the usability features elicitation and specification process. The use of these guidelines provides requirements analysts with a knowledge repository. They can use this repository to ask the right questions and capture precise usability requirements information.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70741","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339231","Usability requirements;usability features elicitation;Requirements/Specifications;Elicitation methods","Guidelines;Usability;Software systems;Application software;Costs;Software architecture;Software quality;Knowledge engineering;Programming;Design engineering","formal specification;software quality;systems analysis","usability functionalities;quality attribute;software components;requirements engineering;specification process;knowledge repository","","53","","56","","","","","","IEEE","IEEE Journals & Magazines"
"A temporal approach for testing distributed systems","A. Khoumsi","Sherbrooke Univ., Que., Canada","IEEE Transactions on Software Engineering","","2002","28","11","1085","1103","This paper deals with testing distributed software systems. In the past, two important problems have been determined for executing tests using a distributed test architecture: controllability and observability problems. A coordinated test method has subsequently been proposed to solve these two problems. In the present article: 1) we show that controllability and observability are indeed resolved if and only if the test system respects timing constraints, even when the system under test is non-real-time; 2) we determine these timing constraints; 3) we determine other timing constraints which optimize the duration of test execution; 4) we show that the communication medium used by the test system does not necessarily have to be FIFO; and 5) we show that the centralized test method can be considered just as a particular case of the proposed coordinated test method.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049406","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049406","","System testing;Timing;Controllability;Observability;Software testing;Constraint optimization;Fault detection;Software systems;Computer architecture;Error correction","program testing;timing;distributed programming","distributed software system testing;distributed test architecture;controllability;observability;temporal approach;coordinated test method;timing constraints;communication medium;centralized test method","","31","","10","","","","","","IEEE","IEEE Journals & Magazines"
"Software Numerical Instability Detection and Diagnosis by Combining Stochastic and Infinite-Precision Testing","E. Tang; X. Zhang; N. T. Müller; Z. Chen; X. Li","State Key Laboratory for Novel Software Technology and Software Institute of Nanjing University, Jiangsu, China; Department of Computer Science, Purdue University, 305 North University Street, West Lafayette, IN; Abteilung Informatik, University of Trier, Trier, Germany; State Key Laboratory for Novel Software Technology and Software Institute of Nanjing University, Jiangsu, China; State Key Laboratory for Novel Software Technology and Software Institute of Nanjing University, Jiangsu, China","IEEE Transactions on Software Engineering","","2017","43","10","975","994","Numerical instability is a well-known problem that may cause serious runtime failures. This paper discusses the reason of instability in software development process, and presents a toolchain that not only detects the potential instability in software, but also diagnoses the reason for such instability. We classify the reason of instability into two categories. When it is introduced by software requirements, we call the instability caused by problem . In this case, it cannot be avoided by improving software development, but requires inspecting the requirements, especially the underlying mathematical properties. Otherwise, we call the instability caused by practice. We design our toolchain as four loosely-coupled tools, which combine stochastic arithmetic with infinite-precision testing. Each tool in our toolchain can be configured with different strategies according to the properties of the analyzed software. We evaluate our toolchain on subjects from literature. The results show that it effectively detects and separates the instabilities caused by problems from others. We also conduct an evaluation on the latest version of GNU Scientific Library, and the toolchain finds a few real bugs in the well-maintained and widely deployed numerical library. With the help of our toolchain, we report the details and fixing advices to the GSL buglist.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2642956","National Basic Research Program of China 973 Program; NSF Award; National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7792694","Numerical analysis;infinite-precision arithmetic;stochastic arithmetic;software testing","Software;Software algorithms;Algorithm design and analysis;Libraries;Computer bugs;Software testing","fault diagnosis;formal specification;mathematics computing;numerical stability;program debugging;program testing;software quality;software reliability;software tools;stochastic processes","software requirements;toolchain;loosely-coupled tools;stochastic arithmetic;numerical library;software development process;potential instability;mathematical properties;infinite-precision testing;software numerical instability detection;software numerical instability diagnosis;runtime failures;GNU scientific library;GSL buglist","","","","64","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Design by Contract to Improve Software Vigilance","Y. Le Traon; B. Baudry; J. -. Jezequel","France Te´le´com R&D, 2, Avenue Pierre Marzin, 22 307 Lannion Cedex, France; IRISA, Universite´ de Rennes1, Campus Universitaire de Beaulieu, 35042 Rennes Cedex, France; IRISA, Universite´ de Rennes1, Campus Universitaire de Beaulieu, 35042 Rennes Cedex, France","IEEE Transactions on Software Engineering","","2006","32","8","571","586","Design by contract is a lightweight technique for embedding elements of formal specification (such as invariants, pre and postconditions) into an object-oriented design. When contracts are made executable, they can play the role of embedded, online oracles. Executable contracts allow components to be responsive to erroneous states and, thus, may help in detecting and locating faults. In this paper, we define vigilance as the degree to which a program is able to detect an erroneous state at runtime. Diagnosability represents the effort needed to locate a fault once it has been detected. In order to estimate the benefit of using design by contract, we formalize both notions of vigilance and diagnosability as software quality measures. The main steps of measure elaboration are given, from informal definitions of the factors to be measured to the mathematical model of the measures. As is the standard in this domain, the parameters are then fixed through actual measures, based on a mutation analysis in our case. Several measures are presented that reveal and estimate the contribution of contracts to the overall quality of a system in terms of vigilance and diagnosability","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703388","Object-oriented design methods;programming by contract;diagnostics;metrics.","Contracts;Fault detection;Embedded software;Formal specifications;Mathematical model;Object oriented modeling;Runtime;Software quality;Software measurement;Measurement standards","formal specification;object-oriented methods;object-oriented programming;program debugging;program diagnostics;program verification;software fault tolerance;software metrics;software quality","design by contract;software vigilance improvement;formal specification;object-oriented design method;embedded online oracle;executable contract;software fault detection;program runtime erroneous state detection;software diagnosability;software quality measure;mathematical model;mutation analysis;software metrics","","28","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Preventing Temporal Violations in Scientific Workflows: Where and How","X. Liu; Y. Yang; Y. Jiang; J. Chen","Swinburne University of Technology, Melbourne; Anhui University, Hefei and Swinburne University of Technology, Melbourne; Hefei University of Technology, Hefei and University of Pittsburgh, Pittsburgh; University of Technology, Sydney and Swinburne University of Technology, Melbourne","IEEE Transactions on Software Engineering","","2011","37","6","805","825","Due to the dynamic nature of the underlying high-performance infrastructures for scientific workflows such as grid and cloud computing, failures of timely completion of important scientific activities, namely, temporal violations, often take place. Unlike conventional exception handling on functional failures, nonfunctional QoS failures such as temporal violations cannot be passively recovered. They need to be proactively prevented through dynamically monitoring and adjusting the temporal consistency states of scientific workflows at runtime. However, current research on workflow temporal verification mainly focuses on runtime monitoring, while the adjusting strategy for temporal consistency states, namely, temporal adjustment, has so far not been thoroughly investigated. For this issue, two fundamental problems of temporal adjustment, namely, where and how, are systematically analyzed and addressed in this paper. Specifically, a novel minimum probability time redundancy-based necessary and sufficient adjustment point selection strategy is proposed to address the problem of where and an innovative genetic-algorithm-based effective and efficient local rescheduling strategy is proposed to tackle the problem of how. The results of large-scale simulation experiments with generic workflows and specific real-world applications demonstrate that our temporal adjustment strategy can remarkably prevent the violations of both local and global temporal constraints in scientific workflows.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.99","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5645643","Workflow management;exception handling;reliability;software verification;statistical methods.","Decision support systems;Software reliability;Quality of service;Workflow management software","genetic algorithms;middleware;quality of service;workflow management software","temporal violation prevention;scientific workflows;grid computing;cloud computing;nonfunctional QoS failures;workflow temporal verification;adjustment point selection;rescheduling strategy;generic workflows","","30","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Software Architecture Reconstruction: A Process-Oriented Taxonomy","S. Ducasse; D. Pollet","INRIA, Lille Nord Europe; University of Lille 1, Francr","IEEE Transactions on Software Engineering","","2009","35","4","573","591","To maintain and understand large applications, it is important to know their architecture. The first problem is that unlike classes and packages, architecture is not explicitly represented in the code. The second problem is that successful applications evolve over time, so their architecture inevitably drifts. Reconstructing the architecture and checking whether it is still valid is therefore an important aid. While there is a plethora of approaches and techniques supporting architecture reconstruction, there is no comprehensive software architecture reconstruction state of the art and it is often difficult to compare the approaches. This paper presents a state of the art in software architecture reconstruction approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815276","Software architecture reconstruction.","Software architecture;Taxonomy;Computer architecture;Application software;Data mining;Programming;Europe;Packaging;Cognitive science;Bridges","software architecture;software maintenance;software packages","software architecture reconstruction;process-oriented taxonomy;software development","","126","","181","","","","","","IEEE","IEEE Journals & Magazines"
"QoS-aware middleware for Web services composition","Liangzhao Zeng; B. Benatallah; A. H. H. Ngu; M. Dumas; J. Kalagnanam; H. Chang","IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","5","311","327","The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online business-to-business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different quality of service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are described and compared: one based on local (task-level) selection of services and the other based on global allocation of tasks to services using integer programming.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291834","Web services;quality of service;service composition;integer programming.","Middleware;Web services;Quality of service;Linear programming;Web and internet services;Financial management;Availability;Computer science;Computer Society;Online Communities/Technical Collaboration","electronic commerce;middleware;quality of service;Internet;integer programming","Web manual interaction;online business-to-business collaboration;B2B;Web services;quality of service;QoS;middleware platform;integer programming;service composition","","1609","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Analysis of Object-Oriented Design Metrics for Predicting High and Low Severity Faults","Yuming Zhou; Hareton Leung","IEEE Computer Society; NA","IEEE Transactions on Software Engineering","","2006","32","10","771","789","In the last decade, empirical studies on object-oriented design metrics have shown some of them to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented design metrics and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented design metrics, specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these design metrics are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these design metrics are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.102","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717471","Object-oriented;faults;fault-proneness;metrics;prediction;cross validation.","Object oriented modeling;Predictive models;Logistics;Learning systems;NASA;Computer Society;Software systems;Fault detection;Decision making;Programming","object-oriented programming;regression analysis;software fault tolerance;software metrics","object-oriented design metrics;fault-proneness prediction;object-oriented software system;fault severity;logistic regression method;machine learning method;public domain NASA data set;fault-prone classes","","152","","31","","","","","","IEEE","IEEE Journals & Magazines"
"The Role of Deliberate Artificial Design Elements in Software Engineering Experiments","J. Hannay; M. Jørgensen","NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","242","259","Increased realism in software engineering experiments is often promoted as an important means of increasing generalizability and industrial relevance. In this context, artificiality, e.g., the use of constructed tasks in place of realistic tasks, is seen as a threat. In this paper, we examine the opposite view that deliberately introduced artificial design elements may increase knowledge gain and enhance both generalizability and relevance. In the first part of this paper, we identify and evaluate arguments and examples in favor of and against deliberately introducing artificiality into software engineering experiments. We find that there are good arguments in favor of deliberately introducing artificial design elements to 1) isolate basic mechanisms, 2) establish the existence of phenomena, 3) enable generalization from particularly unfavorable to more favorable conditions (persistence of phenomena), and 4) relate experiments to theory. In the second part of this paper, we summarize a content analysis of articles that report software engineering experiments published over a 10-year period from 1993 to 2002. The analysis reveals a striving for realism and external validity, but little awareness of for what and when various degrees of artificiality and realism are appropriate. Furthermore, much of the focus on realism seems to be based on a narrow understanding of the nature of generalization. We conclude that an increased awareness and deliberation as to when and for what purposes both artificial and realistic design elements are applied is valuable for better knowledge gain and quality in empirical software engineering experiments. We also conclude that time spent on studies that have obvious threats to validity that are due to artificiality might be better spent on studies that investigate research questions for which artificiality is a strength rather than a weakness. However, arguments in favor of artificial design elements should not be used to justify studies that are badly designed or that have research questions of low relevance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4453833","Software Engineering;Surveys of historical development of one particular area;Software Engineering;Surveys of historical development of one particular area","Software design;Software engineering;Computer industry;Computer Society;Engineering drawings;Displays;Programming;Probability distribution","knowledge acquisition;software engineering","empirical software engineering experiment;content analysis;realistic design element;artificial design element;knowledge acquisition","","28","","110","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic QoS Adaptation for Mobile Middleware","S. Chuang; A. T. S. Chan","The Hong Kong Polytechnic University, Hong Kong; The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Software Engineering","","2008","34","6","738","752","Computation and networking resources in mobile operating environments are much scarcer and more dynamic than in desktop operating environments. Mobile applications can leverage on the benefits of adaptive computing to optimize the QoS delivery based on contextual situations. Fuzzy control models have been successfully applied to various distributed network QoS management systems. However, existing models are either application-specific or limited to abstract modeling and simple conceptual scenarios which do not take into account overall model scalability. Specifically, the large number of QoS parameters in a mobile operating environment causes an exponential increase in the number of rules correspondingly increases the demand for processing power to infer the rules. Hierarchical fuzzy systems were introduced to reduce the number of rules using hierarchical fuzzy control, in which correlated linguistic variables are hierarchically inferred and grouped into abstract linguistic variables. In this paper, we propose a mobile QoS management framework that uses a hierarchical fuzzy control model to support a highly extensible and structured adaptation paradigm. The proposed framework integrates several levels of QoS abstractions derived from user-perceived requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4547430","Mobile Computing;Domain-specific architectures;Mobile Computing;Domain-specific architectures","Middleware;Quality of service;Fuzzy control;Mobile computing;Power system management;Power system modeling;Quality management;Battery management systems;Energy management;Computer network management","fuzzy control;fuzzy systems;middleware;mobile computing;quality of service","dynamic QoS adaptation;mobile middleware;adaptive computing;fuzzy control;distributed network QoS management systems;mobile operating environment;fuzzy systems","","13","","35","","","","","","IEEE","IEEE Journals & Magazines"
"The generic consensus service","R. Guerraoui; A. Schiper","Dept. de Syst. de Commun., Ecole Polytech. Federale de Lausanne, Switzerland; NA","IEEE Transactions on Software Engineering","","2001","27","1","29","41","This paper describes a modular approach for the construction of fault-tolerant agreement protocols. The approach is based on a generic consensus service. Fault-tolerant agreement protocols are built using a client-server interaction, where the clients are the processes that must solve the agreement problem and the servers implement the consensus service. This service is accessed through a generic consensus filter, customized for each specific agreement problem. We illustrate our approach on the construction of various fault-tolerant agreement protocols, such as nonblocking atomic commitment, group membership, view synchronous communication, and total order multicast. Through a systematic reduction to consensus, we provide a simple way to solve agreement problems. In addition to its modularity, our approach enables efficient implementations of agreement protocols and precise characterization of the assumptions underlying their liveness and safety properties.","0098-5589;1939-3520;2326-3881","","10.1109/32.895986","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=895986","","Multicast protocols;Fault tolerance;Access protocols;Broadcasting;Filters;File servers;Fault tolerant systems;Resilience;Modular construction;Safety","protocols;client-server systems;multicast communication;computer network reliability","generic consensus service;fault-tolerant agreement protocols;client-server interaction;generic consensus filter;nonblocking atomic commitment;group membership;view synchronous communication;total order multicast;liveness;safety properties","","50","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Lightweight extraction of object models from bytecode","D. Jackson; A. Waingold","Lab. for Comput. Sci., MIT, Cambridge, MA, USA; NA","IEEE Transactions on Software Engineering","","2001","27","2","156","169","A program's object model captures the essence of its design. For some programs, no object model was developed during design; for others, an object model exists but may be out-of-sync with the code. This paper describes a tool that automatically extracts an object model from the class-files of a Java program. Unlike existing tools, it handles container classes by inferring the types of elements stored in a container and eliding the container itself. This feature is crucial for obtaining models that show the structure of the abstract state and bear some relation to conceptual models. Although the tool performs only a simple, heuristic analysis that is almost entirely local, the resulting object model is surprisingly accurate. The paper explains what object models are and why they are useful; describes the analysis, its assumptions, and limitations; evaluates the tool for accuracy, and illustrates its use on a suite of sample programs.","0098-5589;1939-3520;2326-3881","","10.1109/32.908960","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=908960","","Java;Containers;Performance analysis;Computer Society;Reverse engineering;Feature extraction;Computer science","Java;object-oriented programming;abstract data types;reverse engineering","lightweight object model extraction;bytecode;class-files;Java program;container classes;abstract state;heuristic analysis","","17","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Survey of Program Comprehension through Dynamic Analysis","B. Cornelissen; A. Zaidman; A. van Deursen; L. Moonen; R. Koschke","Delft University of Technology, The Netherlands; Delft University of Technology, The Netherlands; Delft University of Technology, The Netherlands; Simula Research Laboratory, Norway; University of Bremen, Germany","IEEE Transactions on Software Engineering","","2009","35","5","684","702","Program comprehension is an important activity in software maintenance, as software must be sufficiently understood before it can be properly modified. The study of a program's execution, known as dynamic analysis, has become a common technique in this respect and has received substantial attention from the research community, particularly over the last decade. These efforts have resulted in a large research body of which currently there exists no comprehensive overview. This paper reports on a systematic literature survey aimed at the identification and structuring of research on program comprehension through dynamic analysis. From a research body consisting of 4,795 articles published in 14 relevant venues between July 1999 and June 2008 and the references therein, we have systematically selected 176 articles and characterized them in terms of four main facets: activity, target, method, and evaluation. The resulting overview offers insight in what constitutes the main contributions of the field, supports the task of identifying gaps and opportunities, and has motivated our discussion of several important research directions that merit additional consideration in the near future.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815280","Survey;program comprehension;dynamic analysis.","Computer Society;Software maintenance;Software systems;Documentation;Software engineering;Data analysis;Information analysis;Availability;Runtime;Virtual machining","reverse engineering;software maintenance;system monitoring","program comprehension;dynamic analysis;software maintenance;systematic literature survey","","163","","154","","","","","","IEEE","IEEE Journals & Magazines"
"An approach to developing domain requirements as a core asset based on commonality and variability analysis in a product line","Mikyeong Moon; Keunhyuk Yeom; Heung Seok Chae","Dept. of Comput. Sci. & Eng., Pusan Nat. Univ., South Korea; Dept. of Comput. Sci. & Eng., Pusan Nat. Univ., South Korea; Dept. of Comput. Sci. & Eng., Pusan Nat. Univ., South Korea","IEEE Transactions on Software Engineering","","2005","31","7","551","569","The methodologies of product line engineering emphasize proactive reuse to construct high-quality products more quickly that are less costly. Requirements engineering for software product families differs significantly from requirements engineering for single software products. The requirements for a product line are written for the group of systems as a whole, with requirements for individual systems specified by a delta or an increment to the generic set. Therefore, it is necessary to identify and explicitly denote the regions of commonality and points of variation at the requirements level. In this paper, we suggest a method of producing requirements that will be a core asset in the product line. We describe a process for developing domain requirements where commonality and variability in a domain are explicitly considered. A CASE environment, named DREAM, for managing commonality and variability analysis of domain requirements is also described. We also describe a case study for an e-travel system domain where we found that our approach to developing domain requirements based on commonality and variability analysis helped to produce domain requirements as a core asset for product lines.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492371","Index Terms- Requirement engineering;product-line;core asset;commonality;variability;domain analysis;reuse.","Design engineering;Computer architecture;Moon;Computer Society;Computer aided software engineering;Environmental management;Software systems;Control systems;Testing;Costs","computer aided software engineering;software reusability;software quality;software architecture;formal specification;travel industry","product line engineering;software reuse;variability analysis;commonality analysis;high-quality software product;requirements engineering;software product family;CASE environment;DREAM;e-travel system domain","","69","","29","","","","","","IEEE","IEEE Journals & Magazines"
"On the value of static analysis for fault detection in software","J. Zheng; L. Williams; N. Nagappan; W. Snipes; J. P. Hudepohl; M. A. Vouk","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","4","240","253","No single software fault-detection technique is capable of addressing all fault-detection concerns. Similarly to software reviews and testing, static analysis tools (or automated static analysis) can be used to remove defects prior to release of a software product. To determine to what extent automated static analysis can help in the economic production of a high-quality product, we have analyzed static analysis faults and test and customer-reported failures for three large-scale industrial software systems developed at Nortel Networks. The data indicate that automated static analysis is an affordable means of software fault detection. Using the orthogonal defect classification scheme, we found that automated static analysis is effective at identifying assignment and checking faults, allowing the later software production phases to focus on more complex, functional, and algorithmic faults. A majority of the defects found by automated static analysis appear to be produced by a few key types of programmer errors and some of these types have the potential to cause security vulnerabilities. Statistical analysis results indicate the number of automated static analysis faults can be effective for identifying problem modules. Our results indicate static analysis tools are complementary to other fault-detection techniques for the economic production of a high-quality software product.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628970","Code inspections;walkthroughs.","Fault detection;Software tools;Failure analysis;Automatic testing;Software testing;Fault diagnosis;Production systems;System testing;Large-scale systems;Computer industry","program diagnostics;fault diagnosis;software quality","software fault-detection;static analysis tool;automated static analysis;Nortel Networks;orthogonal defect classification scheme;programmer error;security vulnerability;high-quality software product;code inspection;industrial software system","","95","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Bayesian Networks For Evidence-Based Decision-Making in Software Engineering","A. T. Misirli; A. B. Bener","Department of Information Processing Science, University of Oulu, Finland; Mechanical and Industrial Engineering Department, Ryerson University, Toronto, CA","IEEE Transactions on Software Engineering","","2014","40","6","533","554","Recommendation systems in software engineering (SE) should be designed to integrate evidence into practitioners experience. Bayesian networks (BNs) provide a natural statistical framework for evidence-based decision-making by incorporating an integrated summary of the available evidence and associated uncertainty (of consequences). In this study, we follow the lead of computational biology and healthcare decision-making, and investigate the applications of BNs in SE in terms of 1) main software engineering challenges addressed, 2) techniques used to learn causal relationships among variables, 3) techniques used to infer the parameters, and 4) variable types used as BN nodes. We conduct a systematic mapping study to investigate each of these four facets and compare the current usage of BNs in SE with these two domains. Subsequently, we highlight the main limitations of the usage of BNs in SE and propose a Hybrid BN to improve evidence-based decision-making in SE. In two industrial cases, we build sample hybrid BNs and evaluate their performance. The results of our empirical analyses show that hybrid BNs are powerful frameworks that combine expert knowledge with quantitative data. As researchers in SE become more aware of the underlying dynamics of BNs, the proposed models will also advance and naturally contribute to evidence based-decision-making.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2321179","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6808495","Evidence-based decision-making;Bayesian networks;Bayesian statistics;software reliability;software metrics;post-release defects","Software engineering;Decision making;Bayes methods;Software;Medical services;Systematics;Buildings","belief networks;decision making;software metrics;software reliability","Bayesian networks;evidence-based decision-making;software engineering;recommendation systems;SE;natural statistical framework;associated uncertainty;computational biology;health care decision-making;systematic mapping study;hybrid BN node;software reliability;software metrics","","18","","81","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting the Combined Selection of Model-Based Testing Techniques","A. C. Dias-Neto; G. H. Travassos","Institute of Computing, Federal University of Amazonas, Manaus, Brazil; Systems Engineering and Computer Science Programme at COPPE, Federal University of Rio de Janeiro, Rio de Janeiro , Brazil","IEEE Transactions on Software Engineering","","2014","40","10","1025","1041","The technical literature on model-based testing (MBT) offers us several techniques with different characteristics and goals. Contemporary software projects usually need to make use of different software testing techniques. However, a lack of empirical information regarding their scalability and effectiveness is observed. It makes their application difficult in real projects, increasing the technical difficulties to combine two or more MBT techniques for the same software project. In addition, current software testing selection approaches offer limited support for the combined selection of techniques. Therefore, this paper describes the conception and evaluation of an approach aimed at supporting the combined selection of MBT techniques for software projects. It consists of an evidence-based body of knowledge with 219 MBT techniques and their corresponding characteristics and a selection process that provides indicators on the level of adequacy (impact indicator) amongst MBT techniques and software projects characteristics. Results from the data analysis indicate it contributes to improve the effectiveness and efficiency of the selection process when compared to another selection approach available in the technical literature. Aiming at facilitating its use, a computerized infrastructure, evaluated into an industrial context and evolved to implement all the facilities needed to support such selection approach, is presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2312915","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6776497","Software testing;model-based testing;software technology selection;recommendation system;experimental software engineering","Software testing;Context;Computational modeling;Organizations;Software quality","program testing;project management;software development management","model-based testing techniques;technical literature;software projects;software testing techniques;MBT techniques;software testing selection;impact indicator;software project characteristics;data analysis;computerized infrastructure","","7","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Generating Test Data from OCL Constraints with Search Techniques","S. Ali; M. Zohaib Iqbal; A. Arcuri; L. C. Briand","Simula Research Lab, Norway; National University of Computer and Emerging Sciences, Islamabad and University of Luxembourg, Luxembourg; Simula Research Lab, Norway; University of Luxembourg, Luxembourg","IEEE Transactions on Software Engineering","","2013","39","10","1376","1402","Model-based testing (MBT) aims at automated, scalable, and systematic testing solutions for complex industrial software systems. To increase chances of adoption in industrial contexts, software systems can be modeled using well-established standards such as the Unified Modeling Language (UML) and the Object Constraint Language (OCL). Given that test data generation is one of the major challenges to automate MBT, we focus on test data generation from OCL constraints in this paper. This endeavor is all the more challenging given the numerous OCL constructs and operations that are designed to facilitate the definition of constraints. Though search-based software testing has been applied to test data generation for white-box testing (e.g., branch coverage), its application to the MBT of industrial software systems has been limited. In this paper, we propose a set of search heuristics targeted to OCL constraints to guide test data generation and automate MBT in industrial applications. We evaluate these heuristics for three search algorithms: Genetic Algorithm, (1+1) Evolutionary Algorithm, and Alternating Variable Method. We empirically evaluate our heuristics using complex artificial problems, followed by empirical analyses of the feasibility of our approach on one industrial system in the context of robustness testing. Our approach is also compared with the most widely referenced OCL solver (UMLtoCSP) in the literature and shows to be significantly more efficient.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6491405","OCL;search-based testing;test data generation;empirical evaluation;search-based software engineering;model-based testing","Unified modeling language;Search problems;Software algorithms;Genetic algorithms;Standards;Software testing","genetic algorithms;program testing;specification languages","test data generation;OCL constraints;search techniques;model-based testing;MBT;industrial software systems;object constraint language;unified modeling language;UML;OCL constructs;OCL operations;search-based software testing;white-box testing;genetic algorithm;one-plus-one evolutionary algorithm;alternating variable method;empirical analysis;robustness testing context;UMLtoCSP OCL solver","","45","","75","","","","","","IEEE","IEEE Journals & Magazines"
"Toward a Smell-Aware Bug Prediction Model","F. Palomba; M. Zanoni; F. A. Fontana; A. De Lucia; R. Oliveto","Delft University of Technology, Delft, The Netherlands; University of Milano-Bicocca, Milano, MI, Italy; University of Milano-Bicocca, Milano, MI, Italy; University of Salerno, Fisciano, SA, Italy; University of Molise, Campobasso, Italy","IEEE Transactions on Software Engineering","","2019","45","2","194","218","Code smells are symptoms of poor design and implementation choices. Previous studies empirically assessed the impact of smells on code quality and clearly indicate their negative impact on maintainability, including a higher bug-proneness of components affected by code smells. In this paper, we capture previous findings on bug-proneness to build a specialized bug prediction model for smelly classes. Specifically, we evaluate the contribution of a measure of the severity of code smells (i.e., code smell intensity) by adding it to existing bug prediction models based on both product and process metrics, and comparing the results of the new model against the baseline models. Results indicate that the accuracy of a bug prediction model increases by adding the code smell intensity as predictor. We also compare the results achieved by the proposed model with the ones of an alternative technique which considers metrics about the history of code smells in files, finding that our model works generally better. However, we observed interesting complementarities between the set of <italic>buggy and smelly</italic> classes correctly classified by the two models. By evaluating the actual information gain provided by the intensity index with respect to the other metrics in the model, we found that the intensity index is a relevant feature for both product and process metrics-based models. At the same time, the metric counting the average number of code smells in previous versions of a class considered by the alternative model is also able to reduce the entropy of the model. On the basis of this result, we devise and evaluate a <italic>smell-aware</italic> combined bug prediction model that included product, process, and smell-related features. We demonstrate how such model classifies bug-prone code components with an F-Measure at least 13 percent higher than the existing state-of-the-art models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2770122","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8097044","Code smells;bug prediction;empirical study;mining software repositories","Computer bugs;Measurement;Predictive models;Indexes;Software;Complexity theory;Entropy","","","","2","","132","","","","","","IEEE","IEEE Journals & Magazines"
"Refactoring Inspection Support for Manual Refactoring Edits","E. L. G. Alves; M. Song; T. Massoni; P. D. L. Machado; M. Kim","Computer Science Department, Federal University of Campina Grande, Campina Grande, Brazil; Computer Science Department, The University of Nebraska at Omaha, Omaha, NE; Computer Science Department, Universidade Federal de Campina Grande, Campina Grande, PB, Brazil; Systems and Computing Department, Federal University of Campina Grande, Campina Grande, Brazil; Computer Science Department, University of California, Los Angeles, CA","IEEE Transactions on Software Engineering","","2018","44","4","365","383","Refactoring is commonly performed manually, supported by regression testing, which serves as a safety net to provide confidence on the edits performed. However, inadequate test suites may prevent developers from initiating or performing refactorings. We propose RefDistiller, a static analysis approach to support the inspection of manual refactorings. It combines two techniques. First, it applies predefined templates to identify potential missed edits during manual refactoring. Second, it leverages an automated refactoring engine to identify extra edits that might be incorrect. RefDistiller also helps determine the root cause of detected anomalies. In our evaluation, RefDistiller identifies 97 percent of seeded anomalies, of which 24 percent are not detected by generated test suites. Compared to running existing regression test suites, it detects 22 times more anomalies, with 94 percent precision on average. In a study with 15 professional developers, the participants inspected problematic refactorings with RefDistiller versus testing only. With RefDistiller, participants located 90 percent of the seeded anomalies, while they located only 13 percent with testing. The results show RefDistiller can help check the correctness of manual refactorings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2679742","National Science Foundation; Google Faculty Award; National Institute of Science and Technology for Software Engineering; CNPq/Brasil; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7874212","Refactoring;refactoring anomalies;code inspection","Manuals;Inspection;Testing;Computer bugs;Transforms;Engines;Detectors","program diagnostics;program testing;regression analysis;software maintenance","RefDistiller;seeded anomalies;refactoring inspection support;manual refactoring edits;regression testing;inadequate test suites;potential missed edits;automated refactoring engine;extra edits;generated test suites;running existing regression test suites;problematic refactorings","","1","","49","","","","","","IEEE","IEEE Journals & Magazines"
"API-Based and Information-Theoretic Metrics for Measuring the Quality of Software Modularization","S. Sarkar; G. M. Rama; A. C. Kak","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","1","14","32","We present in this paper a new set of metrics that measure the quality of modularization of a non-object-oriented software system. We have proposed a set of design principles to capture the notion of modularity and defined metrics centered around these principles. These metrics characterize the software from a variety of perspectives: structural, architectural, and notions such as the similarity of purpose and commonality of goals. (By structural, we are referring to intermodule coupling-based notions, and by architectural, we mean the horizontal layering of modules in large software systems.) We employ the notion of API (application programming interface) as the basis for our structural metrics. The rest of the metrics we present are in support of those that are based on API. Some of the important support metrics include those that characterize each module on the basis of the similarity of purpose of the services offered by the module. These metrics are based on information-theoretic principles. We tested our metrics on some popular open-source systems and some large legacy-code business applications. To validate the metrics, we compared the results obtained on human-modularized versions of the software (as created by the developers of the software) with those obtained on randomized versions of the code. For randomized versions, the assignment of the individual functions to modules was randomized","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256942","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027146","Metrics/measurement;modules and interfaces;information theory;distribution;maintenance and enhancement;maintainability;coupling;layered architecture.","Software measurement;Software quality;Software systems;Application software;Open source software;Data structures;Software maintenance;System testing;Information theory;Computer architecture","application program interfaces;information theory;object-oriented programming;software architecture;software metrics;software quality","information-theoretic metrics;quality measurement;software modularization;nonobject-oriented software system;application programming interface;open-source systems;legacy-code business applications;human-modularized versions;randomized versions","","51","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental Test Generation for Software Product Lines","E. Uzuncaova; S. Khurshid; D. Batory","Microsoft, Redmond; University of Texas at Austin, Austin; University of Texas at Austin, Austin","IEEE Transactions on Software Engineering","","2010","36","3","309","322","Recent advances in mechanical techniques for systematic testing have increased our ability to automatically find subtle bugs, and hence, to deploy more dependable software. This paper builds on one such systematic technique, scope-bounded testing, to develop a novel specification-based approach for efficiently generating tests for products in a software product line. Given properties of features as first-order logic formulas in Alloy, our approach uses SAT-based analysis to automatically generate test inputs for each product in a product line. To ensure soundness of generation, we introduce an automatic technique for mapping a formula that specifies a feature into a transformation that defines incremental refinement of test suites. Our experimental results using different data structure product lines show that an incremental approach can provide an order of magnitude speedup over conventional techniques. We also present a further optimization using dedicated integer constraint solvers for feature properties that introduce integer constraints, and show how to use a combination of solvers in tandem for solving Alloy formulas.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5456077","Software/program verification;testing and debugging;software engineering.","Software testing;Automatic testing;System testing;Computer bugs;Logic testing;Data structures;Software quality;Automatic logic units;Acoustic testing;Constraint optimization","computability;data structures;program testing;program verification","incremental test generation;software product lines;mechanical techniques;systematic testing;scope-bounded testing;specification-based approach;first-order logic formulas;SAT-based analysis;data structure product lines;dedicated integer constraint solvers;Alloy formulas;program verification","","32","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Software Reliability Analysis Using Weakest Preconditions in Linear Assignment Programs","H. Luo; X. Liu; X. Chen; T. Long; R. Jiang","Department of Measurement and Control Engineering, School of Manufacturing Science and Engineering, Sichuan University, Chengdu, P.R.China; School of Computer Science, McGill University, Montreal H3A0E9, Canada; School of Computer Science, McGill University, Montreal H3A0E9, Canada; Department of Control Engineering, Chengdu University of Information Technology, Shuangliu, P.R.China; School of Electrical Engineering and Information, Sichuan University, Chengdu, P.R.China","IEEE Transactions on Software Engineering","","2016","42","9","866","885","Weakest preconditions derived from triple axiomatic semantics have been widely used to prove the correctness of programs. They can also be applied to evaluate the reliability of software. However, deducing a weakest precondition, as well as determining its propagation path, encounters challenges such as unknown constraint conditions, symbol computation and means of representation. To address these challenges, in this paper, we utilize the disjunctive normal form of if-else branch structure to capture reasonable propagation paths of the weakest precondition. Meanwhile, by removing the sequential dependencies, we demonstrate how to get the weakest precondition of loop-structure by leveraging program function. Moreover, we extensively explore three modeling characteristics (i.e., path extension, innermost connection and condition leap) for deducing the weakest precondition of structured programs. Finally, taking the definition of program node and storage structure of weakest precondition as bases, we design a serial of modeling algorithms. Based on symbol computation and recursive call technology with Depth-First Search (DFS), our algorithms can not only be used to deduce the weakest precondition, but also to capture the propagate path of the weakest precondition. Experiments illustrate the efficacy and effectiveness of our proposed models and designed deductive algorithms.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2521379","Research Foundation of Young Teachers in Sichuan University of P.R. China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7398131","Weakest precondition;path extension;innermost connection;condition leap;node;cell-structure","Algorithm design and analysis;Semantics;Software reliability;Computational modeling;Computer bugs;Cognition","linear programming;search problems;software reliability","software reliability analysis;weakest preconditions;linear assignment program;triple axiomatic semantics;program correctness;propagation path;if-else branch structure;loop-structure precondition;path extension characteristic;innermost connection characteristic;condition leap characteristic;symbol computation;recursive call technology;depth-first search;DFS;deductive algorithms","","1","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic model transformations using extended UML object diagrams in modeling environments","D. Milicev","Sch. of Electr. Eng., Belgrade Univ., Serbia","IEEE Transactions on Software Engineering","","2002","28","4","413","431","One of the most important features of modeling tools is generation of output. The output may be documentation, source code, net list, or any other presentation of the system being constructed. The process of output generation may be considered as automatic creation of a target model from a model in the source modeling domain. This translation does not need to be accomplished in a single step. Instead, a tool may generate multiple intermediate models as other views to the system. These models may be used either as better descriptions of the system, or as a descent down the abstraction levels of the user-defined model, gradually leading to the desired implementation. If the modeling domains have their metamodels defined in terms of object-oriented concepts, the models consist of instances of the abstractions from the metamodels and links between them. A new technique for specifying the mapping between different modeling domains is proposed in the paper. It uses UML object diagrams that show the instances and links of the target model that should be created during automatic translations. The diagrams are extended with the proposed concepts of conditional, repetitive, parameterized, and polymorphic model creation, implemented by the standard UML extensibility mechanisms. Several examples from different engineering domains are provided, illustrating the applicability and benefits of the approach. The first experimental results show that the specifications may lead to better reuse and shorter production time when developing customized output generators.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995438","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995438","","Unified modeling language","specification languages;formal specification;object-oriented programming;diagrams;software tools;software reusability","automatic model transformations;UML object diagrams;modeling tools;documentation;source code;net list;user-defined model;metamodels;object-oriented concepts;polymorphic model;extensibility mechanisms;experimental results;software reuse;domain-specific modeling;Unified Modeling Language;model-based output generation","","25","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Studies of Pair Programming for CS/SE Teaching in Higher Education: A Systematic Literature Review","N. Salleh; E. Mendes; J. Grundy","International Islamic University of Malaysia, Kuala Lumpur and University of Auckland, Auckland; University of Auckland, Auckland; Swinburne University of Technology, Hawthorn","IEEE Transactions on Software Engineering","","2011","37","4","509","525","The objective of this paper is to present the current evidence relative to the effectiveness of pair programming (PP) as a pedagogical tool in higher education CS/SE courses. We performed a systematic literature review (SLR) of empirical studies that investigated factors affecting the effectiveness of PP for CS/SE students and studies that measured the effectiveness of PP for CS/SE students. Seventy-four papers were used in our synthesis of evidence, and 14 compatibility factors that can potentially affect PP's effectiveness as a pedagogical tool were identified. Results showed that students' skill level was the factor that affected PP's effectiveness the most. The most common measure used to gauge PP's effectiveness was time spent on programming. In addition, students' satisfaction when using PP was overall higher than when working solo. Our meta-analyses showed that PP was effective in improving students' grades on assignments. Finally, in the studies that used quality as a measure of effectiveness, the number of test cases succeeded, academic performance, and expert opinion were the quality measures mostly applied. The results of this SLR show two clear gaps in this research field: 1) a lack of studies focusing on pair compatibility factors aimed at making PP an effective pedagogical tool and 2) a lack of studies investigating PP for software design/modeling tasks in conjunction with programming tasks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5482588","Empirical studies;pair programming;systematic review.","Programming profession;Education;Educational programs;Computer science;Performance evaluation;Time measurement;Testing;Software design;Collaborative work;Algorithm design and analysis","educational technology;further education;software prototyping;teaching","pair programming;CS/SE teaching;higher education;systematic literature review;PP","","88","","75","","","","","","IEEE","IEEE Journals & Magazines"
"Quantitative Evaluation of Model-Driven Performance Analysis and Simulation of Component-Based Architectures","F. Brosig; P. Meier; S. Becker; A. Koziolek; H. Koziolek; S. Kounev","Department of Computer Science, University of Würzburg, Am Hubland, W&#x00FC;rzburg, Germany; Codecentric AG, Elsenheimerstr. 55a, M&#x00FC;nchen, Germany; Department of Software Engineering, University of Paderborn, Zukunftsmeile 1, Paderborn, Germany; Karlsruhe Institute of Technology (KIT), Am Fasanengarten 5, Karlsruhe, Germany; ABB Corporate Research, Wallstadter Str. 59, Ladenburg, Germany; Department of Computer Science, University of Würzburg, Am Hubland, W&#x00FC;rzburg, Germany","IEEE Transactions on Software Engineering","","2015","41","2","157","175","During the last decade, researchers have proposed a number of model transformations enabling performance predictions. These transformations map performance-annotated software architecture models into stochastic models solved by analytical means or by simulation. However, so far, a detailed quantitative evaluation of the accuracy and efficiency of different transformations is missing, making it hard to select an adequate transformation for a given context. This paper provides an in-depth comparison and quantitative evaluation of representative model transformations to, e.g., queueing petri nets and layered queueing networks. The semantic gaps between typical source model abstractions and the different analysis techniques are revealed. The accuracy and efficiency of each transformation are evaluated by considering four case studies representing systems of different size and complexity. The presented results and insights gained from the evaluation help software architects and performance engineers to select the appropriate transformation for a given context, thus significantly improving the usability of model transformations for performance prediction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2362755","DFG; Collaborative Research Center “On-The-Fly Computing”; DFG; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6920061","D.2.11 Software architectures;D.2.10.h Quality analysis and evaluation;D.2.2 Design tools and techniques;Software architectures;quality analysis and evaluation;design tools and techniques","Unified modeling language;Analytical models;Predictive models;Phase change materials;Accuracy;Stochastic processes;Software architecture","object-oriented programming;software architecture;software performance evaluation;stochastic processes","quantitative evaluation;model-driven performance analysis;component-based architectures;performance predictions;transformations map performance-annotated software architecture models;stochastic models;representative model transformations;semantic gaps;source model abstractions","","22","","50","","","","","","IEEE","IEEE Journals & Magazines"
"On the Effectiveness of Contracts as Test Oracles in the Detection and Diagnosis of Functional Faults in Concurrent Object-Oriented Software","W. Araujo; L. C. Briand; Y. Labiche","Juniper Networks, 1194 N Mathilda Ave, Sunnyvale, CA 94089; SnT Centre, University of Luxembourg, 6, rue Richard Coudenhove-Kalergi, L-1359 Luxembourg-Kirchberg, Luxembourg; Systems and Computer Engineering Department, Carleton University, 1125 Colonel By Drive, Ottawa, Canada","IEEE Transactions on Software Engineering","","2014","40","10","971","992","Design by contract (DbC) is a software development methodology that focuses on clearly defining the interfaces between components to produce better quality object-oriented software. Though there exists ample support for DbC for sequential programs, applying DbC to concurrent programs presents several challenges. Using Java as the target programming language, we tackle such challenges by augmenting the Java Modelling Language (JML) and modifying the JML compiler (jmlc) to generate runtime assertion checking code to support DbC in concurrent programs. We applied our solution in a carefully designed case study on a highly concurrent industrial software system from the telecommunications domain to assess the effectiveness of contracts as test oracles in detecting and diagnosing functional faults in concurrent software. Based on these results, clear and objective requirements are defined for contracts to be effective test oracles for concurrent programs whilst balancing the effort to design them. Effort is measured indirectly through the contract complexity measure (CCM), a measure we define. Main results include that contracts of a realistic level of completeness and complexity can detect around 76 percent of faults and reduce the diagnosis effort for such faults tenfold. We, therefore, show that DbC can be applied to concurrent software and can be a valuable tool to improve the economics of software engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339829","Juniper Networks; Luxembourg's National Research Fund; NSERC Discovery; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6857355","Software/Program Verification¿Programming by contract;Software Quality/SQA¿Measurement applied to SQA and V&V;Concurrent programming;object-oriented programming","Contracts;Java;Concurrent computing;Programming;Software;Message systems;Interference","concurrency control;contracts;fault diagnosis;Java;object-oriented methods;parallel programming;program compilers;program testing;software quality;software reliability","design by contract;software development methodology;object-oriented software quality;DbC;sequential programs;concurrent programs;target programming language;Java modelling language;JML compiler;JMLC;runtime assertion checking code;concurrent industrial software system;telecommunication domain;test oracles;functional fault diagnosing;functional fault detection;concurrent software;contract complexity measure;CCM;software engineering","","4","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Data Quality: Some Comments on the NASA Software Defect Datasets","M. Shepperd; Q. Song; Z. Sun; C. Mair","Brunel University, Uxbridge; Xi'an Jiaotong University, Xi'an; Xi'an Jiaotong University, Xi'an; Southampton Solent University, Southampton","IEEE Transactions on Software Engineering","","2013","39","9","1208","1215","Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6464273","Empirical software engineering;data quality;machine learning;defect prediction","NASA;Software;PROM;Educational institutions;Sun;Communities;Abstracts","data analysis;learning (artificial intelligence);pattern classification;software reliability","data quality;NASA software defect dataset;National Aeronautics and Space Administration;data replication;machine learning;software module classification;defect-prone classification;not-defect-prone classification;IEEE Transactions on Software Engineering;data preprocessing;dataset provenance","","98","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Using redundancies to find errors","Yichen Xie; D. Engler","Comput. Syst. Lab., Stanford Univ., CA, USA; Comput. Syst. Lab., Stanford Univ., CA, USA","IEEE Transactions on Software Engineering","","2003","29","10","915","928","Programmers generally attempt to perform useful work. If they performed an action, it was because they believed it served some purpose. Redundant operations violate this belief. However, in the past, redundant operations have been typically regarded as minor cosmetic problems rather than serious errors. This paper demonstrates that, in fact, many redundancies are as serious as traditional hard errors (such as race conditions or null pointer dereferences). We experimentally test this idea by writing and applying five redundancy checkers to a number of large open source projects, finding many errors. We then show that, even when redundancies are harmless, they strongly correlate with the presence of traditional hard errors. Finally, we show how flagging redundant operations gives a way to detect mistakes and omissions in specifications. For example, a locking specification that binds shared variables to their protecting locks can use redundancies to detect missing bindings by flagging critical sections that include no shared state.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237172","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237172","","Redundancy;Writing;Programming profession;Testing;Protection;Software quality;System recovery;Computer languages;Robustness;Debugging","program compilers;redundancy;software quality","compilation;error detection;program redundancy;software quality;conceptual errors","","14","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A System for Profiling and Monitoring Database Access Patterns by Application Programs for Anomaly Detection","L. Bossi; E. Bertino; S. R. Hussain","Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN; Department of Computer Science, Purdue University, West Lafayette, IN","IEEE Transactions on Software Engineering","","2017","43","5","415","431","Database Management Systems (DBMSs) provide access control mechanisms that allow database administrators (DBAs) to grant application programs access privileges to databases. Though such mechanisms are powerful, in practice finer-grained access control mechanism tailored to the semantics of the data stored in the DMBS is required as a first class defense mechanism against smart attackers. Hence, custom written applications which access databases implement an additional layer of access control. Therefore, securing a database alone is not enough for such applications, as attackers aiming at stealing data can take advantage of vulnerabilities in the privileged applications and make these applications to issue malicious database queries. An access control mechanism can only prevent application programs from accessing the data to which the programs are not authorized, but it is unable to prevent misuse of the data to which application programs are authorized for access. Hence, we need a mechanism able to detect malicious behavior resulting from previously authorized applications. In this paper, we present the architecture of an anomaly detection mechanism, DetAnom, that aims to solve such problem. Our approach is based the analysis and profiling of the application in order to create a succinct representation of its interaction with the database. Such a profile keeps a signature for every submitted query and also the corresponding constraints that the application program must satisfy to submit the query. Later, in the detection phase, whenever the application issues a query, a module captures the query before it reaches the database and verifies the corresponding signature and constraints against the current context of the application. If there is a mismatch, the query is marked as anomalous. The main advantage of our anomaly detection mechanism is that, in order to build the application profiles, we need neither any previous knowledge of application vulnerabilities nor any example of possible attacks. As a result, our mechanism is able to protect the data from attacks tailored to database applications such as code modification attacks, SQL injections, and also from other data-centric attacks as well. We have implemented our mechanism with a software testing technique called concolic testing and the PostgreSQL DBMS. Experimental results show that our profiling technique is close to accurate, requires acceptable amount of time, and the detection mechanism incurs low runtime overhead.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2598336","Northrop Grumman Systems Corporation; Department of Homeland Security (DHS); Science and Technology Directorate; Homeland Security Advanced Research Projects Agency; Cyber Security Division; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7534833","Database;insider attacks;anomaly detection;application profile;SQL injection","Databases;Access control;Software testing;Software;Engines","authorisation;database management systems;digital signatures;program diagnostics;program testing;query processing;software architecture","database access patterns profiling;database access patterns monitoring;application programs;anomaly detection mechanism architecture;database management systems;DBMS;database administrator;DBA;defense mechanism;smart attackers;access control mechanism;malicious behavior detection;DetAnom;signature;query submission;software testing technique;concolic testing;PostgreSQL","","4","","40","","","","","","IEEE","IEEE Journals & Magazines"
"A Classification Framework for Software Component Models","I. Crnkovic; S. Sentilles; A. Vulgarakis; M. R. V. Chaudron","Mälardalen University, Västerås; Mälardalen University, Västerås; Mälardalen University, Västerås; Universiteit Leiden, Leiden","IEEE Transactions on Software Engineering","","2011","37","5","593","615","In the last decade, a large number of different software component models have been developed, with different aims and using different principles and technologies. This has resulted in a number of models which have many similarities, but also principal differences, and in many cases unclear concepts. Component-based development has not succeeded in providing standard principles, as has, for example, object-oriented development. In order to increase the understanding of the concepts and to differentiate component models more easily, this paper identifies, discusses, and characterizes fundamental principles of component models and provides a Component Model Classification Framework based on these principles. Further, the paper classifies a large number of component models using this framework.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.83","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5587419","Software components;software component models;component lifecycle;extra-functional properties;component composition.","Data models;Bismuth;Packaging","object-oriented programming;pattern classification","software component models;component based development;object oriented development;component model classification framework","","83","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Hipikat: a project memory for software development","D. Cubranic; G. C. Murphy; J. Singer; K. S. Booth","Dept. of Comput. Sci., Victoria Univ., BC, Canada; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","6","446","465","Sociological and technical difficulties, such as a lack of informal encounters, can make it difficult for new members of noncollocated software development teams to learn from their more experienced colleagues. To address this situation, we have developed a tool, named Hipikat that provides developers with efficient and effective access to the group memory for a software development project that is implicitly formed by all of the artifacts produced during the development. This project memory is built automatically with little or no change to existing work practices. After describing the Hipikat tool, we present two studies investigating Hipikat's usefulness in software modification tasks. One study evaluated the usefulness of Hipikat's recommendations on a sample of 20 modification tasks performed on the Eclipse Java IDE during the development of release 2.1 of the Eclipse software. We describe the study, present quantitative measures of Hipikat's performance, and describe in detail three cases that illustrate a range of issues that we have identified in the results. In the other study, we evaluated whether software developers who are new to a project can benefit from the artifacts that Hipikat recommends from the project memory. We describe the study, present qualitative observations, and suggest implications of using project memory as a learning aid for project newcomers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.71","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463229","Index Terms- Software development teams;project memory;software artifacts;recommender system;user studies.","Programming;Virtual groups;Software systems;Computer Society;Software tools;Performance evaluation;Java;Software performance;Recommender systems;Employee welfare","software tools;project management;Java;programming environments;software performance evaluation","Hipikat tool;project memory;software development;software artifacts;recommerider system;user studies;software development project;software modification task;Eclipse Java IDE;Eclipse software;learning aid","","129","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Modular Software Model Checking for Distributed Systems","W. Leungwattanakit; C. Artho; M. Hagiya; Y. Tanabe; M. Yamamoto; K. Takahashi","Department of Mathematics and Informatics, Faculty of Science, Chiba University, 1-3-3 Yayoicho, Inage Ward, Chiba, Japan; Research Institute for Secure Systems at the National Institute of Advanced Industrial Science and Technology (AIST), Amagasaki, Japan; Department of Computer Science, University of Tokyo, Science Building No. 7, 7-3-1 Hongo, Tokyo, Japan; National Institute of Informatics (NII), 2-1-2 Hitotsubashi, Japan; Department of Mathematics and Informatics, Faculty of Science, Chiba University, 1-3-3 Yayoicho, Inage Ward, Chiba, Japan; Research Institute for Secure Systems at the National Institute of Advanced Industrial Science and Technology (AIST), Amagasaki, Japan","IEEE Transactions on Software Engineering","","2014","40","5","483","501","Distributed systems are complex, being usually composed of several subsystems running in parallel. Concurrent execution and inter-process communication in these systems are prone to errors that are difficult to detect by traditional testing, which does not cover every possible program execution. Unlike testing, model checking can detect such faults in a concurrent system by exploring every possible state of the system. However, most model-checking techniques require that a system be described in a modeling language. Although this simplifies verification, faults may be introduced in the implementation. Recently, some model checkers verify program code at runtime but tend to be limited to stand-alone programs. This paper proposes cache-based model checking, which relaxes this limitation to some extent by verifying one process at a time and running other processes in another execution environment. This approach has been implemented as an extension of Java PathFinder, a Java model checker. It is a scalable and promising technique to handle distributed systems. To support a larger class of distributed systems, a checkpointing tool is also integrated into the verification system. Experimental results on various distributed systems show the capability and scalability of cache-based model checking.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6645368","Software model checking;software verification;distributed systems;checkpointing","Model checking;Software;Java;Checkpointing;Synchronization;Scalability;Message systems","cache storage;checkpointing;concurrency control;distributed processing;Java;program testing;program verification","modular software model checking;distributed systems;parallel subsystems;concurrent execution;interprocess communication;program execution;fault detection;model-checking techniques;modeling language;program code verification;stand-alone programs;cache-based model checking;execution environment;Java PathFinder extension;Java model checker;checkpointing tool","","7","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Toward understanding the rhetoric of small source code changes","R. Purushothaman; D. E. Perry","Server Operating Syst. Group, Dell Comput. Corp., Round Rock, TX, USA; NA","IEEE Transactions on Software Engineering","","2005","31","6","511","526","Understanding the impact of software changes has been a challenge since software systems were first developed. With the increasing size and complexity of systems, this problem has become more difficult. There are many ways to identify the impact of changes on the system from the plethora of software artifacts produced during development, maintenance, and evolution. We present the analysis of the software development process using change and defect history data. Specifically, we address the problem of small changes by focusing on the properties of the changes rather than the properties of the code itself. Our study reveals that 1) there is less than 4 percent probability that a one-line change introduces a fault in the code, 2) nearly 10 percent of all changes made during the maintenance of the software under consideration were one-line changes, 3) nearly 50 percent of the changes were small changes, 4) nearly 40 percent of changes to fix faults resulted in further faults, 5) the phenomena of change differs for additions, deletions, and modifications as well as for the number of lines affected, and 6) deletions of up to 10 lines did not cause faults.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.74","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463233","Index Terms- Source code changes;software faults;one-line changes;fault probabilities.","Rhetoric;Software maintenance;Risk management;Software systems;Programming;Costs;Computer Society;History;Computer architecture;Life testing","software fault tolerance;software maintenance;reverse engineering","source code changes;software faults;one-line changes;fault probabilities;software system;plethora;software artifacts;software development process;defect history data;software maintenance","","71","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Program slicing with dynamic points-to sets","M. Mock; D. C. Atkinson; C. Chambers; S. J. Eggers","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","8","657","678","Program slicing is a potentially useful analysis for aiding program understanding. However, in reality even slices of small programs are often too large to be useful. Imprecise pointer analyses have been suggested as one cause of this problem. In this paper, we use dynamic points-to data, which represents optimistic pointer information, to obtain a bound on the best case slice size improvement that can be achieved with improved pointer precision. Our experiments show that slice size can be reduced significantly for programs that make frequent use of calls through function pointers because for them the dynamic pointer data results in a considerably smaller call graph, which leads to fewer data dependences. Programs without or with only few calls through function pointers, however, show considerably less improvement. We discovered that C programs appear to have a significant fraction of direct and nonspurious pointer data dependences so that reducing spurious dependences via pointers is only of limited benefit. Consequently, to make slicing useful in general for such programs, improvements beyond better pointer analyses are necessary. On the other hand, since we show that collecting dynamic function pointer information can be performed with little overhead (average slowdown of 10 percent for our benchmarks), dynamic pointer information may be a practical approach to making slicing of programs with frequent function pointer use more successful in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.94","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498771","Index Terms- Dynamic analysis;points-to analysis;program slicing.","Data analysis;Computer Society;Application software;Programming profession;Debugging;Software maintenance;Software testing;Reverse engineering;Computer languages;Information analysis","program slicing;reverse engineering;software cost estimation","program slicing;optimistic pointer information;function pointers;call graph;data dependences;C programs;dynamic function pointer information;dynamic points-to set analysis;program understanding","","20","","61","","","","","","IEEE","IEEE Journals & Magazines"
"RELAI Testing: A Technique to Assess and Improve Software Reliability","D. Cotroneo; R. Pietrantuono; S. Russo","Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione (DIETI), Università di Napoli Federico II, Via Claudio 21, Naples, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione (DIETI), Università di Napoli Federico II, Via Claudio 21, Naples, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell’Informazione (DIETI), Università di Napoli Federico II, Via Claudio 21, Naples, Italy","IEEE Transactions on Software Engineering","","2016","42","5","452","475","Testing software to assess or improve reliability presents several practical challenges. Conventional operational testing is a fundamental strategy that simulates the real usage of the system in order to expose failures with the highest occurrence probability. However, practitioners find it unsuitable for assessing/achieving very high reliability levels; also, they do not see the adoption of a “real” usage profile estimate as a sensible idea, being it a source of non-quantifiable uncertainty. Oppositely, debug testing aims to expose as many failures as possible, but regardless of their impact on runtime reliability. These strategies are used either to assess or to improve reliability, but cannot improve and assess reliability in the same testing session. This article proposes Reliability Assessment and Improvement (RELAI) testing, a new technique thought to improve the delivered reliability by an adaptive testing scheme, while providing, at the same time, a continuous assessment of reliability attained through testing and fault removal. The technique also quantifies the impact of a partial knowledge of the operational profile. RELAI is positively evaluated on four software applications compared, in separate experiments, with techniques conceived either for reliability improvement or for reliability assessment, demonstrating substantial improvements in both cases.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2491931","European Commission; FP7 Marie Curie Industry-Academia Partnerships and Pathways (IAPP); MIUR; SVEVIA; COSMIC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7299696","Software Testing;Reliability;Operational Testing;Random Testing;Sampling;Operational Profile;Software testing;reliability;operational testing;random testing;sampling;operational profile","Testing;Software reliability;Software;Uncertainty;Estimation error","probability;program debugging;program testing;software reliability","RELAI testing;software testing;operational testing;nonquantifiable uncertainty;debug testing;software failures;runtime reliability;reliability assessment and improvement testing;software reliability;adaptive testing scheme;continuous reliability assessment;fault removal;software applications","","5","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Verifying Linearizability via Optimized Refinement Checking","Y. Liu; W. Chen; Y. A. Liu; J. Sun; S. J. Zhang; J. S. Dong","Nanyang Technological University, Singapore; Microsoft Research Asia, Beijing; State University of New York at Stony Brook, Stony Brook; Singapore University of Technology and Design, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore","IEEE Transactions on Software Engineering","","2013","39","7","1018","1039","Linearizability is an important correctness criterion for implementations of concurrent objects. Automatic checking of linearizability is challenging because it requires checking that: (1) All executions of concurrent operations are serializable, and (2) the serialized executions are correct with respect to the sequential semantics. In this work, we describe a method to automatically check linearizability based on refinement relations from abstract specifications to concrete implementations. The method does not require that linearization points in the implementations be given, which is often difficult or impossible. However, the method takes advantage of linearization points if they are given. The method is based on refinement checking of finite-state systems specified as concurrent processes with shared variables. To tackle state space explosion, we develop and apply symmetry reduction, dynamic partial order reduction, and a combination of both for refinement checking. We have built the method into the PAT model checker, and used PAT to automatically check a variety of implementations of concurrent objects, including the first algorithm for scalable nonzero indicators. Our system is able to find all known and injected bugs in these implementations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.82","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363443","Linearizability;refinement;model checking;PAT","History;Sun;Educational institutions;Optimization;Electronic mail;Semantics","concurrency control;formal specification;program debugging;program verification;programming language semantics;refinement calculus","linearizability verification;optimized refinement checking;concurrent objects;automatic linearizability checking;concurrent operations;serialized executions;sequential semantics;refinement relations;linearization points;finite-state systems;shared variables;symmetry reduction;dynamic partial-order reduction;PAT model checker;scalable nonzero indicators;bug detection","","11","","62","","","","","","IEEE","IEEE Journals & Magazines"
"GossipKit: A Unified ComponentFramework for Gossip","F. Taïani; S. Lin; G. S. Blair","University of Rennes 1 / IRISA Rennes, France; SAP Labs, China; University of Lancaster, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","2","123","136","Although the principles of gossip protocols are relatively easy to grasp, their variety can make their design and evaluation highly time consuming. This problem is compounded by the lack of a unified programming framework for gossip, which means developers cannot easily reuse, compose, or adapt existing solutions to fit their needs, and have limited opportunities to share knowledge and ideas. In this paper, we consider how component frameworks, which have been widely applied to implement middleware solutions, can facilitate the development of gossip-based systems in a way that is both generic and simple. We show how such an approach can maximize code reuse, simplify the implementation of gossip protocols, and facilitate dynamic evolution and redeployment.Also known as “epidemic” protocols.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6645372","Distributed systems;components;frameworks;protocols","Protocols;Peer-to-peer computing;Programming;Wireless sensor networks;Ad hoc networks;Software;Assembly","distributed processing;object-oriented programming;protocols","GossipKit;unified component framework;gossip protocols;protocol design;protocol evaluation;unified programming framework;middleware solutions;gossip-based systems;code reuse;epidemic protocols","","2","","77","","","","","","IEEE","IEEE Journals & Magazines"
"Software defect association mining and defect correction effort prediction","Qinbao Song; M. Shepperd; M. Cartwright; C. Mair","Dept. of Comput. Sci. & Technol., Xi'an Jiaotong Univ., China; NA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","2","69","82","Much current software defect prediction work focuses on the number of defects remaining in a software system. In this paper, we present association rule mining based methods to predict defect associations and defect correction effort. This is to help developers detect software defects and assist project managers in allocating testing resources more effectively. We applied the proposed methods to the SEL defect data consisting of more than 200 projects over more than 15 years. The results show that, for defect association prediction, the accuracy is very high and the false-negative rate is very low. Likewise, for the defect correction effort prediction, the accuracy for both defect isolation effort prediction and defect correction effort prediction are also high. We compared the defect correction effort prediction method with other types of methods - PART, C4.5, and Naive Bayes - and show that accuracy has been improved by at least 23 percent. We also evaluated the impact of support and confidence levels on prediction accuracy, false-negative rate, false-positive rate, and the number of rules. We found that higher support and confidence levels may not result in higher prediction accuracy, and a sufficient number of rules is a precondition for high prediction accuracy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599417","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599417","Software defect prediction;defect association;defect isolation effort;defect correction effort.","Software systems;Accuracy;Software quality;Data mining;Resource management;Job shop scheduling;Inspection;Association rules;Project management;Software development management","program testing;data mining;software quality;software process improvement","software defect association prediction;defect correction effort prediction;SEL defect data;association rule mining;software quality","","80","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Facilitating Coordination between Software Developers: A Study and Techniques for Timely and Efficient Recommendations","K. Blincoe; G. Valetto; D. Damian","Software Engineering Global Interaction Lab, Victoria, BC, Canada; Fondazione Bruno Kessler, Trento, Italy; Software Engineering Global Interaction Lab, Victoria, BC, Canada","IEEE Transactions on Software Engineering","","2015","41","10","969","985","When software developers fail to coordinate, build failures, duplication of work, schedule slips and software defects can result. However, developers are often unaware of when they need to coordinate, and existing methods and tools that help make developers aware of their coordination needs do not provide timely or efficient recommendations. We describe our techniques to identify timely and efficient coordination recommendations, which we developed and evaluated in a study of coordination needs in the Mylyn software project. We describe how data obtained from tools that capture developer actions within their Integrated Development Environment (IDE) as they occur can be used to timely identify coordination needs; we also describe how properties of tasks coupled with machine learning can focus coordination recommendations to those that are more critical to the developers to reduce information overload and provide more efficient recommendations. We motivate our techniques through developer interviews and report on our quantitative analysis of coordination needs in the Mylyn project. Our results suggest that by leveraging IDE logging facilities, properties of tasks and machine learning techniques awareness tools could make developers aware of critical coordination needs in a timely way. We conclude by discussing implications for software engineering research and tool design.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2431680","US National Science Foundation (NSF); NECSIS; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7105409","Human Factors in Software Design;Management;Metrics/Measurement;Productivity;Programming Teams;Computer-supported cooperative work;human factors in software design;management;metrics/measurement;productivity;programming teams","Software;Encoding;Interviews;Statistical analysis;Manuals;Accuracy;Correlation","groupware;learning (artificial intelligence);programming environments;project management;software tools","software developers;coordination recommendation;Mylyn software project;integrated development environment;coordination needs quantitative analysis;IDE logging facilities;task properties;machine learning technique awareness tools;software engineering research;tool design","","3","","70","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing Ordered Throughput Using Autonomic Cloud Bursting Schedulers","S. Kailasam; N. Gnanasambandam; J. Dharanipragada; N. Sharma","Indian Institute of Technology Madras, Chennai; Xerox Research Center Webster, New York; Indian Institute of Technology Madras, Chennai; Xerox Research Center Webster, New York","IEEE Transactions on Software Engineering","","2013","39","11","1564","1581","Optimizing ordered throughput not only improves the system efficiency but also makes cloud bursting transparent to the user. This is critical from the perspective of user fairness in customer-facing systems, correctness in stream processing systems, and so on. In this paper, we consider optimizing ordered throughput for near real-time, data-intensive, independent computations using cloud bursting. Intercloud computation of data-intensive applications is a challenge due to large data transfer requirements, low intercloud bandwidth, and best-effort traffic on the Internet. The system model we consider is comprised of two processing stages. The first stage uses cloud bursting opportunistically for parallel processing, while the second stage (sequential) expects the output of the first stage to be in the same order as the arrival sequence. We propose three scheduling heuristics as part of an autonomic cloud bursting approach that adapt to changing workload characteristics, variation in bandwidth, and available resources to optimize ordered throughput. We also characterize the operational regimes for cloud bursting as stabilization mode versus acceleration mode, depending on the workload characteristics like the size of data to be transferred for a given compute load. The operational regime characterization helps in deciding how many instances can be optimally utilized in the external cloud.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6520852","Cloud bursting;ordered throughput;autonomic;data-intensive","Cloud computing;Optimization;Scheduling","cloud computing;fault tolerant computing;parallel processing;scheduling","external cloud;operational regime characterization;data size;acceleration mode;stabilization mode;bandwidth variation;workload characteristics;scheduling heuristics;arrival sequence;parallel processing;Internet traffic;intercloud bandwidth;large data transfer requirements;data-intensive applications;intercloud computation;near real-time data-intensive independent computation;stream processing system correctness;customer-facing systems;user fairness;system efficiency improvement;autonomic cloud bursting schedulers;ordered throughput optimization","","13","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing the Defect Reduction Benefits of Code Inspection and Test-Driven Development","J. W. Wilkerson; J. F. Nunamaker; R. Mercer","Pennsylvania State University, Erie, Erie; University of Arizona, Tucson; University of Arizona, Tucson","IEEE Transactions on Software Engineering","","2012","38","3","547","560","This study is a quasi experiment comparing the software defect rates and implementation costs of two methods of software defect reduction: code inspection and test-driven development. We divided participants, consisting of junior and senior computer science students at a large Southwestern university, into four groups using a two-by-two, between-subjects, factorial design and asked them to complete the same programming assignment using either test-driven development, code inspection, both, or neither. We compared resulting defect counts and implementation costs across groups. We found that code inspection is more effective than test-driven development at reducing defects, but that code inspection is also more expensive. We also found that test-driven development was no more effective at reducing defects than traditional programming methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5750007","Agile programming;code inspections and walk throughs;reliability;test-driven development;testing strategies;empirical study.","Inspection;Software;Testing;Java;Writing;Programming profession","program testing;system recovery","defect reduction benefits;code inspection;test driven development;quasi experiment;software defect rates;software defect reduction;senior computer science students;junior computer science students;programming assignment","","7","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Detection and Resolution of Lexical Ambiguity in Process Models","F. Pittke; H. Leopold; J. Mendling","Institute for Information Business, Vienna, WU, Austria; Department of Computer Science, VU University Amsterdam, The Netherlands; Institute for Information Business, Vienna, WU, Austria","IEEE Transactions on Software Engineering","","2015","41","6","526","544","System-related engineering tasks are often conducted using process models. In this context, it is essential that these models do not contain structural or terminological inconsistencies. To this end, several automatic analysis techniques have been proposed to support quality assurance. While formal properties of control flow can be checked in an automated fashion, there is a lack of techniques addressing textual quality. More specifically, there is currently no technique available for handling the issue of lexical ambiguity caused by homonyms and synonyms. In this paper, we address this research gap and propose a technique that detects and resolves lexical ambiguities in process models. We evaluate the technique using three process model collections from practice varying in size, domain, and degree of standardization. The evaluation demonstrates that the technique significantly reduces the level of lexical ambiguity and that meaningful candidates are proposed for resolving ambiguity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2396895","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7027184","Identification of Lexical Ambiguity;Resolution of Lexical Ambiguity;Business Process Models;Identification of lexical ambiguity;resolution of lexical ambiguity;business process models","Unified modeling language;Object oriented modeling;Context;Business;Natural languages;Manuals;Vectors","computational linguistics;grammars;natural language processing;software engineering","automatic detection;automatic resolution;lexical ambiguity;system-related engineering task;structural inconsistency;terminological inconsistency;automatic analysis technique;quality assurance;control flow;automated fashion;textual quality;homonyms;synonyms;process model collection","","22","","106","","","","","","IEEE","IEEE Journals & Magazines"
"A layered architecture for uniform version management","B. Westfechtel; B. P. Munch; R. Conradi","Dept. of Comput. Sci. III, Aachen Univ. of Technol., Germany; NA; NA","IEEE Transactions on Software Engineering","","2001","27","12","1111","1133","Version management is a key part of software configuration management. A big variety of version models has been realized in both commercial systems and research prototypes. These version models differ with respect to the objects put under version control (files, directories, entities, objects), the organization of versions (version graphs versus multidimensional version spaces), the granularity of versioning (whole software products versus individual components), emphasis on states versus emphasis on changes (state-versus change-based versioning), rules for version selection, etc. We present a uniform version model-and its support architecture-for software configuration management. Unlike other unification approaches, such as UML for object-oriented modeling, we do not assemble all the concepts having been introduced in previous systems. Instead, we define a base model that is built on a small number of concepts. Specific version models may be expressed in terms of this base model. Our approach to uniform version management is distinguished by its underlying layered architecture. Unlike the main stream of software configuration management systems, our instrumentable version engine is completely orthogonal to the data model used for representing software objects and their relationships. In addition, we introduce version rules at the bottom of the layered architecture and employ them as a uniform mechanism for expressing different version models. This contrasts to the main stream solution, where a specific version model-usually version graphs-is deeply built into the system and version rules are dependent on this model.","0098-5589;1939-3520;2326-3881","","10.1109/32.988710","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988710","","Object oriented modeling;Computer architecture;Software prototyping;Prototypes;Multidimensional systems;Unified modeling language;Assembly systems;Software systems;Instruments;Engines","configuration management;software architecture","uniform version management;layered architecture;support architecture;software configuration management;granularity;states;uniform version model;instrumentable version engine","","31","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Uncertainty Analysis in Software Reliability Modeling by Bayesian Analysis with Maximum-Entropy Principle","Y. Dai; M. Xie; Q. Long; S. Ng","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","11","781","795","In software reliability modeling, the parameters of the model are typically estimated from the test data of the corresponding component. However, the widely used point estimators are subject to random variations in the data, resulting in uncertainties in these estimated parameters. Ignoring the parameter uncertainty can result in grossly underestimating the uncertainty in the total system reliability. This paper attempts to study and quantify the uncertainties in the software reliability modeling of a single component with correlated parameters and in a large system with numerous components. Another characteristic challenge in software testing and reliability is the lack of available failure data from a single test, which often makes modeling difficult. This lack of data poses a bigger challenge in the uncertainty analysis of the software reliability modeling. To overcome this challenge, this paper proposes utilizing experts' opinions and historical data from previous projects to complement the small number of observations to quantify the uncertainties. This is done by combining the maximum-entropy principle (MEP) into the Bayesian approach. This paper further considers the uncertainty analysis at the system level, which contains multiple components, each with its respective model/parameter/ uncertainty, by using a Monte Carlo approach. Some examples with different modeling approaches (NHPP, Markov, Graph theory) are illustrated to show the generality and effectiveness of the proposed approach. Furthermore, we illustrate how the proposed approach for considering the uncertainties in various components improves a large-scale system reliability model.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70739","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339233","Software Reliability;Uncertainty analysis;Bayesian method;Monte Carlo;Markov model;Graph theory","Uncertainty;Software reliability;Bayesian methods;Parameter estimation;Software testing;Uncertain systems;Monte Carlo methods;Graph theory;Reliability theory;Predictive models","Bayes methods;maximum entropy methods;Monte Carlo methods;parameter estimation;program testing;software reliability","uncertainty analysis;software reliability modeling;Bayesian approach;maximum-entropy principle;point estimators;parameter estimation;parameter uncertainty;system reliability;software testing;Monte Carlo approach","","44","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Combining Perceptions and Prescriptions in Requirements Engineering Process Assessment: An Industrial Case Study","N. P. Napier; L. Mathiassen; R. D. Johnson","Georgia Gwinnett College, Lawrenceville; Georgia State University, Atlanta; University of Pretoria, Pretoria","IEEE Transactions on Software Engineering","","2009","35","5","593","606","Requirements engineering (RE) is a key discipline in software development and several methods are available to help assess and improve RE processes. However, these methods rely on prescriptive models of RE; they do not, like other disciplines within software engineering, draw directly on stakeholder perceptions and subjective judgments. Given this backdrop, we present an empirical study in RE process assessment. Our aim was to investigate how stakeholder perceptions and process prescriptions can be combined during assessments to effectively inform RE process improvement. We first describe existing methods for RE process assessment and the role played by stakeholder perceptions and subjective judgments in the software engineering and management literature. We then present a method that combines perceptions and prescriptions in RE assessments together with an industrial case study in which the method was applied and evaluated over a three-year period at TelSoft. The data suggest that the combined method led to a comprehensive and rich assessment and it helped TelSoft consider RE as an important and integral part of the broader engineering context. This, in turn, led to improvements that combined plan-driven and adaptive principles for RE. Overall, the combined method helped TelSoft move from Level 1 to Level 2 in RE maturity, and the employees perceived the resulting engineering practices to be improved. Based on these results, we suggest that software managers and researchers combine stakeholder perceptions and process prescriptions as one way to effectively balance the specificity, comparability, and accuracy of software process assessments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4967614","Process implementation and change;qualitative process analysis;requirements engineering process;software management;software process.","Software engineering;Computer industry;Programming;Engineering management;Software quality;Project management;Risk management;Educational institutions;Data engineering;Quality management","formal specification;formal verification;project management;software development management;software maintenance;software process improvement;software quality;statistical analysis;systems analysis","requirements engineering process assessment;industrial case study;software development;RE prescription model;software engineering;stakeholder perception;subjective judgment;empirical study;software RE process improvement;software project management;TelSoft;plan-driven principle;adaptive principle;software process change;qualitative process analysis","","17","","56","","","","","","IEEE","IEEE Journals & Magazines"
"A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches","S. Herbold; A. Trautsch; J. Grabowski","University of Goettingen, Göttingen, Germany; University of Goettingen, Göttingen, Germany; University of Goettingen, Göttingen, Germany","IEEE Transactions on Software Engineering","","2018","44","9","811","833","Cross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article, we provide a benchmark for CPDP. We replicate 24 approaches proposed by researchers between 2008 and 2015 and evaluate their performance on software products from five different data sets. Based on our benchmark, we determined that an approach proposed by Camargo Cruz and Ochimizu (2009) based on data standardization performs best and is always ranked among the statistically significant best results for all metrics and data sets. Approaches proposed by Turhan et al. (2009), Menzies et al. (2011), and Watanabe et al. (2008) are also nearly always among the best results. Moreover, we determined that predictions only seldom achieve a high performance of 0.75 recall, precision, and accuracy. Thus, CPDP still has not reached a point where the performance of the results is sufficient for the application in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2724538","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7972992","Cross-project defect prediction;benchmark;comparison;replication","Benchmark testing;Prediction methods;Software;Quality assurance;Measurement;Correlation","project management;quality assurance;software metrics;software quality","benchmark cross-Project Defect Prediction approaches;CPDP;quality assurance;software projects;software products;data standardization;performance metrics;data sets","","3","","90","","","","","","IEEE","IEEE Journals & Magazines"
"Designing Autonomic Management Systems by Using Reactive Control Techniques","N. Berthier; É. Rutten; N. De Palma; S. M. Gueye","ERODS team, University of Grenoble LIG Bât. C, 220 rue de la Chimie, St Martin d'Hères, France; LIG/INRIA Grenoble - Rhône-Alpes, Inovallée, 655 av. de l'Europe, Montbonnot, St Ismier, France; ERODS team, University of Grenoble, LIG Bât. C, 220 rue de la Chimie, St Martin d'Hères, France; ERODS team, University of Grenoble, LIG Bât. C, 220 rue de la Chimie, St Martin d'Hères, France","IEEE Transactions on Software Engineering","","2016","42","7","640","657","The ever growing complexity of software systems has led to the emergence of automated solutions for their management. The software assigned to this work is usually called an Autonomic Management System (AMS). It is ordinarily designed as a composition of several managers, which are pieces of software evaluating the dynamics of the system under management through measurements (e.g., workload, memory usage), taking decisions, and acting upon it so that it stays in a set of acceptable operating states. However, careless combination of managers may lead to inconsistencies in the taken decisions, and classical approaches dealing with these coordination problems often rely on intricate and ad hoc solutions. To tackle this problem, we take a global view and underscore that AMSs are intrinsically reactive, as they react to flows of monitoring data by emitting flows of reconfiguration actions. Therefore we propose a new approach for the design of AMSs, based on synchronous programming and discrete controller synthesis techniques. They provide us with high-level languages for modeling the system to manage, as well as means for statically guaranteeing the absence of logical coordination problems. Hence, they suit our main contribution, which is to obtain guarantees at design time about the absence of logical inconsistencies in the taken decisions. We detail our approach, illustrate it by designing an AMS for a realistic multi-tier application, and evaluate its practicality with an implementation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2510004","French ANR project Ctrl-Green; ANR INFRA; MINALOGIC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7360217","Autonomic computing;coordination;discrete control;reactive programming","Software;Programming;Automata;Sensor systems;Actuators","software management;software performance evaluation","autonomic management systems;reactive control techniques;software systems;AMS;software evaluation;software measurements;ad hoc solutions;monitoring data flow;emitting flows;synchronous programming;discrete controller synthesis techniques;logical coordination problems;realistic multitier application","","2","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Validating Second-Order Mutation at System Level","P. Reales Mateo; M. Polo Usaola; J. L. Fernández Alemán","University of Castilla-La Mancha, Ciudad Real; University of Castilla-La Mancha, Ciudad Real; University of Murcia, Murcia","IEEE Transactions on Software Engineering","","2013","39","4","570","587","Mutation has been recognized to be an effective software testing technique. It is based on the insertion of artificial faults in the system under test (SUT) by means of a set of mutation operators. Different operators can mutate each program statement in several ways, which may produce a huge number of mutants. This leads to very high costs for test case execution and result analysis. Several works have approached techniques for cost reduction in mutation testing, like n-order mutation where each mutant contains n artificial faults instead of one. There are two approaches to n-order mutation: increasing the effectiveness of mutation by searching for good n-order mutants, and decreasing the costs of mutation testing by reducing the mutants set through the combination of the first-order mutants into n-order mutants. This paper is focused on the second approach. However, this second use entails a risk: the possibility of leaving undiscovered faults in the SUT, which may distort the perception of the test suite quality. This paper describes an empirical study of different combination strategies to compose second-order mutants at system level as well as a cost-risk analysis of n-order mutation at system level.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6216382","Empirical evaluation;high-order mutation;mutation testing","Algorithm design and analysis;Concrete;Educational institutions;Benchmark testing;Optimization;Software testing","program testing;software fault tolerance","second order mutation;system level;software testing technique;system under test;SUT;mutation operators;test case execution;cost risk analysis","","7","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting Scope Tracking and Visualization for Very Large-Scale Requirements Engineering-Utilizing FSC+, Decision Patterns, and Atomic Decision Visualizations","K. Wnuk; T. Gorschek; D. Callele; E. Karlsson; E. Åhlin; B. Regnell","Software Engineering Research Lab (SERL), Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden; Software Engineering Research Lab (SERL), Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Add a Lot, Sweden; Sony Mobile Communications, Lund, Sweden; Department of Computer Science, Lund University, Lund, Sweden","IEEE Transactions on Software Engineering","","2016","42","1","47","74","Deciding the optimal project scope that fulfills the needs of the most important stakeholders is challenging due to a plethora of aspects that may impact decisions. Large companies that operate in rapidly changing environments experience frequently changing customer needs which force decision makers to continuously adjust the scope of their projects. Change intensity is further fueled by fierce market competition and hard time-to-market deadlines. Staying in control of the changes in thousands of features becomes a major issue as information overload hinders decision makers from rapidly extracting relevant information. This paper presents a visual technique, called Feature Survival Charts+ (FSC+), designed to give a quick and effective overview of the requirements scoping process for Very Large-Scale Requirements Engineering (VLSRE). FSC+ were applied at a large company with thousands of features in the database and supported the transition from plan-driven to a more dynamic and change-tolerant release scope management process. FSC+ provides multiple views, filtering, zooming, state-change intensity views, and support for variable time spans. Moreover, this paper introduces five decision archetypes deduced from the dataset and subsequently analyzed and the atomic decision visualization that shows the frequency of various decisions in the process. The capabilities and usefulness of FSC+, decision patterns (state changes that features undergo) and atomic decision visualizations are evaluated through interviews with practitioners who found utility in all techniques and indicated that their inherent flexibility was necessary to meet the varying needs of the stakeholders.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2445347","IKNOWDM project; Knowledge Foundation in Sweden; SCALARE ITEA2 project; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7123669","- D.2.1Requirements/Specifications D.2.9.d Initiation and scope definition D.2.9 Management;Requirements/specifications;initiation and scope definition;management","Power capacitors;Companies;Planning;Visualization;Software;Software engineering;Electronic mail","data visualisation;formal specification","scope tracking;very large-scale requirements engineering visualization;FSC+;decision patterns;atomic decision visualizations;Feature Survival Charts+;requirements scoping process;very large-scale requirements engineering;VLSRE;change-tolerant release scope management process","","4","","108","","","","","","IEEE","IEEE Journals & Magazines"
"Avoiding packaging mismatch with flexible packaging","R. DeLine","Carnegie Mellon Univ., Pittsburgh, PA, USA","IEEE Transactions on Software Engineering","","2001","27","2","124","143","To integrate a software component into a system, it must interact properly with the system's other components. Unfortunately, the decisions about how a component is to interact with other components are typically committed long before the moment of integration and are difficult to change. This paper introduces the flexible packaging method, which allows a component developer to defer some decisions about component interaction until system integration time. The method divides the component's source into two pieces: the ware, which encapsulates the component's functionality; and the packager, which encapsulates the details of interaction. Both the ware and the packager are independently reusable. A ware, as a reusable part, allows a given piece of functionality to be employed in systems in different architectural styles. A packager, as a reusable part, encapsulates conformance to a component standard, like an ActiveX control or an ODBC database accessor. Because the packager's source code is often formulaic, a tool is provided to generate the packager's source from a high-level description of the intended interaction, a description written in the architectural description language UniCon. The method and tools are evaluated with a series of experiments in which three wares and nine types of packaging are combined to form thirteen components.","0098-5589;1939-3520;2326-3881","","10.1109/32.908958","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=908958","","Packaging;Software packages;Software systems;Databases;Architecture description languages;Software architecture;Filters;System testing;Heart;Maintenance engineering","software reusability;object-oriented programming;software architecture;program processors","packaging mismatch;flexible packaging;software component;ware;component functionality;packager;ActiveX control;ODBC database accessor;architectural description language;UniCon;software packaging;system integration;software architecture","","9","","32","","","","","","IEEE","IEEE Journals & Magazines"
"An Efficient and Scalable Approach to Correct Class Model Refinement","W. Shen; K. Wang; A. Egyed","Western Michigan University, Kalamazoo; Siemens PLM Software, Ann Arbor; Johannes Kepler University, Linz","IEEE Transactions on Software Engineering","","2009","35","4","515","533","Today, programmers benefit immensely from Integrated Development Environments (IDEs), where errors are highlighted within seconds of their introduction. Yet, designers rarely benefit from such an instant feedback in modeling tools. This paper focuses on the refinement of UML-style class models with instant feedback on correctness. Following the Model-Driven Architecture (MDA) paradigm, we strongly believe in the benefit of maintaining high-level and low-level models separately to 1) document the lower level model and 2) continuously ensure the correctness of the low-level model during later evolution (i.e., high- or low-level models may be evolved independently). However, currently the refinement and subsequent evolution lack automated support, let alone an instant feedback on their correctness (i.e., consistency). Traditional approaches to consistency checking fail here because of the computational cost of comparing class models. Our proposed instant approach first transforms the low-level model into an intermediate model that is then easier comparable with the high-level model. The key to computational scalability is the separation of transformation and comparison so that each can react optimally to changes-changes that could happen concurrently in both the high- and low-level class models. We evaluate our approach on eight third-party design models. The empirical data show that the separation of transformation and comparison results in a 6 to 11-fold performance gain and a ninefold reduction in producing irrelevant feedback. While this work emphasizes the refinement of class models, we do believe that the concepts are more generally applicable to other kinds of modeling languages, where transformation and subsequent comparison are computationally expensive.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815278","Class models;consistency checking;refinement;separation of concerns;and UML.","Feedback;Programming profession;Unified modeling language;Software systems;Error correction;Computational efficiency;Concurrent computing;Scalability;Performance gain;System recovery","software architecture;Unified Modeling Language","class model refinement;integrated development environment;UML-style class models;Unified Modeling Language;model-driven architecture;computational scalability","","7","","43","","","","","","IEEE","IEEE Journals & Magazines"
"MODEST: A Compositional Modeling Formalism for Hard and Softly Timed Systems","H. Bohnenkamp; P. R. D'Argenio; H. Hermanns; J. -. Katoen","Software Modeling and Verification Group, Informatik 2, University (RWTH) Aachen, 52056 Aachen, Germany; Computer Science Group, FaMAF, Universidad Nacional de Co´rdoba, Ciudad Universitaria, 5000—Cordoba, Argentina; Dependable Systems and Software Group, Department of Computer Science, Saarland University, 66123 Saarbrucken, Germany; Software Modeling and Verification Group, Informatik 2, University (RWTH) Aachen, 52056 Aachen, Germany","IEEE Transactions on Software Engineering","","2006","32","10","812","830","This paper presents MODEST (modeling and description language for stochastic timed systems), a formalism that is intended to support 1) the modular description of reactive systems' behavior while covering both 2) functional and 3) nonfunctional system aspects such as timing and quality-of-service constraints in a single specification. The language contains, features such as simple and structured data types, structuring mechanisms like parallel composition and abstraction, means to control the granularity of assignments, exception handling, and nondeterministic and random branching and timing. MODEST can be viewed as an overarching notation for a wide spectrum of models, ranging from labeled transition systems to timed automata (and probabilistic variants thereof), as well as prominent stochastic processes such as (generalized semi-) Markov chains and decision processes. The paper describes the design rationales and details of the syntax and semantics","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.104","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717473","Modeling formalism;compositionality;formal semantics;timed automata;stochastic processes.","Embedded software;Learning automata;Stochastic systems;Stochastic processes;Timing;Costs;Algebra;Software systems;Robustness;Unified modeling language","formal languages;formal specification;Markov processes;programming language semantics;specification languages;stochastic automata","compositional modeling formalism;description language;stochastic timed system;modular description;reactive system;nonfunctional system;quality-of-service constraint;formal specification;stochastic process;Markov chain;decision process;formal semantics","","63","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Verisim: formal analysis of network simulations","K. Bhargavan; C. A. Gunter; Moonjoo Kim; Insup Lee; D. Obradovic; O. Sokolsky; M. Viswanathan","Dept. of Comput. & Inf. Sci., Pennsylvania Univ., Philadelphia, PA, USA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","2","129","145","Network protocols are often analyzed using simulations. We demonstrate how to extend such simulations to check propositions expressing safety properties of network event traces in an extended form of linear temporal logic. Our technique uses the INS simulator together with a component of the MaC system to provide a uniform framework. We demonstrate its effectiveness by analyzing simulations of the ad hoc on-demand distance vector (AODV) routing protocol for packet radio networks. Our analysis finds violations of significant properties and we discuss the faults that cause them. Novel aspects of our approach include modest integration costs with other simulation objectives such as performance evaluation, greatly increased flexibility in specifying properties to be checked and techniques for analyzing complex traces of alarms raised by the monitoring software.","0098-5589;1939-3520;2326-3881","","10.1109/32.988495","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988495","","Analytical models;Discrete event simulation;Safety;Logic;Vectors;Routing protocols;Packet radio networks;Costs;Performance analysis;Monitoring","protocols;telecommunication network routing;temporal logic;packet radio networks;program debugging;formal verification;digital simulation;telecommunication computing;system monitoring","formal analysis;network simulations;network protocols;Verisim;safety properties;network event traces;linear temporal logic;MaC system;ad hoc on-demand distance vector routing protocol;packet radio networks;integration costs;performance evaluation;complex alarm traces;monitoring software","","41","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A Survey on Load Testing of Large-Scale Software Systems","Z. M. Jiang; A. E. Hassan","Department of Electrical Engineering and Computer ScienceSoftware Construction, AnaLytics and Evaluation (SCALE) Lab, York University, Toronto, ON, Canada; Software Analysis and Intelligence (SAIL) Lab, School of Computing, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2015","41","11","1091","1118","Many large-scale software systems must service thousands or millions of concurrent requests. These systems must be load tested to ensure that they can function correctly under load (i.e., the rate of the incoming requests). In this paper, we survey the state of load testing research and practice. We compare and contrast current techniques that are used in the three phases of a load test: (1) designing a proper load, (2) executing a load test, and (3) analyzing the results of a load test. This survey will be useful for load testing practitioners and software engineering researchers with interest in the load testing of large-scale software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2445340","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7123673","Software Testing;Load Testing;Software Quality;Software testing;load testing;software quality;large-scale software systems;survey","Testing;Stress;Computer bugs;Robustness;Software systems;Stress measurement","program testing;software quality","load testing;large-scale software systems;concurrent requests;load testing research;contrast current techniques;software engineering researchers","","19","","196","","","","","","IEEE","IEEE Journals & Magazines"
"Software reuse research: status and future","W. B. Frakes; Kyo Kang","Comput. Sci. Dept., Virginia Tech, Falls Church, VA, USA; NA","IEEE Transactions on Software Engineering","","2005","31","7","529","536","This paper briefly summarizes software reuse research, discusses major research contributions and unsolved problems, provides pointers to key publications, and introduces four papers selected from The Eighth International Conference on Software Reuse (ICSR8).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.85","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492369","Index Terms- Software reuse;domain engineering;research;metrics;architectures;generators;finance.","Software quality;Software reusability;Software engineering;Computer science;Finance;Productivity;Reliability engineering;Software systems;Software libraries;Software architecture","software reusability;research and development;software metrics;software architecture","software reuse research;software engineering;software metrics;software architecture","","231","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Build-level components","M. de Jonge","Philips Res. Lab., Eindhoven, Netherlands","IEEE Transactions on Software Engineering","","2005","31","7","588","600","Reuse between software systems is often not optimal. An important reason is that while at the functional level well-known modularization principles are applied for structuring functionality in modules, this is not the case at the build level for structuring files in directories. This leads to a situation where files are entangled in directory hierarchies and build processes, making it hard to extract functionality and to make functionality suitable for reuse. Consequently, software may not come available for reuse at all, or only in rather large chunks of functionality, which may lead to extra software dependencies. In this paper, we propose to improve this situation by applying component-based software engineering (CBSE) principles to the build level. We discuss how existing software systems break CBSE principles, we introduce the notion of build-level components, and we define rules for developing such components. To make our techniques feasible, we define a reengineering process for semiautomatically transforming existing software systems into build-level components. Our techniques are demonstrated in two case studies where we decouple the source tree of Graphviz into 46 build-level components and analyze the source tree of Mozilla.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492373","Index Terms- CBSE;software component;software reuse;software construction;software engineering;source tree composition;build level.","Software systems;Software engineering;Tree graphs;Assembly systems;Functional programming;Debugging;Component architectures","object-oriented programming;software reusability;systems re-engineering","build-level components;software system reusability;component-based software engineering;reengineering process;Graphviz source tree;Mozilla source tree;software component;software construction;software engineering","","19","","34","","","","","","IEEE","IEEE Journals & Magazines"
"ASCENT: An Algorithmic Technique for Designing Hardware and Software in Tandem","J. White; B. Doughtery; D. C. Schmidt","Virginia Tech, Blacksburg; Vanderbilt University, Nashville; Vanderbilt University, Nashville","IEEE Transactions on Software Engineering","","2010","36","6","838","851","Search-based software engineering is an emerging paradigm that uses automated search algorithms to help designers iteratively find solutions to complicated design problems. For example, when designing a climate monitoring satellite, designers may want to use the minimal amount of computing hardware to reduce weight and cost while supporting the image processing algorithms running onboard. A key problem in these situations is that the hardware and software designs are locked in a tightly coupled cost-constrained producer/consumer relationship that makes it hard to find a good hardware/software design configuration. Search-based software engineering can be used to apply algorithmic techniques to automate the search for hardware/software designs that maximize the image processing accuracy while respecting cost constraints. This paper provides the following contributions to research on search-based software engineering: 1) We show how a cost-constrained producer/consumer problem can be modeled as a set of two multidimensional multiple-choice knapsack problems (MMKPs), 2) we present a polynomial-time search-based software engineering technique, called the Allocation-baSed Configuration Exploration Technique (ASCENT), for finding near optimal hardware/software codesign solutions, and 3) we present empirical results showing that ASCENT's solutions average over 95 percent of the optimal solution's value.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5539763","Algorithms;computer aided software engineering;optimization methods;distributed computing.","Software algorithms;Software design;Algorithm design and analysis;Hardware;Software engineering;Iterative algorithms;Costs;Image processing;Monitoring;Satellites","hardware-software codesign;knapsack problems;software engineering","ASCENT;hardware-software codesign;automated search algorithm;climate monitoring satellite;computing hardware;image processing algorithm;hardware/software design configuration;cost-constrained producer/consumer problem;multidimensional multiple choice knapsack problem;polynomial time search-based software engineering;allocation-based configuration exploration technique","","7","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Threat-driven modeling and verification of secure software using aspect-oriented Petri nets","D. Xu; K. E. Nygard","Dept. of Comput. Sci., North Dakota State Univ., Fargo, ND, USA; Dept. of Comput. Sci., North Dakota State Univ., Fargo, ND, USA","IEEE Transactions on Software Engineering","","2006","32","4","265","278","Design-level vulnerabilities are a major source of security risks in software. To improve trustworthiness of software design, this paper presents a formal threat-driven approach, which explores explicit behaviors of security threats as the mediator between security goals and applications of security features. Security threats are potential attacks, i.e., misuses and anomalies that violate the security goals of systems' intended functions. Security threats suggest what, where, and how security features for threat mitigation should be applied. To specify the intended functions, security threats, and threat mitigations of a security design as a whole, we exploit aspect-oriented Petri nets as a unified formalism. Intended functions and security threats are modeled by Petri nets, whereas threat mitigations are modeled by Petri net-based aspects due to the incremental and crosscutting nature of security features. The unified formalism facilitates verifying correctness of security threats against intended functions and verifying absence of security threats from integrated functions and threat mitigations. As a result, our approach can make software design provably secured from anticipated security threats and, thus, reduce significant design-level vulnerabilities. We demonstrate our approach through a systematic case study on the threat-driven modeling and verification of a real-world shopping cart application.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628972","Software security;modeling;verification;threat modeling;Petri nets;aspect-oriented Petri nets;aspect-oriented software development.","Petri nets;Application software;Software design;Programming;Computer security;Protection;Cryptography;Intrusion detection","security of data;object-oriented programming;formal specification;formal verification;Petri nets","design-level vulnerability;software security;software design;security threat;aspect-oriented Petri net;formal verification;real-world shopping cart application;threat-driven modeling","","75","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Virtual benchmarking and model continuity in prototyping embedded multiprocessor signal processing systems","R. S. Janka; L. M. Wills; L. B. Baumstark","Cadence Design Systems Inc., Atlanta, GA, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","9","832","846","The complexity of hardware/software codesign of embedded real-time signal processing systems can be reduced by rapid system prototyping (RSP). However, existing RSP frameworks do not provide a sound specification and design methodology (SDM) because they require the designer to choose the implementation target before specification and design exploration and they do not work together coherently across development stages. This paper presents a new SDM, called MAGIC, that allows the designer to capture an executable specification model for use in design exploration to find the optimal multiprocessor technology before committing to that technology. MAGIC uses a technique called ""virtual benchmarking,"" for early validation of promising architectures. The MAGIC SDM also exploits emerging open-standards computation and communication middleware to establish model continuity between RSP frameworks. This methodology has been validated through the specification and design of a moderately complex system representative of the signal processing domain: the RASSP Synthetic Aperture Radar benchmark. In this case study, MAGIC achieves three orders of magnitude speedup over existing virtual prototyping approaches and demonstrates the ability to evaluate competitive technologies prior to implementation. Transfer of this methodology to the system-on-a-chip domain using Cadence's Virtual Component Codesign infrastructure is also discussed with promising results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033224","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033224","","Virtual prototyping;Signal processing;Software prototyping;Hardware;Embedded software;Real time systems;Design methodology;Computer architecture;Middleware;Signal design","hardware-software codesign;embedded systems;radar computing;formal specification;multiprocessing systems;software prototyping;signal processing;synthetic aperture radar;radar signal processing;application program interfaces","hardware/software codesign;embedded real-time multiprocessor signal processing systems;rapid system prototyping;specification and design methodology;MAGIC;executable specification model;optimal multiprocessor technology;virtual benchmarking;early validation;architectures;open-standards computation;communication middleware;RASSP Synthetic Aperture Radar benchmark;system-on-a-chip domain;Cadence Virtual Component Codesign infrastructure;model continuity","","5","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Precise Calling Context Encoding","W. N. Sumner; Y. Zheng; D. Weeratunge; X. Zhang","Purdue University, West Lafayette; Purdue University, West Lafayette; Purdue University, West Lafayette; Purdue University, West Lafayette","IEEE Transactions on Software Engineering","","2012","38","5","1160","1177","Calling contexts (CCs) are very important for a wide range of applications such as profiling, debugging, and event logging. Most applications perform expensive stack walking to recover contexts. The resulting contexts are often explicitly represented as a sequence of call sites and hence are bulky. We propose a technique to encode the current calling context of any point during an execution. In particular, an acyclic call path is encoded into one number through only integer additions. Recursive call paths are divided into acyclic subsequences and encoded independently. We leverage stack depth in a safe way to optimize encoding: If a calling context can be safely and uniquely identified by its stack depth, we do not perform encoding. We propose an algorithm to seamlessly fuse encoding and stack depth-based identification. The algorithm is safe because different contexts are guaranteed to have different IDs. It also ensures contexts can be faithfully decoded. Our experiments show that our technique incurs negligible overhead (0-6.4 percent). For most medium-sized programs, it can encode all contexts with just one number. For large programs, we are able to encode most calling contexts to a few numbers. We also present our experience of applying context encoding to debugging crash-based failures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5963696","Calling context;context sensitivity;profiling;path encoding;calling context encoding;call graph","Context;Encoding;Instruments;Image edge detection;Runtime;Decoding;Software algorithms","optimisation;program compilers;program debugging","precise calling context encoding;CC;profiling;event logging;stack walking;context recovery;call sites;recursive call paths;acyclic subsequences;encoding optimization;stack depth-based identification;ID;medium-sized programs;crash-based failure debugging","","6","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying high performance ERP projects","E. Stensrud; I. Myrtveit","Norwegian Sch. of Manage., Sandvika, Norway; Norwegian Sch. of Manage., Sandvika, Norway","IEEE Transactions on Software Engineering","","2003","29","5","398","416","Learning from high performance projects is crucial for software process improvement. Therefore, we need to identify outstanding projects that may serve as role models. It is common to measure productivity as an indicator of performance. It is vital that productivity measurements deal correctly with variable returns to scale and multivariate data. Software projects generally exhibit variable returns to scale, and the output from ERP projects is multivariate. We propose to use data envelopment analysis variable returns to scale (DEA VRS) to measure the productivity of software projects. DEA VRS fulfills the two requirements stated above. The results from this empirical study of 30 ERP projects extracted from a benchmarking database in Accenture identified six projects as potential role models. These projects deserve to be studied and probably copied as part of a software process improvement initiative. The results also suggest that there is a 50 percent potential for productivity improvement, on average. Finally, the results support the assumption of variable returns to scale in ERP projects. We recommend DEA VRS be used as the default technique for appropriate productivity comparisons of individual software projects. Used together with methods for hypothesis testing, DEA VRS is also a useful technique for assessing the effect of alleged process improvements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199070","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199070","","Enterprise resource planning;Productivity;Software performance;Data envelopment analysis;Software measurement;Best practices;Software engineering;Project management;Performance evaluation;Data mining","software process improvement;enterprise resource planning;data envelopment analysis;project management;software management;management science","high-performance ERP project identification;data envelopment analysis variable returns;software process improvement;multivariate data;software projects;DEA VRS;software project productivity measurement;benchmarking database;Accenture;enterprise resource planning","","47","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Time and Probability-Based Information Flow Analysis","R. Lanotte; A. Maggiolo-Schettini; A. Troina","Universit&#x0E0; dell'Insubria, Como; Universit&#x0E0; di Pisa, Pisa; Universit&#x0E0; di Torino, Torino","IEEE Transactions on Software Engineering","","2010","36","5","719","734","In multilevel systems, it is important to avoid unwanted indirect information flow from higher levels to lower levels, namely, the so-called covert channels. Initial studies of information flow analysis were performed by abstracting away from time and probability. It is already known that systems that are proven to be secure in a possibilistic framework may turn out to be insecure when time or probability is considered. Recently, work has been done in order to consider also aspects either of time or of probability, but not both. In this paper, we propose a general framework based on Probabilistic Timed Automata, where both probabilistic and timing covert channels can be studied. We define a Noninterference security property and a Nondeducibility on Composition security property, which allow expressing information flow in a timed and probabilistic setting. We then compare these properties with analogous ones defined in contexts where either time or probability or neither of them are taken into account. This permits a classification of the properties depending on their discerning power. As an application, we study a system with covert channels that we are able to discover by applying our techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383372","Probabilistic timed automata;multilevel security;information flow analysis;weak bisimulation.","Information analysis;Information security;Automata;Clocks;Multilevel systems;Timing;Communication system control;Performance analysis;Power system security;Control systems","data flow analysis;probabilistic automata;probability;security of data;software agents;software engineering","probability based information flow analysis;multilevel system;indirect unwanted information flow;covert channel;probabilistic timed automata;probabilistic covert channel;timing covert channel;noninterference security property;nondeducibility;composition security property","","7","","45","","","","","","IEEE","IEEE Journals & Magazines"
"Reduction methods for real-time systems using Delay Time Petri Nets","E. Y. T. Juan; J. J. P. Tsai; T. Murata; Yi Zhou","Dept. of Electr. Eng. & Comput. Sci., Illinois Univ., Chicago, IL, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","5","422","448","We present a new net-reduction methodology to facilitate the analysis of real-time systems using Delay Time Petri Nets (DTPNs). Net reduction is one of the most important techniques for reducing the state-explosion problem of Petri nets. However, the application of net reduction to current timed-extensions of Petri nets (such as Merlin's Time PNs) is very limited due to the difficulty faced in the preservation of timing constraints. To overcome this problem, we introduce DTPNs which are inspired by Merlin's (1976) Time PNs, Senac's (1994) Hierarchical Time Stream PNs, and Little's (1991) Timed PNs. We show that DTPNs are much more suitable for net reduction. Then, we present a new set of DTPN reduction rules for the analysis of schedule and deadlock analysis. Our work is distinct from the others since our goal is to analyze real-time systems and the reduction methods we propose preserve both timing properties (schedule) and deadlock. To evaluate our framework, we have implemented an automated analysis tool whose main functions include net reduction and class-graph generation. The experimental results show that our net-reduction methodology leads to a significant contribution to the efficient analysis of real-time systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.922714","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=922714","","Real time systems;Delay systems;Delay effects;Petri nets;Timing;System recovery;Control systems;Aerospace control;Aerospace electronics;Formal verification","real-time systems;Petri nets;scheduling;systems analysis","real-time systems;Delay Time Petri Nets;net-reduction methodology;state-explosion problem;timing constraints;Time Petri nets;Hierarchical Time Stream Petri nets;schedule analysis;deadlock analysis;timing;class-graph generation;experiment","","37","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A Uniform Representation of Hybrid Criteria for Regression Testing","S. Sampath; R. Bryce; A. M. Memon","University of Maryland, Baltimore; University of North Texas, Denton; University of Maryland, Baltimore","IEEE Transactions on Software Engineering","","2013","39","10","1326","1344","Regression testing tasks of test case prioritization, test suite reduction/minimization, and regression test selection are typically centered around criteria that are based on code coverage, test execution costs, and code modifications. Researchers have developed and evaluated new individual criteria; others have combined existing criteria in different ways to form what we--and some others--call hybrid criteria. In this paper, we formalize the notion of combining multiple criteria into a hybrid. Our goal is to create a uniform representation of such combinations so that they can be described unambiguously and shared among researchers. We envision that such sharing will allow researchers to implement, study, extend, and evaluate the hybrids using a common set of techniques and tools. We precisely formulate three hybrid combinations, Rank, Merge, and Choice, and demonstrate their usefulness in two ways. First, we recast, in terms of our formulations, others' previously reported work on hybrid criteria. Second, we use our previous results on test case prioritization to create and evaluate new hybrid criteria. Our findings suggest that hybrid criteria of others can be described using our Merge and Rank formulations, and that the hybrid criteria we developed most often outperformed their constituent individual criteria.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6484067","Test case prioritization;test criteria;hybrid test criteria;web testing;GUI testing","Testing;Fault detection;Educational institutions;Genetic algorithms;Vectors;Loss measurement;Minimization","program testing;regression analysis","uniform representation;hybrid test criteria;regression testing;rank-merge-and-choice hybrid combination;test case prioritization;merge-and-rank formulations","","13","","74","","","","","","IEEE","IEEE Journals & Magazines"
"NDT. A Model-Driven Approach for Web Requirements","M. J. Escalona; G. Aragón","University of Seville, Seville; Everis, Seville","IEEE Transactions on Software Engineering","","2008","34","3","377","390","Web engineering is a new research line in software engineering that covers the definition of processes, techniques, and models suitable for Web environments in order to guarantee the quality of results. The research community is working in this area and, as a very recent line, they are assuming the Model-Driven paradigm to support and solve some classic problems detected in Web developments. However, there is a lack in Web requirements treatment. This paper presents a general vision of Navigational Development Techniques (NDT), which is an approach to deal with requirements in Web systems. It is based on conclusions obtained in several comparative studies and it tries to fill some gaps detected by the research community. This paper presents its scope, its most important contributions, and offers a global vision of its associated tool: NDT-Tool. Furthermore, it analyzes how Web Engineering can be applied in the enterprise environment. NDT is being applied in real projects and has been adopted by several companies as a requirements methodology. The approach offers a Web requirements solution based on a Model-Driven paradigm that follows the most accepted tendencies by Web engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4497213","Software engineering for Internet projects;Surveys of historical development of one particular area;Requirements/Specifications;Software engineering for Internet projects;Surveys of historical development of one particular area;Requirements/Specifications","Model driven engineering;Software engineering;Navigation;Application software;Standards development;Companies;Internet;Software systems;Systems engineering and theory;Proposals","Internet;software engineering;Web design","Web requirements;Web engineering;software engineering;model-driven paradigm;Web developments;navigational development techniques;NDT-Tool","","42","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical Principles and an Industrial Case Study in Retrieving Equivalent Requirements via Natural Language Processing Techniques","D. Falessi; G. Cantone; G. Canfora","Simula Research Laboratory, Lysaker and University of Rome "TorVergata", Rome; University of Rome "TorVergata", Rome; University of Sannio, Benevento","IEEE Transactions on Software Engineering","","2013","39","1","18","44","Though very important in software engineering, linking artifacts of the same type (clone detection) or different types (traceability recovery) is extremely tedious, error-prone, and effort-intensive. Past research focused on supporting analysts with techniques based on Natural Language Processing (NLP) to identify candidate links. Because many NLP techniques exist and their performance varies according to context, it is crucial to define and use reliable evaluation procedures. The aim of this paper is to propose a set of seven principles for evaluating the performance of NLP techniques in identifying equivalent requirements. In this paper, we conjecture, and verify, that NLP techniques perform on a given dataset according to both ability and the odds of identifying equivalent requirements correctly. For instance, when the odds of identifying equivalent requirements are very high, then it is reasonable to expect that NLP techniques will result in good performance. Our key idea is to measure this random factor of the specific dataset(s) in use and then adjust the observed performance accordingly. To support the application of the principles we report their practical application to a case study that evaluates the performance of a large number of NLP techniques for identifying equivalent requirements in the context of an Italian company in the defense and aerospace domain. The current application context is the evaluation of NLP techniques to identify equivalent requirements. However, most of the proposed principles seem applicable to evaluating any estimation technique aimed at supporting a binary decision (e.g., equivalent/nonequivalent), with the estimate in the range [0,1] (e.g., the similarity provided by the NLP), when the dataset(s) is used as a benchmark (i.e., testbed), independently of the type of estimator (i.e., requirements text) and of the estimation method (e.g., NLP).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.122","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6112783","Empirical software engineering;traceability recovery;natural language processing;equivalent requirements;metrics and measurement","Natural language processing;Context;Semantics;Measurement;Matrix decomposition;Monitoring;Thesauri","information retrieval;natural language processing;program diagnostics;software engineering","industrial case study;equivalent requirement retrieval;natural language processing techniques;software engineering;traceability recovery;clone detection;NLP techniques;reliable evaluation procedures;Italian company;aerospace domain;defense domain;estimation technique;binary decision","","40","","116","","","","","","IEEE","IEEE Journals & Magazines"
"A Study of Uncertainty in Software Cost and Its Impact on Optimal Software Release Time","B. Yang; H. Hu; L. Jia","University of Electronic Science and Technology of China, Chengdu; University of Electronic Science and Technology of China, Chengdu; Xi'an Jiaotong University, Xi'an","IEEE Transactions on Software Engineering","","2008","34","6","813","825","For a software development project, management often faces the dilemma of when to stop testing the software and release it for operation, which requires careful decision making as it has great impact on both software reliability and project cost. In most existing research on the optimal software release problem, the cost considered was the Expected Cost (EC) of the project. However, what concerns management is the Actual Cost (AC) of the project rather than the EC. Treatment (such as minimization) of the EC may not ensure the desired low level of the AC due to the uncertainty (variability) involved in the AC. In this paper, we study the uncertainty in software cost and its impact on optimal software release time in detail. The uncertainty is quantified by the variance of the AC and several risk functions. A risk-control approach to the optimal software release problem is proposed. New formulations of the problem which are extensions of current formulations are developed and solution procedures are established. Several examples are presented. Results reveal that it seems crucial to take into account the uncertainty in software cost in the optimal software release problem; otherwise, unsafe decisions may be reached which could be a false dawn to management.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4553721","Cost estimation;Time estimation;Project control and modeling;Reliability;Cost estimation;Time estimation;Project control and modeling;Reliability","Uncertainty;Cost function;Software testing;Programming;Project management;Decision making;Software reliability;Software safety;Software development management;Software quality","project management;software cost estimation;software development management;software reliability","optimal software release time;nonhomogeneous Poisson process;software reliability;actual cost;expected cost;risk-control approach;software cost uncertainty","","36","","36","","","","","","IEEE","IEEE Journals & Magazines"
"The Use of Summation to Aggregate Software Metrics Hinders the Performance of Defect Prediction Models","F. Zhang; A. E. Hassan; S. McIntosh; Y. Zou","School of Computing, Queen's University, Kingston, ON, Canada; School of Computing, Queen's University, Kingston, ON, Canada; Department of Electrical and Computer Engineering, McGill University, Montréal, QC, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2017","43","5","476","491","Defect prediction models help software organizations to anticipate where defects will appear in the future. When training a defect prediction model, historical defect data is often mined from a Version Control System (VCS, e.g., Subversion), which records software changes at the file-level. Software metrics, on the other hand, are often calculated at the class- or method-level (e.g., McCabe's Cyclomatic Complexity). To address the disagreement in granularity, the class- and method-level software metrics are aggregated to file-level, often using summation (i.e., McCabe of a file is the sum of the McCabe of all methods within the file). A recent study shows that summation significantly inflates the correlation between lines of code (Sloc) and cyclomatic complexity (Cc) in Java projects. While there are many other aggregation schemes (e.g., central tendency, dispersion), they have remained unexplored in the scope of defect prediction. In this study, we set out to investigate how different aggregation schemes impact defect prediction models. Through an analysis of 11 aggregation schemes using data collected from 255 open source projects, we find that: (1) aggregation schemes can significantly alter correlations among metrics, as well as the correlations between metrics and the defect count; (2) when constructing models to predict defect proneness, applying only the summation scheme (i.e., the most commonly used aggregation scheme in the literature) only achieves the best performance (the best among the 12 studied configurations) in 11 percent of the studied projects, while applying all of the studied aggregation schemes achieves the best performance in 40 percent of the studied projects; (3) when constructing models to predict defect rank or count, either applying only the summation or applying all of the studied aggregation schemes achieves similar performance, with both achieving the closest to the best performance more often than the other studied aggregation schemes; and (4) when constructing models for effort-aware defect prediction, the mean or median aggregation schemes yield performance values that are significantly closer to the best performance than any of the other studied aggregation schemes. Broadly speaking, the performance of defect prediction models are often underestimated due to our community's tendency to only use the summation aggregation scheme. Given the potential benefit of applying additional aggregation schemes, we advise that future defect prediction models should explore a variety of aggregation schemes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2599161","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7539677","Defect prediction;aggregation scheme;software metrics","Predictive models;Correlation;Software metrics;Indexes;Software;Data models","data aggregation;data mining;Java;public domain software;software metrics","software metrics aggregation;defect prediction models;software organizations;historical defect data mining;version control system;software changes recording;McCabe cyclomatic complexity;granularity disagreement;class-level software metrics;method-level software metrics;lines of code;Sloc;Cc;Java projects;open source projects;effort-aware defect prediction;summation","","6","","87","","","","","","IEEE","IEEE Journals & Magazines"
"Effects of Personality on Pair Programming","J. E. Hannay; E. Arisholm; H. Engvik; D. I. K. Sjoberg","Simula Research Laboratory, Lysaker and University of Oslo, Oslo; Simula Research Laboratory, Lysaker and University of Oslo, Oslo; University of Oslo, Oslo; University of Oslo, Oslo","IEEE Transactions on Software Engineering","","2010","36","1","61","80","Personality tests in various guises are commonly used in recruitment and career counseling industries. Such tests have also been considered as instruments for predicting the job performance of software professionals both individually and in teams. However, research suggests that other human-related factors such as motivation, general mental ability, expertise, and task complexity also affect the performance in general. This paper reports on a study of the impact of the Big Five personality traits on the performance of pair programmers together with the impact of expertise and task complexity. The study involved 196 software professionals in three countries forming 98 pairs. The analysis consisted of a confirmatory part and an exploratory part. The results show that: (1) Our data do not confirm a meta-analysis-based model of the impact of certain personality traits on performance and (2) personality traits, in general, have modest predictive value on pair programming performance compared with expertise, task complexity, and country. We conclude that more effort should be spent on investigating other performance-related predictors such as expertise, and task complexity, as well as other promising predictors, such as programming skill and learning. We also conclude that effort should be spent on elaborating on the effects of personality on various measures of collaboration, which, in turn, may be used to predict and influence performance. Insights into such malleable, rather than static, factors may then be used to improve pair programming performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5089333","Pair programming;personality;Big Five;expertise;task complexity;performance.","Programming profession;Collaborative work;Keyboards;Books;Recruitment;Engineering profession;Employee welfare;Software testing;Instruments;Software performance","human factors;personnel;programming;recruitment;team working","personality tests;pair programming;recruitment industries;career counseling industries;job performance;software professionals;personality traits;task complexity;meta-analysis-based model;performance-related predictors","","47","","121","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Extraction and Clustering of Requirements Glossary Terms","C. Arora; M. Sabetzadeh; L. Briand; F. Zimmer","SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Alphonse Weicker, Luxembourg; SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Alphonse Weicker, Luxembourg; SnT Centre for Security, Reliability, and Trust, University of Luxembourg, Alphonse Weicker, Luxembourg; SES Techcom, Betzdorf, Luxembourg","IEEE Transactions on Software Engineering","","2017","43","10","918","945","A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary helps mitigate imprecision and ambiguity. A key step in building a glossary is to decide upon the terms to include in the glossary and to find any related terms. Doing so manually is laborious, particularly for large requirements documents. In this article, we develop an automated approach for extracting candidate glossary terms and their related terms from natural language requirements documents. Our approach differs from existing work on term extraction mainly in that it clusters the extracted terms by relevance, instead of providing a flat list of terms. We provide an automated, mathematically-based procedure for selecting the number of clusters. This procedure makes the underlying clustering algorithm transparent to users, thus alleviating the need for any user-specified parameters. To evaluate our approach, we report on three industrial case studies, as part of which we also examine the perceptions of the involved subject matter experts about the usefulness of our approach. Our evaluation notably suggests that: (1) Over requirements documents, our approach is more accurate than major generic term extraction tools. Specifically, in our case studies, our approach leads to gains of 20 percent or more in terms of recall when compared to existing tools, while at the same time either improving precision or leaving it virtually unchanged. And, (2) the experts involved in our case studies find the clusters generated by our approach useful as an aid for glossary construction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2635134","Luxembourg’s National Research Fund; European Research Council; European Union’s Horizon 2020 research and innovation program; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7765062","Requirements glossaries;term extraction;natural language processing;clustering;case study research","Terminology;Servers;Pipelines;Natural languages;Monitoring;Software;Clustering algorithms","natural language processing;pattern clustering;text analysis","natural language requirements documents;mathematically-based procedure;user-specified parameters;candidate glossary terms;technical terms;clustering algorithm;automated approach;software requirements document;requirements glossary terms;glossary construction;generic term extraction tools","","2","","100","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Crossover Designs in Software Engineering Experiments: Benefits and Perils","S. Vegas; C. Apa; N. Juristo","Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, Madrid, Spain; Instituto de Computación, Facultad de Ingeniería, Montevideo, Uruguay; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, Madrid, Spain","IEEE Transactions on Software Engineering","","2016","42","2","120","135","In experiments with crossover design subjects apply more than one treatment. Crossover designs are widespread in software engineering experimentation: they require fewer subjects and control the variability among subjects. However, some researchers disapprove of crossover designs. The main criticisms are: the carryover threat and its troublesome analysis. Carryover is the persistence of the effect of one treatment when another treatment is applied later. It may invalidate the results of an experiment. Additionally, crossover designs are often not properly designed and/or analysed, limiting the validity of the results. In this paper, we aim to make SE researchers aware of the perils of crossover experiments and provide risk avoidance good practices. We study how another discipline (medicine) runs crossover experiments. We review the SE literature and discuss which good practices tend not to be adhered to, giving advice on how they should be applied in SE experiments. We illustrate the concepts discussed analysing a crossover experiment that we have run. We conclude that crossover experiments can yield valid results, provided they are properly designed and analysed, and that, if correctly addressed, carryover is no worse than other validity threats.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2467378","Spanish Ministry of Economy and Competitiveness research; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7192651","Data analysis;crossover designs;carryover;Experimental software engineering;controlled experiment;data analysis;crossover design;carryover","Software engineering;Animals;Atmospheric measurements;Particle measurements;Psychology;US Government agencies;Information processing","software engineering","crossover design;software engineering experiments;SE research;crossover experiment","","7","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient Dynamic Updates of Distributed Components Through Version Consistency","L. Baresi; C. Ghezzi; X. Ma; V. P. L. Manna","Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy; State Key Laboratory for Novel Software Technology and the Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing University, Nanjing, Jiangsu, China; Holst Centre/imec the Netherlands, Eindhoven, AE, The Netherlands","IEEE Transactions on Software Engineering","","2017","43","4","340","358","Modern component-based distributed software systems are increasingly required to offer non-stop service and thus their updates must be carried out at runtime. Different authors have already proposed solutions for the safe management of dynamic updates. Our contribution aims at improving their efficiency without compromising safety. We propose a new criterion, called version consistency, which defines when a dynamic update can be safely and efficiently applied to the components that execute distributed transactions. Version consistency ensures that distributed transactions be served as if they were operated on a single coherent version of the system despite possible concurrent updates. The paper presents a distributed algorithm for checking version consistency efficiently, formalizes the proposed approach by means of a graph transformation system, and verifies its correctness through model checking. The paper also presents ConUp, a novel prototype framework that supports the approach and offers a viable, concrete solution for the use of version consistency. Both the approach and ConUp are evaluated on a significant third-party application. Obtained results witness the benefits of the proposed solution with respect to both timeliness and disruption.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2592913","The 973 Program of China; NSFC; EEB-Edifici a zero consumo energetico in distretti urbani intelligenti; Italian Technology Cluster For Smart Communities; Telecom Italia; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7516718","Component-based distributed system;dynamic update;version-consistency","Portals;Runtime;Software systems;Safety;Model checking;Concrete","distributed programming;formal verification;graph theory","distributed components;version consistency;distributed software systems;dynamic update;graph transformation system","","2","","41","","","","","","IEEE","IEEE Journals & Magazines"
"An Autonomous Engine for Services Configuration and Deployment","F. Cuadrado; J. C. Duenas; R. Garcia-Carmona","Queen Mary University of London, London; Universidad Politécnica de Madrid, Madrid; Universidad Politécnica de Madrid, Madrid","IEEE Transactions on Software Engineering","","2012","38","3","520","536","The runtime management of the infrastructure providing service-based systems is a complex task, up to the point where manual operation struggles to be cost effective. As the functionality is provided by a set of dynamically composed distributed services, in order to achieve a management objective multiple operations have to be applied over the distributed elements of the managed infrastructure. Moreover, the manager must cope with the highly heterogeneous characteristics and management interfaces of the runtime resources. With this in mind, this paper proposes to support the configuration and deployment of services with an automated closed control loop. The automation is enabled by the definition of a generic information model, which captures all the information relevant to the management of the services with the same abstractions, describing the runtime elements, service dependencies, and business objectives. On top of that, a technique based on satisfiability is described which automatically diagnoses the state of the managed environment and obtains the required changes for correcting it (e.g., installation, service binding, update, or configuration). The results from a set of case studies extracted from the banking domain are provided to validate the feasibility of this proposal.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728830","Autonomic systems;model-based management;satisfiability;service configuration.","Runtime;Containers;Servers;Web services;Context;Business;Engines","computability;fault tolerant computing;service-oriented architecture","autonomous engine;services configuration;runtime management;service-based systems;dynamically composed distributed services;distributed elements;runtime resources;automated closed control loop;generic information model;runtime elements;service dependencies;business objectives;satisfiability;banking domain","","9","","38","","","","","","IEEE","IEEE Journals & Magazines"
"From live sequence charts to state machines and back: a guided tour","Y. Bontemps; P. Heymans; P. -. Schobbens","SmalS-MvM/eGov, Bruxelles, Belgium; NA; NA","IEEE Transactions on Software Engineering","","2005","31","12","999","1014","The problem of relating state-based intraagent (or intraobject) behavioral descriptions with scenario-based interagent (interobject) descriptions has recently focused much interest among the software engineering community. This paper compiles the results of our investigation of this problem. As interagent formalism, we adopt a simple variant of live sequence charts. For the intraagent perspective, we consider a game-theoretic foundation, looking at agents as ""strategies,"" which encompasses the popular ""state-based"" paradigm. Three classes of relationships between models are studied: scenario checking (called eLSC checking), synthesis, and verification. We set a formally defined theoretical stage that allows us to express these three problems very simply, to discuss their complexity, and to describe optimal solutions. Our study reveals the intrinsic high computational difficulty of these tasks. Consequently, many related problems and solutions are surveyed, some of which can be the basis for practical solutions. In this, we also offer a panorama of current research and directions for the future.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.137","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566603","Index Terms- Requirements engineering;life cycle;program verification.","Systems engineering and theory;Humans;Automotive engineering;Hardware;Computer Society;Software engineering;Concurrent computing;Distributed computing;Computer industry;Embedded software","program verification;systems analysis;software agents;flowcharting;finite state machines","live sequence charts;state machines;guided tour;state-based intraagent description;scenario-based interagent description;software engineering;game-theoretic foundation;scenario checking;eLSC checking;requirements engineering;life cycle;program verification","","27","","72","","","","","","IEEE","IEEE Journals & Magazines"
"TACO: Efficient SAT-Based Bounded Verification Using Symmetry Breaking and Tight Bounds","J. P. Galeotti; N. Rosner; C. G. López Pombo; M. F. Frias","Universidad de Buenos Aires and CONICET, Argentina; Universidad de Buenos Aires, Argentina; Universidad de Buenos Aires and CONICET, Argentina; Instituto Tecnológico de Buenos Aires and CONICET, Argentina","IEEE Transactions on Software Engineering","","2013","39","9","1283","1307","SAT-based bounded verification of annotated code consists of translating the code together with the annotations to a propositional formula, and analyzing the formula for specification violations using a SAT-solver. If a violation is found, an execution trace exposing the failure is exhibited. Code involving linked data structures with intricate invariants is particularly hard to analyze using these techniques. In this paper, we present Translation of Annotated COde (TACO), a prototype tool which implements a novel, general, and fully automated technique for the SAT-based analysis of JML-annotated Java sequential programs dealing with complex linked data structures. We instrument code analysis with a symmetry-breaking predicate which, on one hand, reduces the size of the search space by ignoring certain classes of isomorphic models and, on the other hand, allows for the parallel, automated computation of tight bounds for Java fields. Experiments show that the translations to propositional formulas require significantly less propositional variables, leading to an improvement of the efficiency of the analysis of orders of magnitude, compared to the noninstrumented SAT--based analysis. We show that in some cases our tool can uncover bugs that cannot be detected by state-of-the-art tools based on SAT-solving, model checking, or SMT-solving.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6482141","Static analysis;SAT-based code analysis;Alloy;KodKod;DynAlloy","Metals;Java;Cost accounting;Instruments;Analytical models;Contracts;Context","computability;formal specification;formal verification;program diagnostics;program interpreters","TACO tool;translation-of-annotated code tool;SAT-based bounded verification;satisfiability;code translation;specification violation;JML-annotated Java sequential program;data structure;code analysis;symmetry-breaking predicate;isomorphic model;automated tight bound computation;model checking;SAT-solving;SMT-solving","","10","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing high-change modules and modules with the highest measurement values in two large-scale open-source products","A. G. Koru; J. Tian","Dept. of Inf. Syst., Maryland Univ., Baltimore, MD, USA; NA","IEEE Transactions on Software Engineering","","2005","31","8","625","642","Identifying change-prone modules can enable software developers to take focused preventive actions that can reduce maintenance costs and improve quality. Some researchers observed a correlation between change proneness and structural measures, such as size, coupling, cohesion, and inheritance measures. However, the modules with the highest measurement values were not found to be the most troublesome modules by some of our colleagues in industry, which was confirmed by our previous study of six large-scale industrial products. To obtain additional evidence, we identified and compared high-change modules and modules with the highest measurement values in two large-scale open-source products, Mozilla and OpenOffice, and we characterized the relationship between them. Contrary to common intuition, we found through formal hypothesis testing that the top modules in change-count rankings and the modules with the highest measurement values were different. In addition, we observed that high-change modules had fairly high places in measurement rankings, but not the highest places. The accumulated findings from these two open-source products, together with our previous similar findings for six closed-source products, should provide practitioners with additional guidance in identifying the change-prone modules.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498769","Index Terms- Product metrics;maintainability;maintenance planning;open-source software.","Large-scale systems;Open source software;Size measurement;Software measurement;Software maintenance;Costs;Industrial relations;Testing;Software quality;Inspection","public domain software;software maintenance;software metrics;software quality;software cost estimation","large-scale open-source products;software developers;software maintenance cost reduction;large-scale industrial products;Mozilla software product;OpenOffice software product;formal hypothesis testing;change-prone modules;product metrics;software maintenance planning;open-source software","","38","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Discipline Matters: Refactoring of Preprocessor Directives in the<monospace>#ifdef</monospace>Hell","F. Medeiros; M. Ribeiro; R. Gheyi; S. Apel; C. Kästner; B. Ferreira; L. Carvalho; B. Fonseca","Federal Institute of Alagoas, AL, Brazil; Computing Institute, Federal University of Alagoas, Maceió, AL, Brazil; Department of Computing and Systems, Federal University of Campina Grande, PB, Brazil; Department of Informatics and Mathematics, University of Passau, Passau, Germany; Institute for Software Research, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA; Computing Institute, Federal University of Alagoas, Maceió, AL, Brazil; Computing Institute, Federal University of Alagoas, Maceió, AL, Brazil; Computing Institute, Federal University of Alagoas, Maceió, AL, Brazil","IEEE Transactions on Software Engineering","","2018","44","5","453","469","The C preprocessor is used in many C projects to support variability and portability. However, researchers and practitioners criticize the C preprocessor because of its negative effect on code understanding and maintainability and its error proneness. More importantly, the use of the preprocessor hinders the development of tool support that is standard in other languages, such as automated refactoring. Developers aggravate these problems when using the preprocessor in undisciplined ways (e.g., conditional blocks that do not align with the syntactic structure of the code). In this article, we proposed a catalogue of refactorings and we evaluated the number of application possibilities of the refactorings in practice, the opinion of developers about the usefulness of the refactorings, and whether the refactorings preserve behavior. Overall, we found 5,670 application possibilities for the refactorings in 63 real-world C projects. In addition, we performed an online survey among 246 developers, and we submitted 28 patches to convert undisciplined directives into disciplined ones. According to our results, 63 percent of developers prefer to use the refactored (i.e., disciplined) version of the code instead of the original code with undisciplined preprocessor usage. To verify that the refactorings are indeed behavior preserving, we applied them to more than 36 thousand programs generated automatically using a model of a subset of the C language, running the same test cases in the original and refactored programs. Furthermore, we applied the refactorings to three real-world projects: BusyBox, OpenSSL, and SQLite. This way, we detected and fixed a few behavioral changes, 62 percent caused by unspecified behavior in the C programming language.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2688333","CNPq; FAPEAL PPGs; CAPES; DEVASSES; European Union's Seventh Framework Programme for research, technological development and demonstration; NSF; Science of Security Lablet; AFRL; DARPA; German Research Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7888579","Configurable systems;preprocessors;and refactoring","Syntactics;C languages;Guidelines;Linux;Kernel;Standards","C language;program compilers;program diagnostics;program processors;public domain software;software maintenance","refactored programs;preprocessor directives;C preprocessor;automated refactoring;preprocessor usage;BusyBox;OpenSSL;SQLite;C programming language","","1","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic inclusion of middleware performance attributes into architectural UML software models","T. Verdickt; B. Dhoedt; F. Gielen; P. Demeester","Dept. of Inf. Technol., Ghent Univ., Belgium; Dept. of Inf. Technol., Ghent Univ., Belgium; Dept. of Inf. Technol., Ghent Univ., Belgium; Dept. of Inf. Technol., Ghent Univ., Belgium","IEEE Transactions on Software Engineering","","2005","31","8","695","711","Distributed systems often use a form of communication middleware to cope with different forms of heterogeneity, including geographical spreading of the components, different programming languages and platform architectures, etc. The middleware, of course, impact the architecture and the performance of the system. This paper presents a model transformation framework to automatically include the architectural impact and the overhead incurred by using a middleware layer between several system components. Using this framework, architects can model the system in a middleware-independent fashion. Accurate, middleware-aware models can then be obtained automatically using a middleware model repository. The actual transformation algorithm is presented in more detail. The resulting models can be used to obtain performance models of the system. From those performance models, early indications of the system performance can be extracted.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498773","Index Terms- Distributed software engineering tools and techniques;performance of systems: modeling techniques.","Middleware;Unified modeling language;Software performance;System performance;Software engineering;Computer architecture;Design engineering;Communication system software;Computer languages;Hardware","middleware;Unified Modeling Language;software architecture;object-oriented programming;software performance evaluation;distributed object management","middleware performance attributes;architectural UML software model;distributed systems;communication middleware;geographical spreading components;programming languages;model transformation framework;middleware model repository;system performance;distributed software engineering tools;middleware-aware model","","20","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Watermarking, tamper-proofing, and obfuscation - tools for software protection","C. S. Collberg; C. Thomborson","Dept. of Comput. Sci., Arizona Univ., Tucson, AZ, USA; NA","IEEE Transactions on Software Engineering","","2002","28","8","735","746","We identify three types of attack on the intellectual property contained in software and three corresponding technical defenses. A defense against reverse engineering is obfuscation, a process that renders software unintelligible but still functional. A defense against software piracy is watermarking, a process that makes it possible to determine the origin of software. A defense against tampering is tamper-proofing, so that unauthorized modifications to software (for example, to remove a watermark) will result in nonfunctional code. We briefly survey the available technology for each type of defense.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027797","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027797","","Watermarking;Software protection;Computer crime;Data security;Intellectual property;Computer security;Java;Manufacturing;Computer Society;Reverse engineering","industrial property;reverse engineering;computer crime;copy protection;cryptography","watermarking;tamper proofing;obfuscation tools;software protection;intellectual property;reverse engineering;software piracy","","273","","98","","","","","","IEEE","IEEE Journals & Magazines"
"Gray Computing: A Framework for Computing with Background JavaScript Tasks","Y. Pan; J. White; Y. Sun; J. Gray","Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN; Department of Computer Science, California State Polytechnic University, Pomona, CA; Department of Computer Science, University of Alabama, Tuscaloosa, AL","IEEE Transactions on Software Engineering","","2019","45","2","171","193","Website visitors are performing increasingly complex computational work on the websites’ behalf, such as validating forms, rendering animations, and producing data visualizations. In this article, we explore the possibility of increasing the work offloaded to web visitors’ browsers. The idle computing cycles of web visitors can be turned into a large-scale distributed data processing engine, which we term gray computing. Past research has looked primarily at either volunteer computing with specialized clients or browser-based volunteer computing where the visitors keep their browsers open to a single web page for a long period of time. This article provides a comprehensive analysis of the architecture, performance, security, cost effectiveness, user experience, and other issues of gray computing distributed data processing engines with heterogeneous computing power, non-uniform page view times, and high computing pool volatility. Several real-world applications are examined and gray computing is shown to be cost effective for a number of complex tasks ranging from computer vision to bioinformatics to cryptology.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2772812","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8105894","Software economics;JavaScript;web browser;cloud computing","Distributed processing;Browsers;Data processing;Web pages;Cloud computing","","","","","","96","","","","","","IEEE","IEEE Journals & Magazines"
"Runtime analysis of atomicity for multithreaded programs","L. Wang; S. D. Stoller","Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA; Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA","IEEE Transactions on Software Engineering","","2006","32","2","93","110","Atomicity is a correctness condition for concurrent systems. Informally, atomicity is the property that every concurrent execution of a set of transactions is equivalent to some serial execution of the same transactions. In multithreaded programs, executions of procedures (or methods) can be regarded as transactions. Correctness in the presence of concurrency typically requires atomicity of these transactions. Tools that automatically detect atomicity violations can uncover subtle errors that are hard to find with traditional debugging and testing techniques. This paper describes two algorithms for runtime detection of atomicity violations and compares their cost and effectiveness. The reduction-based algorithm checks atomicity based on commutativity properties of events in a trace; the block-based algorithm efficiently represents the relevant information about a trace as a set of blocks (i.e., pairs of events plus associated synchronizations) and checks atomicity by comparing each block with other blocks. To improve the efficiency and accuracy of both algorithms, we incorporate a multilockset algorithm for checking data races, dynamic escape analysis, and happen-before analysis. Experiments show that both algorithms are effective in finding atomicity violations. The block-based algorithm is more accurate but more expensive than the reduction-based algorithm.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599419","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599419","Concurrent programming;testing and debugging;Java;data race;atomicity.","Runtime;System recovery;Concurrent computing;Debugging;Algorithm design and analysis;Automatic testing;Costs;Java;Operating systems","multi-threading;program diagnostics;program testing;multiprocessing systems","runtime analysis;multithreaded programs;concurrent systems;atomicity violations;reduction-based algorithm;block-based algorithm;multilockset algorithm;data race checking;dynamic escape analysis;happen-before analysis","","63","","32","","","","","","IEEE","IEEE Journals & Magazines"
"The Mobius framework and its implementation","D. D. Deavours; G. Clark; T. Courtney; D. Daly; S. Derisavi; J. M. Doyle; W. H. Sanders; P. G. Webster","Inf. & Telecommun. Technol. Center, Kansas Univ., Lawrence, KS, USA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","10","956","969","The Mobius framework is an environment for supporting multiple modeling formalisms and solution techniques. Models expressed in formalisms that are compatible with the framework are translated into equivalent models using Mobius framework components. This translation preserves the structure of the models, allowing efficient solutions. The framework is implemented in the tool by a well-defined abstract functional interface. Models and solution techniques interact with one another through the use of the standard interface, allowing them to interact with Mobius framework components, not formalism components. This permits novel combinations of modeling techniques, and will be a catalyst for new research in modeling techniques. This paper describes our approach, focusing on the ""atomic model"". We describe the formal description of the Mobius components as well as their implementations in our software tool.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041052","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041052","","Software tools;Stochastic systems;Petri nets;Stochastic processes;Computer networks;Process design;Computer network reliability;Telecommunication network reliability;Availability;System performance","digital simulation;Petri nets;Markov processes;software tools;formal specification","stochastic Petri nets;Mobius framework;multiple modeling formalisms;equivalent models;abstract functional interface;formal description;software tool;PEPA;execution policy;modeling tools;Markov models","","143","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Estimation and prediction metrics for adaptive maintenance effort of object-oriented systems","F. Fioravanti; P. Nesi","Dept. of Syst. & Inf., Florence Univ., Italy; NA","IEEE Transactions on Software Engineering","","2001","27","12","1062","1084","Many software systems built in recent years have been developed using object-oriented technology and, in some cases, they already need adaptive maintenance in order to satisfy market and customer needs. In most cases, the estimation and prediction of maintenance effort is performed with difficulty due to the lack of metrics and suitable models. In this paper, a model and metrics for estimation/prediction of adaptive maintenance effort are presented and compared with some other solutions taken from the literature. The model proposed can be used as a general approach for adopting well-known metrics (typically used for the estimation of development effort) for the estimation/prediction of adaptive maintenance effort. The model and metrics proposed have been validated against real data by using multilinear regression analysis. The validation has shown that several well-known metrics can be profitably employed for the estimation/prediction of maintenance effort.","0098-5589;1939-3520;2326-3881","","10.1109/32.988708","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988708","","Costs;Object oriented modeling;Predictive models;Control systems;Software performance;Performance evaluation;Software systems;Regression analysis;Software maintenance;Tree data structures","statistical analysis;software maintenance;software metrics;object-oriented methods","software systems;object-oriented systems;prediction metrics;estimation metrics;adaptive maintenance effort;multilinear regression analysis","","58","","44","","","","","","IEEE","IEEE Journals & Magazines"
"On Fault Representativeness of Software Fault Injection","R. Natella; D. Cotroneo; J. A. Duraes; H. S. Madeira","Federico II University of Naples, Naples; Federico II University of Naples, Naples; Rua Pedro Nunes-Quinta da Nora, Coimbra; University of Coimbra, Polo II-Pinhal de Marrocos, Coimbra","IEEE Transactions on Software Engineering","","2013","39","1","80","96","The injection of software faults in software components to assess the impact of these faults on other components or on the system as a whole, allowing the evaluation of fault tolerance, is relatively new compared to decades of research on hardware fault injection. This paper presents an extensive experimental study (more than 3.8 million individual experiments in three real systems) to evaluate the representativeness of faults injected by a state-of-the-art approach (G-SWFIT). Results show that a significant share (up to 72 percent) of injected faults cannot be considered representative of residual software faults as they are consistently detected by regression tests, and that the representativeness of injected faults is affected by the fault location within the system, resulting in different distributions of representative/nonrepresentative faults across files and functions. Therefore, we propose a new approach to refine the faultload by removing faults that are not representative of residual software faults. This filtering is essential to assure meaningful results and to reduce the cost (in terms of number of faults) of software fault injection campaigns in complex software. The proposed approach is based on classification algorithms, is fully automatic, and can be used for improving fault representativeness of existing software fault injection approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.124","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6122035","Software fault injection;experimental dependability evaluation;software reliability;fault-tolerant systems","Software;Testing;Fault tolerance;Fault tolerant systems;Hardware;Fault location;Emulation","software fault tolerance","fault representativeness;software components;software fault injection approaches;fault tolerance;hardware fault injection;G-SWFIT;regression tests;fault location;nonrepresentative faults;classification algorithms","","52","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Scalable and Effective Test Generation for Role-Based Access Control Systems","A. Masood; R. Bhatti; A. Ghafoor; A. P. Mathur","Air University, Islamabad; Oracle, Redwood Shores; Purdue University, West Lafayette; Purdue University, West Lafayette","IEEE Transactions on Software Engineering","","2009","35","5","654","668","Conformance testing procedures for generating tests from the finite state model representation of Role-Based Access Control (RBAC) policies are proposed and evaluated. A test suite generated using one of these procedures has excellent fault detection ability but is astronomically large. Two approaches to reduce the size of the generated test suite were investigated. One is based on a set of six heuristics and the other directly generates a test suite from the finite state model using random selection of paths in the policy model. Empirical studies revealed that the second approach to test suite generation, combined with one or more heuristics, is most effective in the detection of both first-order mutation and malicious faults and generates a significantly smaller test suite than the one generated directly from the finite state models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4967616","Role-Based Access Control (RBAC);finite state models;fault model;first-order mutants;malicious faults.","System testing;Access control;Fault detection;Application software;Genetic mutations;Permission;Aerospace electronics;Computer Society;Authentication;Operating systems","authorisation;fault tolerance;finite state machines","for role-based access control system;conformance testing;finite state model;malicious fault detection;first-order mutant","","19","","30","","","","","","IEEE","IEEE Journals & Magazines"
"A State-of-the-Practice Survey of Risk Management in Development with Off-the-Shelf Software Components","J. Li; R. Conradi; O. P. Slyngstad; M. Torchiano; M. Morisio; C. Bunse","Norwegian University of Science and Technology; Norwegian University of Science and Technology; Norwegian University of Science and Technology; Politecnico di Torino; Politecnico di Torino, Italy; the International University in Germany","IEEE Transactions on Software Engineering","","2008","34","2","271","286","An international survey on risk management in software development with off-the-shelf (OTS) components is reported upon and discussed. The survey investigated actual risk-management activities and their correlations with the occurrences of typical risks in OTS component-based development. Data from 133 software projects in Norway, Italy, and Germany were collected using a stratified random sample of IT companies. The results show that OTS components normally do not contribute negatively to the quality of the software system as a whole, as is commonly expected. However, issues such as the underestimation of integration effort and inefficient debugging remain problematic and require further investigation. The results also illustrate several promising effective risk-reduction activities, e.g., putting more effort into learning relevant OTS components, integrating unfamiliar components first, thoroughly evaluating the quality of candidate OTS components, and regularly monitoring the support capability of OTS providers. Five hypotheses are proposed regarding these risk-reduction activities. The results also indicate that several other factors, such as project, cultural, and human-social factors, have to be investigated to thoroughly deal with the possible risks of OTS-based projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4459339","Software Engineering/Reusable Software;Software Engineering/Management;Software Engineering/Software Engineering Process;Software Engineering/Reusable Software;Software Engineering/Management;Software Engineering/Software Engineering Process","Risk management;Software engineering;Computer Society;Software systems;Programming;Computer industry;Investments;Open source software;Project management;Debugging","project management;risk management;software development management;software packages","risk management;off-the-shelf software component;software development;component-based development;software project;software system quality;debugging;risk-reduction activity","","38","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Elaborating Requirements Using Model Checking and Inductive Learning","D. Alrajeh; J. Kramer; A. Russo; S. Uchitel","Imperial College London, London; Imperial College London, London; Imperial College London, London; Imperial College London, London","IEEE Transactions on Software Engineering","","2013","39","3","361","383","The process of Requirements Engineering (RE) includes many activities, from goal elicitation to requirements specification. The aim is to develop an operational requirements specification that is guaranteed to satisfy the goals. In this paper, we propose a formal, systematic approach for generating a set of operational requirements that are complete with respect to given goals. We show how the integration of model checking and inductive learning can be effectively used to do this. The model checking formally verifies the satisfaction of the goals and produces counterexamples when incompleteness in the operational requirements is detected. The inductive learning process then computes operational requirements from the counterexamples and user-provided positive examples. These learned operational requirements are guaranteed to eliminate the counterexamples and be consistent with the goals. This process is performed iteratively until no goal violation is detected. The proposed framework is a rigorous, tool-supported requirements elaboration technique which is formally guided by the engineer's knowledge of the domain and the envisioned system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6216384","Requirements elaboration;goal operationalization;behavior model refinement;model checking;inductive learning","Wheels;Computational modeling;Software;Adaptation models;Calculus;Switches;Semantics","formal specification;formal verification;learning by example","model checking;requirement engineering;RE;goal elicitation;requirement specification;operational requirements specification;formal verification;inductive learning process;tool-supported requirements elaboration","","5","","52","","","","","","IEEE","IEEE Journals & Magazines"
"A programming methodology for dual-tier multicomputers","S. B. Baden; S. J. Fink","Dept. of Comput. Sci. & Eng., California Univ., San Diego, La Jolla, CA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","3","212","226","Hierarchically organized ensembles of shared memory multiprocessors possess a richer and more complex model of locality than previous generation multicomputers with single processor nodes. These dual-tier computers introduce many new factors into the programmer's performance model. We present a methodology for implementing block-structured numerical applications on dual-tier computers and a run-time infrastructure, called KeLP2, that implements the methodology. KeLP2 supports two levels of locality and parallelism via hierarchical SPMD control flow, run-time geometric meta-data, and asynchronous collective communication. KeLP applications can effectively overlap communication with computation under conditions where nonblocking point-to-point message passing fails to do so. KeLP's abstractions hide considerable detail without sacrificing performance and dual-tier applications written in KeLP consistently outperform equivalent single-tier implementations written in MPI. We describe the KeLP2 model and show how it facilitates the implementation of five block-structured applications specially formulated to hide communication latency on dual-tiered architectures. We support our arguments with empirical data from applications running on various single- and dual-tier multicomputers. KeLP2 supports a migration path from single-tier to dual-tier platforms and we illustrate this capability with a detailed programming example.","0098-5589;1939-3520;2326-3881","","10.1109/32.842948","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842948","","Application software;Concurrent computing;Parallel processing;Programming profession;Computer architecture;Computer applications;Runtime;Communication system control;Message passing;Delay","message passing;shared memory systems;parallel processing;parallel programming","programming methodology;dual-tier multicomputers;shared memory multiprocessors;performance model;block-structured numerical applications;run-time infrastructure;KeLP2;hierarchical SPMD control flow;run-time geometric meta-data;asynchronous collective communication;point-to-point message passing;communication latency;dual-tiered architectures","","15","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Test Code Quality and Its Relation to Issue Handling Performance","D. Athanasiou; A. Nugroho; J. Visser; A. Zaidman","Software Improvement Group, Amstelplein 1, 1096HA Amsterdam, The Netherlands; Software Improvement Group, Amstelplein 1, 1096HA Amsterdam, The Netherlands; Software Improvement Group, Amstelplein 1, 1096HA Amsterdam, The Netherlands; Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Mekelweg 4, 2628CD Delft, The Netherlands","IEEE Transactions on Software Engineering","","2014","40","11","1100","1125","Automated testing is a basic principle of agile development. Its benefits include early defect detection, defect causelocalization and removal of fear to apply changes to the code. Therefore, maintaining high quality test code is essential. This study introduces a model that assesses test code quality by combining source code metrics that reflect three main aspects of test codequality: completeness, effectiveness and maintainability. The model is inspired by the Software Quality Model of the SoftwareImprovement Group which aggregates source code metrics into quality ratings based on benchmarking. To validate the model we assess the relation between test code quality, as measured by the model, and issue handling performance. An experiment isconducted in which the test code quality model is applied to<inline-formula><tex-math notation=""LaTeX"">$18$</tex-math><alternatives><inline-graphic xlink:href=""zaidman-ieq1-2342227.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>open source systems. The test quality ratings are tested for correlation with issue handling indicators, which are obtained by mining issue repositories. In particular, we study the (1) defect resolution speed, (2) throughput and (3) productivity issue handling metrics. The results reveal a significant positive correlation between test code quality and two out of the three issue handling metrics (throughput and productivity), indicating that good test code quality positively influences issue handling performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2342227","NWO TestRoots project; RAAK-PRO project EQuA; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6862882","Testing;defects;bugs;metrics;measurement","Measurement;Software;Productivity;Throughput;Benchmark testing;Correlation","","","","28","","90","","","","","","IEEE","IEEE Journals & Magazines"
"Test case prioritization: a family of empirical studies","S. Elbaum; A. G. Malishevsky; G. Rothermel","Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","2","159","182","To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of such prioritization is to increase a test suite's rate of fault detection. Previous work reported results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: 1) Can prioritization techniques be effective when targeted at specific modified versions; 2) what trade-offs exist between fine granularity and coarse granularity prioritization techniques; 3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? To address these questions, we have performed several new studies in which we empirically compared prioritization techniques using both controlled experiments and case studies.","0098-5589;1939-3520;2326-3881","","10.1109/32.988497","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988497","","Computer aided software engineering;Fault detection;Software testing;System testing;Costs;Software measurement;Radio access networks;Feedback;Debugging;Instruments","program testing","test case prioritization;regression testing;software testing;fault detection rate;fine granularity prioritization techniques;coarse granularity prioritization techniques;fault proneness measures","","377","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying Renaming Opportunities by Expanding Conducted Rename Refactorings","H. Liu; Q. Liu; Y. Liu; Z. Wang","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Software Engineering","","2015","41","9","887","900","To facilitate software refactoring, a number of approaches and tools have been proposed to suggest where refactorings should be conducted. However, identification of such refactoring opportunities is usually difficult because it often involves difficult semantic analysis and it is often influenced by many factors besides source code. For example, whether a software entity should be renamed depends on the meaning of its original name (natural language understanding), the semantics of the entity (source code semantics), experience and preference of developers, and culture of companies. As a result, it is difficult to identify renaming opportunities. To this end, in this paper we propose an approach to identify renaming opportunities by expanding conducted renamings. Once a rename refactoring is conducted manually or with tool support, the proposed approach recommends to rename closely related software entities whose names are similar to that of the renamed entity. The rationale is that if an engineer makes a mistake in naming a software entity it is likely for her to make the same mistake in naming similar and closely related software entities. The main advantage of the proposed approach is that it does not involve difficult semantic analysis of source code or complex natural language understanding. Another advantage of this approach is that it is less influenced by subjective factors, e.g., experience and preference of software engineers. The proposed approach has been evaluated on four open-source applications. Our evaluation results show that the proposed approach is accurate in recommending entities to be renamed (average precision 82 percent) and in recommending new names for such entities (average precision 93 percent). Evaluation results also suggest that a substantial percentage (varying from 20 to 23 percent) of rename refactorings are expansible.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2427831","National Natural Science Foundation of China; Program for New Century Excellent Talents in University; Beijing Higher Education Young Elite Teacher Project; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7097720","Software Refactoring;Rename;Code Smells;Refactoring Opportunity;Identification;Software refactoring;rename;code smells;refactoring opportunity;identification","Semantics;Natural languages;Open source software;Engines;Context;IEEE Potentials","software maintenance","renaming opportunities identification;conducted rename refactorings;software refactoring;semantic analysis;software entity naming","","3","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Design, construction, and application of a generic visual language generation environment","K. Zhang; D. -. Zhang; J. Cao","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","4","289","307","The implementation of visual programming languages (VPLs) and their supporting environments is time-consuming and tedious. To ease the task, researchers have developed some high-level tools to reduce the development effort. None of these tools, however, can be easily used to create a complete visual language in a seamless way as the lex/yacc tools do for textual language constructions. This paper presents the design, construction and application of a generic visual language generation environment, called VisPro. The VisPro design model improves the conventional model-view-controller framework in that its functional modules are decoupled to allow independent development and integration. The VisPro environment consists of a set of visual programming tools. Using VisPro, the process of VPL construction can be divided into two steps: lexicon definition and grammar specification. The former step defines visual objects and a visual editor, and the latter step provides language grammars with graph rewriting rules. The compiler for the VPL is automatically created according to the grammar specification. A target VPL is generated as a programming environment which contains the compiler and the visual editor. The paper demonstrates how VisPro is used by building a simple visual language and a more complex visual modeling language for distributed programming.","0098-5589;1939-3520;2326-3881","","10.1109/32.917521","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917521","","Programming environments;Graphical user interfaces;Computer languages;Software systems;User interfaces;Program processors;Buildings;Productivity;Application software;Education","visual languages;programming environments;compiler generators;subroutines;graph grammars;rewriting systems;distributed programming","generic visual language generation environment;visual programming language construction;VisPro;design model;model-view-controller framework;decoupled functional modules;independent module development;module integration;visual programming tools;lexicon definition;grammar specification;visual objects;visual editor;language grammars;graph rewriting rules;compiler construction;programming environment;visual modeling language;distributed programming","","39","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Timed Automata Patterns","J. S. Dong; P. Hao; S. Qin; J. Sun; W. Yi","National University of Singapore, Singapore; National University of Singapore, Singapore; Durham University, Durham; National University of Singapore, Singapore; North Eastern University, China, and Uppsala University, Sweden","IEEE Transactions on Software Engineering","","2008","34","6","844","859","Timed automata have proven to be useful for specification and verification of real-time systems. System design using timed automata relies on explicit manipulation of clock variables. A number of automated analyzers for timed automata have been developed. However, timed automata lack composable patterns for high-level system design. Specification languages like Timed Communicating Sequential Process (CSP) and Timed Communicating Object-Z (TCOZ) are well suited for presenting compositional models of complex real-time systems. In this work, we define a set of composable Timed Automata patterns based on hierarchical constructs in time-enriched process algebras. The patterns facilitate the hierarchical design of complex systems using timed automata. They also allow a systematic translation from Timed CSP/TCOZ models to timed automata so that analyzers for timed automata can be used to reason about TCOZ models. A prototype has been developed to support system design using timed automata patterns or, if given a TCOZ specification, to automate the translation from TCOZ to timed automata.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4586397","Specification;Validation;Specification;Validation","Automata;Real time systems;Algebra;Clocks;Power system modeling;Sun;Specification languages;Prototypes;Formal specifications;Timing","automata theory;formal specification;formal verification;process algebra;specification languages;systems analysis","timed automata patterns;system design;specification languages;timed communicating sequential process;timed communicating object-Z;time-enriched process algebras;TCOZ specification;real-time systems","","28","","43","","","","","","IEEE","IEEE Journals & Magazines"
"A probabilistic model for predicting software development effort","P. C. Pendharkar; G. H. Subramanian; J. A. Rodger","Sch. of Bus. Adm., Pennsylvania State Univ., University Park, PA, USA; Sch. of Bus. Adm., Pennsylvania State Univ., University Park, PA, USA; NA","IEEE Transactions on Software Engineering","","2005","31","7","615","624","Recently, Bayesian probabilistic models have been used for predicting software development effort. One of the reasons for the interest in the use of Bayesian probabilistic models, when compared to traditional point forecast estimation models, is that Bayesian models provide tools for risk estimation and allow decision-makers to combine historical data with subjective expert estimates. In this paper, we use a Bayesian network model and illustrate how a belief updating procedure can be used to incorporate decision-making risks. We develop a causal model from the literature and, using a data set of 33 real-world software projects, we illustrate how decision-making risks can be incorporated in the Bayesian networks. We compare the predictive performance of the Bayesian model with popular nonparametric neural-network and regression tree forecasting models and show that the Bayesian model is a competitive model for forecasting software development effort.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.75","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492375","Index Terms- Bayesian belief networks;software effort estimation;probability theory.","Predictive models;Programming;Bayesian methods;Regression tree analysis;Neural networks;Costs;Probability distribution;Decision making;Linear regression;Risk management","software development management;software cost estimation;risk management;decision making;belief networks;neural nets;Bayes methods;probability;regression analysis","Bayesian probabilistic model;software development effort prediction;point forecast estimation model;risk estimation;decision-making risk;Bayesian network model;belief updating procedure;real-world software project;nonparametric neural-network;regression tree forecasting model;software development effort forecasting;Bayesian belief network;software effort estimation;probability theory","","108","","37","","","","","","IEEE","IEEE Journals & Magazines"
"The Risks of Coverage-Directed Test Case Generation","G. Gay; M. Staats; M. Whalen; M. P. E. Heimdahl","Department of Computer Science & Engineering, University of South Carolina; Google, Inc; Department of Computer Science and Engineering, University of Minnesota; Department of Computer Science and Engineering, University of Minnesota","IEEE Transactions on Software Engineering","","2015","41","8","803","819","A number of structural coverage criteria have been proposed to measure the adequacy of testing efforts. In the avionics and other critical systems domains, test suites satisfying structural coverage criteria are mandated by standards. With the advent of powerful automated test generation tools, it is tempting to simply generate test inputs to satisfy these structural coverage criteria. However, while techniques to produce coverage-providing tests are well established, the effectiveness of such approaches in terms of fault detection ability has not been adequately studied. In this work, we evaluate the effectiveness of test suites generated to satisfy four coverage criteria through counterexample-based test generation and a random generation approach-where tests are randomly generated until coverage is achieved-contrasted against purely random test suites of equal size. Our results yield three key conclusions. First, coverage criteria satisfaction alone can be a poor indication of fault finding effectiveness, with inconsistent results between the seven case examples (and random test suites of equal size often providing similar-or even higher-levels of fault finding). Second, the use of structural coverage as a supplement-rather than a target-for test generation can have a positive impact, with random test suites reduced to a coverage-providing subset detecting up to 13.5 percent more faults than test suites generated specifically to achieve coverage. Finally, Observable MC/DC, a criterion designed to account for program structure and the selection of the test oracle, can-in part-address the failings of traditional structural coverage criteria, allowing for the generation of test suites achieving higher levels of fault detection than random test suites of equal size. These observations point to risks inherent in the increase in test automation in critical systems, and the need for more research in how coverage criteria, test generation approaches, the test oracle used, and system structure jointly influence test effectiveness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2421011","NASA; NSF; Fonds National de la Recherche, Luxembourg; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7081779","Software Testing;System Testing;Software testing;system testing","Testing;Aerospace electronics;NASA;Standards;Fault detection;Measurement;Software packages","program testing;risk management;software fault tolerance","risks;coverage-directed test case generation;structural coverage criteria;automated test generation tools;fault detection;counterexample-based test generation;random generation approach;random test suites;coverage criteria satisfaction;fault finding effectiveness;observable MC/DC;program structure;test oracle selection;test automation;critical systems;system structure;software testing","","26","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Generating Event Sequence-Based Test Cases Using GUI Runtime State Feedback","X. Yuan; A. M. Memon","University of Maryland, College Park; University of Maryland, College Park","IEEE Transactions on Software Engineering","","2010","36","1","81","95","This paper presents a fully automatic model-driven technique to generate test cases for graphical user interfaces (GUIs)-based applications. The technique uses feedback from the execution of a ¿seed test suite,¿ which is generated automatically using an existing structural event interaction graph model of the GUI. During its execution, the runtime effect of each GUI event on all other events pinpoints event semantic interaction (ESI) relationships, which are used to automatically generate new test cases. Two studies on eight applications demonstrate that the feedback-based technique 1) is able to significantly improve existing techniques and helps identify serious problems in the software and 2) the ESI relationships captured via GUI state yield test suites that most often detect more faults than their code, event, and event-interaction-coverage equivalent counterparts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5306073","GUI testing;automated testing;model-based testing;GUITAR testing system.","Graphical user interfaces;Runtime;State feedback;Automatic testing;Software testing;System testing;Application software;Costs;Fault diagnosis;Event detection","graphical user interfaces;program testing;software quality","event sequence based test cases;graphical user interfaces;GUI runtime state feedback;automatic model driven technique;event interaction coverage equivalent counterparts;software quality;event semantic interaction relationships","","61","","49","","","","","","IEEE","IEEE Journals & Magazines"
"A formal model of the software test process","J. W. Cangussu; R. A. DeCarlo; A. P. Mathur","Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","8","782","796","A novel approach to model the system test phase of the software life cycle is presented. This approach is based on concepts and techniques from control theory and is useful in computing the effort required to reduce the number of errors and the schedule slippage under a changing process environment. Results from these computations are used, and possibly revised, at specific checkpoints in a feedback-control structure to meet the schedule and quality objectives. Two case studies were conducted to study the behavior of the proposed model. One study reported here uses data from a commercial project. The outcome from these two studies suggests that the proposed model might well be the first significant milestone along the road to a formal and practical theory of software process control.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027800","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027800","","Software testing;Process control;Temperature control;Control theory;System testing;Processor scheduling;Control systems;Life testing;Error correction;Roads","program testing;software process improvement","formal model;software test process;system test phase;software life cycle;schedule slippage;checkpoints;feedback-control structure;software process control","","54","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Keeping the Development Environment Up to Date—A Study of the Situated Practices of Appropriating the Eclipse IDE","S. Draxler; G. Stevens; A. Boden","Department for Information Systems and New Media, University of Siegen, Siegen, Germany; Department for Information Systems and New Media, University of Siegen, Siegen, Germany; Usability and User Experience Design Competence Center, Fraunhofer Institute for Applied Information Technology FIT, Sankt Augustin, Germany","IEEE Transactions on Software Engineering","","2014","40","11","1061","1074","Software engineers and developers are surrounded by highly complex software systems. What does it take to cope with these? We introduce a field study that explores the maintenance of the Eclipse Integrated Development Environment by software developers as part of their daily work. The study focuses on appropriation of the Eclipse IDE. We present an empirical view on appropriation as a means to maintain the collective ability to work. We visited seven different organizations and observed and interviewed their members. Each organization was chosen to provide an overall picture of Eclipse use throughout the industry. The results decompose the appropriation of Eclipse by software developers in organizations into four categories: learning, tailoring and discovering, as well as the cross-cutting category: collaboration. The categories are grounded in situations that provoked a need to change as well as in policies adopted for coping with this need. By discussing these categories against the background of Eclipse and its ecosystem, we want to illustrate in what ways appropriation of component- or plugin- based software is nowadays a common and highly complex challenge for Eclipse users, and how the related appropriation practices can be supported by IT systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2354047","German Federal Ministry for Education and Research; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6898825","Programmer workbench;human factors in software design;deployment;usage experience","Software;Organizations;Computer aided software engineering;Interviews;Context;Employment;Ecosystems","object-oriented languages;object-oriented programming;software engineering","development environment;Eclipse IDE;software engineers;software developers;highly complex software systems;Eclipse Integrated Development Environment;component-based software;plugin-based software;IT systems","","2","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Measurement programs in software development: determinants of success","A. Gopal; M. S. Krishnan; T. Mukhopadhyay; D. R. Goldenson","PRTM, Waltham, MA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","9","863","875","Measurement programs in software organizations are an important source of control over quality and cost in software development. The findings of this research presented here are based on an industry-wide survey conducted to examine the factors that influence success in software metrics programs. Our approach is to go beyond the anecdotal information on metrics programs that exists in the literature and use the industry-wide survey data to rigorously test for the effects of various factors that affect metrics programs success. We measure success in metrics programs using two variables-use of metrics information in decision-making and improved organizational performance. The various determinants of metrics program success are divided into two sets-organizational variables and technical variables. The influence of these variables on metrics programs success is tested using regression analysis. Our results support some of the factors discussed in the anecdotal literature such as management support, goal alignment, and communication and feedback. Certain other factors such as metrics quality and the ease of data collection are not as strongly influential on success. We conclude the paper with a detailed discussion of our results and suggestions for future work.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033226","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033226","","Software measurement;Programming;Testing;Software quality;Costs;Computer industry;Software metrics;Decision making;Regression analysis;Feedback","software metrics;software development management;software quality;economics","measurement programs;software development;quality;cost;software metrics programs;decision making;improved organizational performance;technical variables;organizational variables;regression analysis;management support;goal alignment;communication;feedback;data collection;success factors","","65","","28","","","","","","IEEE","IEEE Journals & Magazines"
"Model Checking Timed and Stochastic Properties with CSL^{TA}","S. Donatelli; S. Haddad; J. Sproston","Università di Torino, Torino; LSV, CBRS and École Normale Supérieure de Cachan, Cachan; Università di Torino, Torino","IEEE Transactions on Software Engineering","","2009","35","2","224","240","Markov chains are a well-known stochastic process that provide a balance between being able to adequately model the system's behavior and being able to afford the cost of the model solution. The definition of stochastic temporal logics like continuous stochastic logic (CSL) and its variant asCSL, and of their model-checking algorithms, allows a unified approach to the verification of systems, allowing the mix of performance evaluation and probabilistic verification. In this paper we present the stochastic logic CSLTA, which is more expressive than CSL and asCSL, and in which properties can be specified using automata (more precisely, timed automata with a single clock). The extension with respect to expressiveness allows the specification of properties referring to the probability of a finite sequence of timed events. A typical example is the responsiveness property ""with probability at least 0.75, a message sent at time 0 by a system A will be received before time 5 by system B and the acknowledgment will be back at A before time 7"", a property that cannot be expressed in either CSL or asCSL. We also present a model-checking algorithm for CSLTA.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.108","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4721440","Model checking;Markov processes;Temporal logic","Stochastic processes;Stochastic systems;Costs;Automata;Probabilistic logic;Quality of service;Unified modeling language;Telecommunication computing;Clocks;Delay","formal logic;formal verification;Markov processes;probability","timed properties;stochastic properties;Markov chains;stochastic process;stochastic temporal logics;continuous stochastic logic;model checking algorithm;systems verification;performance evaluation;probabilistic verification;finite sequence;timed events","","40","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Dealing with Traceability in the MDDof Model Transformations","J. M. Vara; V. A. Bollati; Á. Jiménez; E. Marcos","Computing Languages and Systems - II, University Rey Juan Carlos, Mostoles, Madrid, Spain; Computing Languages and Systems - II, University Rey Juan Carlos, Mostoles, Madrid, Spain; Computing Languages and Systems - II, University Rey Juan Carlos, Mostoles, Madrid, Spain; Computing Languages and Systems - II, University Rey Juan Carlos, Mostoles, Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","6","555","583","Traceability has always been acknowledged as a relevant topic in Software Engineering. However, keeping track of the relationships between the different assets involved in a development process is a complex and tedious task. The fact that the main assets handled in any model-driven engineering project are models and model transformations eases the task. In order to take advantage of this scenario, which has not been appropriately capitalized on by the most widely adopted model transformation languages before, this work presents MeTAGeM-Trace, a methodological and technical proposal with which to support the model-driven development of model transformations that include trace generation. The underlying idea is to start from a high-level specification of the transformation which is subsequently refined into lower-level transformation models in terms of a set of DSLs until the source code that implements the transformation can be generated. Running this transformation produces not only the corresponding target models, but also a trace model between the elements of the source and target models. As part of the proposal, an EMF-based toolkit has been developed to support the development of ATL and ETL model transformations. This toolkit has been empirically validated by conducting a set of case studies following a systematic research methodology.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2316132","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6784505","Model-driven engineering;model transformations;traceability","Proposals;Object oriented modeling;Software;DSL;Complexity theory;Data models;Software engineering","research and development;software engineering;source code (software)","MDD;traceability;software engineering;model-driven engineering project;model transformation languages;MeTAGeM-Trace;trace generation;lower-level transformation models;DSL;source code;EMF-based toolkit;ATL model transformations;ETL model transformations;systematic research methodology","","6","","79","","","","","","IEEE","IEEE Journals & Magazines"
"The Probabilistic Program Dependence Graph and Its Application to Fault Diagnosis","G. K. Baah; A. Podgurski; M. J. Harrold","Georgia Institute of Technology, Atlanta; Case Western Reserve University, Cleveland; Georgia Institute of Technology, Atlanta","IEEE Transactions on Software Engineering","","2010","36","4","528","545","This paper presents an innovative model of a program's internal behavior over a set of test inputs, called the probabilistic program dependence graph (PPDG), which facilitates probabilistic analysis and reasoning about uncertain program behavior, particularly that associated with faults. The PPDG construction augments the structural dependences represented by a program dependence graph with estimates of statistical dependences between node states, which are computed from the test set. The PPDG is based on the established framework of probabilistic graphical models, which are used widely in a variety of applications. This paper presents algorithms for constructing PPDGs and applying them to fault diagnosis. The paper also presents preliminary evidence indicating that a PPDG-based fault localization technique compares favorably with existing techniques. The paper also presents evidence indicating that PPDGs can be useful for fault comprehension.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374423","Probabilistic graphical models;machine learning;fault diagnosis;program analysis.","Fault diagnosis;Graphical models;Application software;Testing;Software engineering;Automatic control;Information analysis;Runtime;Probability distribution;Computer Society","fault diagnosis;graph theory;probability;program diagnostics;reasoning about programs;uncertainty handling","probabilistic program dependence graph;fault diagnosis;probabilistic analysis;reasoning;uncertain program behavior;fault localization technique;probabilistic graphical models","","39","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Imprecise Matching of Requirements Specifications for Software Services Using Fuzzy Logic","M. C. Platenius; A. Shaker; M. Becker; E. Hüllermeier; W. Schäfer","Software Engineering Group, Heinz Nixdorf Institute, Paderborn University, Germany; Intelligent Systems Group, Department of Computer Science, Paderborn University, Germany; Software Engineering Group, Fraunhofer IEM, Paderborn, Germany; Intelligent Systems Group, Department of Computer Science, Paderborn University, Germany; Software Engineering Group, Heinz Nixdorf Institute, Paderborn University, Germany","IEEE Transactions on Software Engineering","","2017","43","8","739","759","Today, software components are provided by global markets in the form of services. In order to optimally satisfy service requesters and service providers, adequate techniques for automatic service matching are needed. However, a requester's requirements may be vague and the information available about a provided service may be incomplete. As a consequence, fuzziness is induced into the matching procedure. The contribution of this paper is the development of a systematic matching procedure that leverages concepts and techniques from fuzzy logic and possibility theory based on our formal distinction between different sources and types of fuzziness in the context of service matching. In contrast to existing methods, our approach is able to deal with imprecision and incompleteness in service specifications and to inform users about the extent of induced fuzziness in order to improve the user's decision-making. We demonstrate our approach on the example of specifications for service reputation based on ratings given by previous users. Our evaluation based on real service ratings shows the utility and applicability of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2632115","German Research Foundation (DFG); Collaborative Research Center “On-The-Fly Computing”; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7755807","Service selection;service matching;requirements specifications;non-functional properties;fuzzy logic;uncertainty;decision making","Uncertainty;Fuzzy logic;Context;Security;Software;Software engineering;Decision making","decision making;fuzzy logic;fuzzy set theory;pattern matching;software engineering","software components;requirement specification imprecise matching;software services;fuzzy logic;automatic service matching;possibility theory;user decision-making","","","","93","","","","","","IEEE","IEEE Journals & Magazines"
"Using sensitivity analysis to validate a state variable model of the software test process","J. W. Cangussu; R. A. DeCarlo; A. P. Mathur","Dept. of Comput. Sci., Texas Univ. at Dallas, Richardson, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","5","430","443","We report on the sensitivity analysis of a state variable model (Model S) proposed earlier. Model S captures the dominant behavior of the system test phase of the software test process. Sensitivity analysis is a mathematical methodology to compute changes in the system behavior due to changes in system parameters or variables. This is particularly important when parameters are calibrated using noisy or small data sets. Nevertheless, by mathematically quantifying the effects of parameter variations on the behavior of the model, and thereby the STP, one can easily and quickly evaluate the effect of such variations on the process performance without having to perform extensive simulations. In all cases studied, model S behaved according to empirical observations which serves to validate the model. It is also shown that sensitivity analysis can suggest structural improvements in a model when the model does not behave as expected.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199072","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199072","","Sensitivity analysis;Software testing;Mathematical model;System testing;Feedback control;Acoustic testing;Performance evaluation;Calibration;Software quality;Quality management","sensitivity analysis;program testing","sensitivity analysis;state variable model;software test process;dominant behavior;feedback control;state model","","24","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Common Trends in Software Fault and Failure Data","M. Hamill; K. Goseva-Popstojanova","West Virginia University, Morgantown; West Virginia University, Morgantown","IEEE Transactions on Software Engineering","","2009","35","4","484","496","The benefits of the analysis of software faults and failures have been widely recognized. However, detailed studies based on empirical data are rare. In this paper, we analyze the fault and failure data from two large, real-world case studies. Specifically, we explore: 1) the localization of faults that lead to individual software failures and 2) the distribution of different types of software faults. Our results show that individual failures are often caused by multiple faults spread throughout the system. This observation is important since it does not support several heuristics and assumptions used in the past. In addition, it clearly indicates that finding and fixing faults that lead to such software failures in large, complex systems are often difficult and challenging tasks despite the advances in software development. Our results also show that requirement faults, coding faults, and data problems are the three most common types of software faults. Furthermore, these results show that contrary to the popular belief, a significant percentage of failures are linked to late life cycle activities. Another important aspect of our work is that we conduct intra- and interproject comparisons, as well as comparisons with the findings from related studies. The consistency of several main trends across software systems in this paper and several related research efforts suggests that these trends are likely to be intrinsic characteristics of software faults and failures rather than project specific.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.3","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4760152","Software faults and failures;fault location;fault types;software fault distribution;software reliability;empirical studies.","Failure analysis;Software quality;Programming;Software systems;Fault location;Software reliability;Fault detection;Humans;Terminology;Computer bugs","software fault tolerance;system recovery;systems analysis","software fault analysis;software failure data;complex system;software development;requirement fault;coding fault;software life cycle activity;software system","","51","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Software Reliability and Testing Time Allocation: An Architecture-Based Approach","R. Pietrantuono; S. Russo; K. S. Trivedi","Federico II University of Naples, Naples; Federico II University of Naples, Naples; Duke University, Durham","IEEE Transactions on Software Engineering","","2010","36","3","323","337","With software systems increasingly being employed in critical contexts, assuring high reliability levels for large, complex systems can incur huge verification costs. Existing standards usually assign predefined risk levels to components in the design phase, to provide some guidelines for the verification. It is a rough-grained assignment that does not consider the costs and does not provide sufficient modeling basis to let engineers quantitatively optimize resources usage. Software reliability allocation models partially address such issues, but they usually make so many assumptions on the input parameters that their application is difficult in practice. In this paper, we try to reduce this gap, proposing a reliability and testing resources allocation model that is able to provide solutions at various levels of detail, depending upon the information the engineer has about the system. The model aims to quantitatively identify the most critical components of software architecture in order to best assign the testing resources to them. A tool for the solution of the model is also developed. The model is applied to an empirical case study, a program developed for the European Space Agency, to verify model's prediction abilities and evaluate the impact of the parameter estimation errors on the prediction accuracy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383374","Reliability;software architecture;software testing.","Software reliability;Software testing;Reliability engineering;Predictive models;Software systems;Guidelines;Cost function;Application software;System testing;Resource management","program testing;software architecture;software reliability","software reliability;testing time allocation;architecture-based approach;rough-grained assignment","","41","","40","","","","","","IEEE","IEEE Journals & Magazines"
"FSM-based incremental conformance testing methods","K. EI-Fakih; N. Yevtushenko; G. Bochmann","Dept. of Comput. Sci., American Univ. of Sharjah, United Arab Emirates; NA; NA","IEEE Transactions on Software Engineering","","2004","30","7","425","436","The development of appropriate test cases is an important issue for conformance testing of protocol implementations and other reactive software systems. A number of methods are known for the development of a test suite based on a specification given in the form of a finite state machine. In practice, the system requirements evolve throughout the lifetime of the system and the specifications are modified incrementally. We adapt four well-known test derivation methods, namely, the HIS, W, Wp, and UIOv methods, for generating tests that would test only the modified parts of an evolving specification. Some application examples and experimental results are provided. These results show significant gains when using incremental testing in comparison with complete testing, especially when the modified part represents less than 20 percent of the whole specification.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318604","Protocol conformance testing;finite state machines;test derivation;incremental testing.","System testing;Protocols;Automata;Software testing;Software systems;Unified modeling language;Hardware;Programming","formal verification;conformance testing;protocols;finite state machines;formal specification;program testing","protocol conformance testing;reactive software systems;system requirements;finite state machine based incremental conformance testing methods;test derivation methods","","29","","23","","","","","","IEEE","IEEE Journals & Magazines"
"An experiment measuring the effects of personal software process (PSP) training","L. Prechelt; B. Unger","abaXXX Technol. AG, Stuttgart, Germany; NA","IEEE Transactions on Software Engineering","","2001","27","5","465","472","The personal software process is a process improvement methodology aimed at individual software engineers. It claims to improve software quality (in particular defect content), effort estimation capability, and process adaptation and improvement capabilities. We have tested some of these claims in an experiment comparing the performance of participants who had just previously received a PSP course to a different group of participants who had received other technical training instead. Each participant of both groups performed the same task. We found the following positive effects: the PSP group estimated their productivity (though not their effort) more accurately, made fewer trivial mistakes, and their programs performed more careful error-checking; further, the performance variability was smaller in the PSP group in various respects. However, the improvements are smaller than the PSP proponents usually assume, possibly due to the low actual usage of PSP techniques in the PSP group. We conjecture that PSP training alone does not automatically realize the PSP's potential benefits (as seen in some industrial PSP success stories) when programmers are left alone with motivating themselves to actually use the PSP techniques.","0098-5589;1939-3520;2326-3881","","10.1109/32.922716","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=922716","","Software measurement;Software quality;Productivity;Testing;Industrial training;Coordinate measuring machines;Data analysis;Management training;Programming profession;Quality management","software process improvement;training;software quality","experiment;personal software process training;software process improvement methodology;software quality;software effort estimation;productivity","","28","","12","","","","","","IEEE","IEEE Journals & Magazines"
"Goal-Centric Traceability: Using Virtual Plumblines to Maintain Critical Systemic Qualities","J. Cleland-Huang; W. Marrero; B. Berenbach","DePaul University, Chicago; DePaul University, Chicago; Siemens Corporate Research, Inc., Princeton","IEEE Transactions on Software Engineering","","2008","34","5","685","699","Successful software development involves the elicitation, implementation, and management of critical systemic requirements related to qualities such as security, usability, and performance. Unfortunately, even when such qualities are carefully incorporated into the initial design and implemented code, there are no guarantees that they will be consistently maintained throughout the lifetime of the software system. Even though it is well known that system qualities tend to erode as functional and environmental changes are introduced, existing regression testing techniques are primarily designed to test the impact of change upon system functionality rather than to evaluate how it might affect more global qualities. The concept of using goal-centric traceability to establish relationships between a set of strategically placed assessment models and system goals is introduced. This paper describes the process, algorithms, and techniques for utilizing goal models to establish executable traces between goals and assessment models, detect change impact points through the use of automated traceability techniques, propagate impact events, and assess the impact of change upon systemic qualities. The approach is illustrated through two case studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4553719","Management;Maintenance management;Management;Maintenance management","Usability;System testing;Programming;Quality management;Software development management;Security;Software systems;Change detection algorithms;Event detection;Real time systems","formal specification;program testing;regression analysis;software maintenance;software quality","goal-centric traceability;virtual plumblines;critical systemic qualities;software development;critical systemic requirements;software system lifetime;regression testing techniques;system functionality;automated traceability techniques","","18","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic Software Updating Using a Relaxed Consistency Model","H. Chen; J. Yu; C. Hang; B. Zang; P. Yew","Fudan University, Shanghai; University of Michigan, Ann Arbor; Microsoft (China) Ltd., Shanghai; Fudan University, Shanghai; University of Minnesota at Twin Cities, Minneapolis","IEEE Transactions on Software Engineering","","2011","37","5","679","694","Software is inevitably subject to changes. There are patches and upgrades that close vulnerabilities, fix bugs, and evolve software with new features. Unfortunately, most traditional dynamic software updating approaches suffer some level of limitations; few of them can update multithreaded applications when involving data structure changes, while some of them lose binary compatibility or incur nonnegligible performance overhead. This paper presents POLUS, a software maintenance tool capable of iteratively evolving running unmodified multithreaded software into newer versions, yet with very low performance overhead. The main idea in POLUS is a relaxed consistency model that permits the concurrent activity of the old and new code. POLUS borrows the idea of cache-coherence protocol in computer architecture and uses a ”bidirectional write-through” synchronization protocol to ensure system consistency. To demonstrate the applicability of POLUS, we report our experience in using POLUS to dynamically update three prevalent server applications: vsftpd, sshd, and Apache HTTP server. Performance measurements show that POLUS incurs negligible runtime overhead on the three applications-a less than 1 percent performance degradation (but 5 percent for one case). The time to apply an update is also minimal.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5551162","Maintainability;reliability;runtime environments.","Software;Synchronization;Protocols;Bidirectional control;Registers;Runtime","computer architecture;hypermedia;multi-threading;program testing;software maintenance;software tools;transport protocols","dynamic software update;relaxed consistency model;data structure;binary compatibility;nonnegligible performance;POLUS;software maintenance tool;iteratively evolving running unmodified multithreaded software;concurrent activity;cache-coherence protocol;computer architecture;bidirectional write-through synchronization protocol;prevalent server application;HTTP server","","14","","42","","","","","","IEEE","IEEE Journals & Magazines"
"A design methodology for data-parallel applications","L. S. Nyland; J. F. Prins; A. Goldberg; P. H. Mills","Dept. of Comput. Sci., North Carolina Univ., Chapel Hill, NC, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","4","293","314","A methodology for the design and development of data-parallel applications and components is presented. Data-parallelism is a well understood form of parallel computation, yet developing simple applications can involve substantial efforts to express the problem in low level notations. We describe a process of software development for data-parallel applications starting from high level specifications, generating repeated refinements of designs to match different architectural models and performance constraints, enabling a development activity with cost benefit analysis. Primary issues are algorithm choice, correctness, and efficiency, followed by data decomposition, load balancing, and message passing coordination. Development of a data-parallel multitarget tracking application is used as a case study, showing the progression from high to low level refinements. We conclude by describing tool support for the process.","0098-5589;1939-3520;2326-3881","","10.1109/32.844491","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=844491","","Design methodology;Concurrent computing;Application software;Parallel processing;Milling machines;Parallel programming;Cost benefit analysis;Load management;Process design;Algorithm design and analysis","parallel programming;parallel algorithms;cost-benefit analysis;message passing;resource allocation","data-parallel application design;design methodology;data-parallelism;parallel computation;low level notations;high level specifications;repeated refinements;architectural models;performance constraints;development activity;cost benefit analysis;algorithm choice;data decomposition;load balancing;message passing coordination;data-parallel multitarget tracking application;case study;tool support","","8","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Patterns of Knowledge in API Reference Documentation","W. Maalej; M. P. Robillard","University of Hamburg, Germany; McGill University, Montréal","IEEE Transactions on Software Engineering","","2013","39","9","1264","1282","Reading reference documentation is an important part of programming with application programming interfaces (APIs). Reference documentation complements the API by providing information not obvious from the API syntax. To improve the quality of reference documentation and the efficiency with which the relevant information it contains can be accessed, we must first understand its content. We report on a study of the nature and organization of knowledge contained in the reference documentation of the hundreds of APIs provided as a part of two major technology platforms: Java SDK 6 and .NET 4.0. Our study involved the development of a taxonomy of knowledge types based on grounded methods and independent empirical validation. Seventeen trained coders used the taxonomy to rate a total of 5,574 randomly sampled documentation units to assess the knowledge they contain. Our results provide a comprehensive perspective on the patterns of knowledge in API documentation: observations about the types of knowledge it contains and how this knowledge is distributed throughout the documentation. The taxonomy and patterns of knowledge we present in this paper can be used to help practitioners evaluate the content of their API documentation, better organize their documentation, and limit the amount of low-value content. They also provide a vocabulary that can help structure and facilitate discussions about the content of APIs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6473801","API documentation;software documentation;empirical study;content analysis;grounded method;data mining;pattern mining;Java;.NET","Documentation;Taxonomy;Encoding;Reliability;Java;Software;Sociology","application program interfaces;learning (artificial intelligence);pattern classification","knowledge taxonomy;.NET 4.0 API;Java SDK 6 API;knowledge organization;knowledge nature;reference documentation efficiency;reference documentation quality;application program interface;API reference documentation;knowledge pattern","","32","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Reverse Engineering Input Syntactic Structure from Program Execution and Its Applications","Z. Lin; X. Zhang; D. Xu","Purdue University, West Lafayette; Purdue University, West Lafayette; Purdue University, West Lafayette","IEEE Transactions on Software Engineering","","2010","36","5","688","703","Program input syntactic structure is essential for a wide range of applications such as test case generation, software debugging, and network security. However, such important information is often not available (e.g., most malware programs make use of secret protocols to communicate) or not directly usable by machines (e.g., many programs specify their inputs in plain text or other random formats). Furthermore, many programs claim they accept inputs with a published format, but their implementations actually support a subset or a variant. Based on the observations that input structure is manifested by the way input symbols are used during execution and most programs take input with top-down or bottom-up grammars, we devise two dynamic analyses, one for each grammar category. Our evaluation on a set of real-world programs shows that our technique is able to precisely reverse engineer input syntactic structure from execution. We apply our technique to hierarchical delta debugging (HDD) and network protocol reverse engineering. Our technique enables the complete automation of HDD, in which programmers were originally required to provide input grammars, and improves the runtime performance of HDD. Our client study on network protocol reverse engineering also shows that our technique supersedes existing techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5210120","Input syntactic structure;reverse engineering;control dependence;grammar inference;delta debugging;top-down grammar;bottom-up grammar.","Reverse engineering;Application software;Protocols;Computer science;Software debugging;Information security;Runtime;XML;Software testing;Automation","data structures;grammars;program debugging;protocols;reverse engineering","program input syntactic structure;test case generation;software debugging;network security;bottom-up grammars;top-down grammars;hierarchical delta debugging;network protocol reverse engineering;HDD automation","","7","","36","","","","","","IEEE","IEEE Journals & Magazines"
"The Effect of Pairs in Program Design Tasks","K. M. Lui; K. C. C. Chan; J. Nosek","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","197","211","Pair programming involves-two developers simultaneously collaborating with each other on the same programming task to design and code a solution. Algorithm design and its implementation are normally interwoven in that implementation often provides feedback to enhance the design. Previous controlled pair programming experiments did not explore the efficacy of pairs versus individuals in program design-related tasks separately from coding. Variations in programmer skills in a particular language or an integrated development environment and the understanding of programming instructions can mask the skill of subjects in program design-related tasks. Programming aptitude tests (PATs) have been shown to correlate with programming performance. PATs do not require understanding of programming instructions and do not require a skill in any specific computer language. Two controlled experiments were conducted, with full-time professional programmers being the subjects who worked on increasingly complex programming aptitude tasks related to problem solving and algorithmic design. In both experiments, pairs significantly outperformed individuals, providing evidence of the value of pairs in program design-related tasks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70755","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4378344","Experimental design;Programming teams;Experimental design;Programming teams","Programming profession;Algorithm design and analysis;Dynamic programming;Testing;Collaborative software;Collaborative work;Switches;Productivity;Time measurement;Collaboration","programming;reverse engineering","pair programming;program design task;integrated development environment;programming aptitude test;program understanding","","24","","62","","","","","","IEEE","IEEE Journals & Magazines"
"On the Evolution of Services","V. Andrikopoulos; S. Benbernou; M. P. Papazoglou","IAAS, University of Stuttgart, Stuttgart; Universit&#x0E9; Paris Descartes, Paris; ERISS, Tilburg University, Tilburg","IEEE Transactions on Software Engineering","","2012","38","3","609","628","In an environment of constant change and variation driven by competition and innovation, a software service can rarely remain stable. Being able to manage and control the evolution of services is therefore an important goal for the Service-Oriented paradigm. This work extends existing and widely adopted theories from software engineering, programming languages, service-oriented computing, and other related fields to provide the fundamental ingredients required to guarantee that spurious results and inconsistencies that may occur due to uncontrolled service changes are avoided. The paper provides a unifying theoretical framework for controlling the evolution of services that deals with structural, behavioral, and QoS level-induced service changes in a type-safe manner, ensuring correct versioning transitions so that previous clients can use a versioned service in a consistent manner.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728828","Services engineering;service evolution;versioning;service compatibility.","XML;Guidelines;Protocols;Business;Availability;Quality of service;Software","service-oriented architecture;Web services","software service;service-oriented paradigm;software engineering;programming languages;service-oriented computing","","60","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Replicating and Re-Evaluating the Theory of Relative Defect-Proneness","M. D. Syer; M. Nagappan; B. Adams; A. E. Hassan","School of Computing, Queen’s University, Kingston, ON, Canada; School of Computing, Queen’s University, Kingston, ON, Canada; Genie Informatique et Genie Logiciel, Ecole Polytechnique de Montreal, Campus de l’Universite de Montreal; School of Computing, Queen’s University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2015","41","2","176","197","A good understanding of the factors impacting defects in software systems is essential for software practitioners, because it helps them prioritize quality improvement efforts (e.g., testing and code reviews). Defect prediction models are typically built using classification or regression analysis on product and/or process metrics collected at a single point in time (e.g., a release date). However, current defect prediction models only predict if a defect will occur, but not when, which makes the prioritization of software quality improvements efforts difficult. To address this problem, Koru et al. applied survival analysis techniques to a large number of software systems to study how size (i.e., lines of code) influences the probability that a source code module (e.g., class or file) will experience a defect at any given time. Given that 1) the work of Koru et al. has been instrumental to our understanding of the size-defect relationship, 2) the use of survival analysis in the context of defect modelling has not been well studied and 3) replication studies are an important component of balanced scholarly debate, we present a replication study of the work by Koru et al. In particular, we present the details necessary to use survival analysis in the context of defect modelling (such details were missing from the original paper by Koru et al.). We also explore how differences between the traditional domains of survival analysis (i.e., medicine and epidemiology) and defect modelling impact our understanding of the size-defect relationship. Practitioners and researchers considering the use of survival analysis should be aware of the implications of our findings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2361131","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6914599","Survival Analysis;Cox Models;Defect Modelling;Survival analysis;Cox models;defect modelling","Analytical models;Hazards;Software;Measurement;Data models;Mathematical model;Predictive models","program diagnostics;software quality;software reliability","relative defect-proneness theory;survival analysis techniques;source code module;size-defect relationship;defect modelling;software system defects","","3","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluation and Measurement of Software Process Improvement—A Systematic Literature Review","M. Unterkalmsteiner; T. Gorschek; A. K. M. M. Islam; C. K. Cheng; R. B. Permadi; R. Feldt","Blekinge Institute of Technology, Karlskrona; Blekinge Institute of Technology, Karlskrona; University of Kaiserslautern, Kaiserslautern; General Electrics Healthcare, Freiburg; Amadeus S.A.S, Sophia Antipolis; Blekinge Institute of Technology, Karlskrona","IEEE Transactions on Software Engineering","","2012","38","2","398","424","BACKGROUND-Software Process Improvement (SPI) is a systematic approach to increase the efficiency and effectiveness of a software development organization and to enhance software products. OBJECTIVE-This paper aims to identify and characterize evaluation strategies and measurements used to assess the impact of different SPI initiatives. METHOD-The systematic literature review includes 148 papers published between 1991 and 2008. The selected papers were classified according to SPI initiative, applied evaluation strategies, and measurement perspectives. Potential confounding factors interfering with the evaluation of the improvement effort were assessed. RESULTS-Seven distinct evaluation strategies were identified, wherein the most common one, “Pre-Post Comparison,” was applied in 49 percent of the inspected papers. Quality was the most measured attribute (62 percent), followed by Cost (41 percent), and Schedule (18 percent). Looking at measurement perspectives, “Project” represents the majority with 66 percent. CONCLUSION-The evaluation validity of SPI initiatives is challenged by the scarce consideration of potential confounding factors, particularly given that “Pre-Post Comparison” was identified as the most common evaluation strategy, and the inaccurate descriptions of the evaluation context. Measurements to assess the short and mid-term impact of SPI initiatives prevail, whereas long-term measurements in terms of customer satisfaction and return on investment tend to be less used.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728832","Process implementation and change;process measurement;metrics/measurement;systematic literature review.","Software;Software measurement;Systematics;Current measurement;Data mining;Organizations","software process improvement","software process improvement;SPI;software development organization;customer satisfaction;return on investment","","77","","266","","","","","","IEEE","IEEE Journals & Magazines"
"A scenario-matching approach to the description and model checking of real-time properties","V. Braberman; N. Kicillof; A. Olivero","Comput. Sci. Dept., Buenos Aires Univ., Argentina; Comput. Sci. Dept., Buenos Aires Univ., Argentina; NA","IEEE Transactions on Software Engineering","","2005","31","12","1028","1041","A major obstacle in the technology transfer agenda of behavioral analysis and design methods is the need for logics or automata to express properties for control-intensive systems. Interaction-modeling notations may offer a replacement or a complement, with a practitioner-appealing and lightweight flavor, due partly to the sub specification of intended behavior by means of scenarios. We propose a novel approach consisting of engineering a new formal notation of this sort based on a simple compact declarative semantics: VTS (visual timed event scenarios). Scenarios represent event patterns, graphically depicting conditions over traces. They predicate general system events and provide features to describe complex properties not expressible with MSC-like notations. The underlying formalism supports partial orders and real-time constraints. The problem of checking whether a timed-automaton model has a matching trace is proven decidable. On top of this kernel, we introduce a notation to state properties over all system traces: conditional scenarios, allowing engineers to describe uniquely rich connections between antecedent and consequent portions of the scenario. An undecidability result is presented for the general case of the model-checking problem over dense-time domains, to later identify a decidable-yet practically relevant-subclass, where verification is solvable by generating antiscenarios expressed in the VTS-kernel notation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.131","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566605","Index Terms- Requirements/specifications;model checking;formal methods;scenario-based verification.","Real time systems;Design methodology;Logic design;Automata;Automatic control;Control system synthesis;Kernel;Computer industry;Electrical equipment industry;Control systems","formal verification;formal specification;automata theory;real-time systems","scenario-matching approach;model checking;behavioral analysis;design methods;control-intensive system;interaction-modeling;visual timed event scenarios;event patterns;real-time constraints;timed-automaton model;formal verification","","14","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Where Do Configuration Constraints Stem From? An Extraction Approach and an Empirical Study","S. Nadi; T. Berger; C. Kästner; K. Czarnecki","Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Hessen, Germany; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON","IEEE Transactions on Software Engineering","","2015","41","8","820","841","Highly configurable systems allow users to tailor software to specific needs. Valid combinations of configuration options are often restricted by intricate constraints. Describing options and constraints in a variability model allows reasoning about the supported configurations. To automate creating and verifying such models, we need to identify the origin of such constraints. We propose a static analysis approach, based on two rules, to extract configuration constraints from code. We apply it on four highly configurable systems to evaluate the accuracy of our approach and to determine which constraints are recoverable from the code. We find that our approach is highly accurate (93% and 77% respectively) and that we can recover 28% of existing constraints. We complement our approach with a qualitative study to identify constraint sources, triangulating results from our automatic extraction, manual inspections, and interviews with 27 developers. We find that, apart from low-level implementation dependencies, configuration constraints enforce correct runtime behavior, improve users' configuration experience, and prevent corner cases. While the majority of constraints is extractable from code, our results indicate that creating a complete model requires further substantial domain knowledge and testing. Our results aim at supporting researchers and practitioners working on variability model engineering, evolution, and verification techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2415793","NSERC; ARTEMIS JU; NSF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7065312","Variability models;Reverse-engineering;qualitative studies;Variability models;reverse-engineering;qualitative studies;static analyses;configuration constraints","Feature extraction;Kernel;Accuracy;Linux;Manuals;Interviews","configuration management;program diagnostics","configuration constraints;extraction approach;configuration combination;variability model;static analysis approach;configuration constraints extraction;constraint sources identification;variability model engineering;variability model evolution;variability model verification techniques","","13","","80","","","","","","IEEE","IEEE Journals & Magazines"
"Testing homogeneous spreadsheet grids with the ""what you see is what you test"" methodology","M. Burnett; A. Sheretov; Bing Ren; G. Rothermel","Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","6","576","594","Although there has been recent research into ways to design environments that enable end users to create their own programs, little attention has been given to helping these end users systematically test their programs. To help address this need in spreadsheet systems (the most widely used type of end-user programming language), we previously introduced a visual approach to systematically testing individual cells in spreadsheet systems. However, the previous approach did not scale well in the presence of largely homogeneous grids, which introduce problems somewhat analogous to the array-testing problems of imperative programs. We present two approaches to spreadsheet testing that explicitly support such grids. We present the algorithms, time complexities, and performance data comparing the two approaches. This is part of our continuing work to bring to end users at least some of the benefits of formalized notions of testing without requiring knowledge of testing beyond a naive level.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010060","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010060","","System testing;Computer Society;Computer languages;Data visualization;Graphical user interfaces;Standards development;Error analysis;Proposals;Programming;Spreadsheet programs","spreadsheet programs;program testing;visual programming","homogeneous spreadsheet grid testing;design environments;end users;spreadsheet systems;end-user programming language;visual approach;homogeneous grids;array-testing problems;imperative programs;spreadsheet testing;time complexities;performance data;formalized notions;software testing;visual programming","","24","","32","","","","","","IEEE","IEEE Journals & Magazines"
"HYDRA: Massively Compositional Model for Cross-Project Defect Prediction","X. Xia; D. Lo; S. J. Pan; N. Nagappan; X. Wang","College of Computer Science and Technology, Zhejiang University Hangzhou, Zhejiang, China; School of Information Systems, Singapore Management University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; Testing, Verification and Measurement Research, Microsoft Research, Redmond, WA; College of Computer Science and Technology, Zhejiang University Hangzhou, Zhejiang, China","IEEE Transactions on Software Engineering","","2016","42","10","977","998","Most software defect prediction approaches are trained and applied on data from the same project. However, often a new project does not have enough training data. Cross-project defect prediction, which uses data from other projects to predict defects in a particular project, provides a new perspective to defect prediction. In this work, we propose a HYbrid moDel Reconstruction Approach (HYDRA) for cross-project defect prediction, which includes two phases: genetic algorithm (GA) phase and ensemble learning (EL) phase. These two phases create a massive composition of classifiers. To examine the benefits of HYDRA, we perform experiments on 29 datasets from the PROMISE repository which contains a total of 11,196 instances (i.e., Java classes) labeled as defective or clean. We experiment with logistic regression as the underlying classification algorithm of HYDRA. We compare our approach with the most recently proposed cross-project defect prediction approaches: TCA+ by Nam et al., Peters filter by Peters et al., GP by Liu et al., MO by Canfora et al., and CODEP by Panichella et al. Our results show that HYDRA achieves an average F1-score of 0.544. On average, across the 29 datasets, these results correspond to an improvement in the F1-scores of 26.22 , 34.99, 47.43, 28.61, and 30.14 percent over TCA+, Peters filter, GP, MO, and CODEP, respectively. In addition, HYDRA on average can discover 33 percent of all bugs if developers inspect the top 20 percent lines of code, which improves the best baseline approach (TCA+) by 44.41 percent. We also find that HYDRA improves the F1-score of Zero-R which predict all the instances to be defective by 5.42 percent, but improves Zero-R by 58.65 percent when inspecting the top 20 percent lines of code. In practice, Zero-R can be hard to use since it simply predicts all of the instances to be defective, and thus developers have to inspect all of the instances to find the defective ones. Moreover, we notice the improvement of HYDRA over other baseline approaches in terms of F1-score and when inspecting the top 20 percent lines of code are substantial, and in most cases the improvements are significant and have large effect sizes across the 29 datasets.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2543218","National Basic Research Program of China; NSFC; National Key Technology R&D Program; Ministry of Science and Technology of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7435328","Cross-project defect prediction;transfer learning;genetic algorithm;ensemble learning","Genetic algorithms;Predictive models;Training;Buildings;Architecture;Data models;Measurement","genetic algorithms;learning (artificial intelligence);pattern classification;regression analysis;software fault tolerance","HYDRA model;massively compositional model;cross-project defect prediction;software defect prediction approach;hybrid model reconstruction approach;genetic algorithm phase;GA phase;phase and ensemble learning phase;EL phase;PROMISE repository;logistic regression;classification algorithm","","44","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Advanced exception handling mechanisms","P. A. Buhr; W. Y. R. Mok","Dept. of Comput. Sci., Waterloo Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","2000","26","9","820","836","It is no longer possible to consider exception handling as a secondary issue in language design, or even worse, a mechanism added after the fact via a library approach. Exception handling is a primary feature in language design and must be integrated with other major features, including advanced control flow, objects, coroutines, concurrency, real-time, and polymorphism. Integration is crucial as there are both obvious and subtle interactions between exception handling and other language features. Unfortunately, many exception handling mechanisms work only with a subset of the features and in the sequential domain. A framework for a comprehensive, easy to use, and extensible exception handling mechanism is presented for a concurrent, object-oriented environment. The environment includes language constructs with separate execution stacks, e.g. coroutines and tasks, so the exception environment is significantly more complex than the normal single-stack situation. The pros and cons of various exception features are examined, along with feature interaction with other language mechanisms. Both exception termination and resumption models are examined in this environment, and previous criticisms of the resumption model, a feature commonly missing in modern languages, are addressed.","0098-5589;1939-3520;2326-3881","","10.1109/32.877844","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877844","","Testing;Object oriented modeling;Libraries;Concurrent computing;Robustness;Process control;Writing;Robust control;Programming profession","exception handling;object-oriented programming;high level languages;multiprocessing programs","advanced exception handling mechanisms;language design;advanced control flow;objects;coroutines;concurrency;real-time systems;polymorphism;concurrent object-oriented environment;language constructs;execution stacks;feature interaction;exception termination model;exception resumption model","","24","","","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic detection and exploitation of branch constraints for timing analysis","C. A. Healy; D. B. Whalley","Comput. Sci. Dept., Furman Univ., Greenville, SC, USA; NA","IEEE Transactions on Software Engineering","","2002","28","8","763","781","Predicting the worst-case execution time (WCET) and best-case execution time (BCET) of a real-time program is a challenging task. Though much progress has been made in obtaining tighter timing predictions by using techniques that model the architectural features of a machine, significant overestimations of WCET and underestimations of GCET can still occur. Even with perfect architectural modeling, dependencies on data values can constrain the outcome of conditional branches and the corresponding set of paths that can be taken in a program. While branch constraint information has been used in the past by some timing analyzers, it has typically been specified manually, which is both tedious and error prone. This paper describes efficient techniques for automatically detecting branch constraints by a compiler and automatically exploiting these constraints within a timing analyzer. The result is significantly tighter timing analysis predictions without requiring additional interaction with a user.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027799","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027799","","Timing;Automatic control;Performance analysis;Information analysis;Real time systems;Computer Society;Predictive models;Time measurement;Programming profession;Pipelines","timing;real-time systems;program testing;program compilers","branch constraints;timing analysis;worst-case execution time;best-case execution time;real-time program;architectural features;branch constraint information;compiler;real-time systems","","19","","25","","","","","","IEEE","IEEE Journals & Magazines"
"Automated aspect-oriented decomposition of process-control systems for ultra-high dependability assurance","D. Wang; F. B. Bastani; L. -. Yen","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA","IEEE Transactions on Software Engineering","","2005","31","9","713","732","This paper presents a method for decomposing process-control systems. This decomposition method is automated, meaning that a series of principles that can be evolved to support automated tools are given to help a designer decompose complex systems into a collection of simpler components. Each component resulting from the decomposition process can be designed and implemented independently of the other components. Also, these components can be tested or verified by the end-user independently of each other. Moreover, the system properties, such as safety, stability, and reliability, can be mathematically inferred from the properties of the individual components. These components are referred to as IDEAL (independently developable end-user assessable logical) components. This decomposition method is applied to a case study specified by the High-Integrity Systems group at Sandia National Labs, which involves the control of a future version of the Bay Area Rapid Transit (BART) system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.99","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514442","Index Terms- Software decomposition;dependability assurance;process-control systems;aspect-oriented modeling.","Software safety;Testing;State-space methods;Protocols;Medical control systems;Control systems;Power system modeling;Application software;Robust stability;Software quality","object-oriented programming;safety-critical software;formal specification;formal verification","aspect-oriented decomposition;process-control systems;ultra-high dependability assurance;formal specification;formal verification;independently developable end-user assessable logical component;software decomposition;aspect-oriented modeling","","","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Compositional schedulability analysis of real-time systems using time Petri nets","Dianxiang Xu; Xudong He; Yi Deng","Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","10","984","996","This paper presents an approach to the schedulability analysis of real-time systems modeled in time Petri nets by separating timing properties from other behavioral properties. The analysis of behavioral properties is conducted based on the reachability graph of the underlying Petri net, whereas timing constraints are checked in terms of absolute and relative firing domains. If a specific task execution is schedulable, we calculate the time span of the task execution, and pinpoint nonschedulable transitions to help adjust timing constraints. A technique for compositional timing analysis is also proposed to deal with complex task sequences, which not only improves efficiency but also facilitates the discussion of the reachability issue with regard to schedulability. We identified a class of well-structured time Petri nets such that their reachability can be easily analyzed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041054","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041054","","Real time systems;Petri nets;Timing;Reachability analysis;Processor scheduling;Computer Society;Delay effects;Monitoring;Logic","real-time systems;Petri nets;reachability analysis;flexible manufacturing systems;computer aided production planning;graph theory;production control","real-time systems;time Petri nets;schedulability;reachability graph;time span;compositional timing analysis;production control;flexible manufacturing systems;timing constraints","","37","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Optimum control limits for employing statistical process control in software process","P. Jalote; A. Saxena","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Kanpur, India; NA","IEEE Transactions on Software Engineering","","2002","28","12","1126","1134","There is increasing interest in using control charts for monitoring and improving software processes, particularly quality control processes like reviews and testing. In a control chart, control limits are established for attributes and, if any point falls outside the limits, it is assumed to be due to special causes that need to be identified and eliminated. If the control limits are too tight, they may raise too many ""false alarms"" and, if they are too wide, they may miss special situations. Optimal control limits will try to minimize the cost of these errors. In this paper, we develop a cost model for employing control charts for software processes using optimum control limits which can be determined. Our applications of the model suggest that, for quality control processes like inspection, optimum control limits may be tighter than those commonly used in manufacturing. We have also implemented this model as a Web service that can be used for determining optimum control limits.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158286","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158286","","Process control;Control charts;Quality control;Cost function;Monitoring;Software quality;Software testing;Optimal control;Error correction;Application software","statistical process control;software metrics;software process improvement;software quality","optimum control limits;statistical process control;control charts;software process monitoring;software process improvement;quality control processes;reviews;testing;cost model;inspection process;Web service","","34","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs","J. Xuan; M. Martinez; F. DeMarco; M. Clément; S. L. Marcote; T. Durieux; D. Le Berre; M. Monperrus","State Key Lab of Software Engineering, School of Computer, Wuhan University, Wuhan, China; Faculty of Informatics, University of Lugano, Lugano, Switzerland; University of Buenos Aires, Buenos Aires, Argentina; Department of Computer Science, University of Lille, Lille, France; University of Buenos Aires, Buenos Aires, Argentina; Department of Computer Science, University of Lille, Lille, France; University of Artois & CNRS, Lens, France; University of Lille & INRIA, Lille, France","IEEE Transactions on Software Engineering","","2017","43","1","34","55","We propose Nopol, an approach to automatic repair of buggy conditional statements (i.e., if-then-else statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of Nopol consists of three major phases. First, Nopol employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, Nopol encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem; then a feasible solution to the SMT instance is translated back into a code patch. We evaluate Nopol on 22 real-world bugs (16 bugs with buggy if conditions and six bugs with missing preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with buggy if conditions and missing preconditions. We illustrate the capabilities and limitations of Nopol using case studies of real bug fixes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2560811","INRIA Internship program; INRIA postdoctoral research fellowship; CNRS delegation program; National Natural Science Foundation of China; Young Talent Development Program of the China Computer Federation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7463060","Automatic repair;patch generation;SMT;fault localization","Maintenance engineering;Computer bugs;Runtime;Java;Encoding;Open source software;Indexes","computability;Java;object-oriented programming;program debugging;public domain software;software fault tolerance;software maintenance","Nopol;automatic conditional statement bug repairing;Java programs;buggy program;conditional expression;angelic fix localization;test execution;runtime trace collection;objected-oriented features;patch generation;satisfiability modulo theory problem;SMT problem;code patch;buggy IF conditions;open-source projects;Apache Commons Math;Apache Commons Lang","","36","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical studies of a prediction model for regression test selection","M. J. Harrold; D. Rosenblum; G. Rothermel; E. Weyuker","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","248","263","Regression testing is an important activity that can account for a large proportion of the cost of software maintenance. One approach to reducing the cost of regression testing is to employ a selective regression testing technique that: chooses a subset of a test suite that was used to test the software before the modifications; then uses this subset to test the modified software. Selective regression testing techniques reduce the cost of regression testing if the cost of selecting the subset from the test suite together with the cost of running the selected subset of test cases is less than the cost of rerunning the entire test suite. Rosenblum and Weyuker (1997) proposed coverage-based predictors for use in predicting the effectiveness of regression test selection strategies. Using the regression testing cost model of Leung and White (1989; 1990), Rosenblum and Weyuker demonstrated the applicability of these predictors by performing a case study involving 31 versions of the KornShell. To further investigate the applicability of the Rosenblum-Weyuker (RW) predictor, additional empirical studies have been performed. The RW predictor was applied to a number of subjects, using two different selective regression testing tools, Deja vu and TestTube. These studies support two conclusions. First, they show that there is some variability in the success with which the predictors work and second, they suggest that these results can be improved by incorporating information about the distribution of modifications. It is shown how the RW prediction model can be improved to provide such an accounting.","0098-5589;1939-3520;2326-3881","","10.1109/32.910860","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910860","","Predictive models;Software testing;Costs;Performance evaluation;Computer Society;Software maintenance;Software performance;Software quality","program testing;software maintenance;statistical analysis","prediction model;regression test selection;software maintenance;regression testing;test suite;coverage-based predictors;KornShell;Deja vu;TestTube","","35","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Is it possible to decorate graphical software design and architecture models with qualitative Information?-An experiment","L. Bratthall; C. Wohlin","Corporate Res. Dept., ABB AS, Billingstad, Norway; NA","IEEE Transactions on Software Engineering","","2002","28","12","1181","1193","Software systems evolve over time and it is often difficult to maintain them. One reason for this is that often it is hard to understand the previous release. Further, even if architecture and design models are available and up to date, they primarily represent the functional behavior of the system. To evaluate whether it is possible to also represent some nonfunctional aspects, an experiment has been conducted. The objective of the experiment is to evaluate the cognitive suitability of some visual representations that can be used to represent a control relation, software component size and component external and internal complexity. Ten different representations are evaluated in a controlled environment using 35 subjects. The results from the experiment show that representations with low cognitive accessibility weight can be found. In an example, these representations are used to illustrate some qualities in an SDL block diagram. It is concluded that the incorporation of these representations in architecture and design descriptions is both easy and probably worthwhile. The incorporation of the representations should enhance the understanding of previous releases and, hence, help software developers in evolving and maintaining complex software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158290","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158290","","Software design;Computer architecture;Software maintenance;Software systems;Software quality;Software testing;Size control;Software architecture;Unified modeling language;Fault diagnosis","specification languages;software architecture;software quality;software maintenance","graphical software design;architecture models;qualitative information;software systems;software component size;SDL block diagram;software evolution;software maintenance;software quality representation","","13","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Data Mining Techniques for Software Effort Estimation: A Comparative Study","K. Dejaeger; W. Verbeke; D. Martens; B. Baesens","Katholieke Universiteit Leuven, Leuven; Katholieke Universiteit Leuven, Leuven; University of Antwerp, Antwerp; Katholieke Universiteit Leuven, Leuven and University of Southampton, Highfield Southampton","IEEE Transactions on Software Engineering","","2012","38","2","375","397","A predictive model is required to be accurate and comprehensible in order to inspire confidence in a business setting. Both aspects have been assessed in a software effort estimation setting by previous studies. However, no univocal conclusion as to which technique is the most suited has been reached. This study addresses this issue by reporting on the results of a large scale benchmarking study. Different types of techniques are under consideration, including techniques inducing tree/rule-based models like M5 and CART, linear models such as various types of linear regression, nonlinear models (MARS, multilayered perceptron neural networks, radial basis function networks, and least squares support vector machines), and estimation techniques that do not explicitly induce a model (e.g., a case-based reasoning approach). Furthermore, the aspect of feature subset selection by using a generic backward input selection wrapper is investigated. The results are subjected to rigorous statistical testing and indicate that ordinary least squares regression in combination with a logarithmic transformation performs best. Another key finding is that by selecting a subset of highly predictive attributes such as project size, development, and environment related attributes, typically a significant increase in estimation accuracy can be obtained.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928350","Data mining;software effort estimation;regression.","Software;Estimation;Data models;Data mining;Cognition;Artificial neural networks;Regression tree analysis","data mining;program testing;regression analysis;software cost estimation","data mining techniques;software effort estimation;predictive model;rule-based models;CART;M5;linear regression;nonlinear models;estimation techniques;feature subset selection;generic backward input selection wrapper;rigorous statistical testing;ordinary least squares regression;logarithmic transformation","","81","","108","","","","","","IEEE","IEEE Journals & Magazines"
"Aspect-Oriented Race Detection in Java","E. Bodden; K. Havelund","Technical University Darmstadt, Darmstadt; California Institute of Technology, Pasadena","IEEE Transactions on Software Engineering","","2010","36","4","509","527","In the past, researchers have developed specialized programs to aid programmers in detecting concurrent programming errors such as deadlocks, livelocks, starvation, and data races. In this work, we propose a language extension to the aspect-oriented programming language AspectJ, in the form of three new pointcuts, lock(), unlock(), and maybeShared(). These pointcuts allow programmers to monitor program events where locks are granted or handed back, and where values are accessed that may be shared among multiple Java threads. We decide thread locality using a static thread-local-objects analysis developed by others. Using the three new primitive pointcuts, researchers can directly implement efficient monitoring algorithms to detect concurrent-programming errors online. As an example, we describe a new algorithm which we call RACER, an adaption of the well-known ERASER algorithm to the memory model of Java. We implemented the new pointcuts as an extension to the AspectBench Compiler, implemented the RACER algorithm using this language extension, and then applied the algorithm to the NASA K9 Rover Executive and two smaller programs. Our experiments demonstrate that our implementation is effective in finding subtle data races. In the Rover Executive, RACER finds 12 data races, with no false warnings. Only one of these races was previously known.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5406531","Race detection;runtime verification;aspect-oriented programming;semantic pointcuts;static analysis.","Java;Programming profession;Computer languages;System recovery;Monitoring;Runtime;Protection;Instruments;Libraries","aspect-oriented programming;concurrency control;Java;multi-threading;program compilers;program debugging","aspect-oriented race detection;concurrent programming error detection;aspect-oriented programming language;AspectJ;multiple Java threads;static thread-local-objects analysis;ERASER algorithm;primitive pointcuts;AspectBench compiler;RACER algorithm;NASA K9 Rover Executive","","13","","52","","","","","","IEEE","IEEE Journals & Magazines"
"To Be Optimal or Not in Test-Case Prioritization","D. Hao; L. Zhang; L. Zang; Y. Wang; X. Wu; T. Xie","Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Department of Computer Science, University of Illinois at Urbana-Champaign","IEEE Transactions on Software Engineering","","2016","42","5","490","505","Software testing aims to assure the quality of software under test. To improve the efficiency of software testing, especially regression testing, test-case prioritization is proposed to schedule the execution order of test cases in software testing. Among various test-case prioritization techniques, the simple additional coverage-based technique, which is a greedy strategy, achieves surprisingly competitive empirical results. To investigate how much difference there is between the order produced by the additional technique and the optimal order in terms of coverage, we conduct a study on various empirical properties of optimal coverage-based test-case prioritization. To enable us to achieve the optimal order in acceptable time for our object programs, we formulate optimal coverage-based test-case prioritization as an integer linear programming (ILP) problem. Then we conduct an empirical study for comparing the optimal technique with the simple additional coverage-based technique. From this empirical study, the optimal technique can only slightly outperform the additional coverage-based technique with no statistically significant difference in terms of coverage, and the latter significantly outperforms the former in terms of either fault detection or execution time. As the optimal technique schedules the execution order of test cases based on their structural coverage rather than detected faults, we further implement the ideal optimal test-case prioritization technique, which schedules the execution order of test cases based on their detected faults. Taking this ideal technique as the upper bound of test-case prioritization, we conduct another empirical study for comparing the optimal technique and the simple additional technique with this ideal technique. From this empirical study, both the optimal technique and the additional technique significantly outperform the ideal technique in terms of coverage, but the latter significantly outperforms the former two techniques in terms of fault detection. Our findings indicate that researchers may need take cautions in pursuing the optimal techniques in test-case prioritization with intermediate goals.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2496939","National 973 Program of China; National Natural Science Foundation of China; National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7314957","Test-Case Prioritization;Integer Linear Programming;Greedy Algorithm;Empirical Study;Test-case prioritization;integer linear programming;greedy algorithm;empirical study","Software;Measurement;Schedules;Fault detection;Integer linear programming;Software testing","integer programming;linear programming;program testing","test-case prioritization techniques;software testing;software quality;regression testing;simple additional coverage-based technique;optimal coverage-based test-case prioritization;integer linear programming;ILP problem","","14","","47","","","","","","IEEE","IEEE Journals & Magazines"
"EMERALDS: a small-memory real-time microkernel","K. M. Zuberi; K. G. Shin","Microsoft Corp., Redmond, WA, USA; NA","IEEE Transactions on Software Engineering","","2001","27","10","909","928","EMERALDS (Extensible Microkernel for Embedded, ReAL-time, Distributed Systems) is a real-time microkernel designed for small-memory embedded applications. These applications must run on slow (15-25 MHz) processors with just 32-128 kbytes of memory, either to keep production costs down in mass produced systems or to keep weight and power consumption low. To be feasible for such applications, the OS must not only be small in size (less than 20 kbytes), but also have low overhead kernel services. Unlike commercial embedded OSs which rely on carefully optimized code to achieve efficiency, EMERALDS takes the approach of redesigning the basic OS services of task scheduling, synchronization, communication, and system call mechanism by using characteristics found in small-memory embedded systems, such as small code size and a priori knowledge of task execution and communication patterns. With these new schemes, the overheads of various OS services are reduced 20-40 percent without compromising any OS functionality.","0098-5589;1939-3520;2326-3881","","10.1109/32.962561","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=962561","","Application software;Real time systems;Hardware;Costs;Energy consumption;Embedded system;Control systems;Automotive engineering;Engines;Production systems","real-time systems;operating system kernels;network operating systems;scheduling;synchronisation","EMERALDS;real time distributed systems;real-time microkernel;small-memory embedded applications;slow processors;power consumption;real-time operating systems;real-time scheduling;task synchronization;system call mechanism;small-memory embedded systems;32 to 128 kbyte","","11","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Domain-Specific Service Selection for Composite Services","O. Moser; F. Rosenberg; S. Dustdar","Vienna University of Technology, Vienna; IBM T.J. Watson Research Center, Hawthorne; Vienna University of Technology, Vienna","IEEE Transactions on Software Engineering","","2012","38","4","828","843","We propose a domain-specific service selection mechanism and system implementation to address the issue of runtime adaptation of composite services that implement mission-critical business processes. To this end, we leverage quality of service (QoS) as a means to specify rigid dependability requirements. QoS does not include only common attributes such as availability or response time but also attributes specific to certain business domains and processes. Therefore, we combine both domain-agnostic and domain-specific QoS attributes in an adaptive QoS model. For specifying the service selection strategy, we propose a domain-specific language called VieDASSL to specify so-called selectors. This language can be used to specify selector implementations based on the available QoS attributes. Both the QoS model implementation and the selectors can be adapted at runtime to deal with changing business and QoS requirements. Our approach is implemented on top of an existing WS-BPEL engine. We demonstrate its feasibility by implementing a case study from the telecommunication domain.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6231591","Service composition;quality of service;monitoring;service selection;domain specific languages","Quality of service;Runtime;Business;Adaptation models;Time factors;Availability;Engines","business data processing;quality of service;reliability;specification languages;Web services","domain-specific service selection mechanism;composite services;runtime adaptation;mission-critical business processes;quality of service;domain-agnostic QoS attributes;domain-specific QoS attributes;adaptive QoS model;domain-specific language;VieDASSL;selectors;business requirements;QoS requirements;WS-BPEL engine;telecommunication;Web services","","18","","56","","","","","","IEEE","IEEE Journals & Magazines"
"CCFinder: a multilinguistic token-based code clone detection system for large scale source code","T. Kamiya; S. Kusumoto; K. Inoue","Graduate Sch. of Eng. Sci., Osaka Univ., Japan; NA; NA","IEEE Transactions on Software Engineering","","2002","28","7","654","670","A code clone is a code portion in source files that is identical or similar to another. Since code clones are believed to reduce the maintainability of software, several code clone detection techniques and tools have been proposed. This paper proposes a new clone detection technique, which consists of the transformation of input source text and a token-by-token comparison. For its implementation with several useful optimization techniques, we have developed a tool, named CCFinder (Code Clone Finder), which extracts code clones in C, C++, Java, COBOL and other source files. In addition, metrics for the code clones have been developed. In order to evaluate the usefulness of CCFinder and metrics, we conducted several case studies where we applied the new tool to the source code of JDK, FreeBSD, NetBSD, Linux, and many other systems. As a result, CCFinder has effectively found clones and the metrics have been able to effectively identify the characteristics of the systems. In addition, we have compared the proposed technique with other clone detection techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019480","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019480","","Cloning;Large-scale systems;Software systems;Maintenance engineering;Software maintenance;Software tools;Java;Linux;Computer aided software engineering;Programming profession","high level languages;large-scale systems;software maintenance;software metrics;computer aided software engineering;software tools;optimising compilers","CCFinder;multi-linguistic token-based code clone detection system;large-scale source code;software maintainability;input source text transformation;token-by-token comparison;optimization techniques;C language;C++ language;Java;COBOL;software metrics;case studies;JDK;Java Development Kit;FreeBSD;NetBSD;Linux;system characteristics identification;duplicated code;CASE tool","","583","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Software Development in Startup Companies: The Greenfield Startup Model","C. Giardino; N. Paternoster; M. Unterkalmsteiner; T. Gorschek; P. Abrahamsson","Faculty of Computer Science, Free University of Bolzano/Bozen, Dominikanerplatz 3, Italy; Software Engineering Research Lab Sweden, Blekinge Institute of Technology, Campus Gräsvik, 371 79 Karlskrona, Sweden; Software Engineering Research Lab Sweden, Blekinge Institute of Technology, Campus Gräsvik, 371 79 Karlskrona, Sweden; Software Engineering Research Lab Sweden, Blekinge Institute of Technology, Campus Gräsvik, 371 79 Karlskrona, Sweden; Department of Computer and Information Science, Norwegian University of Science and Technology NTNU, Sem Saelandsvei 7-9, Trondheim, Norway","IEEE Transactions on Software Engineering","","2016","42","6","585","604","Software startups are newly created companies with no operating history and oriented towards producing cutting-edge products. However, despite the increasing importance of startups in the economy, few scientific studies attempt to address software engineering issues, especially for early-stage startups. If anything, startups need engineering practices of the same level or better than those of larger companies, as their time and resources are more scarce, and one failed project can put them out of business. In this study we aim to improve understanding of the software development strategies employed by startups. We performed this state-of-practice investigation using a grounded theory approach. We packaged the results in the Greenfield Startup Model (GSM), which explains the priority of startups to release the product as quickly as possible. This strategy allows startups to verify product and market fit, and to adjust the product trajectory according to early collected user feedback. The need to shorten time-to-market, by speeding up the development through low-precision engineering activities, is counterbalanced by the need to restructure the product before targeting further growth. The resulting implications of the GSM outline challenges and gaps, pointing out opportunities for future research to develop and validate engineering practices in the startup context.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2509970","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7360225","Software Development;Startups;Grounded Theory;Software development;startups;grounded theory","Software;Companies;GSM;Context;Software engineering;History","software development management","Greenfield startup model;software startups;software engineering issues;engineering practices;software development strategies;state-of-practice investigation;grounded theory approach;GSM;user feedback;time-to-market","","24","","119","","","","","","IEEE","IEEE Journals & Magazines"
"Bayesian Approaches to Matching Architectural Diagrams","D. Kimelman; M. Kimelman; D. Mandelin; D. Yellin","IBM Thomas J. Watson Research Center, Yorktown Heights; Independent Consultant; Mozilla Corporation, Mountain View; IBM Israel Software Lab, Jerusalem","IEEE Transactions on Software Engineering","","2010","36","2","248","274","IT system architectures and many other kinds of structured artifacts are often described by formal models or informal diagrams. In practice, there are often a number of versions of a model or diagram, such as a series of revisions, divergent variants, or multiple views of a system. Understanding how versions correspond or differ is crucial, and thus, automated assistance for matching models and diagrams is essential. We have designed a framework for finding these correspondences automatically based on Bayesian methods. We represent models and diagrams as graphs whose nodes have attributes such as name, type, connections to other nodes, and containment relations, and we have developed probabilistic models for rating the quality of candidate correspondences based on various features of the nodes in the graphs. Given the probabilistic models, we can find high-quality correspondences using search algorithms. Preliminary experiments focusing on architectural models suggest that the technique is promising.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5232811","Bayesian techniques;IT system architecture;modeling tools;change control.","Bayesian methods;Context modeling;Large-scale systems;Network topology;Centralized control;Merging;Collaboration;Adaptation model;Application software;Security","Bayes methods;configuration management;graphs;probability;software architecture","Bayesian methods;formal models;informal diagrams;IT system architectures;divergent variants;graphs;probabilistic models;architectural diagram matching;search algorithms","","3","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Multiphase stabilization","M. G. Gouda","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA","IEEE Transactions on Software Engineering","","2002","28","2","201","208","We generalize the concept of stabilization of computing systems. According to this generalization, the actions of a system S are partitioned into n partitions, called phase 1 through phase n. In this case, system S is said to be n-stabilizing to a state predicate Q iff S has state predicates P.0, ..., P.n such that P.0=true, P.n=Q, and the following two conditions hold for every j, 1/spl les/j/spl les/n. First, if S starts at a state satisfying P.(j-1) and if the only actions of S that are allowed to be executed are those of phase j or less, then S will reach a state satisfying P.j. Second, the set of states satisfying P.j is closed under any execution of the actions of phase j or less. By choosing n=1, this generalization degenerates to the traditional definition of stabilization. We discuss three advantages of this generalization over the traditional definition. First, this generalization captures many stabilization properties of systems that are traditionally considered nonstabilizing. Second, verifying stabilization when n>1 is usually easier than when n=1. Third, this generalization suggests a new method of fault recovery, called multiphase recovery.","0098-5589;1939-3520;2326-3881","","10.1109/32.988499","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988499","","Convergence;Fault tolerant systems;Tail","system recovery;stability","multiphase stabilization;computing systems;state predicate;multiphase recovery","","8","","13","","","","","","IEEE","IEEE Journals & Magazines"
"A vector-based approach to software size measurement and effort estimation","T. E. Hastings; A. S. M. Sajeev","Sch. of Comput. Sci. & Software Eng., Monash Univ., Caulfield East, Vic., Australia; NA","IEEE Transactions on Software Engineering","","2001","27","4","337","350","Software size is a fundamental product measure that can be used for assessment, prediction and improvement purposes. However, existing software size measures, such as function points, do not address the underlying problem complexity of software systems adequately. This can result in disproportional measures of software size for different types of systems. We propose a vector size measure (VSM) that incorporates both functionality and problem complexity in a balanced and orthogonal manner. The VSM is used as the input to a vector prediction model (VPM) which can be used to estimate development effort early in the software life-cycle. We theoretically validate the approach against a formal framework. We also empirically validate the approach with a pilot study. The results indicate that the approach provides a mechanism to measure the size of software systems, classify software systems, and estimate development effort early in the software life-cycle to within /spl plusmn/20% across a range of application types.","0098-5589;1939-3520;2326-3881","","10.1109/32.917523","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917523","","Size measurement;Software measurement;Software systems;Software quality;Predictive models;Life estimation;Application software;Testing;Software metrics;Programming","software cost estimation;software metrics;size measurement;algebraic specification;computational complexity;vectors","software size measurement;software development effort estimation;vector size measure;vector prediction model;problem complexity;functionality;software life-cycle;formal framework;pilot study;software systems classification;application types;algebraic specification;gradient;magnitude;semantic properties;software metrics;software specification;syntactic properties;validation","","45","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Rate-Based Queueing Simulation Model of Open Source Software Debugging Activities","C. Lin; Y. Li","Department of Computer Science and Information Engineering, National Chiayi University, Chiayi, Taiwan; Laboratory of Industrial Engineering, Ecole Centrale Paris, Paris, France","IEEE Transactions on Software Engineering","","2014","40","11","1075","1099","Open source software (OSS) approach has become increasingly prevalent for software development. As the widespread utilization of OSS, the reliability of OSS products becomes an important issue. By simulating the testing and debugging processes of software life cycle, the rate-based queueing simulation model has shown its feasibility for closed source software (CSS) reliability assessment. However, the debugging activities of OSS projects are different in many ways from those of CSS projects and thus the simulation approach needs to be calibrated for OSS projects. In this paper, we first characterize the debugging activities of OSS projects. Based on this, we propose a new rate-based queueing simulation framework for OSS reliability assessment including the model and the procedures. Then a decision model is developed to determine the optimal version-updating time with respect to two objectives: minimizing the time for version update, and maximizing OSS reliability. To illustrate the proposed framework, three real datasets from Apache and GNOME projects are used. The empirical results indicate that our framework is able to effectively approximate the real scenarios. Moreover, the influences of the core contributor staffing levels are analyzed and the optimal version-updating times are obtained.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2354032","National Science Council, Taiwan; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6891380","Queueing theory;rate-based simulation;open source software (OSS);bug reporting;report judgment;bug fixing;optimal version-updating time;non-homogeneous continuous time Markov chain (NHCTMC);multi-attribute utility theory (MAUT)","Software reliability;Debugging;Software;Stochastic processes;Analytical models;Cascading style sheets","configuration management;program debugging;program testing;project management;public domain software;queueing theory;software reliability","open source software;OSS approach;software development;OSS products reliability;testing processes;debugging processes;software life cycle;rate-based queueing simulation model;closed source software;CSS reliability assessment;debugging activities;OSS projects;decision model;optimal version-updating time","","8","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Random Testing: Theoretical Results and Practical Implications","A. Arcuri; M. Z. Iqbal; L. Briand","Simula, Oslo; Simula Research Laboratory, Lysaker; Simula Research Laboratory, Lysaker","IEEE Transactions on Software Engineering","","2012","38","2","258","277","A substantial amount of work has shed light on whether random testing is actually a useful testing technique. Despite its simplicity, several successful real-world applications have been reported in the literature. Although it is not going to solve all possible testing problems, random testing appears to be an essential tool in the hands of software testers. In this paper, we review and analyze the debate about random testing. Its benefits and drawbacks are discussed. Novel results addressing general questions about random testing are also presented, such as how long does random testing need, on average, to achieve testing targets (e.g., coverage), how does it scale, and how likely is it to yield similar results if we rerun it on the same testing problem (predictability). Due to its simplicity that makes the mathematical analysis of random testing tractable, we provide precise and rigorous answers to these questions. Results show that there are practical situations in which random testing is a viable option. Our theorems are backed up by simulations and we show how they can be applied to most types of software and testing criteria. In light of these results, we then assess the validity of empirical analyzes reported in the literature and derive guidelines for both practitioners and scientists.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.121","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6104067","Coupon collector;random testing;theory;Schur function;predictability;partition testing;adaptive random testing.","Testing;Software;Upper bound;Color;Random variables;Algorithm design and analysis;Generators","program testing;software tools","random testing;software testing;software tool;mathematical analysis;partition testing","","52","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Linear and Branching System Metrics","L. de Alfaro; M. Faella; M. Stoelinga","University of California, Santa Cruz, Santa Cruz; Università di Napoli, Napoli; University of Twente, Enschede","IEEE Transactions on Software Engineering","","2009","35","2","258","273","We extend the classical system relations of trace inclusion, trace equivalence, simulation, and bisimulation to a quantitative setting in which propositions are interpreted not as boolean values, but as elements of arbitrary metric spaces. Trace inclusion and equivalence give rise to asymmetrical and symmetrical linear distances, while simulation and bisimulation give rise to asymmetrical and symmetrical branching distances. We study the relationships among these distances and we provide a full logical characterization of the distances in terms of quantitative versions of LTL and mu-calculus. We show that, while trace inclusion (respectively, equivalence) coincides with simulation (respectively, bisimulation) for deterministic boolean transition systems, linear and branching distances do not coincide for deterministic metric transition systems. Finally, we provide algorithms for computing the distances over finite systems, together with a matching lower complexity bound.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.106","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4721438","Logics of programs;Specification techniques;Modal logic","Logic;Extraterrestrial measurements;Cost accounting;Computational modeling;Reasoning about programs;Formal languages;Software tools;Digital audio players;Clocks;Automata","Boolean functions;formal specification;process algebra;program diagnostics;program verification;software metrics;temporal logic","linear system metrics;branching system metric;software trace inclusion;software trace equivalence;software bisimulation;LTL;mu-calculus;deterministic Boolean transition system;software verification;linear temporal logic property;system specification","","40","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Cognitive heuristics in software engineering applying and extending anchoring and adjustment to artifact reuse","J. Parsons; C. Saunders","Fac. of Bus. Adm., Memorial Univ. of Newfoundland, St. John's, Nfld., Canada; NA","IEEE Transactions on Software Engineering","","2004","30","12","873","888","The extensive literature on reuse in software engineering has focused on technical and organizational factors, largely ignoring cognitive characteristics of individual developers. Despite anecdotal evidence that cognitive heuristics play a role in successful artifact reuse, few empirical studies have explored this relationship. This paper proposes how a cognitive heuristic, called anchoring, and the resulting adjustment bias can be adapted and extended to predict issues that might arise when developers reuse code and/or designs. The research proposes that anchoring and adjustment can be manifested in three ways: propagation of errors in reuse artifacts, failure to include requested functionality absent from reuse artifacts, and inclusion of unrequested functionality present in reuse artifacts. Results from two empirical studies are presented. The first study examines reuse of object classes in a programming task, using a combination of practicing programmers and students. The second study uses a database design task with student participants. Results from both studies indicate that anchoring occurs. Specifically, there is strong evidence that developers tend to use the extraneous functionality in the artifacts they are reusing and some evidence of anchoring to errors and omissions in reused artifacts. Implications of these findings for both practice and future research are explored.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.94","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377186","Index Terms- Software psychology;requirements/specifications;reusable software;reusable libraries;reuse models;human factors in software design.","Software engineering;Software libraries;Human factors;Application software;Computer Society;Programming profession;Databases;Psychology;Software reusability;Object oriented modeling","formal specification;software reusability;human factors;software libraries;object-oriented programming;psychology","cognitive heuristics;software engineering;programming task;database design task;reusable software;software psychology;human factor;requirement specification;reusable libraries;software design","","29","","54","","","","","","IEEE","IEEE Journals & Magazines"
"On object systems and behavioral inheritance","D. Harel; O. Kupferman","Dept. of Comput. Sci. & Appl. Math., Weizmann Inst. of Sci., Rehovot, Israel; NA","IEEE Transactions on Software Engineering","","2002","28","9","889","903","We consider state-based behavior in object-oriented analysis and design, as it arises, for example, in specifying behavior in the UML using statecharts. We first provide a rigorous and analyzable model of object systems and their reactivity. The definition is for basic one-thread systems, but can be extended in appropriate ways to more elaborate models. We then address the notion of inheritance and behavioral conformity and the resulting substitutability of classes, whereby inheriting should retain the system's original behaviors. Inheritance is a central issue of crucial importance to the modeling, design, and verification of object-oriented systems, and the many deep and unresolved questions around it cannot be addressed without a precise definition of the systems under consideration. We use our definition to give a clear and rigorous picture of what exactly is meant by behavioral conformity and how computationally complex it is to detect.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033228","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033228","","Object oriented modeling;Unified modeling language;Computational modeling;Analytical models;Runtime;Proposals;Object oriented programming;Writing;Computer languages","inheritance;object-oriented methods;formal verification;object-oriented programming;specification languages;formal specification","object-oriented analysis and design;state-based behavior;UML;statecharts;object systems;reactivity;one-thread systems;behavioral inheritance;behavioral conformity;class substitutability;modeling;verification","","26","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Effect of Domain Knowledge on Elicitation Effectiveness: An Internally Replicated Controlled Experiment","A. M. Aranda; O. Dieste; N. Juristo","Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Campus de Montegancedo, Boadilla del Monte, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Campus de Montegancedo, Boadilla del Monte, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Campus de Montegancedo, Boadilla del Monte, Spain","IEEE Transactions on Software Engineering","","2016","42","5","427","451","Context. Requirements elicitation is a highly communicative activity in which human interactions play a critical role. A number of analyst characteristics or skills may influence elicitation process effectiveness. Aim. Study the influence of analyst problem domain knowledge on elicitation effectiveness. Method. We executed a controlled experiment with post-graduate students. The experimental task was to elicit requirements using open interview and consolidate the elicited information immediately afterwards. We used four different problem domains about which students had different levels of knowledge. Two tasks were used in the experiment, whereas the other two were used in an internal replication of the experiment; that is, we repeated the experiment with the same subjects but with different domains. Results. Analyst problem domain knowledge has a small but statistically significant effect on the effectiveness of the requirements elicitation activity. The interviewee has a big positive and significant influence, as does general training in requirements activities and interview experience. Conclusion. During early contacts with the customer, a key factor is the interviewee; however, training in tasks related to requirements elicitation and knowledge of the problem domain helps requirements analysts to be more effective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2494588","Spanish Ministry of Ministry of Economy and Competitiveness; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7307191","Controlled experiment;domain knowledge;requirements elicitation;internal replication;Controlled experiment;domain knowledge;requirements elicitation;internal replication","Interviews;Knowledge engineering;Computer science;Software engineering;Requirements engineering;Training","software engineering","requirements elicitation;elicitation effectiveness;internally replicated controlled experiment;problem domain knowledge","","4","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Reducing Masking Effects in CombinatorialInteraction Testing: A Feedback DrivenAdaptive Approach","C. Yilmaz; E. Dumlu; M. B. Cohen; A. Porter","Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Borsa Istanbul, Istanbul, Turkey; Department of Computer Science and Engineering, University of Nebraska-Lincoln, NE; Department of Computer Science , University of Maryland, College Park, MD","IEEE Transactions on Software Engineering","","2014","40","1","43","66","The configuration spaces of modern software systems are too large to test exhaustively. Combinatorial interaction testing (CIT) approaches, such as covering arrays, systematically sample the configuration space and test only the selected configurations. The basic justification for CIT approaches is that they can cost-effectively exercise all system behaviors caused by the settings of t or fewer options. We conjecture, however, that in practice some of these behaviors are not actually tested because of unanticipated masking effects - test case failures that perturb system execution so as to prevent some behaviors from being exercised. While prior research has identified this problem, most solutions require knowing the masking effects a priori. In practice this is impractical, if not impossible. In this work, we reduce the harmful consequences of masking effects. First we define a novel interaction testing criterion, which aims to ensure that each test case has a fair chance to test all valid t-way combinations of option settings. We then introduce a feedback driven adaptive combinatorial testing process (FDA-CIT) to materialize this criterion in practice. At each iteration of FDA-CIT, we detect potential masking effects, heuristically isolate their likely causes (i.e., fault characterization), and then generate new samples that allow previously masked combinations to be tested in configurations that avoid the likely failure causes. The iterations end when the new interaction testing criterion has been satisfied. This paper compares two different fault characterization approaches - an integral part of the proposed approach, and empirically assesses their effectiveness and efficiency in removing masking effects on two widely used open source software systems. It also compares FDA-CIT against error locating arrays, a state of the art approach for detecting and locating failures. Furthermore, the scalability of the proposed approach is evaluated by comparing it with perfect test scenarios, in which all masking effects are known a priori. Our results suggest that masking effects do exist in practice, and that our approach provides a promising and efficient way to work around them, without requiring that masking effects be known a priori.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6654147","Combinatorial testing;adaptive testing;covering arrays;software quality assurance","Testing;Adaptive arrays;Educational institutions;Scalability;Servers;Electronic mail;Software systems","program testing;public domain software;software fault tolerance","perfect test scenarios;fault location;fault detection;error locating arrays;open source software systems;fault characterization approaches;potential masking effects detection;t-way combinations;FDA-CIT process;interaction testing criterion;system execution;test case failures;covering arrays;software systems;configuration spaces;feedback driven adaptive approach;CIT approach;combinatorial interaction testing;masking effects reduction","","14","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Learning Communicating Automata from MSCs","B. Bollig; J. Katoen; C. Kern; M. Leucker","ENS Cachan and CNRS, Cachan; RWTH Aachen University, Aachen; RWTH Aachen University, Aachen; Technical University Munich, Munich","IEEE Transactions on Software Engineering","","2010","36","3","390","408","This paper is concerned with bridging the gap between requirements and distributed systems. Requirements are defined as basic message sequence charts (MSCs) specifying positive and negative scenarios. Communicating finite-state machines (CFMs), i.e., finite automata that communicate via FIFO buffers, act as system realizations. The key contribution is a generalization of Angluin's learning algorithm for synthesizing CFMs from MSCs. This approach is exact-the resulting CFM precisely accepts the set of positive scenarios and rejects all negative ones-and yields fully asynchronous implementations. The paper investigates for which classes of MSC languages CFMs can be learned, presents an optimization technique for learning partial orders, and provides substantial empirical evidence indicating the practical feasibility of the approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374425","Software engineering/requirements/specifications/elicitation methods;software engineering/design/design concepts;computing methodologies/artificial intelligence/learning/induction;theory of computation/computation by abstract devices/models of computation/automata.","Learning automata;Software engineering;Software design;Unified modeling language;Communication channels;System recovery;Computer Society;Design engineering;Design methodology;Artificial intelligence","distributed processing;finite state machines;learning (artificial intelligence)","communicating automata;MSC;message sequence charts;distributed systems;communicating finite-state machines;finite automata;FIFO buffers;Angluin learning algorithm;optimization technique","","6","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Contract Insertion with CCBot","S. A. Carr; F. Logozzo; M. Payer","Purdue University, West Lafayette, IN; FaceBook, Seattle, WA; Purdue University, West Lafayette, IN","IEEE Transactions on Software Engineering","","2017","43","8","701","714","Existing static analysis tools require significant programmer effort. On large code bases, static analysis tools produce thousands of warnings. It is unrealistic to expect users to review such a massive list and to manually make changes for each warning. To address this issue we propose CCBot (short for CodeContracts Bot), a new tool that applies the results of static analysis to existing code through automatic code transformation. Specifically, CCBot instruments the code with method preconditions, postconditions, and object invariants which detect faults at runtime or statically using a static contract checker. The only configuration the programmer needs to perform is to give CCBot the file paths to code she wants instrumented. This allows the programmer to adopt contract-based static analysis with little effort. CCBot's instrumented version of the code is guaranteed to compile if the original code did. This guarantee means the programmer can deploy or test the instrumented code immediately without additional manual effort. The inserted contracts can detect common errors such as null pointer dereferences and out-of-bounds array accesses. CCBot is a robust large-scale tool with an open-source C# implementation. We have tested it on real world projects with tens of thousands of lines of code. We discuss several projects as case studies, highlighting undiscovered bugs found by CCBot, including 22 new contracts that were accepted by the project authors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2625248","NSF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7736073","Contract-based verification;automated patching;assertions;class invariants","Contracts;C# languages;Instruments;Computer bugs;Reactive power;Semantics;Runtime","C# language;program compilers;program diagnostics;program verification;software fault tolerance","automatic contract insertion;CCBot;static analysis tools;CodeContracts Bot;automatic code transformation;object invariants;fault detection;static contract checker;file paths;contract-based static analysis;null pointer dereferences;out-of-bounds array accesses;open-source C# implementation;contract-based verification","","1","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Formalization of the Whole-Part relationship in the Unified Modeling Language","F. Barbier; B. Henderson-Sellers; A. Le Parc-Lacayrelle; J. -. Bruel","Pau Univ., France; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","5","459","470","A formal definition for the semantics of the Whole-Part relationship in the Unified Modeling Language or UML is introduced. This provides a fully directly usable specification which can be incorporated into version 2.0 of UML. An improvement to the current metamodel fragment relating to relationships is proposed, supplemented by the introduction of axioms expressed in the Object Constraint Language or OCL. The overall formalization relates to a clear and concise emphasis on carefully enunciated (primary) characteristics that apply to all instances of a new Whole-Part metatype. Specific kinds of the Whole-Part relationship are defined in terms of secondary characteristics, which must be possessed by subtypes: In UML 1.4, these are Aggregation (a.k.a. white diamond) and Composition (a.k.a. black diamond). Primary and secondary characteristics may then be consistently combined with each other. Consequently, this allows the possible introduction of supplementary forms of Whole-Part. Such a revision is necessary since Aggregation and Composition in UML 1.4 do not cover the full spectrum of Whole-Part theory.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199074","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199074","","Unified modeling language;Object oriented modeling;Bibliographies;Design methodology;Assembly;Composite materials;Terminology","specification languages;object-oriented languages","formal definition;semantics;Whole-Part relationship;metamodel fragment;Object Constraint Language;OCL;object-oriented modeling;aggregation;composition;whole-part theory;Unified Modeling Language;UML;directly usable specification","","44","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Power-Laws in a Large Object-Oriented Software System","G. Concas; M. Marchesi; S. Pinna; N. Serra","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","10","687","708","We present a comprehensive study of an implementation of the Smalltalk object oriented system, one of the first and purest object-oriented programming environment, searching for scaling laws in its properties. We study ten system properties, including the distributions of variable and method names, inheritance hierarchies, class and method sizes, system architecture graph. We systematically found Pareto - or sometimes log-normal - distributions in these properties. This denotes that the programming activity, even when modeled from a statistical perspective, can in no way be simply modeled as a random addition of independent increments with finite variance, but exhibits strong organic dependencies on what has been already developed. We compare our results with similar ones obtained for large Java systems, reported in the literature or computed by ourselves for those properties never studied before, showing that the behavior found is similar in all studied object oriented systems. We show how the Yule process is able to stochastically model the generation of several of the power-laws found, identifying the process parameters and comparing theoretical and empirical tail indexes. Lastly, we discuss how the distributions found are related to existing object-oriented metrics, like Chidamber and Kemerer's, and how they could provide a starting point for measuring the quality of a whole system, versus that of single classes. In fact, the usual evaluation of systems based on mean and standard deviation of metrics can be misleading. It is more interesting to measure differences in the shape and coefficients of the data?s statistical distributions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1019","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302780","D.2.3.a Object-oriented programming;D.2.4.h Statistical methods;D.2.8.a Complexity measures;D.2.8.d Product metrics;D.2.8.e Software science;D.3.2.p Object-oriented languages;G.3.p Stochastic processes","Software systems;Object oriented modeling;Object oriented programming;Power system modeling;Shape measurement;Java;Power generation;Tail;Statistical distributions;Statistical analysis","","","","119","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Exception Handling for Repair in Service-Based Processes","G. Friedrich; M. G. Fugini; E. Mussi; B. Pernici; G. Tagni","Alpen-Adria Universit&#x0E4;t Klagenfurt, Kalgenfurt; Politecnico di Milano, Milano; Politecnico di Milano, Milano; Politecnico di Milano, Milano; Vrije Universiteit Amsterdam, Amsterdam","IEEE Transactions on Software Engineering","","2010","36","2","198","215","This paper proposes a self-healing approach to handle exceptions in service-based processes and to repair the faulty activities with a model-based approach. In particular, a set of repair actions is defined in the process model, and repairability of the process is assessed by analyzing the process structure and the available repair actions. During execution, when an exception arises, repair plans are generated by taking into account constraints posed by the process structure, dependencies among data, and available repair actions. The paper also describes the main features of the prototype developed to validate the proposed repair approach for composed Web services; the self-healing architecture for repair handling and the experimental results are illustrated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383376","Exception handling;failures;faults;repair;self-healing processes;Web services;process management.","Web services;Prototypes;Service oriented architecture;Logic design;Proposals","exception handling;program verification;software fault tolerance;software maintenance;software prototyping;Web services","exception handling;service based process repairing;Web services;self-healing architecture;process structure analysis;prototype development","","60","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Enforcing Exception Handling Policies with a Domain-Specific Language","E. A. Barbosa; A. Garcia; M. P. Robillard; B. Jakobus","OPUS Research Group, Informatics Department, Pontifical Catholic University of Rio de Janeiro, Rua Marquês de São Vicente, 255-Gávea, Rio de Janeiro, Brazil; OPUS Research Group, Informatics Department, Pontifical Catholic University of Rio de Janeiro, Rua Marquês de São Vicente, 255-Gávea, Rio de Janeiro, Brazil; School of Computer Science, McGill University, Montreal, Canada; OPUS Research Group, Informatics Department, Pontifical Catholic University of Rio de Janeiro, Rua Marquês de São Vicente, 255-Gávea, Rio de Janeiro, Brazil","IEEE Transactions on Software Engineering","","2016","42","6","559","584","Current software projects deal with exceptions in implementation and maintenance phases without a clear definition of exception handling policies. We call an exception handling policy the set of design decisions that govern the use of exceptions in a software project. Without an explicit exception handling policy, developers can remain unaware of the originally intended use of exceptions. In this paper, we present Exception Handling Policies Language (EPL), a domain-specific language to specify and verify exception handling policies. The evaluation of EPL was based on a user-centric observational study and case studies. The user-centric study was performed to observe how potential users of the language actually use it. With this study, we could better understand the trade-offs related to different language design decisions based on concrete and well-documented observations and experiences reported by participants. We identified some language characteristics that hindered its use and that motivated new language constructs. In addition, we performed case studies with one open-source project and two industry-strength systems to investigate how specifying and verifying exception handling policies may assist in detecting exception handling problems. The results show that violations of exception handling policies help to indicate potential faults in the exception handling code.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2506164","Fundação Carlos Chagas Filho de Amparo à Pesquisa do Estado do Rio de Janeiro (FAPERJ); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7348692","Exception handling;Exception handling policy;Policy specification;Domain-specific language;Exception handling;exception handling policy;policy specification;domain-specific language","Java;Software reliability;Robustness;Software systems","exception handling;formal specification;formal verification;programming languages;project management;public domain software;software maintenance","industry-strength systems;open-source project;user-centric study;exception handling policy verification;exception handling policy specification;EPL evaluation;language design decisions;maintenance phase;implementation phase;software projects;domain-specific language;exception handling policies language","","4","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Shortening matching time in OPS5 production systems","J. A. Kang; A. M. K. Cheng","Humax Co. Ltd., South Korea; NA","IEEE Transactions on Software Engineering","","2004","30","7","448","457","A rule-based system must satisfy stringent timing constraints when applied to a real-time environment. As the scale of rule-based expert systems increases, the efficiency of systems becomes a pressing concern. The most critical performance factor in the implementation of a production system is the condition-testing algorithm. We propose a new method based on the widely used RETE match algorithm. We show an approach designed to reduce the response time of rule-based expert systems by reducing the matching time. There are two steps in the method we propose: The first makes an index structure of the tokens to reduce the /spl alpha/-node-level join candidates. The second chooses the highest time tag for certain /spl beta/-nodes to reduce the amount of combinatorial match that is problematical in a real-time production system application. For this purpose, a simple compiler is implemented in C and the response time of test programs is measured.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318606","Matching;knowledge-based systems;expert systems;rule-based systems;OPS5;Rete;response time.","Production systems;Real time systems;Expert systems;Delay;Knowledge based systems;Timing;Pressing;Program processors;Testing;Time measurement","expert systems;real-time systems;program testing;optimising compilers","real-time environment;rule-based expert systems;condition-testing algorithm;RETE match algorithm;/spl alpha/-node-level join candidates;OPS5 production system;program compiler;test programs;knowledge-based systems","","13","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Source Code Summarization of Context for Java Methods","P. W. McBurney; C. McMillan","College of Computer Science and Engineering, University Notre Dame, Notre Dame, IN; Computer Science, University of Notre Dame, South Bend, VA","IEEE Transactions on Software Engineering","","2016","42","2","103","119","Source code summarization is the task of creating readable summaries that describe the functionality of software. Source code summarization is a critical component of documentation generation, for example as Javadocs formed from short paragraphs attached to each method in a Java program. At present, a majority of source code summarization is manual, in that the paragraphs are written by human experts. However, new automated technologies are becoming feasible. These automated techniques have been shown to be effective in select situations, though a key weakness is that they do not explain the source code's context. That is, they can describe the behavior of a Java method, but not why the method exists or what role it plays in the software. In this paper, we propose a source code summarization technique that writes English descriptions of Java methods by analyzing how those methods are invoked. We then performed two user studies to evaluate our approach. First, we compared our generated summaries to summaries written manually by experts. Then, we compared our summaries to summaries written by a state-of-the-art automatic summarization tool. We found that while our approach does not reach the quality of human-written summaries, we do improve over the state-of-the-art summarization tool in several dimensions by a statistically-significant margin.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2465386","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7181703","Source code summarization;automatic documentation;program comprehension;Source code summarization;automatic documentation;program comprehension","Context;Documentation;Java;Natural languages;Software;Generators;XML","Java;object-oriented methods","source code summarization technique;Java methods;software functionality;documentation generation;Javadocs;Java program;user studies;automatic summarization tool;human-written summaries","","14","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Range Fixes: Interactive Error Resolution for Software Configuration","Y. Xiong; H. Zhang; A. Hubaux; S. She; J. Wang; K. Czarnecki","School of Electronics Engineering and Computer Science, Institute of Software, Peking University, Beijing, PR China; School of Electronics Engineering and Computer Science, Institute of Software, Peking University, Beijing, PR China; ASML, Eindhoven, the Netherlands; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; School of Electronics Engineering and Computer Science, Institute of Software, Peking University, Beijing, PR China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada","IEEE Transactions on Software Engineering","","2015","41","6","603","619","To prevent ill-formed configurations, highly configurable software often allows defining constraints over the available options. As these constraints can be complex, fixing a configuration that violates one or more constraints can be challenging. Although several fix-generation approaches exist, their applicability is limited because (1) they typically generate only one fix or a very long fix list, difficult for the user to identify the desirable fix; and (2) they do not fully support non-Boolean constraints, which contain arithmetic, inequality, and string operators. This paper proposes a novel concept, range fix, for software configuration. A range fix specifies the options to change and the ranges of values for these options. We also design an algorithm that automatically generates range fixes for a violated constraint. We have evaluated our approach with three different strategies for handling constraint interactions, on data from nine open source projects over two configuration platforms. The evaluation shows that our notion of range fix leads to mostly simple yet complete sets of fixes, and our algorithm is able to generate fixes within one second for configuration systems with a few thousands options and constraints.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2383381","National Basic Research Program of China; High-Tech Research and Development Program of China; National Natural Science Foundation of China; NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6991616","Consistency Management;Error Resolution;Range Fix;Software Configuration;Consistency management;error resolution;range fix;software configuration","Concrete;Linux;Biological system modeling;Reactive power;Kernel;Navigation","constraint handling;software engineering","range fixes;interactive error resolution;software configuration;fix-generation approaches;constraint interaction handling;open source projects","","8","","46","","","","","","IEEE","IEEE Journals & Magazines"
"VERTAF: an application framework for the design and verification of embedded real-time software","Pao-Ann Hsiung; Shang-Wei Lin; Chih-Hao Tseng; Trong-Yen Lee; Jin-Ming Fu; Win-Bin See","Dept. of Comput. Sci. & Inf. Eng., Nat. Chung Chen Univ., Ming-Hsiung, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Chung Chen Univ., Ming-Hsiung, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Chung Chen Univ., Ming-Hsiung, Taiwan; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","10","656","674","The growing complexity of embedded real-time software requirements calls for the design of reusable software components, the synthesis and generation of software code, and the automatic guarantee of nonfunctional properties such as performance, time constraints, reliability, and security. Available application frameworks targeted at the automatic design of embedded real-time software are poor in integrating functional and nonfunctional requirements. To bridge this gap, we reveal the design flow and the internal architecture of a newly proposed framework called verifiable embedded real-time application framework (VERTAF), which integrates software component-based reuse, formal synthesis, and formal verification. A formal UML-based embedded real-time object model is proposed for component reuse. Formal synthesis employs quasistatic and quasidynamic scheduling with automatic generation of multilayer portable efficient code. Formal verification integrates a model checker kernel from SGM, by adapting it for embedded software. The proposed architecture for VERTAF is component-based and allows plug-and-play for the scheduler and the verifier. Using VERTAF to develop application examples significantly reduced design effort and illustrated how high-level reuse of software components combined with automatic synthesis and verification can increase design productivity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339277","Index Terms- Application framework;code generation;embedded real-time software;formal synthesis;formal verification;scheduling;software components;UML modeling.","Application software;Embedded software;Software performance;Software reusability;Computer architecture;Formal verification;Software design;Time factors;Security;Bridges","embedded systems;object-oriented programming;software reusability;program verification;program compilers;software portability;distributed object management;processor scheduling;software architecture","embedded real-time software requirements;reusable software components;software code generation;formal verification;UML-based embedded real-time object model;formal synthesis;quasidynamic scheduling;multilayer portable efficient code;model checker kernel;design productivity;verifiable embedded real-time application framework","","24","","61","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting source code changes by mining change history","A. T. T. Ying; G. C. Murphy; R. Ng; M. C. Chu-Carroll","IBM Thomas J. Watson Res. Center, Hawthorne, NY, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","9","574","586","Software developers are often faced with modification tasks that involve source which is spread across a code base. Some dependencies between source code, such as those between source code written in different languages, are difficult to determine using existing static and dynamic analyses. To augment existing analyses and to help developers identify relevant source code during a modification task, we have developed an approach that applies data mining techniques to determine change patterns - sets of files that were changed together frequently in the past - from the change history of the code base. Our hypothesis is that the change patterns can be used to recommend potentially relevant source code to a developer performing a modification task. We show that this approach can reveal valuable dependencies by applying the approach to the Eclipse and Mozilla open source projects and by evaluating the predictability and interestingness of the recommendations produced for actual modification tasks on these systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324645","Index Terms- Enhancement;maintainability;clustering;classification;association rules;data mining.","History;Data mining;Association rules;Pattern analysis;Computer Society;Software systems;Computer languages;Frequency;Programming profession;Computer science","data mining;software tools;program verification;software maintenance;configuration management","software developers;modification task;code base;source code changes prediction;data mining technique;change history;Eclipse open source project;Mozilla open source project;software maintainability;pattern clustering;pattern classification;association rules","","241","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Fault Localization for Dynamic Web Applications","S. Artzi; J. Dolby; F. Tip; M. Pistoia","IBM Software Group, Littleton; IBM Thomas J. Watson Research Center, Yorktown Heights; IBM Thomas J. Watson Research Center, Yorktown Heights; IBM Thomas J. Watson Research Center, Yorktown Heights","IEEE Transactions on Software Engineering","","2012","38","2","314","335","In recent years, there has been significant interest in fault-localization techniques that are based on statistical analysis of program constructs executed by passing and failing executions. This paper shows how the Tarantula, Ochiai, and Jaccard fault-localization algorithms can be enhanced to localize faults effectively in web applications written in PHP by using an extended domain for conditional and function-call statements and by using a source mapping. We also propose several novel test-generation strategies that are geared toward producing test suites that have maximal fault-localization effectiveness. We implemented various fault-localization techniques and test-generation strategies in Apollo, and evaluated them on several open-source PHP applications. Our results indicate that a variant of the Ochiai algorithm that includes all our enhancements localizes 87.8 percent of all faults to within 1 percent of all executed statements, compared to only 37.4 percent for the unenhanced Ochiai algorithm. We also found that all the test-generation strategies that we considered are capable of generating test suites with maximal fault-localization effectiveness when given an infinite time budget for test generation. However, on average, a directed strategy based on path-constraint similarity achieves this maximal effectiveness after generating only 6.5 tests, compared to 46.8 tests for an undirected test-generation strategy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975173","Fault localization;statistical debugging;program analysis;web applications;PHP.","HTML;Databases;Servers;Open source software;Browsers;Algorithm design and analysis;Concrete","program testing;software fault tolerance;statistical analysis","fault localization;dynamic Web applications;statistical analysis;source mapping;fault localization effectiveness;test generation strategies;path constraint;Tarantula;Ochiai;Jaccard;Apollo;open-source PHP applications","","14","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Formulating Criticality-Based Cost-Effective Fault Tolerance Strategies for Multi-Tenant Service-Based Systems","Y. Wang; Q. He; D. Ye; Y. Yang","State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Vic, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Vic, Australia","IEEE Transactions on Software Engineering","","2018","44","3","291","307","The proliferation of cloud computing has fueled the rapid growth of multi-tenant service-based systems (SBSs), which serve multiple tenants simultaneously by composing existing services in the form of business processes. In a distributed and volatile operating environment, runtime anomalies may occur to the component services of an SBS and cause end-to-end quality violations. Engineering multi-tenant SBSs that can quickly handle runtime anomalies cost effectively has become a significant challenge. Different approaches have been proposed to formulate fault tolerance strategies for engineering SBSs. However, none of the existing approaches has sufficiently considered the service criticality based on multi-tenancy where multiple tenants share the same SBS instance with different multi-dimensional quality preferences. In this paper, we propose Criticality-based Fault Tolerance for Multi-Tenant SBSs (CFT4MTS), a novel approach that formulates cost-effective fault tolerance strategies for multi-tenant SBSs by providing redundancy for the critical component services. First, the criticality of each component service is evaluated based on its multi-dimensional quality and multiple tenants sharing the component service with differentiated quality preferences. Then, the fault tolerance problem is modelled as an Integer Programming problem to identify the optimal fault tolerance strategy. The experimental results show that, compared with three existing representative approaches, CFT4MTS can alleviate degradation in the quality of multi-tenant SBSs in a much more effective and efficient way.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2681667","Australian Research Council Discovery; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7876832","Cloud computing;criticality;fault tolerance;multi-tenancy;redundancy;service-based system","Fault tolerant systems;Streaming media;Runtime;Redundancy;Cloud computing;Business","cloud computing;integer programming;service-oriented architecture;software fault tolerance","optimal fault tolerance;criticality-based cost-effective fault tolerance;multi-tenant service-based systems;multidimensional quality preferences;cloud computing;Criticality-based Fault Tolerance for Multi-Tenant SBSs;CFT4MTS;end-to-end quality violations;runtime anomalies cost;service criticality;multitenancy;critical component services;Integer Programming problem","","2","","46","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Comparison of Model Validation Techniques for Defect Prediction Models","C. Tantithamthavorn; S. McIntosh; A. E. Hassan; K. Matsumoto","Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan; Department of Electrical and Computer Engineering, Montreal, QC, McGill UniversityCanada; School of Computing, Queen’s University, Kingston, ON, Canada; Graduate School of Information Science, Nara Institute of Science and Technology, Ikoma, Japan","IEEE Transactions on Software Engineering","","2017","43","1","1","18","Defect prediction models help software quality assurance teams to allocate their limited resources to the most defect-prone modules. Model validation techniques, such as<inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><inline-graphic xlink:href=""tantithamthavorn-ieq1-2584050.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>-fold cross-validation, use historical data to estimate how well a model will perform in the future. However, little is known about how accurate the estimates of model validation techniques tend to be. In this paper, we investigate the bias and variance of model validation techniques in the domain of defect prediction. Analysis of 101 public defect datasets suggests that 77 percent of them are highly susceptible to producing unstable results– - selecting an appropriate model validation technique is a critical experimental design choice. Based on an analysis of 256 studies in the defect prediction literature, we select the 12 most commonly adopted model validation techniques for evaluation. Through a case study of 18 systems, we find that single-repetition holdout validation tends to produce estimates with 46-229 percent more bias and 53-863 percent more variance than the top-ranked model validation techniques. On the other hand, out-of-sample bootstrap validation yields the best balance between the bias and variance of estimates in the context of our study. Therefore, we recommend that future defect prediction studies avoid single-repetition holdout validation, and instead, use out-of-sample bootstrap validation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2584050","JSPS; Advancing Strategic International Networks to Accelerate the Circulation of Talented Researchers; Interdisciplinary Global Networks for Accelerating Theory and Practice in Software Ecosystem; JSPS Fellows; Natural Sciences and Engineering Research Council of Canada (NSERC); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7497471","Defect prediction models;model validation techniques;bootstrap validation;cross validation;holdout validation","Predictive models;Data models;Analytical models;Context;Context modeling;Software;Logistics","","","","39","","122","","","","","","IEEE","IEEE Journals & Magazines"
"Self-Management of Adaptable Component-Based Applications","L. Rosa; L. Rodrigues; A. Lopes; M. Hiltunen; R. Schlichting","INESC-ID and Universidade Técnica de Lisboa, Lisboa; INESC-ID and Universidade Técnica de Lisboa, Lisboa; University of Lisbon, Lisbon; AT&T Labs-Research, Florham Park; AT&T Labs Research, Florham Park","IEEE Transactions on Software Engineering","","2013","39","3","403","421","The problem of self-optimization and adaptation in the context of customizable systems is becoming increasingly important with the emergence of complex software systems and unpredictable execution environments. Here, a general framework for automatically deciding on when and how to adapt a system whenever it deviates from the desired behavior is presented. In this framework, the system's target behavior is described as a high-level policy that establishes goals for a set of performance indicators. The decision process is based on information provided independently for each component that describes the available adaptations, their impact on performance indicators, and any limitations or requirements. The technique consists of both offline and online phases. Offline, rules are generated specifying component adaptations that may help to achieve the established goals when a given change in the execution context occurs. Online, the corresponding rules are evaluated when a change occurs to choose which adaptations to perform. Experimental results using a prototype framework in the context of a web-based application demonstrate the effectiveness of this approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6197201","Adaptive systems;self-management;autonomic computing;goal policies","Runtime;Context;Software systems;Optimization;Catalogs","fault tolerant computing;optimisation","adaptable component based applications;customizable systems;complex software systems;unpredictable execution environments;desired behavior;target behavior;performance indicators;decision process;Web based application;autonomic computing","","14","","28","","","","","","IEEE","IEEE Journals & Magazines"
"A Taxonomy and Qualitative Comparison of Program Analysis Techniques for Security Assessment of Android Software","A. Sadeghi; H. Bagheri; J. Garcia; S. Malek","School of Information and Computer Sciences, University of California, Irvine, CA; Department of Computer Science and Engineering, University of Nebraska, Lincoln, NE; School of Information and Computer Sciences, University of California, Irvine, CA; School of Information and Computer Sciences, University of California, Irvine, CA","IEEE Transactions on Software Engineering","","2017","43","6","492","530","In parallel with the meteoric rise of mobile software, we are witnessing an alarming escalation in the number and sophistication of the security threats targeted at mobile platforms, particularly Android, as the dominant platform. While existing research has made significant progress towards detection and mitigation of Android security, gaps and challenges remain. This paper contributes a comprehensive taxonomy to classify and characterize the state-of-the-art research in this area. We have carefully followed the systematic literature review process, and analyzed the results of more than 300 research papers, resulting in the most comprehensive and elaborate investigation of the literature in this area of research. The systematic analysis of the research literature has revealed patterns, trends, and gaps in the existing literature, and underlined key challenges and opportunities that will shape the focus of future research efforts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2615307","National Science Foundation; Defense Advanced Research Projects Agency; Army Research Office; Department of Homeland Security; Air Force Office of Scientific Research; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7583740","Taxonomy and survey;security assessment;android platform;program analysis","Androids;Humanoid robots;Security;Taxonomy;Mobile communication;Malware;Systematics","Android (operating system);mobile computing;program diagnostics;security of data","taxonomy;program analysis;security assessment;Android software;mobile software;security threats;mobile platforms;dominant platform;Android security","","18","","517","","","","","","IEEE","IEEE Journals & Magazines"
"Metamorphic Testing for Software Quality Assessment: A Study of Search Engines","Z. Q. Zhou; S. Xiang; T. Y. Chen","School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; Department of Computer Science and Software Engineering, Swinburne University of Technology, Hawthorn, Victoria, Australia","IEEE Transactions on Software Engineering","","2016","42","3","264","284","Metamorphic testing is a testing technique that can be used to verify the functional correctness of software in the absence of an ideal oracle. This paper extends metamorphic testing into a user-oriented approach to software verification, validation, and quality assessment, and conducts large scale empirical studies with four major web search engines: Google, Bing, Chinese Bing, and Baidu. These search engines are very difficult to test and assess using conventional approaches owing to the lack of an objective and generally recognized oracle. The results are useful for both search engine developers and users, and demonstrate that our approach can effectively alleviate the oracle problem and challenges surrounding a lack of specifications when verifying, validating, and evaluating large and complex software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2478001","Australian Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7254235","Software quality;verification;validation;quality assessment;oracle problem;lack of system specification;metamorphic testing;user-oriented testing;search engine;Software quality;verification;validation;quality assessment;oracle problem;lack of system specification;metamorphic testing;user-oriented testing;search engine","Search engines;Testing;Web pages;Google;Software algorithms;Software quality","Internet;program testing;program verification;search engines;software quality;user interfaces","metamorphic testing;software quality assessment;user-oriented approach;software verification;software validation;Web search engines;Google;Chinese Bing;Baidu","","22","","53","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Review of the Application and Empirical Investigation of Search-Based Test Case Generation","S. Ali; L. C. Briand; H. Hemmati; R. K. Panesar-Walawege","Simula Research Laboratory, Lysaker and University of Oslo, Norway; Simula Research Laboratory, Lysaker and University of Oslo, Norway; Simula Research Laboratory, Lysaker and University of Oslo, Norway; Simula Research Laboratory, Lysaker and University of Oslo, Norway","IEEE Transactions on Software Engineering","","2010","36","6","742","762","Metaheuristic search techniques have been extensively used to automate the process of generating test cases, and thus providing solutions for a more cost-effective testing process. This approach to test automation, often coined “Search-based Software Testing” (SBST), has been used for a wide variety of test case generation purposes. Since SBST techniques are heuristic by nature, they must be empirically investigated in terms of how costly and effective they are at reaching their test objectives and whether they scale up to realistic development artifacts. However, approaches to empirically study SBST techniques have shown wide variation in the literature. This paper presents the results of a systematic, comprehensive review that aims at characterizing how empirical studies have been designed to investigate SBST cost-effectiveness and what empirical evidence is available in the literature regarding SBST cost-effectiveness and scalability. We also provide a framework that drives the data collection process of this systematic review and can be the starting point of guidelines on how SBST techniques can be empirically assessed. The intent is to aid future researchers doing empirical studies in SBST by providing an unbiased view of the body of empirical evidence and by guiding them in performing well-designed and executed empirical studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5210118","Evolutionary computing and genetic algorithms;frameworks;heuristics design;review and evaluation;test generation;testing strategies;validation.","System testing;Automatic testing;Software testing;Automation;Costs;Logic testing;Scalability;Guidelines;Genetic algorithms;Algorithm design and analysis","program testing;search problems","search based test case generation;metaheuristic search technique;cost effective testing process;test automation;search based software testing","","154","","55","","","","","","IEEE","IEEE Journals & Magazines"
"A distributed parallel programming framework","N. Stankovic; Kang Zhang","Nokia, Burlington, MA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","5","478","493","This paper presents Visper, a novel object-oriented framework that identifies and enhances common services and programming primitives, and implements a generic set of classes applicable to multiple programming models in a distributed environment. Groups of objects, which can be programmed in a uniform and transparent manner, and agent-based distributed system management, are also featured in Visper. A prototype system is designed and implemented in Java, with a number of visual utilities that facilitate program development and portability. As a use case, Visper integrates parallel programming in an MPI-like message-passing paradigm at a high level with services such as checkpointing and fault tolerance at a lower level. The paper reports a range of performance evaluation on the prototype and compares it to related works.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000451","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000451","","Parallel programming","parallel programming;application program interfaces;message passing;object-oriented programming;visual programming;software fault tolerance","distributed parallel programming framework;Visper;object-oriented framework;programming primitives;multiple programming models;distributed environment;agent-based distributed system management;visual utilities;MPI-like message passing paradigm;visual programming;fault tolerance","","15","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Name-Based Analysis of Equally Typed Method Arguments","M. Pradel; T. R. Gross","ETH Zurich, Zurich; ETH Zurich, Zurich","IEEE Transactions on Software Engineering","","2013","39","8","1127","1143","When calling a method that requires multiple arguments, programmers must pass the arguments in the expected order. For statically typed languages, the compiler helps programmers by checking that the type of each argument matches the type of the formal parameter. Unfortunately, types are futile for methods with multiple parameters of the same type. How can a programmer check that equally typed arguments are passed in the correct order? This paper presents two simple, yet effective, static program analyses that detect problems related to the order of equally typed arguments. The key idea is to leverage identifier names to infer the semantics of arguments and their intended positions. The analyses reveal problems that affect the correctness, understandability, and maintainability of a program, such as accidentally reversed arguments and misleading parameter names. Most parts of the analyses are language-agnostic. We evaluate the approach with 24 real-world programs written in Java and C. Our results show the analyses to be effective and efficient. One analysis reveals anomalies in the order of equally typed arguments; it finds 54 relevant problems with a precision of 82 percent. The other analysis warns about misleading parameter names and finds 31 naming bugs with a precision of 39 percent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6419711","Testing and debugging;maintenance;documentation;static program analysis;anomaly detection;method arguments","Java;Access control;Engines;Feature extraction;Context;Robustness;Program processors","C language;Java;program diagnostics","name-based analysis;equally typed method argument;statically typed language;formal parameter;static program analysis;program correctness;program understandability;program maintainability;Java language;C language","","3","","40","","","","","","IEEE","IEEE Journals & Magazines"
"General test result checking with log file analysis","J. H. Andrews; Yingjun Zhang","Dept. of Comput. Sci., Univ. of Western Ontario, London, Ont., Canada; Dept. of Comput. Sci., Univ. of Western Ontario, London, Ont., Canada","IEEE Transactions on Software Engineering","","2003","29","7","634","648","We describe and apply a lightweight formal method for checking test results. The method assumes that the software under test writes a text log file; this log file is then analyzed by a program to see if it reveals failures. We suggest a state-machine-based formalism for specifying the log file analyzer programs and describe a language and implementation based on that formalism. We report on empirical studies of the application of log file analysis to random testing of units. We describe the results of experiments done to compare the performance and effectiveness of random unit testing with coverage checking and log file analysis to other unit testing procedures. The experiments suggest that writing a formal log file analyzer and using random testing is competitive with other formal and informal methods for unit testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214327","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214327","","Software testing;Failure analysis;Software safety;Inspection;Automatic testing;Humans;System testing;Performance analysis;Writing;Real time systems","program testing;program debugging;program verification;formal specification;finite state machines","software testing;event-based debugging;log file;state-machine-based formalism;safety verification;lightweight formal methods;test oracles;unit testing","","36","","45","","","","","","IEEE","IEEE Journals & Magazines"
"On the Need for Mixed Media in Distributed Requirements Negotiations","D. Damian; F. Lanubile; T. Mallardo","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","116","132","Achieving agreement with respect to software requirements is a collaborative process that traditionally relies on same-time, same-place interactions. As the trend toward geographically distributed software development continues, colocated meetings are becoming increasingly problematic. Our research investigates the impact of computer-mediated communication on the performance of distributed client/developer teams involved in the collaborative development of a requirements specification. Drawing on media-selection theories, we posit that a combination of lean and rich media is needed for an effective process of requirements negotiations when stakeholders are geographically dispersed. In this paper, we present an empirical study that investigates the performance of six educational global project teams involved in a negotiation process using both asynchronous text-based and synchronous videoconferencing-based communication modes. The findings indicate that requirement negotiations were more effective when the groups conducted asynchronous structured discussions of requirement issues prior to the synchronous negotiation meeting. Asynchronous discussions were useful in resolving issues related to uncertainty in requirements, thus allowing synchronous negotiations to focus more on removing ambiguities in the requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70758","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4378346","Requirements/Specifications;Distributed/Internet based software engineering tools and techniques;Experimental design;Asynchronous interaction;Synchronous interaction;Requirements/Specifications;Distributed/Internet based software engineering tools and techniques;Experimental design;Asynchronous interaction;Synchronous interaction","Collaboration;Programming;Collaborative software;Computer mediated communication;Grounding;Computer science;Dispersion;Uncertainty;Globalization;Computer industry","formal specification;multimedia communication","mixed media;distributed requirements negotiations;software requirements;geographically distributed software development;computer-mediated communication;distributed client-developer teams;requirements specification;media-selection theories;educational global project teams;asynchronous text-based communication;synchronous videoconferencing-based communication;synchronous negotiation meeting","","26","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Using origin analysis to detect merging and splitting of source code entities","M. W. Godfrey; L. Zou","Sch. of Comput. Sci., Waterloo Univ., Ont., Canada; Sch. of Comput. Sci., Waterloo Univ., Ont., Canada","IEEE Transactions on Software Engineering","","2005","31","2","166","181","Merging and splitting source code entities is a common activity during the lifespan of a software system; as developers rethink the essential structure of a system or plan for a new evolutionary direction, so must they be able to reorganize the design artifacts at various abstraction levels as seems appropriate. However, while the raw effects of such changes may be plainly evident in the new artifacts, the original context of the design changes is often lost. That is, it may be obvious which characters of which files have changed, but it may not be obvious where or why moving, renaming, merging, and/or splitting of design elements has occurred. In this paper, we discuss how we have extended origin analysis (Q. Tu et al., 2002), (M.W. Godfrey et al., 2002) to aid in the detection of merging and splitting of files and functions in procedural code; in particular, we show how reasoning about how call relationships have changed can aid a developer in locating where merges and splits have occurred, thereby helping to recover some information about the context of the design change. We also describe a case study of these techniques (as implemented in the Beagle tool) using the PostgreSQL database system as the subject.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401931","Index Terms- Software evolution;origin analysis;restructuring;reverse engineering;and reengineering.","Merging;Software systems;History;Software maintenance;Information analysis;Database systems;Reverse engineering;Environmental management;Software tools;Documentation","reverse engineering;systems re-engineering;software maintenance;software development management;merging;SQL;reasoning about programs","merging;source code entity;software system;origin analysis;PostgreSQL database system","","97","","25","","","","","","IEEE","IEEE Journals & Magazines"
"A tool to help tune where computation is performed","Hyeonsang Eom; J. K. Hollingsworth","Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; NA","IEEE Transactions on Software Engineering","","2001","27","7","618","629","We introduce a new performance metric, called load balancing factor (LBF), to assist programmers when evaluating different tuning alternatives. The LBF metric differs from traditional performance metrics since it is intended to measure the performance implications of a specific tuning alternative rather than quantifying where time is spent in the current version of the program. A second unique aspect of the metric is that it provides guidance about moving work within a distributed or parallel program rather than reducing it. A variation of the LBF metric can also be used to predict the performance impact of changing the underlying network. The LBF metric is computed incrementally and online during the execution of the program to be tuned. We also present a case study that shows that our metric can accurately predict the actual performance gains for a test suite of six programs.","0098-5589;1939-3520;2326-3881","","10.1109/32.935854","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=935854","","Programming profession;Computational modeling;Performance evaluation;Current measurement;Performance analysis;Load management;Time measurement;Performance gain;Testing;Distributed computing","software performance evaluation;software metrics;distributed programming;parallel programming","performance metric;load balancing factor;tuning;parallel program;distributed program","","","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Towards Prioritizing Documentation Effort","P. W. McBurney; S. Jiang; M. Kessentini; N. A. Kraft; A. Armaly; M. W. Mkaouer; C. McMillan","Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer and Information Science, University of Michigan-Dearborn, Dearborn, MI; ABB Corporate Research, Raleigh, NC; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer and Information Science, University of Michigan-Dearborn, Dearborn, MI; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN","IEEE Transactions on Software Engineering","","2018","44","9","897","913","Programmers need documentation to comprehend software, but they often lack the time to write it. Thus, programmers must prioritize their documentation effort to ensure that sections of code important to program comprehension are thoroughly explained. In this paper, we explore the possibility of automatically prioritizing documentation effort. We performed two user studies to evaluate the effectiveness of static source code attributes and textual analysis of source code towards prioritizing documentation effort. The first study used open-source API Libraries while the second study was conducted using closed-source industrial software from ABB. Our findings suggest that static source code attributes are poor predictors of documentation effort priority, whereas textual analysis of source code consistently performed well as a predictor of documentation effort priority.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2716950","National Science Foundation Graduate Research Fellowship Program; National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7953505","Code documentation;program comprehension;software maintenance","Documentation;Libraries;Java;Gold;Programming;Software;Neural networks","application program interfaces;program diagnostics;public domain software;software maintenance","prioritizing documentation effort;static source code attributes;open-source API Libraries;closed-source industrial software;documentation effort priority","","","","78","","","","","","IEEE","IEEE Journals & Magazines"
"A methodology for architecture-level reliability risk analysis","S. M. Yacoub; H. H. Ammar","Hewlett-Packard Labs., Palo Alto, CA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","6","529","547","The paper presents a methodology for reliability risk assessment at the early stages of the development lifecycle, namely, the architecture level. We describe a heuristic risk assessment methodology that is based on dynamic metrics. The methodology uses dynamic complexity and dynamic coupling metrics to define complexity factors for the architecture elements (components and connectors). Severity analysis is performed using Failure Mode and Effect Analysis (FMEA) as applied to architecture models. We combine severity and complexity factors to develop heuristic risk factors for the architecture components and connectors. Based on analysis scenarios, we develop a risk assessment model that represents components, connectors, component risk factors, connector risk factors, and probabilities of component interactions. We also develop a risk analysis algorithm that aggregates risk factors of components and connectors to the architectural level. Using the risk aggregation and the risk analysis model, we show how to analyze the overall risk factor of the architecture as the function of the risk factors of its constituting components and connectors. A case study of a pacemaker architecture is used to illustrate the application of the methodology. The methodology is used to identify critical components and connectors and to investigate the sensitivity of the architecture risk factor to changes in the heuristic risk factors of the architecture elements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010058","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010058","","Risk analysis;Risk management;Connectors;Failure analysis;Performance analysis;Computer architecture;Computer Society;Frequency;Humans;Computer errors","software reliability;software metrics;risk management;software architecture","architecture-level reliability risk analysis methodology;software risk management plan;subjective judgement;domain experts;subjective risk assessment techniques;product attributes;product metrics;reliability risk assessment;development lifecycle;architecture level;heuristic risk assessment methodology;dynamic metrics;dynamic complexity;dynamic coupling metrics;complexity factors;architecture elements;severity analysis;Failure Mode and Effect Analysis;FMEA;architecture models;severity factors;heuristic risk factors;architecture components;analysis scenarios;risk assessment model;component risk factors;connector risk factors;component interactions;risk factors;architectural level;risk aggregation;pacemaker architecture;architecture risk factor;reliability risk analysis;software architecture","","88","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Invariant-Based Automatic Testing of Modern Web Applications","A. Mesbah; A. van Deursen; D. Roest","University of British Columbia, Vancouver; Delft University of Technology, Delft; Delft University of Technology, Delft","IEEE Transactions on Software Engineering","","2012","38","1","35","53","Ajax-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing Ajax applications automatically, based on a crawler to infer a state-flow graph for all (client-side) user interface states. We identify Ajax-specific faults that can occur in such states (related to, e.g., DOM validity, error messages, discoverability, back-button compatibility) as well as DOM-tree invariants that can serve as oracles to detect such faults. Our approach, called Atusa, is implemented in a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe three case studies, consisting of six subjects, evaluating the type of invariants that can be obtained for Ajax applications as well as the fault revealing capabilities, scalability, required manual effort, and level of automation of our testing approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728834","Automated testing;web applications;Ajax.","Web and internet services;Browsers;User interfaces;Robots;Servers","automatic testing;client-server systems;Internet;Java;program testing;trees (mathematics);user interfaces;XML","invariant-based automatic testing;AJAX-based Web 2.0 application;stateful asynchronous client-server communication;client-side runtime manipulation;state-flow graph;user interface;AJAX-specific fault identification;DOM-tree invariant;fault detection;generic invariant checking component;application-specific state validator;fault revealing capability","","53","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Generating annotated behavior models from end-user scenarios","C. Damas; B. Lambeau; P. Dupont; A. van Lamsweerde","Dept. of Compt. Sci. & Eng., Univ. Catholique de Louvain, Belgium; Dept. of Compt. Sci. & Eng., Univ. Catholique de Louvain, Belgium; Dept. of Compt. Sci. & Eng., Univ. Catholique de Louvain, Belgium; Dept. of Compt. Sci. & Eng., Univ. Catholique de Louvain, Belgium","IEEE Transactions on Software Engineering","","2005","31","12","1056","1073","Requirements-related scenarios capture typical examples of system behaviors through sequences of desired interactions between the software-to-be and its environment. Their concrete, narrative style of expression makes them very effective for eliciting software requirements and for validating behavior models. However, scenarios raise coverage problems as they only capture partial histories of interaction among-system component instances. Moreover, they often leave the actual requirements implicit. Numerous efforts have therefore been made recently to synthesize requirements or behavior models inductively from scenarios. Two problems arise from those efforts. On the one hand, the, scenarios must be complemented with additional input such as state assertions along episodes or flowcharts on such episodes. This makes such techniques difficult to use by the nonexpert end-users who provide the scenarios. On the other hand, the generated state machines may be hard to understand as their nodes generally convey no domain- specific properties. Their validation by analysts, complementary to model checking and animation by may therefore be quite difficult. This paper describes tool-supported techniques that overcome those two problems. Our tool generates a labeled transition system (LTS) for each system component from simple forms of message sequence charts (MSC) taken as examples or counterexamples of desired behavior. No additional input is required. A global LTS for the entire system is synthesized first. This LTS covers all scenario examples and excludes all counterexamples. It is inductively generated through an interactive procedure that extends known learning techniques for grammar induction. The procedure is incremental on training examples. It interactively produces additional scenarios that the end-user has to classify as examples or counterexamples of desired behavior. The LTS synthesis procedure may thus also be used independently for requirements elicitation through scenario questions generated by the tool. The synthesized system LTS is then projected on local LTS for each system component. For model validation by analysts, the tool generates state invariants that decorate the nodes of the local LTS.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.138","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566607","Index Terms- Scenario-based elicitation;synthesis of behavior models;scenario generation;invariant generation;labeled transition systems;message sequence charts;model validation;incremental learning;analysis tools.","Concrete;Flowcharts;Induction generators;Application software;History;Animation;Independent component analysis;Documentation;Production;Control systems","formal verification;learning (artificial intelligence);flowcharting;grammars;Petri nets","annotated behavior model;end-user scenarios;software requirement;flowcharts;state machines;domain- specific property;model checking;computer animation;labeled transition system;message sequence charts;learning technique;grammar induction;incremental training;system component;model validation by analysts","","49","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Static analysis and dynamic steering of time-dependent systems","E. Vicario","Dept. of Syst. & Inf., Florence Univ., Italy","IEEE Transactions on Software Engineering","","2001","27","8","728","748","An enumerative technique is presented which supports reachability and timeliness analysis of time-dependent models. The technique assumes a dense model of time and uses equivalence classes to enable discrete and compact enumeration of the state space. Properties of timed reachability among states are recovered through the analysis of timing constraints embedded within equivalence classes. In particular, algorithms are given to evaluate a tight profile for the set of feasible timings of any untimed run. Runtime refinement of static profiles supports a mixed static/dynamic strategy in the development of a failure avoidance mechanism for dynamic acceptance and a guarantee of hard real-time processes.","0098-5589;1939-3520;2326-3881","","10.1109/32.940727","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=940727","","Real time systems;Timing;Scheduling;Runtime;Testing;Polynomials;Time factors;Computer Society;State-space methods;Petri nets","program diagnostics;reachability analysis;equivalence classes;real-time systems;Petri nets","static analysis;dynamic steering;time-dependent systems;enumerative technique;reachability;timeliness analysis;time-dependent models;dense model;equivalence classes;compact enumeration;state space;timed reachability;timing constraints;tight profile;feasible timings;untimed run;runtime refinement;static profiles;mixed static/dynamic strategy;failure avoidance mechanism;dynamic acceptance;hard real-time processes","","67","","48","","","","","","IEEE","IEEE Journals & Magazines"
"How Developers' Experience and Ability Influence Web Application Comprehension Tasks Supported by UML Stereotypes: A Series of Four Experiments","F. Ricca; M. Di Penta; M. Torchiano; P. Tonella; M. Ceccato","University of Genova, Italy; University of Sannio, Benevento; Politecnico di Torino, Torino; Fondazione Bruno Kessler, Trento; Fondazione Bruno Kessler, Trento","IEEE Transactions on Software Engineering","","2010","36","1","96","118","In recent years, several design notations have been proposed to model domain-specific applications or reference architectures. In particular, Conallen has proposed the UML Web Application Extension (WAE): a UML extension to model Web applications. The aim of our empirical investigation is to test whether the usage of the Conallen notation supports comprehension and maintenance activities with significant benefits, and whether such benefits depend on developers ability and experience. This paper reports and discusses the results of a series of four experiments performed in different locations and with subjects possessing different experience-namely, undergraduate students, graduate students, and research associates-and different ability levels. The experiments aim at comparing performances of subjects in comprehension tasks where they have the source code complemented either by standard UML diagrams or by diagrams stereotyped using the Conallen notation. Results indicate that, although, in general, it is not possible to observe any significant benefit associated with the usage of stereotyped diagrams, the availability of stereotypes reduces the gap between subjects with low skill or experience and highly skilled or experienced subjects. Results suggest that organizations employing developers with low experience can achieve a significant performance improvement by adopting stereotyped UML diagrams for Web applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5332231","Documentation;maintenance;and enhancement;software engineering;software/software engineering.","Unified modeling language;Application software;Object oriented modeling;Software maintenance;Web pages;Software engineering;Computer Society;Computer architecture;Service oriented architecture;Testing","Internet;Unified Modeling Language","Web application comprehension tasks;UML stereotypes;source code;stereotyped diagrams","","41","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Incremental integration testing of concurrent programs","P. V. Koppol; R. H. Carver; Kuo-Chung Tai","High Speed Networks Res. Dept., Lucent Technol. Bell Labs, Holmdel, NJ, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","6","607","623","We present a method for selecting test sequences for concurrent programs from labeled transitions systems (LTS). A common approach to selecting test sequences from a set of LTSs is to derive a global LTS, called the reachability graph, and then force deterministic program executions according to paths selected from the graph. However, using a reachability graph for test path selection introduces a state explosion problem. To overcome this problem, a reduced graph can be generated using incremental reachability analysis, which consists of repeatedly generating a reachability graph for a subset of LTSs, reducing this graph, and using the reduced graph in place of the original LTSs. Unfortunately, existing incremental reachability analysis techniques generate reduced graphs with insufficient information for deterministic testing. We present an incremental approach to testing concurrent programs. Incremental testing consists of incremental reachability analysis for test path selection and deterministic testing for test execution. We define a new type of reachability graph for incremental analysis, called an annotated labeled transition system (ALTS). An ALTS is an LTS annotated with information necessary for deterministic testing. We propose practical coverage criteria for selecting tests paths from an ALTS and present an ALTS reduction algorithm. The results of several case studies are reported.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1010062","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1010062","","Reachability analysis;System testing;Concrete;Explosions;Specification languages;Performance evaluation;Carbon capture and storage","parallel programming;program testing;reachability analysis","incremental integration testing;concurrent program testing;test sequences;labeled transitions systems;global LTS;reachability graph;deterministic program executions;test path selection;state explosion problem;reduced graph;incremental reachability analysis techniques;deterministic testing;test execution;annotated labeled transition system;ALTS reduction algorithm;structural testing;incremental testing","","21","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Trustrace: Mining Software Repositories to Improve the Accuracy of Requirement Traceability Links","N. Ali; Y. Guéhéneuc; G. Antoniol","École Polytechnique de Montréal, Montréal; École Polytechnique de Montréal, Montréal; École Polytechnique de Montréal, Montréal","IEEE Transactions on Software Engineering","","2013","39","5","725","741","Traceability is the only means to ensure that the source code of a system is consistent with its requirements and that all and only the specified requirements have been implemented by developers. During software maintenance and evolution, requirement traceability links become obsolete because developers do not/cannot devote effort to updating them. Yet, recovering these traceability links later is a daunting and costly task for developers. Consequently, the literature has proposed methods, techniques, and tools to recover these traceability links semi-automatically or automatically. Among the proposed techniques, the literature showed that information retrieval (IR) techniques can automatically recover traceability links between free-text requirements and source code. However, IR techniques lack accuracy (precision and recall). In this paper, we show that mining software repositories and combining mined results with IR techniques can improve the accuracy (precision and recall) of IR techniques and we propose Trustrace, a trust--based traceability recovery approach. We apply Trustrace on four medium-size open-source systems to compare the accuracy of its traceability links with those recovered using state-of-the-art IR techniques from the literature, based on the Vector Space Model and Jensen-Shannon model. The results of Trustrace are up to 22.7 percent more precise and have 7.66 percent better recall values than those of the other techniques, on average. We thus show that mining software repositories and combining the mined data with existing results from IR techniques improves the precision and recall of requirement traceability links.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.71","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6341764","Traceability;requirements;feature;source code;repositories;experts;trust-based model","Accuracy;Data mining;Software maintenance;Information retrieval;Open source software;Principal component analysis","data mining;data privacy;information retrieval;software maintenance","software repository mining;requirement traceability link;traceability method;software maintenance;software evolution;information retrieval technique;IR technique;precision accuracy;recall accuracy;Trustrace approach;trust-based traceability recovery approach;medium-size open-source system;vector space model;Jensen-Shannon model","","29","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis and testing of programs with exception handling constructs","S. Sinha; M. J. Harrold","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","9","849","871","Analysis techniques, such as control flow, data flow, and control dependence, are used for a variety of software engineering tasks, including structural and regression testing, dynamic execution profiling, static and dynamic slicing, and program understanding. To be applicable to programs in languages such as Java and C++, these analysis techniques must account for the effects of exception occurrences and exception handling constructs; failure to do so can cause the analysis techniques to compute incorrect results and, thus, limit the usefulness of the applications that use them. This paper discusses the effects of exception handling constructs on several analysis techniques. The paper presents techniques to construct representations for programs with explicit exception occurrences-exceptions that are raised explicitly through throw statements-and exception handling constructs. The paper presents algorithms that use these representations to perform the desired analyses. The paper also discusses several software engineering applications that use these analyses. Finally, the paper describes empirical results pertaining to the occurrence of exception handling constructs in Java programs and their effect on some analysis tasks.","0098-5589;1939-3520;2326-3881","","10.1109/32.877846","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877846","","Java;Performance analysis;Information analysis;Software testing;Failure analysis;Software engineering;Cause effect analysis;Data analysis;Frequency;Computer Society","program testing;exception handling;software engineering;Java;C++ language;data flow analysis;program slicing;object-oriented programming","program testing;program analysis;exception handling constructs;Java;C++;control flow analysis;data flow analysis;control dependence analysis;throw statements;software engineering","","59","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic test generation: a use case driven approach","C. Nebut; F. Fleurey; Y. Le Traon; J. -. Jezequel","LIRMM, Montpellier, France; LIRMM, Montpellier, France; LIRMM, Montpellier, France; LIRMM, Montpellier, France","IEEE Transactions on Software Engineering","","2006","32","3","140","155","Use cases are believed to be a good basis for system testing. Yet, to automate the test generation process, there is a large gap to bridge between high-level use cases and concrete test cases. We propose a new approach for automating the generation of system test scenarios in the context of object-oriented embedded software, taking into account traceability problems between high-level views and concrete test case execution. Starting from a formalization of the requirements based on use cases extended with contracts, we automatically build a transition system from which we synthesize test cases. Our objective is to cover the system in terms of statement coverage with those generated tests: an empirical evaluation of our approach is given based on this objective and several case studies. We briefly discuss the experimental deployment of our approach in the field at Thales Airborne Systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610607","Use case;test generation;scenarios;contracts;UML.","Automatic testing;Computer aided software engineering;System testing;Object oriented modeling;Contracts;Software testing;Unified modeling language;Embedded software;Concrete;Costs","program testing;automatic test pattern generation;object-oriented programming;program diagnostics;embedded systems;formal specification;Unified Modeling Language","automatic test generation;use case driven approach;system testing;object-oriented embedded software;traceability problem;high-level view;concrete test case execution;requirements formalization;Thales Airborne Systems;UML","","120","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Improving the Usability of E-Commerce Applications using Business Processes","Y. Zou; Q. Zhang; X. Zhao","IEEE Computer Society; NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","837","855","E-commerce applications automate many daily business activities. Users interact with e-commerce applications through menu-driven User Interface (Ul) components such as toolbars, dialogs, and windows. However, the tremendous number of functionalities may overwhelm the users. Users struggle to locate the appropriate Ul components to accomplish the tasks required by business processes. In this paper, we enhance e-commerce applications by improving their usability using the knowledge embedded in business process definitions. Our improved application provides contextual information to fulfill each business task. The improved application guides users through the various tasks in a step-by-step fashion. Through a controlled experiment, we demonstrate that our improved application offers a better usability experience for novice users by giving them more guidance and reducing the time needed to locate the next Ul component in a complex Ul.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70709","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359465","Graphical User Interface;User Interface Reengineering;Business Process;Process Definition;and Usability;Graphical User Interface;User Interface Reengineering;Business Process;Process Definition;and Usability","Usability;Application software;Books;User interfaces;Business process re-engineering;Computer architecture;Navigation;Computer Society;Credit cards;Printing","electronic commerce;graphical user interfaces;user interface management systems","e-commerce application usability;business process task;menu-driven user interface component;contextual information","","10","","74","","","","","","IEEE","IEEE Journals & Magazines"
"A comparison of software project overruns - flexible versus sequential development models","K. Molokken-Ostvold; M. Jorgensen","Simula Res. Lab., Lysaker, Norway; Simula Res. Lab., Lysaker, Norway","IEEE Transactions on Software Engineering","","2005","31","9","754","766","Flexible software development models, e.g., evolutionary and incremental models, have become increasingly popular. Advocates claim that among the benefits of using these models is reduced overruns, which is one of the main challenges of software project management. This paper describes an in-depth survey of software development projects. The results support the claim that projects which employ a flexible development model experience less effort overruns than do those which employ a sequential model. The reason for the difference is not obvious. We found, for example, no variation in project size, estimation process, or delivered proportion of planned functionality between projects applying different types of development model. When the managers were asked to provide reasons for software overruns and/or estimation accuracy, the largest difference was that more of flexible projects than sequential projects cited good requirement specifications-and good collaboration/communication with clients as contributing to accurate estimates.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.96","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514444","Index Terms- Cost estimation;management;project control and modeling;software development models.","Project management;Programming;Costs;Collaborative software;Software engineering;Software development management;Communication system control;Software performance;Contracts;Productivity","project management;formal specification;formal verification;software cost estimation;software development management;software process improvement","software project overruns;software development model;software project management;formal specifications;software cost estimation;project control","","42","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Selecting Best Practices for Effort Estimation","T. Menzies; Z. Chen; J. Hihn; K. Lum","Department of Computer Science, West Virginia University, Morgantown, WV 26506-610; 941 W. 37th Place, SAL 337, Los Angeles, CA 90089; Jet Propulsion Laboratory, 4800 Oak Grove Drive, Pasadena, CA 91109-8099; Jet Propulsion Laboratory, 4800 Oak Grove Drive, Pasadena, CA 91109-8099","IEEE Transactions on Software Engineering","","2006","32","11","883","895","Effort estimation often requires generalizing from a small number of historical projects. Generalization from such limited experience is an inherently underconstrained problem. Hence, the learned effort models can exhibit large deviations that prevent standard statistical methods (e.g., t-tests) from distinguishing the performance of alternative effort-estimation methods. The COSEEKMO effort-modeling workbench applies a set of heuristic rejection rules to comparatively assess results from alternative models. Using these rules, and despite the presence of large deviations, COSEEKMO can rank alternative methods for generating effort models. Based on our experiments with COSEEKMO, we advise a new view on supposed ""best practices"" in model-based effort estimation: 1) Each such practice should be viewed as a candidate technique which may or may not be useful in a particular domain, and 2) tools like COSEEKMO should be used to help analysts explore and select the best method for a particular domain","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.114","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015511","Model-based effort estimation;COCOMO;deviation;data mining.","Best practices;Testing;Statistical analysis;Data mining;Predictive models;Humans;Guidelines;Software safety;Estimation error;Linear regression","data mining;project management;software cost estimation;statistical analysis","effort estimation method;standard statistical method;COSEEKMO toolkit;heuristic rejection rule","","103","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Proactive and Reactive Runtime Service Discovery: A Framework and Its Evaluation","A. Zisman; G. Spanoudakis; J. Dooley; I. Siveroni","City University London, London; City University London, London; University of Essex, Colchester; City University London, London","IEEE Transactions on Software Engineering","","2013","39","7","954","974","The identification of services during the execution of service-based applications to replace services in them that are no longer available and/or fail to satisfy certain requirements is an important issue. In this paper, we present a framework to support runtime service discovery. This framework can execute service discovery queries in pull and push mode. In pull mode, it executes queries when a need for finding a replacement service arises. In push mode, queries are subscribed to the framework to be executed proactively and, in parallel with the operation of the application, to identify adequate services that could be used if the need for replacing a service arises. Hence, the proactive (push) mode of query execution makes it more likely to avoid interruptions in the operation of service-based applications when a service in them needs to be replaced at runtime. In both modes of query execution, the identification of services relies on distance-based matching of structural, behavioral, quality, and contextual characteristics of services and applications. A prototype implementation of the framework has been developed and an evaluation was carried out to assess the performance of the framework. This evaluation has shown positive results, which are discussed in the paper.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.84","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6375710","Web-services discovery;composite web services;context-aware QoS model;application development in services","Runtime;Context;Servers;Educational institutions;Database languages;Unified modeling language;Informatics","quality of service;query processing;software quality;ubiquitous computing;Web services","proactive runtime service discovery;reactive runtime service discovery;service identification;service-based applications;service discovery queries execution;pull mode;push mode;replacement service;distance-based matching;structural characteristics;behavioral characteristics;quality characteristics;contextual characteristics;composite Web-services discovery;context-aware QoS model","","16","","62","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Study on Views of Importance of Change Impact Analysis Issues","P. Rovegård; L. Angelis; C. Wohlin","Ericsson AB, Karlskrona; Aristotle University of Thessaloniki, Thessaloniki; Blekinge Institute of Technology, Ronneby","IEEE Transactions on Software Engineering","","2008","34","4","516","530","Change impact analysis is a change management activity that previously has been studied much from a technical perspective. For example, much work focuses on methods for determining the impact of a change. In this paper, we present results from a study on the role of impact analysis in the change management process. In the study, impact analysis issues were prioritised with respect to criticality by software professionals from an organisational perspective and a self-perspective. The software professionals belonged to three organisational levels: operative, tactical and strategic. Qualitative and statistical analyses with respect to differences between perspectives as well as levels are presented. The results show that important issues for a particular level are tightly related to how the level is defined. Similarly, issues important from an organisational perspective are more holistic than those important from a self-perspective. However, our data indicate that the self-perspective colours the organisational perspective, meaning that personal opinions and attitudes cannot easily be disregarded. In comparing the perspectives and the levels, we visualise the differences in a way that allow us to discuss two classes of issues: high-priority and medium-priority. The most important issues from this point of view concern fundamental aspects of impact analysis and its execution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527253","General;Configuration management process;Qualitative process analysis;General;Configuration management process;Qualitative process analysis","Software systems;Statistical analysis;Data visualization;Life estimation;Programming;Algorithm design and analysis;Automatic control;Process control;Telecommunication control;Engineering management","management of change;software development management","change impact analysis;change management;software professionals;organisational perspective","","45","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Whitening SOA Testing via Event Exposure","C. Ye; H. Jacobsen","University of Toronto, Toronto; University of Toronto, Toronto","IEEE Transactions on Software Engineering","","2013","39","10","1444","1465","Whitening the testing of service-oriented applications can provide service consumers confidence on how well an application has been tested. However, to protect business interests of service providers and to prevent information leakage, the implementation details of services are usually invisible to service consumers. This makes it challenging to determine the test coverage of a service composition as a whole and design test cases effectively. To address this problem, we propose an approach to whiten the testing of service compositions based on events exposed by services. By deriving event interfaces to explore only necessary test coverage information from service implementations, our approach allows service consumers to determine test coverage based on selected events exposed by services at runtime without releasing the service implementation details. We also develop an approach to design test cases effectively based on event interfaces concerning both effectiveness and information leakage. The experimental results show that our approach outperforms existing testing approaches for service compositions with up to 49 percent more test coverage and an up to 24 percent higher fault-detection rate. Moreover, our solution can trade off effectiveness, efficiency, and information leakage for test case generation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6495456","Web service composition;white-box testing;event interface;events","Testing;Service-oriented architecture;Books;Runtime;Catalogs;Jacobian matrices","program testing;service-oriented architecture;Web services","SOA testing whitening;service-oriented architecture;event exposure;service consumers;service providers;service composition;test coverage;test case design approach;fault-detection rate;test case generation;information leakage;Web services","","15","","84","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical analysis of c preprocessor use","M. D. Ernst; G. J. Badros; D. Notkin","Dept. of Electr. Eng. & Comput. Sci., MIT, Cambridge, MA, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","12","1146","1170","This is the first empirical study of the use of the C macro preprocessor, Cpp. To determine how the preprocessor is used in practice, this paper analyzes 26 packages comprising 1.4 million lines of publicly available C code. We determine the incidence of C preprocessor usage-whether in macro definitions, macro uses, or dependences upon macros-that is complex, potentially problematic, or inexpressible in terms of other C or C++ language features. We taxonomize these various aspects of preprocessor use and particularly note data that are material to the development of tools for C or C++, including translating from C to C++ to reduce preprocessor usage. Our results show that, while most Cpp usage follows fairly simple patterns, an effective program analysis tool must address the preprocessor. The intimate connection between the C programming language and Cpp, and Cpp's unstructured transformations of token streams often hinder both programmer understanding of C programs and tools built to engineer C programs, such as compilers, debuggers, call graph extractors, and translators. Most tools make no attempt to analyze macro usage, but simply preprocess their input, which results in a number of negative consequences; an analysis that takes Cpp into account is preferable, but building such tools requires an understanding of actual usage. Differences between the semantics of Cpp and those of C can lead to subtle bugs stemming from the use of the preprocessor, but there are no previous reports of the prevalence of such errors. Use of C++ can reduce some preprocessor usage, but such usage has not been previously measured. Our data and analyses shed light on these issues and others related to practical understanding or manipulation of real C programs. The results are of interest to language designers, tool writers, programmers, and software engineers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158288","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158288","","Programming profession;Packaging;Pattern analysis;Computer languages;Program processors;Data preprocessing;Computer bugs;Data analysis;Software tools;Design engineering","program processors;C language","empirical analysis;C preprocessor;Cpp;C code;C++;program analysis tool;tool writers;software engineers","","70","","40","","","","","","IEEE","IEEE Journals & Magazines"
"An atomicity-generating protocol for anonymous currencies","L. J. Camp","Sch. of Gov., Harvard Univ., Cambridge, MA, USA","IEEE Transactions on Software Engineering","","2001","27","3","272","278","Atomicity is necessary for reliable electronic commerce transactions. Anonymity is also an issue of great importance not only to designers of commerce systems, but also to those concerned with the societal effects of information technologies, providing atomicity and anonymity is not trivial. Reliable systems, which provide highly atomic transactions, offer limited anonymity. Many anonymous systems (Rivest and Shamir, 1996) do not offer anonymous reliable transactions (Yee, 1994). Three basic approaches have been used: secure hardware for trusted record-keeping (Brands, 1993), storage of identity information with trustees for conditional anonymity (Low et al., 1993) or by providing dispute resolution only with the removal of anonymity (Chaum, 1988). In this work, the problem of anonymous atomic transactions for a generic token currency is solved using distributed trust and with the assumption that any single party may be corrupt. Defined is a transaction to include the provision of information goods or a contract to deliver specified goods, allowing for the highest degree of atomicity. The cryptographic strength of the atomicity guarantee can be made to the user's specification on a per transaction basis. The atomicity-generating protocol includes provision for dispute resolution and anonymous refunds. Also illustrated, is that any electronic token currency can be made reliable with the addition of this atomicity-generating protocol.","0098-5589;1939-3520;2326-3881","","10.1109/32.910862","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910862","","Electronic commerce;Privacy;Business;Cryptographic protocols;Protection;Information technology;Hardware;Secure storage;Contracts;Cryptography","electronic commerce;cryptography;data privacy;protocols;transaction processing","atomicity generating protocol;anonymous currencies;electronic commerce;social effects;atomic transactions;anonymous systems;trusted record-keeping;identity information;dispute resolution;anonymous atomic transactions;generic token currency;distributed trust;cryptography;anonymous refunds;electronic token currency","","2","","28","","","","","","IEEE","IEEE Journals & Magazines"
"A Semi-Automatic Approach for Extracting Software Product Lines","M. T. Valente; V. Borges; L. Passos","University of Minas Gerais, Belo Horizonte; COTEMIG, Brazil; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2012","38","4","737","754","The extraction of nontrivial software product lines (SPL) from a legacy application is a time-consuming task. First, developers must identify the components responsible for the implementation of each program feature. Next, they must locate the lines of code that reference the components discovered in the previous step. Finally, they must extract those lines to independent modules or annotate them in some way. To speed up product line extraction, this paper describes a semi-automatic approach to annotate the code of optional features in SPLs. The proposed approach is based on an existing tool for product line development, called CIDE, that enhances standard IDEs with the ability to associate background colors with the lines of code that implement a feature. We have evaluated and successfully applied our approach to the extraction of optional features from three nontrivial systems: Prevayler (an in-memory database system), JFreeChart (a chart library), and ArgoUML (a UML modeling tool).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928352","Software product lines;virtual separation of concerns;refactoring tools;annotations","Feature extraction;Color;Image color analysis;Multithreading;Semantics;Software;Context","feature extraction;product development;software maintenance;software reusability;Unified Modeling Language","semiautomatic approach;software product lines extraction;SPL;legacy application;program feature;code lines localization;optional feature code annotation;product line development;CIDE;background colors;optional feature extraction;Prevayler nontrivial systems;JFreeChart nontrivial systems;ArgoUML nontrivial systems","","11","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Using version control data to evaluate the impact of software tools: a case study of the Version Editor","D. L. Atkins; T. Ball; T. L. Graves; A. Mockus","Oregon Univ., Eugene, OR, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","7","625","637","Software tools can improve the quality and maintainability of software, but are expensive to acquire, deploy, and maintain, especially in large organizations. We explore how to quantify the effects of a software tool once it has been deployed in a development environment. We present an effort-analysis method that derives tool usage statistics and developer actions from a project's change history (version control system) and uses a novel effort estimation algorithm to quantify the effort savings attributable to tool usage. We apply this method to assess the impact of a software tool called VE, a version-sensitive editor used in Bell Labs. VE aids software developers in coping with the rampant use of certain preprocessor directives (similar to #if/#endif in C source files). Our analysis found that developers were approximately 40 percent more productive when using VE than when using standard text editors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019478","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019478","","Software tools;Computer aided software engineering;Software maintenance;Control systems;History;Software quality;Statistics;Standards development;Control system analysis;Statistical analysis","configuration management;software tools;software metrics;text editing;software quality","version control data;software tool impact evaluation;Version Editor;software quality;software maintainability;large organizations;software effort analysis method;VE tool;preprocessor directives;C source files;text editors;tool usage statistics;change history","","34","","32","","","","","","IEEE","IEEE Journals & Magazines"
"A Developer Centered Bug Prediction Model","D. Di Nucci; F. Palomba; G. De Rosa; G. Bavota; R. Oliveto; A. De Lucia","University of Salerno, Fisciano, SA, Italy; University of Salerno, Fisciano, SA, Italy; University of Salerno, Fisciano, SA, Italy; Università della Svizzera Italiana (USI), Lugano, Switzerland; University of Molise, Pesche (IS), Campobasso, Italy; University of Salerno, Fisciano, SA, Italy","IEEE Transactions on Software Engineering","","2018","44","1","5","24","Several techniques have been proposed to accurately predict software defects. These techniques generally exploit characteristics of the code artefacts (e.g., size, complexity, etc.) and/or of the process adopted during their development and maintenance (e.g., the number of developers working on a component) to spot out components likely containing bugs. While these bug prediction models achieve good levels of accuracy, they mostly ignore the major role played by human-related factors in the introduction of bugs. Previous studies have demonstrated that focused developers are less prone to introduce defects than non-focused developers. According to this observation, software components changed by focused developers should also be less error prone than components changed by less focused developers. We capture this observation by measuring the scattering of changes performed by developers working on a component and use this information to build a bug prediction model. Such a model has been evaluated on 26 systems and compared with four competitive techniques. The achieved results show the superiority of our model, and its high complementarity with respect to predictors commonly used in the literature. Based on this result, we also show the results of a “hybrid” prediction model combining our predictors with the existing ones.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2659747","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7835258","Scattering metrics;bug prediction;empirical study;mining software repositories","Measurement;Computer bugs;Predictive models;Complexity theory;Scattering;Entropy;Software","object-oriented programming;program debugging;software maintenance;software reliability","software components;hybrid prediction model;code artefacts;software defects prediction;developer centered bug prediction model","","5","","54","","","","","","IEEE","IEEE Journals & Magazines"
"On comparisons of random, partition, and proportional partition testing","S. C. Ntafos","Comput. Sci. Prog., Texas Univ. at Dallas, Richardson, TX, USA","IEEE Transactions on Software Engineering","","2001","27","10","949","960","Early studies of random versus partition testing used the probability of detecting at least one failure as a measure of test effectiveness and indicated that partition testing is not significantly more effective than random testing. More recent studies have focused on proportional partition testing because a proportional allocation of the test cases (according to the probabilities of the subdomains) can guarantee that partition testing will perform at least as well as random testing. We show that this goal for partition testing is not a worthwhile one. Guaranteeing that partition testing has at least as high a probability of detecting a failure comes at the expense of decreasing its relative advantage over random testing. We then discuss other problems with previous studies and show that failure to include important factors (cost, relative effectiveness) can lead to misleading results.","0098-5589;1939-3520;2326-3881","","10.1109/32.962563","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=962563","","Software testing;Performance evaluation;Costs;Helium;Genetic mutations;Failure analysis;Information analysis;Fault detection;Sampling methods;Statistical analysis","program testing;software reliability","random testing;partition testing;proportional partition testing;probability;test effectiveness;test cases;software failure detection;program testing;software reliability","","32","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Model-Based Self-Aware Performance and Resource Management Using the Descartes Modeling Language","N. Huber; F. Brosig; S. Spinner; S. Kounev; M. Bähr","Department of Computer Science, Chair of Software Engineering, University of Würzburg, Würzburg, Germany; Department of Computer Science, Chair of Software Engineering, University of Würzburg, Würzburg, Germany; Department of Computer Science, Chair of Software Engineering, University of Würzburg, Würzburg, Germany; Department of Computer Science, Chair of Software Engineering, University of Würzburg, Würzburg, Germany; Blue Yonder GmbH & Co. KG., Karlsruhe, Germany","IEEE Transactions on Software Engineering","","2017","43","5","432","452","Modern IT systems have increasingly distributed and dynamic architectures providing flexibility to adapt to changes in the environment and thus enabling higher resource efficiency. However, these benefits come at the cost of higher system complexity and dynamics. Thus, engineering systems that manage their end-to-end application performance and resource efficiency in an autonomic manner is a challenge. In this article, we present a holistic model-based approach for self-aware performance and resource management leveraging the Descartes Modeling Language (DML), an architecture-level modeling language for online performance and resource management. We propose a novel online performance prediction process that dynamically tailors the model solving depending on the requirements regarding accuracy and overhead. Using these prediction capabilities, we implement a generic model-based control loop for proactive system adaptation. We evaluate our model-based approach in the context of two representative case studies showing that with the proposed methods, significant resource efficiency gains can be achieved while maintaining performance requirements. These results represent the first end-to-end validation of our approach, demonstrating its potential for self-aware performance and resource management in the context of modern IT systems and infrastructures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2613863","Deutsche Forschungsgemeinschaft (DFG); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7577879","Autonomic;self-aware;adaptation;model-based;modeling language;performance;efficiency","Adaptation models;Resource management;Computer architecture;Predictive models;Unified modeling language;Software;Dynamic scheduling","software architecture;software performance evaluation","model-based self-aware performance;resource management;Descartes modeling language;IT systems;dynamic architectures;distributed architectures;system complexity;engineering systems;end-to-end application performance;resource efficiency;holistic model-based approach;DML;architecture-level modeling language;online performance prediction process;generic model-based control loop;proactive system adaptation;resource efficiency gains","","13","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Targeted Scrum: Applying Mission Command to Agile Software Development","D. P. Harvie; A. Agah","Department of Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS; Department of Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS","IEEE Transactions on Software Engineering","","2016","42","5","476","489","Software engineering and mission command are two separate but similar fields, as both are instances of complex problem solving in environments with ever changing requirements. Our research hypothesis is that modifications to agile software development based on inspirations from mission command can improve the software engineering process in terms of planning, prioritizing, and communication of software requirements and progress, as well as improving the overall software product. Targeted Scrum is a modification of Traditional Scrum based on three inspirations from Mission Command: End State, Line of Effort, and Targeting. These inspirations have led to the introduction of the Product Design Meeting and modifications of some current Scrum meetings and artifacts. We tested our research hypothesis using a semester-long undergraduate level software engineering class. Students developed two software projects, one using Traditional Scrum and the other using Targeted Scrum. We then assessed how well both methodologies assisted the software development teams in planning and developing the software architecture, prioritizing requirements, and communicating progress. We also evaluated the software product produced by both methodologies. We found that Targeted Scrum did better in assisting the software development teams in the planning and prioritization of the requirements. However, Targeted Scrum had a negligible effect on improving the software development teams external and internal communications. Finally, Targeted Scrum did not have an impact on the product quality by the top performing and worst performing teams. Targeted Scrum did assist the product quality of the teams in the middle of the performance spectrum.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2489654","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7296686","Scrum, Mission Command;Line of Effort;Product Design Meeting;Agile;Empirical Software Engineering;Scrum;mission command;line of effort;product design meeting;agile;empirical software engineering","Software;Planning;Scrum (Software development);Product design;Software engineering;Force","software architecture;software prototyping","targeted Scrum;mission command;agile software development;software engineering;software requirements;software product improvement;end state;line of effort;targeting;product design meeting;software projects;traditional Scrum;software architecture;product quality;performance spectrum","","4","","34","","","","","","IEEE","IEEE Journals & Magazines"
"A dynamic coordination policy for software system construction","V. S. Mookerjee; I. R. Chiang","Sch. of Manage., Texas Univ., Richardson, TX, USA; NA","IEEE Transactions on Software Engineering","","2002","28","7","684","694","In constructing a software system, extended periods of coding without adequate coordination (such as system integration and testing) can result in considerable fault correction effort. On the other hand, too much coordination can also prove counterproductive by disrupting the smooth flow of development work. The goal, therefore, is to find an optimal level of coordination so as to minimize system construction effort while adhering to functionality and schedule constraints. Previous research, however, has not considered dynamic project factors such as system growth, system stability and team learning when addressing the above coordination problem. Dynamic factors are important because they could lead to differences in the intensity (frequency) of coordination needed at different stages of system construction. Unlike existing studies, we propose a dynamic coordination policy that places coordination activities at optimal (and often nonuniform) intervals during the construction of a system. Our analysis shows that, if a system stabilizes slowly, more intense coordination should occur early in the project. Also, if the team's knowledge of the system improves with time (i.e. learning effects are present), more intense coordination should occur both near the beginning and near the end of the project. Our analysis also shows that, by encouraging more frequent coordination, superior development tools could facilitate team learning. Finally, the application of the coordination model to data from a NASA software project demonstrates that optimally coordinating a project could significantly reduce the system construction cost.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019482","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019482","","Software systems;System testing;Project management;Programming;Productivity;Software development management;Software testing;Scheduling;Stability;Frequency","software development management;project management","dynamic coordination policy;software system construction;extended coding periods;system integration;system testing;fault correction effort;development workflow;optimal coordination level;system construction effort minimization;functionality constraints;schedule constraints;dynamic project factors;system growth;system stability;team learning;system stabilization;software development projects;team system knowledge;team learning effects;coordination intensity;software development tools;NASA software project;system construction cost;software project management","","19","","30","","","","","","IEEE","IEEE Journals & Magazines"
"The Role of the Tester's Knowledge in Exploratory Software Testing","J. Itkonen; M. V. Mäntylä; C. Lassenius","Aalto University School of Science, Espoo; Aalto University School of Science, Espoo; Aalto University School of Science, Espoo","IEEE Transactions on Software Engineering","","2013","39","5","707","724","We present a field study on how testers use knowledge while performing exploratory software testing (ET) in industrial settings. We video recorded 12 testing sessions in four industrial organizations, having our subjects think aloud while performing their usual functional testing work. Using applied grounded theory, we analyzed how the subjects performed tests and what type of knowledge they utilized. We discuss how testers recognize failures based on their personal knowledge without detailed test case descriptions. The knowledge is classified under the categories of domain knowledge, system knowledge, and general software engineering knowledge. We found that testers applied their knowledge either as a test oracle to determine whether a result was correct or not, or for test design, to guide them in selecting objects for test and designing tests. Interestingly, a large number of failures, windfall failures, were found outside the actual focus areas of testing as a result of exploratory investigation. We conclude that the way exploratory testers apply their knowledge for test design and failure recognition differs clearly from the test-case-based paradigm and is one of the explanatory factors of the effectiveness of the exploratory testing approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6298893","Software testing;exploratory testing;validation;test execution;test design;human factors;methods for SQA;and V&V","Software testing;Context;Software;Knowledge engineering;Observers;Organizations","program testing;software engineering","tester knowledge;exploratory software testing;ET;functional testing;grounded theory;personal knowledge;domain knowledge;system knowledge;general software engineering knowledge;test oracle;windfall failures;test design;failure recognition;test-case-based paradigm","","26","","76","","","","","","IEEE","IEEE Journals & Magazines"
"Software reflexion models: bridging the gap between design and implementation","G. C. Murphy; D. Notkin; K. J. Sullivan","Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada; NA; NA","IEEE Transactions on Software Engineering","","2001","27","4","364","380","The artifacts constituting a software system often drift apart over time. We have developed the software reflexion model technique to help engineers perform various software engineering tasks by exploiting, rather than removing, the drift between design and implementation. More specifically, the technique helps an engineer compare artifacts by summarizing where one artifact (such as a design) is consistent with and inconsistent with another artifact (such as source). The technique can be applied to help a software engineer evolve a structural mental model of a system to the point that it is ""good enough"" to be used for reasoning about a task at hand. The software reflexion model technique has been applied to support a variety of tasks, including design conformance, change assessment, and an experimental reengineering of the million-lines-of-code Microsoft Excel product. We provide a formal characterization of the reflexion model technique, discuss practical aspects of the approach, relate experiences of applying the approach and tools, and place the technique into the context of related work.","0098-5589;1939-3520;2326-3881","","10.1109/32.917525","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917525","","Design engineering;Software performance;Reverse engineering;Software systems;Software engineering;Maintenance engineering;Computer Society;Cognitive science;Spreadsheet programs;Context modeling","reverse engineering;systems re-engineering;software maintenance;formal specification","software reflexion models;software system;software engineering tasks;structural mental model;design conformance;change assessment;experimental reengineering;Microsoft Excel product;formal characterization;reverse engineering;program understanding;software structure;program representation;model differencing","","184","","39","","","","","","IEEE","IEEE Journals & Magazines"
"MobiGATE: a mobile computing middleware for the active deployment of transport services","Y. Zheng; A. T. S. Chan","Dept. of Comput. & Inf. Sci. & Eng., Florida Univ., Gainesville, FL, USA; NA","IEEE Transactions on Software Engineering","","2006","32","1","35","50","The use of gateway proxies is one important approach to facilitating adaptation across wireless and mobile environments. Importantly, augmented service entities deployed within the gateway proxy residing on the wired network can be composed and deployed to shield mobile clients from the effects of poor network characteristics. The usual approach to the static composition of service entities on the gateway proxy is to have these service entities interact with each other by explicitly invoking procedures on the named interface, but such a tight coupling of interfaces inhibits the flexible composition and adaptation of the service entities to the dynamic operating characteristics of wireless networks. In this paper, we present a mobile gateway for the active deployment of transport entities or, for short, MobiGATE (pronounced Mobi-Gate). MobiGATE is a mobile middleware framework that supports the robust and flexible composition of transport entities, known as streamlets. The flow of data traffic is subjected to processing by a chain of streamlets. Each streamlet encapsulates a service entity that adapts the flow of traffic across the wireless network. To facilitate the dynamic reconfiguration of the streamlets, we advocate applying the concept of coordination as the unifying approach to composing these transport service entities. Importantly, MobiGATE delineates a clear separation of interdependent parts from the service-specific computational codes of those service entities. It does this by using a separate coordination language, called MobiGATE coordination language (MCL), to describe the coordination among streamlet service entities. The complete design, implementation, and evaluation of the MobiGATE system are presented in this paper. Initial experimental results validate the flexibility of the coordination approach in promoting separation-of-concern in the reconfiguration of services, while achieving low computation and delay overheads.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583601","Mobile computing;coordination languages;adaptive middleware;dynamic reconfiguration;infrastructural proxies.","Mobile computing;Middleware;Wireless networks;Telecommunication traffic;Application software;Robustness;Delay;Wireless communication;Computer displays;Batteries","middleware;mobile computing;internetworking;telecommunication traffic","MobiGATE system;mobile computing middleware;gateway proxy;wireless environment;mobile environment;augmented service entity deployment;active transport service entity deployment;streamlet service entity;data traffic flow;MobiGATE coordination language;MCL;dynamic service reconfiguration","","5","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Systematic Elaboration of Scalability Requirements through Goal-Obstacle Analysis","L. Duboc; E. Letier; D. S. Rosenblum","State University of Rio de Janeiro, Rio de Janeiro; University College London, London; National University of Singapore, Singapore","IEEE Transactions on Software Engineering","","2013","39","1","119","140","Scalability is a critical concern for many software systems. Despite the recognized importance of considering scalability from the earliest stages of development, there is currently little support for reasoning about scalability at the requirements level. This paper presents a goal-oriented approach for eliciting, modeling, and reasoning about scalability requirements. The approach consists of systematically identifying scalability-related obstacles to the satisfaction of goals, assessing the likelihood and severity of these obstacles, and generating new goals to deal with them. The result is a consolidated set of requirements in which important scalability concerns are anticipated through the precise, quantified specification of scaling assumptions and scalability goals. The paper presents results from applying the approach to a complex, large-scale financial fraud detection system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6152130","Requirements/specifications;analysis;performance measures;quality analysis and evaluation;goal-oriented requirements engineering;KAOS;scalability","Scalability;Software;Batch production systems;Educational institutions;Analytical models;Natural languages","financial data processing;fraud;large-scale systems;reasoning about programs;systems analysis","systematic elaboration;goal-obstacle analysis;software systems;reasoning about scalability;scalability requirement elicitation;scalability requirement modeling;systematic scalability-related obstacle identification;goal satisfaction;complex large-scale financial fraud detection system;goal-oriented requirements engineering","","13","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Defect frequency and design patterns: an empirical study of industrial code","M. Vokac","Simula Res. Lab., Lysaker, Norway","IEEE Transactions on Software Engineering","","2004","30","12","904","917","Software ""design patterns"" seek to package proven solutions to design problems in a form that makes it possible to find, adapt, and reuse them. A common claim is that a design based on properly applied patterns will have fewer defects than more ad hoc solutions. This case study analyzes the weekly evolution and maintenance of a large commercial product (C++, 500,000 LOC) over three years, comparing defect rates for classes that participated in selected design patterns to the code at large. We found that there are significant differences in defect rates among the patterns, ranging from 63 percent to 154 percent of the average rate. We developed a new set of tools able to extract design pattern information at a rate of 3/spl times/10/sup 6/ lines of code per hour, with relatively high precision. Based on a qualitative analysis of the code and the nature of the patterns, we conclude that the Observer and Singleton patterns are correlated with larger code structures and, so, can serve as indicators of code that requires special attention. Conversely, code designed with the Factory pattern is more compact and possibly less closely coupled and, consequently, has lower defect numbers. The Template Method pattern was used in both simple and complex situations, leading to no clear tendency.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.99","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377188","Index Terms- Design patterns;defects;defect frequency;industrial code;case study;maintenance.","Frequency;Pattern analysis;Production facilities;Testing;Packaging;Lab-on-a-chip;Data mining;Computer architecture;User interfaces;Computer industry","object-oriented programming;software prototyping;software maintenance;software fault tolerance","software design patterns;industrial code;software evolution;software maintenance;lines of code;Observer patterns;Singleton patterns;Template Method pattern;defect frequency","","50","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Vulnerability Discovery with Attack Injection","J. Antunes; N. Neves; M. Correia; P. Verissimo; R. Neves","University of Lisboa, Lisboa; University of Lisboa, Lisboa; University of Lisboa, Lisboa; University of Lisboa, Lisboa; Technical University of Lisbon, Lisboa","IEEE Transactions on Software Engineering","","2010","36","3","357","370","The increasing reliance put on networked computer systems demands higher levels of dependability. This is even more relevant as new threats and forms of attack are constantly being revealed, compromising the security of systems. This paper addresses this problem by presenting an attack injection methodology for the automatic discovery of vulnerabilities in software components. The proposed methodology, implemented in AJECT, follows an approach similar to hackers and security analysts to discover vulnerabilities in network-connected servers. AJECT uses a specification of the server's communication protocol and predefined test case generation algorithms to automatically create a large number of attacks. Then, while it injects these attacks through the network, it monitors the execution of the server in the target system and the responses returned to the clients. The observation of an unexpected behavior suggests the presence of a vulnerability that was triggered by some particular attack (or group of attacks). This attack can then be used to reproduce the anomaly and to assist the removal of the error. To assess the usefulness of this approach, several attack injection campaigns were performed with 16 publicly available POP and IMAP servers. The results show that AJECT could effectively be used to locate vulnerabilities, even on well-known servers tested throughout the years.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5374427","Testing and debugging;software engineering;test design;testing tools;experimental evaluation;fault injection;attack injection.","Network servers;Software testing;Protocols;Debugging;Application software;Computer networks;Computer hacking;Communication system security;Automatic testing;Software engineering","computer crime;software engineering","vulnerability discovery;attack injection;networked computer systems;software components;security analysts;hackers analysts;AJECT;IMAP servers;POP servers","","24","","33","","","","","","IEEE","IEEE Journals & Magazines"
"On the use of clone detection for identifying crosscutting concern code","M. Bruntink; A. van Deursen; R. van Engelen; T. Tourwe","Dept. of Software Eng., CWI, Amsterdam, Netherlands; Dept. of Software Eng., CWI, Amsterdam, Netherlands; NA; NA","IEEE Transactions on Software Engineering","","2005","31","10","804","818","In systems developed without aspect-oriented programming, code implementing a crosscutting concern may be spread over many different parts of a system. Identifying such code automatically could be of great help during maintenance of the system. First of all, it allows a developer to more easily find the places in the code that must be changed when the concern changes and, thus, makes such changes less time consuming and less prone to errors. Second, it allows the code to be refactored to an aspect-oriented solution, thereby improving its modularity. In this paper, we evaluate the suitability of clone detection as a technique for the identification of crosscutting concerns. To that end, we manually identify five specific crosscutting concerns in an industrial C system and analyze to what extent clone detection is capable of finding them. We consider our results as a stepping stone toward an automated ""aspect miner"" based on clone detection.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.114","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542064","Index Terms- Clone detection;reverse engineering;aspect-oriented programming;crosscutting concerns;aspect mining.","Cloning;Scattering;Computer languages;Computer Society;Reverse engineering;Software systems;Programming;Automation;Application software","object-oriented programming;object-oriented methods;software maintenance;reverse engineering","clone detection;crosscutting concern code;system maintenance;aspect-oriented solution;industrial C system;aspect miner","","60","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Object analysis patterns for embedded systems","S. Konrad; B. H. C. Cheng; L. A. Campbell","Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA; Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA; Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA","IEEE Transactions on Software Engineering","","2004","30","12","970","992","Some of the most challenging tasks in building a software system are capturing, refining, and analyzing requirements. How well these tasks are performed significantly impacts the quality of the developed software system. The difficulty of these tasks is greatly exacerbated for the software of embedded systems as these systems are commonly used for critical applications, have to operate reliably for long periods of time, and usually have a high degree of complexity. Current embedded systems software development practice, however, often deals with the (requirements) analysis phase in a superficial manner, instead emphasizing design and implementation. This research investigates how an approach similar to the well-known design patterns, termed object analysis patterns, can be applied in the analysis phase of embedded systems development, prior to design and coding. Specifically, our research explores how object-oriented modeling notations, such as the Unified Modeling Language (UML), can be used to represent structural and behavioral information as part of commonly occurring object analysis patterns. This work also investigates how UML-based conceptual models of embedded systems, based on the diagram templates in the object analysis patterns, can be automatically analyzed using the Spin model checker for adherence to properties specified in linear-time temporal logic (LTL) using a previously developed UML formalization framework. We have applied these patterns to several embedded systems applications obtained from the automotive industry. This paper describes one of our case studies and illustrates how our approach facilitates the construction of UML-based conceptual models of embedded systems and the analysis of these models for adherence to functional requirements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.102","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377192","Index Terms- Object-oriented modeling;embedded systems;requirements;patterns;conceptual modeling;object analysis;formal specification;model checking.","Pattern analysis;Embedded system;Object oriented modeling;Software systems;Unified modeling language;Embedded software;Application software;Programming;Information analysis;Automatic logic units","object-oriented methods;embedded systems;formal specification;formal verification;software quality;Unified Modeling Language;temporal logic;object-oriented programming","object analysis pattern;embedded system;software system;software development;design pattern;object-oriented modeling;Unified Modeling Language;structural information;behavioral information;Spin model checker;linear-time temporal logic;automotive industry","","24","","53","","","","","","IEEE","IEEE Journals & Magazines"
"The Design of a Multicore Extension of the SPIN Model Checker","G. J. Holzmann; D. Bosnacki","NA; NA","IEEE Transactions on Software Engineering","","2007","33","10","659","674","We describe an extension of the SPIN model checker for use on multicore shared-memory systems and report on its performance. We show how, with proper load balancing, the time requirements of a verification run can, in some cases, be reduced close to N-fold when N processing cores are used. We also analyze the types of verification problems for which multicore algorithms cannot provide relief. The extensions discussed here require only relatively small changes in the SPIN source code and are compatible with most existing verification modes such as partial order reduction, the verification of temporal logic formulas, bitstate hashing, and hash-compact compression.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70724","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302778","Software/Program Verification;Model Checking;Models of Computation;Logics and meanings of Programs;Distributed Programming","Multicore processing;Power system modeling;Central Processing Unit;Logic programming;Software systems;Problem-solving;Load management;Algorithm design and analysis;Computational modeling;Distributed computing","distributed algorithms;file organisation;program verification;resource allocation;shared memory systems;temporal logic","SPIN model checker;multicore shared-memory system;load balancing;partial order reduction;temporal logic formula;bitstate hashing;hash-compact compression;program verification;distributed algorithm","","65","","38","","","","","","IEEE","IEEE Journals & Magazines"
"A scenario-driven approach to trace dependency analysis","A. Egyed","Teknowledge Corp., Marina del Rey, CA, USA","IEEE Transactions on Software Engineering","","2003","29","2","116","132","Software development artifacts-such as model descriptions, diagrammatic languages, abstract (formal) specifications, and source code-are highly interrelated where changes in some of them affect others. Trace dependencies characterize such relationships abstractly. This paper presents an automated approach to generating and validating trace dependencies. It addresses the severe problem that the absence of trace information or the uncertainty of its correctness limits the usefulness of software models during software development. It also automates what is normally a time consuming and costly activity due to the quadratic explosion of potential trace dependencies between development artifacts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178051","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178051","","Unified modeling language;Programming;Software testing;Software systems;Explosions;Uncertainty;Costs;Design engineering;Software quality;Iterative methods","specification languages;formal specification;reverse engineering;software engineering","model descriptions;diagrammatic languages;abstract specifications;formal specifications;source code;trace dependencies;correctness limits;software models;software development;Unified Modeling Language;UML;traceability","","71","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Implementing and Evaluating Candidate-Based Invariant Generation","A. Betts; N. Chong; P. Deligiannis; A. F. Donaldson; J. Ketema","Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom; Department of Computing, Imperial College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","7","631","650","The discovery of inductive invariants lies at the heart of static program verification. Presently, many automatic solutions to inductive invariant generation are inflexible, only applicable to certain classes of programs, or unpredictable. An automatic technique that circumvents these deficiencies to some extent is candidate-based invariant generation, whereby a large number of candidate invariants are guessed and then proven to be inductive or rejected using a sound program analyzer. This paper describes our efforts to apply candidate-based invariant generation in GPUVerify, a static checker for programs that run on GPUs. We study a set of 383 GPU programs that contain loops, drawn from a number of open source suites and vendor SDKs. Among this set, 253 benchmarks require provision of loop invariants for verification to succeed. We describe the methodology we used to incrementally improve the invariant generation capabilities of GPUVerify to handle these benchmarks, through candidate-based invariant generation, using cheap static analysis to speculate potential program invariants. We also describe a set of experiments that we used to examine the effectiveness of our rules for candidate generation, assessing rules based on their generality (the extent to which they generate candidate invariants), hit rate (the extent to which the generated candidates hold), worth (the extent to which provable candidates actually help in allowing verification to succeed), and influence (the extent to which the success of one generation rule depends on candidates generated by another rule). We believe that our methodology may serve as a useful framework for other researchers interested in candidate-based invariant generation. The candidates produced by GPUVerify help to verify 231 of the 253 programs. This increase in precision, however, makes GPUVerify sluggish: the more candidates that are generated, the more time is spent determining which are inductive invariants. To speed up this process, we have investigated four under-approximating program analyses that aim to reject false candidates quickly and a framework whereby these analyses can run in sequence or in parallel. Across two platforms, running Windows and Linux, our results show that the best combination of these techniques running sequentially-speeds up invariant generation across our benchmarks by 1.17× (Windows) and 1.01× (Linux), with per-benchmark best speedups of 93.58× (Windows) and 48.34× (Linux), and worst slowdowns of 10.24× (Windows) and 43.31× (Linux). We find that parallelizing the strategies marginally improves overall invariant generation speedups to 1.27× (Windows) and 1.11× (Linux), maintains good best-case speedups of 91.18× (Windows) and 44.60× (Linux), and, importantly, dramatically reduces worst-case slowdowns to 3.15× (Windows) and 3.17× (Linux).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2718516","EU FP7 STREP project CARP; EPSRC PSL; Imperial College London’s EPSRC Impact Acceleration Account; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7955079","Formal verification;GPUs;invariant generation","Linux;Graphics processing units;Benchmark testing;Tools;Cognition;Acceleration","formal verification;graphics processing units;Linux;Microsoft Windows (operating systems);program verification","invariant generation speedups;inductive invariant generation;candidate invariants;invariant generation capabilities;potential program invariants;GPUVerify;SDK;candidate-based invariant generation evaluation;GPU programs;Windows;Linux;static program verification","","","","62","","","","","","IEEE","IEEE Journals & Magazines"
"TURTLE: a real-time UML profile supported by a formal validation toolkit","L. Apvrille; J. -. Courtiat; C. Lohr; P. de Saqui-Sannes","Inst. Eurecom, Ecole Nat. Superieure des Telecommun., Sophia-Antipolis, France; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","7","473","487","We present a UML 1.5 profile named TURTLE (Timed UML and RT-LOTOS Environment) endowed with a formal semantics given in terms of RT-LOTOS. TURTLE relies on UML's extensibility mechanisms to enhance class and activity diagrams. Class diagrams are extended with specialized classes named Tclasses, which communicate and synchronize through gates. Also, associations between Tclasses are attributed by a composition operator (Parallel, Synchro, Invocation, Sequence, or Preemption) which provides them with a formal semantics. TURTLE extends UML activity diagrams with synchronization actions and temporal operators (deterministic delay, nondeterministic delay, time-limited offer, and time-capture). The real-time dimension of TURTLE has been further improved by the addition of two composition operators, periodic and suspend, as well as suspendable delay, latency, and time-limited offer operators at the activity diagram level. Core characteristics of TURLE are supported by TTool - the TURTLE toolkit - which includes a diagram editor, a RT-LOTOS code generator and a result analyzer. The toolkit reuses RTL, a RT-LOTOS validation tool offering debug-oriented simulation and exhaustive analysis. TTool hides RT-LOTOS to the end-user and allows him/her to directly check TURTLE modeling against logical errors and timing inconsistencies. Besides the foundations of the TURTLE profile, this paper also discusses its application in the context of space-based embedded software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318608","Real-time systems;UML;RT-LOTOS;formal validation.","Unified modeling language;Delay;Real time systems;Timing;Embedded software;Prototypes;Character generation;Analytical models;Computer errors;Application software","specification languages;formal verification;formal specification;real-time systems;program compilers","Timed UML and RT-LOTOS Environment;TURTLE real-time UML 1.5 profile;formal semantics;UML activity diagrams;class diagrams;Tclasses;TTool TURTLE toolkit;RT-LOTOS code generator;RT-LOTOS validation tool;debug-oriented simulation;space-based embedded software;formal validation toolkit","","40","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Feature Location Using Probabilistic Ranking of Methods Based on Execution Scenarios and Information Retrieval","D. Poshyvanyk; Y. Gueheneuc; A. Marcus; G. Antoniol; V. Rajlich","Department of Computer Science, Wayne State University, Detroit, MI; Departement d'informatique et recherche operationnelle, Universite de Montreal, Center-Ville, Monrreal Quebec, Canada; Department of Computer Science, Wayne State University, Detroit, MI; Departement d'informatique, Ecole Polytechnique de Montreal, Centre-Ville, Montreal, Quebec, Canada; Department of Computer Science, Wayne State University, Detroit, MI","IEEE Transactions on Software Engineering","","2007","33","6","420","432","This paper recasts the problem of feature location in source code as a decision-making problem in the presence of uncertainty. The solution to the problem is formulated as a combination of the opinions of different experts. The experts in this work are two existing techniques for feature location: a scenario-based probabilistic ranking of events and an information-retrieval-based technique that uses latent semantic indexing. The combination of these two experts is empirically evaluated through several case studies, which use the source code of the Mozilla Web browser and the Eclipse integrated development environment. The results show that the combination of experts significantly improves the effectiveness of feature location as compared to each of the experts used independently","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1016","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181710","Program understanding;feature identification;concept location;dynamic and static analyses;information retrieval;Latent Semantic Indexing;scenario-based probabilistic ranking;open source software.","Information retrieval;Performance analysis;Indexing;Programming profession;Computer Society;Decision making;Uncertainty;Information analysis;Open source software;Computer bugs","decision making;information retrieval;probability;program diagnostics","feature location;information retrieval;decision-making problem;scenario-based probabilistic event ranking;latent semantic indexing;Mozilla Web browser;Eclipse integrated development","","209","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Categorization of common coupling and its application to the maintainability of the Linux kernel","L. Yu; S. R. Schach; K. Chen; J. Offutt","Dept. of Comput. Sci., Tennessee Technol. Univ., Cookeville, TN, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","10","694","706","Data coupling between modules, especially common coupling, has long been considered a source of concern in software design, but the issue is somewhat more complicated for products that are comprised of kernel modules together with optional nonkernel modules. This paper presents a refined categorization of common coupling based on definitions and uses between kernel and nonkernel modules and applies the categorization to a case study. Common coupling is usually avoided when possible because of the potential for introducing risky dependencies among software modules. The relative risk of these dependencies is strongly related to the specific definition-use relationships. In a previous paper, we presented results from a longitudinal analysis of multiple versions of the open-source operating system Linux. This paper applies the new common coupling categorization to version 2.4.20 of Linux, counting the number of instances of common coupling between each of the 26 kernel modules and all the other nonkernel modules. We also categorize each coupling in terms of the definition-use relationships. Results show that the Linux kernel contains a large number of common couplings of all types, raising a concern about the long-term maintainability of Linux.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.58","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339279","Index Terms- Modularity;dependencies;common coupling;definition-use analysis;kernel-based software;open-source software;Linux.","Linux;Kernel;Open source software;Operating systems;Application software;Computer Society;Computer architecture;Database systems;Software design;System software","Unix;operating system kernels;software reliability;public domain software;software maintenance;software metrics;software reusability","data coupling;software design;common coupling categorization;software modules;Linux kernel;definition-use analysis;open source software;kernel based software","","38","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating software reuse alternatives: a model and its application to an industrial case study","A. Tomer; L. Goldin; T. Kuflik; E. Kimchi; S. R. Schach","RAFAEL Ltd., Haifa, Israel; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","9","601","612","We propose a model that enables software developers to systematically evaluate and compare all possible alternative reuse scenarios. The model supports the clear identification of the basic operations involved and associates a cost component with each basic operation in a focused and precise way. The model is a practical tool that assists developers to weigh and evaluate different reuse scenarios, based on accumulated organizational data, and then to decide which option to select in a given situation. The model is currently being used at six different companies for cost-benefit analysis of alternative reuse scenarios; we give a case study that illustrates how it has been used in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324647","Index Terms- Reuse models;cost estimation;maintenance management;software libraries;process metrics;process measurement;planning.","Application software;Computer industry;Computer aided software engineering;Computer Society;Software systems;Cost benefit analysis;Software maintenance;Software libraries;Software measurement;Process planning","software cost estimation;software reusability;software maintenance;software management;software metrics;cost-benefit analysis;software libraries","cost-benefit analysis;software reuse model;software cost estimation;maintenance management;software libraries;process metrics;process measurement;industrial case study","","31","","13","","","","","","IEEE","IEEE Journals & Magazines"
"A Comparison of Tabular Expression-Based Testing Strategies","X. Feng; D. L. Parnas; T. H. Tse; T. O'Callaghan","University of Limerick, Limerick and United International College, Zhuhai, Guangdong; University of Limerick, Limerick; The University of Hong Kong, Hong Kong; University of Limerick, Limerick","IEEE Transactions on Software Engineering","","2011","37","5","616","634","Tabular expressions have been proposed as a notation to document mathematically precise but readable software specifications. One of the many roles of such documentation is to guide testers. This paper 1) explores the application of four testing strategies (the partition strategy, decision table-based testing, the basic meaningful impact strategy, and fault-based testing) to tabular expression-based specifications, and 2) compares the strategies on a mathematical basis through formal and precise definitions of the subsumption relationship. We also compare these strategies through experimental studies. These results will help researchers improve current methods and will enable testers to select appropriate testing strategies for tabular expression-based specifications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.78","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5975175","Tabular expression;test case constraint;subsume;unconditionally subsume;conditionally subsume.","Testing;Redundancy;Documentation;Software quality;Software engineering;Electronic mail","formal specification;program testing;system documentation","tabular expression-based testing strategies;readable software specifications;partition strategy;decision table-based testing;meaningful impact strategy;fault-based testing","","4","","49","","","","","","IEEE","IEEE Journals & Magazines"
"On the Positive Effect of Reactive Programming on Software Comprehension: An Empirical Study","G. Salvaneschi; S. Proksch; S. Amann; S. Nadi; M. Mezini","Department of Computer Science, Reactive Systems Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany; Department of Computer Science, Software Technology Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany; Department of Computer Science, Software Technology Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany; Department of Computing Science, AB, University of AlbertaCanada; Department of Computer Science, Software Technology Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany","IEEE Transactions on Software Engineering","","2017","43","12","1125","1143","Starting from the first investigations with strictly functional languages, reactive programming has been proposed as the programming paradigm for reactive applications. Over the years, researchers have enriched reactive languages with more powerful abstractions, embedded these abstractions into mainstream languages-including object-oriented languages-and applied reactive programming to several domains, such as GUIs, animations, Web applications, robotics, and sensor networks. However, an important assumption behind this line of research is that, beside other claimed advantages, reactive programming makes a wide class of otherwise cumbersome applications more comprehensible. This claim has never been evaluated. In this paper, we present the first empirical study that evaluates the effect of reactive programming on comprehension. The study involves 127 subjects and compares reactive programming to the traditional object-oriented style with the Observer design pattern. Our findings show that program comprehension is significantly enhanced by the reactive-programming paradigm-a result that suggests to further develop research in this field.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2655524","European Research Council; German Federal Ministry of Education and Research; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7827078","Reactive programming;empirical study;controlled experiment;software comprehension","Programming;Runtime;Software development;Robot sensing systems","functional languages;object-oriented languages;object-oriented programming","object-oriented languages;software comprehension;functional languages;observer design pattern;program comprehension;reactive languages;reactive programming","","3","","72","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Generating Test Cases for Real-Time Systems Based on Symbolic Models","W. L. Andrade; P. D. L. Machado","Federal University of Campina Grande, Campina Grande; Federal University of Campina Grande, Campina Grande","IEEE Transactions on Software Engineering","","2013","39","9","1216","1229","The state space explosion problem is one of the challenges to be faced by test case generation techniques, particularly when data values need to be enumerated. This problem gets even worse for real-time systems (RTS) that also have time constraints. The usual solution in this context, based on finite state machines or time automata, consists of enumerating data values (restricted to finite domains) while treating time symbolically. In this paper, a symbolic model for conformance testing of real-time systems software named TIOSTS that addresses both data and time symbolically is presented. Moreover, a test case generation process is defined to select more general test cases with variables and parameters that can be instantiated at testing execution time. Generation is based on a combination of symbolic execution and constraint solving for the data part and symbolic analysis for timed aspects. Furthermore, the practical application of the process is investigated through a case study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6475130","Real-time systems and embedded systems;formal methods;symbolic execution;testing strategies","Testing;Clocks;Cost accounting;Real-time systems;Data models;Automata;Semantics","conformance testing;finite state machines;program testing;real-time systems","test case generation techniques;real-time systems;symbolic models;state space explosion problem;RTS;time constraints;finite state machines;time automata;conformance testing;TIOSTS;symbolic execution;constraint solving;data part;symbolic analysis;timed aspects","","6","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Resource Management for Complex, Dynamic Environments","M. S. Raunak; L. J. Osterweil","Loyola University MD, Baltimore; University of Massachusetts Amherst, Amherst","IEEE Transactions on Software Engineering","","2013","39","3","384","402","This paper describes an approach to the specification and management of the agents and resources that are required to support the execution of complex systems and processes. The paper suggests that a resource should be viewed as a provider of a set of capabilities that are needed by a system or process, where that set may vary dynamically over time and with circumstances. This view of resources is defined and then made the basis for the framework of an approach to specifying, managing, and allocating resources in the presence of real-world complexity and dynamism. The ROMEO prototype resource management system is presented as an example of how this framework can be instantiated. Some case studies of the use of ROMEO to support system execution are presented and used to evaluate the framework, the ROMEO prototype, and our view of the nature of resources.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6197203","Resources management;process modeling;discrete event simulation;healthcare processes","Resource management;Hospitals;Surgery;Software;Context;Databases","formal specification;software agents;software engineering","resource management;dynamic environments;complex environments;complex systems;complex processes;ROMEO prototype resource management system;software engineering","","18","","40","","","","","","IEEE","IEEE Journals & Magazines"
"A state-of-the-art survey on software merging","T. Mens","Programming Technol. Lab., Vrije Univ., Brussels, Belgium","IEEE Transactions on Software Engineering","","2002","28","5","449","462","Software merging is an essential aspect of the maintenance and evolution of large-scale software systems. This paper provides a comprehensive survey and analysis of available merge approaches. Over the years, a wide variety of different merge techniques has been proposed. While initial techniques were purely based on textual merging, more powerful approaches also take the syntax and semantics of the software into account. There is a tendency towards operation-based merging because of its increased expressiveness. Another tendency is to try to define merge techniques that are as general, accurate, scalable, and customizable as possible, so that they can be used in any phase in the software life-cycle and detect as many conflicts as possible. After comparing the possible merge techniques, we suggest a number of important open problems and future research directions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000449","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000449","","Merging","configuration management;software maintenance;merging","software merging;software maintenance;merge approaches;textual merging;software life-cycle;conflict detection;conflict resolution;large-scale software systems","","179","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Search Algorithms for Regression Test Case Prioritization","Z. Li; M. Harman; R. M. Hierons","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","4","225","237","Regression testing is an expensive, but important, process. Unfortunately, there may be insufficient resources to allow for the reexecution of all test cases during regression testing. In this situation, test case prioritization techniques aim to improve the effectiveness of regression testing by ordering the test cases so that the most beneficial are executed first. Previous work on regression test case prioritization has focused on greedy algorithms. However, it is known that these algorithms may produce suboptimal results because they may construct results that denote only local minima within the search space. By contrast, metaheuristic and evolutionary search algorithms aim to avoid such problems. This paper presents results from an empirical study of the application of several greedy, metaheuristic, and evolutionary search algorithms to six programs, ranging from 374 to 11,148 lines of code for three choices of fitness metric. The paper addresses the problems of choice of fitness metric, characterization of landscape modality, and determination of the most suitable search technique to apply. The empirical results replicate previous results concerning greedy algorithms. They shed light on the nature of the regression testing search space, indicating that it is multimodal. The results also show that genetic algorithms perform well, although greedy approaches are surprisingly effective, given the multimodal nature of the landscape","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123325","Search techniques;test case prioritization;regression testing.","Greedy algorithms;Cost function;Genetic algorithms;Software testing;Libraries;Fault detection","genetic algorithms;greedy algorithms;program testing;search problems","regression testing;test case prioritization technique;greedy algorithm;metaheuristics;evolutionary search algorithm;fitness metric;genetic algorithm","","269","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Asymptotic Perturbation Bounds for Probabilistic Model Checking with Empirically Determined Probability Parameters","G. Su; Y. Feng; T. Chen; D. S. Rosenblum","Department of Computer Science, School of Computing, National University of Singapore; Quantum Computation and Intelligent Systems, University of Technology Sydney; Department of Computer Science, Middlesex University London; Department of Computer Science, School of Computing, National University of Singapore","IEEE Transactions on Software Engineering","","2016","42","7","623","639","Probabilistic model checking is a verification technique that has been the focus of intensive research for over a decade. One important issue with probabilistic model checking, which is crucial for its practical significance but is overlooked by the state-of-the-art largely, is the potential discrepancy between a stochastic model and the real-world system it represents when the model is built from statistical data. In the worst case, a tiny but nontrivial change to some model quantities might lead to misleading or even invalid verification results. To address this issue, in this paper, we present a mathematical characterization of the consequences of model perturbations on the verification distance. The formal model that we adopt is a parametric variant of discrete-time Markov chains equipped with a vector norm to measure the perturbation. Our main technical contributions include a closed-form formulation of asymptotic perturbation bounds, and computational methods for two arguably most useful forms of those bounds, namely linear bounds and quadratic bounds. We focus on verification of reachability properties but also address automata-based verification of omega-regular properties. We present the results of a selection of case studies that demonstrate that asymptotic perturbation bounds can accurately estimate maximum variations of verification results induced by model perturbations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2508444","Singapore Ministry of Education; Australian Research Council; National Natural Science Foundation of China; CAS/SAFEA; State Key Laboratory of Novel Software Technology at Nanjing University; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7355393","Asymptotic perturbation bound;discrete-time Markov chain;numerical iteration;optimization;parametric Markov chain;perturbation analysis;probabilistic model checking;quadratic programming","Model checking;Markov processes;Probabilistic logic;Computational modeling;Mathematical model;Perturbation methods","automata theory;formal verification;linear programming;Markov processes;probability;quadratic programming;reachability analysis","probabilistic model checking;empirically determined probability parameters;verification technique;stochastic model;real-world system;statistical data;mathematical characterization;model perturbations;verification distance;formal model;parametric discrete-time Markov chains;vector norm;perturbation measure;closed-form formulation;asymptotic perturbation bounds;computational methods;linear bounds;quadratic bounds;reachability property verification;automata-based verification;omega-regular properties;maximum verification variation estimation","","3","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Bayesian graphical models for software testing","D. A. Wooff; M. Goldstein; F. P. A. Coolen","Dept. of Math. Sci., Durham Univ., UK; Dept. of Math. Sci., Durham Univ., UK; Dept. of Math. Sci., Durham Univ., UK","IEEE Transactions on Software Engineering","","2002","28","5","510","525","This paper describes a new approach to the problem of software testing. The approach is based on Bayesian graphical models and presents formal mechanisms for the logical structuring of the software testing problem, the probabilistic and statistical treatment of the uncertainties to be addressed, the test design and analysis process, and the incorporation and implication of test results. Once constructed, the models produced are dynamic representations of the software testing problem. They may be used to drive test design, answer what-if questions, and provide decision support to managers and testers. The models capture the knowledge of the software tester for further use. Experiences of the approach in case studies are briefly discussed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1000453","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1000453","","Bayesian methods;Graphical models;Software testing","software reliability;program testing;belief networks","Bayesian graphical models;software testing;formal mechanisms;logical structuring;test design;software reliability","","37","","27","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical investigation of the influence of a type of side effects on program comprehension","J. J. Dolado; M. Harman; M. C. Otero; L. Hu","Dept. of Comput. Languages & Syst., Univ. of the Basque Country, San Sebastian, Spain; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","7","665","670","This paper reports the results of a study on the impact of a type of side effect (SE) upon program comprehension. We applied a crossover design on different tests involving fragments of C code that include increment and decrement operators. Each test had an SE version and a side-effect-free counterpart. The variables measured in the treatments were the number of correct answers and the time spent in answering. The results show that the side-effect operators considered significantly reduce performance in comprehension-related tasks, providing empirical justification for the belief that side effects are harmful.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214329","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214329","","Testing;Programming profession;Time measurement;Impedance;Computer languages;Humans;Algorithm design and analysis;Software maintenance","program testing;software engineering","side-effect-free programs;program comprehension;LinSERT algorithm;crossover design;program tests;side-effect operators","","17","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Coverage Estimation in Model Checking with Bitstate Hashing","S. Ikeda; M. Jibiki; Y. Kuno","NEC Corporation, Kawasaki; National Institute of Information and Communication Technology, Koganei; University of Tsukuba, Bunkyo","IEEE Transactions on Software Engineering","","2013","39","4","477","486","Explicit-state model checking which is conducted by state space search has difficulty in exploring satisfactory state space because of its memory requirements. Though bitstate hashing achieves memory efficiency, it cannot guarantee complete verification. Thus, it is desirable to provide a reliability indicator such as a coverage estimate. However, the existing approaches for coverage estimation are not very accurate when a verification run covers a small portion of state space. This mainly stems from the lack of information that reflects characteristics of models. Therefore, we propose coverage estimation methods using a growth curve that approximates an increase in reached states by enlarging a bloom filter. Our approaches improve estimation accuracy by leveraging the statistics from multiple verification runs. Coverage is estimated by fitting the growth curve to these statistics. Experimental results confirm the validity of the proposed growth curve and the applicability of our approaches to practical models. In fact, for practical models, our approaches outperformed the conventional ones when the actual coverage is relatively low.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6226428","Coverage estimation;model checking;bitstate hashing","Estimation;Reliability;Probabilistic logic;Accuracy;Mathematical model;Space exploration;Equations","curve fitting;file organisation;formal verification;statistics","bitstate hashing;explicit-state model checking;state space search;memory requirement;memory efficiency;formal verification;growth curve fitting;Bloom filter;statistics;coverage estimation;reliability indicator","","1","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient relational calculation for software analysis","D. Beyer; A. Noack; C. Lewerentz","Dept. of Electr. Eng. & Comput. Sci., California Univ., Berkeley, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","2","137","149","Calculating with graphs and relations has many applications in the analysis of software systems, for example, the detection of design patterns or patterns of problematic design and the computation of design metrics. These applications require an expressive query language, in particular, for the detection of graph patterns, and an efficient evaluation of the queries even for large graphs. In this paper, we introduce RML, a simple language for querying and manipulating relations based on predicate calculus, and CrocoPat, an interpreter for RML programs. RML is general because it enables the manipulation not only of graphs (i.e., binary relations), but of relations of arbitrary arity. CrocoPat executes RML programs efficiently because it internally represents relations as binary decision diagrams, a data structure that is well-known as a compact representation of large relations in computer-aided verification. We evaluate RML by giving example programs for several software analyses and CrocoPat by comparing its performance with calculators for binary relations, a Prolog system, and a relational database management system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401929","Index Terms- Logic programming;graph algorithms;data structures;reverse engineering;reengineering.","Application software;Data structures;Pattern analysis;Software systems;Database languages;Calculus;Boolean functions;Software performance;Performance analysis;Calculators","query languages;binary decision diagrams;data structures;formal verification;relational databases;PROLOG;graph theory;reverse engineering;object-oriented programming;software metrics;systems re-engineering","software analysis;design patterns;design metrics;query language;graph pattern;relation manipulation language;RML;binary decision diagram;data structure;computer-aided verification;Prolog system;relational database management system","","49","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Using Timed Automata for Modeling Distributed Systems with Clocks: Challenges and Solutions","G. Rodriguez-Navas; J. Proenza","Universitat de les Illes Balears, Palma de Mallorca; Universitat de les Illes Balears, Palma de Mallorca","IEEE Transactions on Software Engineering","","2013","39","6","857","868","The application of model checking for the formal verification of distributed embedded systems requires the adoption of techniques for realistically modeling the temporal behavior of such systems. This paper discusses how to model with timed automata the different types of relationships that may be found among the computer clocks of a distributed system, namely, ideal clocks, drifting clocks, and synchronized clocks. For each kind of relationship, a suitable modeling pattern is thoroughly described and formally verified.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6374193","Embedded systems;real-time systems;clock synchronization;model checking;timed automata;hybrid automata","Real-time systems;Automata;Formal verification;Distributed processing;Embedded systems","automata theory;distributed processing;embedded systems;formal verification","timed automata;modeling distributed systems;model checking;formal verification;distributed embedded systems;temporal behavior;distributed system computer clocks;ideal clocks;drifting clocks;synchronized clocks","","8","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A Survey on Software Fault Localization","W. E. Wong; R. Gao; Y. Li; R. Abreu; F. Wotawa","State Key Laboratory of Software Engineering, Wuhan University, Department of Computer Science, University of Texas at Dallas, Richardson, TX; Department of Computer Science, University of Texas at Dallas, Richardson, TX; Department of Computer Science, University of Texas at Dallas, Richardson, TX; Department of Informatics Engineering, University of Porto, Palo Alto Research Center (PARC), Palo Alto, CA; Institute for Software Technology, Graz University of Technology, Austria","IEEE Transactions on Software Engineering","","2016","42","8","707","740","Software fault localization, the act of identifying the locations of faults in a program, is widely recognized to be one of the most tedious, time consuming, and expensive - yet equally critical - activities in program debugging. Due to the increasing scale and complexity of software today, manually locating faults when failures occur is rapidly becoming infeasible, and consequently, there is a strong demand for techniques that can guide software developers to the locations of faults in a program with minimal human intervention. This demand in turn has fueled the proposal and development of a broad spectrum of fault localization techniques, each of which aims to streamline the fault localization process and make it more effective by attacking the problem in a unique way. In this article, we catalog and provide a comprehensive overview of such techniques and discuss key issues and concerns that are pertinent to software fault localization as a whole.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2521368","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7390282","Software fault localization;program debugging;software testing;execution trace;suspicious code;survey","Debugging;Software engineering;Computer bugs;Software debugging;Fault diagnosis;Complexity theory","program debugging;software reliability","software fault localization;program debugging;software developers;program fault locations;human intervention","","85","","427","","","","","","IEEE","IEEE Journals & Magazines"
"Engineering Privacy","S. Spiekermann; L. F. Cranor","Humboldt University, Berin; Carnegie Mellon University, Pittsburgh","IEEE Transactions on Software Engineering","","2009","35","1","67","82","In this paper we integrate insights from diverse islands of research on electronic privacy to offer a holistic view of privacy engineering and a systematic structure for the discipline's topics. First we discuss privacy requirements grounded in both historic and contemporary perspectives on privacy. We use a three-layer model of user privacy concerns to relate them to system operations (data transfer, storage and processing) and examine their effects on user behavior. In the second part of the paper we develop guidelines for building privacy-friendly systems. We distinguish two approaches: ""privacy-by-policy"" and ""privacy-by-architecture."" The privacy-by-policy approach focuses on the implementation of the notice and choice principles of fair information practices (FIPs), while the privacy-by-architecture approach minimizes the collection of identifiable personal data and emphasizes anonymization and client-side data storage and processing. We discuss both approaches with a view to their technical overlaps and boundaries as well as to economic feasibility. The paper aims to introduce engineers and computer scientists to the privacy research domain and provide concrete guidance on how to design privacy-friendly systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4657365","Privacy;Legal Aspects of Computing;Security and Protection;Requirements/Specifications;Privacy;Legal Aspects of Computing;Security and Protection;Requirements/Specifications","Design engineering;Protection;Systems engineering and theory;Data privacy;Companies;Social network services;Law;Radiofrequency identification;Guidelines;Memory","data privacy;security of data","electronic privacy;privacy engineering;user privacy;privacy-friendly systems;privacy-by-policy;privacy-by-architecture","","123","","116","","","","","","IEEE","IEEE Journals & Magazines"
"Approaches to Co-Evolution of Metamodels and Models: A Survey","R. Hebig; D. E. Khelladi; R. Bendraou","Computer Science and Engineering Göteborg, Chalmers University of Technology, Göteborg, Sweden; Sorbonne Universités, UPMC Univ Paris 06, UMR 7606, LIP6, Paris, France; Sorbonne Universités, UPMC Univ Paris 06, UMR 7606, LIP6, Paris, France","IEEE Transactions on Software Engineering","","2017","43","5","396","414","Modeling languages, just as all software artifacts, evolve. This poses the risk that legacy models of a company get lost, when they become incompatible with the new language version. To address this risk, a multitude of approaches for metamodel-model co-evolution were proposed in the last 10 years. However, the high number of solutions makes it difficult for practitioners to choose an appropriate approach. In this paper, we present a survey on 31 approaches to support metamodel-model co-evolution. We introduce a taxonomy of solution techniques and classify the existing approaches. To support researchers, we discuss the state of the art, in order to better identify open issues. Furthermore, we use the results to provide a decision support for practitioners, who aim to adopt solutions from research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2610424","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7569018","Survey;software engineering;metamodels;models;design notations and documentation","Unified modeling language;Companies;Taxonomy;Biological system modeling;Atmospheric modeling;Libraries;Productivity","software engineering","coevolution approaches;metamodel-model coevolution;solution technique taxonomy;decision support","","8","","84","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic analysis of consistency between requirements and designs","M. Chechik; J. Gannon","Dept. of Comput. Sci., Toronto Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","2001","27","7","651","672","Writing requirements in a formal notation permits automatic assessment of such properties as ambiguity, consistency, and completeness. However, verifying that the properties expressed in requirements are preserved in other software life cycle artifacts remains difficult. The existing techniques either require substantial manual effort and skill or suffer from exponential explosion of the number of states in the generated state spaces. ""Light-weight"" formal methods is an approach to achieve scalability in fully automatic verification by checking an abstraction of the system for only certain properties. We describe light-weight techniques for automatic analysis of consistency between software requirements (expressed in SCR) and detailed designs in low-degree-polynomial time, achieved at the expense of using imprecise data-flow analysis techniques. A specification language SCR describes the systems as state machines with event-driven transitions. We define detailed designs to be consistent with their SCR requirements if they contain exactly the same transitions. We have developed a language for specifying detailed designs, an analysis technique to create a model of a design through data-flow analysis of the language constructs, and a method to automatically generate and check properties derived from requirements to ensure a design's consistency with them. These ideas are implemented in a tool named CORD, which we used to uncover errors in designs of some existing systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.935856","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=935856","","Thyristors;Data analysis;Software quality;Costs;Page description languages;Computer Society;Manuals;Explosions;State-space methods;Scalability","formal specification;formal verification;data flow analysis;specification languages;software tools","requirements analysis;software design;formal notation;requirements verification;software life cycle artifacts;formal methods;scalability;automatic verification;polynomial time;data-flow analysis;specification language;SCR language;state machines;event-driven transitions;CORD tool","","17","","66","","","","","","IEEE","IEEE Journals & Magazines"
"A Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems","C. Andersson; P. Runeson","Department of Computer Science, Lund University, Box 118, SE-221 00 Lund, Sweden; Department of Computer Science, Lund University, Box 118, SE-221 00 Lund, Sweden","IEEE Transactions on Software Engineering","","2007","33","5","273","286","To contribute to the body of empirical research on fault distributions during development of complex software systems, a replication of a study of Fenton and Ohlsson is conducted. The hypotheses from the original study are investigated using data taken from an environment that differs in terms of system size, project duration, and programming language. We have investigated four sets of hypotheses on data from three successive telecommunications projects: 1) the Pareto principle, that is, a small number of modules contain a majority of the faults (in the replication, the Pareto principle is confirmed), 2) fault persistence between test phases (a high fault incidence in function testing is shown to imply the same in system testing, as well as prerelease versus postrelease fault incidence), 3) the relation between number of faults and lines of code (the size relation from the original study could be neither confirmed nor disproved in the replication), and 4) fault density similarities across test phases and projects (in the replication study, fault densities are confirmed to be similar across projects). Through this replication study, we have contributed to what is known on fault distributions, which seem to be stable across environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1005","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160967","Empirical research;replication;software fault distributions.","Software systems;System testing;Software engineering;Computer languages;Quality management;Telecommunication switching;Conducting materials","Pareto optimisation;software fault tolerance","Pareto principle;complex software system;software fault distribution","","72","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Software Dependencies, Work Dependencies, and Their Impact on Failures","M. Cataldo; A. Mockus; J. A. Roberts; J. D. Herbsleb","Research and Technology Center, Robert Bosch LLC, Pittsburgh; Avaya Labs Research, Basking Ridge; Duquesne University, Pittsburgh; Carnegie Mellon University, Pittsburgh","IEEE Transactions on Software Engineering","","2009","35","6","864","878","Prior research has shown that customer-reported software faults are often the result of violated dependencies that are not recognized by developers implementing software. Many types of dependencies and corresponding measures have been proposed to help address this problem. The objective of this research is to compare the relative performance of several of these dependency measures as they relate to customer-reported defects. Our analysis is based on data collected from two projects from two independent companies. Combined, our data set encompasses eight years of development activity involving 154 developers. The principal contribution of this study is the examination of the relative impact that syntactic, logical, and work dependencies have on the failure proneness of a software system. While all dependencies increase the fault proneness, the logical dependencies explained most of the variance in fault proneness, while workflow dependencies had more impact than syntactic dependencies. These results suggest that practices such as rearchitecting, guided by the network structure of logical dependencies, hold promise for reducing defects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5166450","Distribution/maintenance/enhancement;metrics/measurement;organizational management and coordination;quality analysis and evaluation.","Software systems;Predictive models;Quality management;Software engineering;Humans;Software development management;Programming","software fault tolerance;software maintenance;software metrics;software quality","software dependencies;work dependencies;customer-reported software faults;quality analysis","","108","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting Vulnerable Software Components via Text Mining","R. Scandariato; J. Walden; A. Hovsepyan; W. Joosen","IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium; Department of Computer Science, Northern Kentucky University, Highland Heights, KY; IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium; IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium","IEEE Transactions on Software Engineering","","2014","40","10","993","1006","This paper presents an approach based on machine learning to predict which components of a software application contain security vulnerabilities. The approach is based on text mining the source code of the components. Namely, each component is characterized as a series of terms contained in its source code, with the associated frequencies. These features are used to forecast whether each component is likely to contain vulnerabilities. In an exploratory validation with 20 Android applications, we discovered that a dependable prediction model can be built. Such model could be useful to prioritize the validation activities, e.g., to identify the components needing special scrutiny.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2340398","EU FP7; Research Fund KU Leuven; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6860243","Vulnerabilities;prediction model;machine learning","Software;Predictive models;Measurement;Security;Androids;Humanoid robots;Text mining","data mining;learning (artificial intelligence);program verification;security of data","vulnerable software component;text mining;machine learning;security vulnerability;source code;Android application","","45","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Interactive, Evolutionary Search in Upstream Object-Oriented Class Design","C. L. Simons; I. C. Parmee; R. Gwynllyw","University of the West of England, Frenchay; University of the West of England, Frenchay; University of the West of England, Frenchay","IEEE Transactions on Software Engineering","","2010","36","6","798","816","Although much evidence exists to suggest that early life cycle software engineering design is a difficult task for software engineers to perform, current computational tool support for software engineers is limited. To address this limitation, interactive search-based approaches using evolutionary computation and software agents are investigated in experimental upstream design episodes for two example design domains. Results show that interactive evolutionary search, supported by software agents, appears highly promising. As an open system, search is steered jointly by designer preferences and software agents. Directly traceable to the design problem domain, a mass of useful and interesting class designs is arrived at which may be visualized by the designer with quantitative measures of structural integrity, such as design coupling and class cohesion. The class designs are found to be of equivalent or better coupling and cohesion when compared to a manual class design for the example design domains, and by exploiting concurrent execution, the runtime performance of the software agents is highly favorable.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5432223","Software design;evolutionary computation;interactive search.","Software agents;Software design;Software performance;Software tools;Design engineering;Software engineering;Evolutionary computation;Open systems;Visualization;Runtime","evolutionary computation;interactive systems;object-oriented methods;open systems;search problems;software agents;software engineering","upstream object oriented class design;life cycle software engineering design;software agent;interactive evolutionary search;open system;design problem domain;structural integrity;concurrent execution;runtime performance","","43","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Customizing the Representation Capabilities of Process Models: Understanding the Effects of Perceived Modeling Impediments","B. M. Samuel; L. A. Watkins III; A. Ehle; V. Khatri","Ivey Business School, Western University 1255 Western Road, London, ON, Canada; Kelley School of Business, Indiana University 1309 East 10th Street BU 570, Bloomington, IN; Kelley School of Business, Indiana University 1309 East 10th Street BU 570, Bloomington, IN; Kelley School of Business, Indiana University 1309 East 10th Street BU 570, Bloomington, IN","IEEE Transactions on Software Engineering","","2015","41","1","19","39","Process modeling is useful during the analysis and design of systems. Prior research acknowledges both impediments to process modeling that limits its use as well as customizations that can be employed to help improve the creation of process models. However, no research to date has provided a rich examination of the linkages between perceived process modeling impediments and process modeling customizations. In order to help address this gap, we first conceptualized perceived impediments to using process models as a “lack of fit” between process modeling and another factor: (1) the role the process model is intended for; and (2) the task at hand. We conducted a case study at two large health insurance carriers to understand why the lack of fit existed and then show different types of process modeling customizations used to address the lack of fit and found a variety of “physical” and “process” customizations employed to overcome the lack of fits. We generalize our findings into propositions for future research that examine the dynamic interaction between process models and their need to be understood by individuals during systems analysis and design.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2354043","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6898868","Software process models;requirements/specifications management;requirements/specifications process;requirements/specification stools;UML;use cases;activity diagrams","Unified modeling language;Analytical models;Organizations;Software;Interviews;Context","formal specification;software process improvement","process model representation capability customization;system design;system analysis;process model creation improvement;perceived process modeling impediments;process modeling customizations;lack-of-fit;health insurance carriers;physical customization;process customization;dynamic process model interaction","","5","","97","","","","","","IEEE","IEEE Journals & Magazines"
"Eliminating exception handling errors with dependability cases: a comparative, empirical study","R. A. Maxion; R. T. Olszewski","Dept. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA","IEEE Transactions on Software Engineering","","2000","26","9","888","906","Programs fail mainly for two reasons: logic errors in the code and exception failures. Exception failures can account for up to two-thirds of system crashes, hence, are worthy of serious attention. Traditional approaches to reducing exception failures, such as code reviews, walkthroughs, and formal testing, while very useful, are limited in their ability to address a core problem: the programmer's inadequate coverage of exceptional conditions. The problem of coverage might be rooted in cognitive factors that impede the mental generation (or recollection) of exception cases that would pertain in a particular situation, resulting in insufficient software robustness. This paper describes controlled experiments for testing the hypothesis that robustness for exception failures can be improved through the use of various coverage-enhancing techniques: N-version programming, group collaboration, and dependability cases. N-version programming and collaboration are well known. Dependability cases, derived from safety cases, comprise a new methodology based on structured taxonomies and memory aids for helping software designers think about and improve exception handling coverage. All three methods showed improvements over control conditions in increasing robustness to exception failures but dependability cases proved most efficacious in terms of balancing cost and effectiveness.","0098-5589;1939-3520;2326-3881","","10.1109/32.877848","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=877848","","Robust control;Collaboration;Software safety;Logic;Computer crashes;Vehicle crash testing;Impedance;Robustness;Taxonomy;Software design","exception handling;program debugging;software reliability","exception handling errors;dependability cases;logic errors;exception failures;system crashes;cognitive factors;software robustness;experiments;coverage-enhancing techniques;N-version programming;group collaboration;software designers;cost effectiveness;software reliability","","16","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Model Transformation Modularization as a Many-Objective Optimization Problem","M. Fleck; J. Troya; M. Kessentini; M. Wimmer; B. Alkhazi","TU Wien, Wien, Austria; Universidad de Sevilla, Sevilla, Spain; University of Michigan, Ann Arbor, MI; TU Wien, Wien, Austria; University of Michigan, Ann Arbor, MI","IEEE Transactions on Software Engineering","","2017","43","11","1009","1032","Model transformation programs are iteratively refined, restructured, and evolved due to many reasons such as fixing bugs and adapting existing transformation rules to new metamodels version. Thus, modular design is a desirable property for model transformations as it can significantly improve their evolution, comprehensibility, maintainability, reusability, and thus, their overall quality. Although language support for modularization of model transformations is emerging, model transformations are created as monolithic artifacts containing a huge number of rules. To the best of our knowledge, the problem of automatically modularizing model transformation programs was not addressed before in the current literature. These programs written in transformation languages, such as ATL, are implemented as one main module including a huge number of rules. To tackle this problem and improve the quality and maintainability of model transformation programs, we propose an automated search-based approach to modularize model transformations based on higher-order transformations. Their application and execution is guided by our search framework which combines an in-place transformation engine and a search-based algorithm framework. We demonstrate the feasibility of our approach by using ATL as concrete transformation language and NSGA-III as search algorithm to find a trade-off between different well-known conflicting design metrics for the fitness functions to evaluate the generated modularized solutions. To validate our approach, we apply it to a comprehensive dataset of model transformations. As the study shows, ATL transformations can be modularized automatically, efficiently, and effectively by our approach. We found that, on average, the majority of recommended modules, for all the ATL programs, by NSGA-III are considered correct with more than 84 percent of precision and 86 percent of recall when compared to manual solutions provided by active developers. The statistical analysis of our experiments over several runs shows that NSGA-III performed significantly better than multi-objective algorithms and random search. We were not able to compare with existing model transformations modularization approaches since our study is the first to address this problem. The software developers considered in our experiments confirm the relevance of the recommended modularization solutions for several maintenance activities based on different scenarios and interviews.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2654255","Christian Doppler Forschungsgesellschaft; BMWFW; European Commission (FEDER); Spanish Government; CICYT project BELI; SEBASE; Andalusian Government project COPAS; Ford Motor Company; Ford Alliance Program; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7820199","Model transformation;modularization;ATL;NSGA-III;MDE;SBSE","Unified modeling language;Object oriented modeling;Adaptation models;Measurement;Algorithm design and analysis;Software engineering;Computer bugs","genetic algorithms;program debugging;search problems;software maintenance;software quality","model transformations modularization;model transformation modularization;model transformation programs;transformation languages;higher-order transformations;in-place transformation engine;concrete transformation language;ATL transformations;bug fixing;transformation rules;many-objective optimization problem;metamodels version;monolithic artifacts;automated search-based approach;NSGA-III;statistical analysis;maintenance activities","","1","","96","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"CP-Miner: finding copy-paste and related bugs in large-scale software code","Z. Li; S. Lu; S. Myagmar; Y. Zhou","Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA","IEEE Transactions on Software Engineering","","2006","32","3","176","192","Recent studies have shown that large software suites contain significant amounts of replicated code. It is assumed that some of this replication is due to copy-and-paste activity and that a significant proportion of bugs in operating systems are due to copy-paste errors. Existing static code analyzers are either not scalable to large software suites or do not perform robustly where replicated code is modified with insertions and deletions. Furthermore, the existing tools do not detect copy-paste related bugs. In this paper, we propose a tool, CP-Miner, that uses data mining techniques to efficiently identify copy-pasted code in large software suites and detects copy-paste bugs. Specifically, it takes less than 20 minutes for CP-Miner to identify 190,000 copy-pasted segments in Linux and 150,000 in FreeBSD. Moreover, CP-Miner has detected many new bugs in popular operating systems, 49 in Linux and 31 in FreeBSD, most of which have since been confirmed by the corresponding developers and have been rectified in the following releases. In addition, we have found some interesting characteristics of copy-paste in operating system code. Specifically, we analyze the distribution of copy-pasted code by size (number lines of code), granularity (basic blocks and functions), and modification within copy-pasted code. We also analyze copy-paste across different modules and various software versions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610609","Software analysis;code reuse;code duplication;debugging aids;data mining.","Computer bugs;Large-scale systems;Linux;Operating systems;Data mining;Cloning;Programming profession;Kernel;Performance analysis;Software performance","program debugging;program diagnostics;operating systems (computers);data mining;software tools;software reusability;software maintenance","CP-Miner tool;debugging aids;large-scale software code;replicated code;copy-paste bugs;operating system code bugs;static code analyzer;data mining technique;Linux;FreeBSD;code reuse;code duplication","","187","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Clone Management for Evolving Software","H. A. Nguyen; T. T. Nguyen; N. H. Pham; J. Al-Kofahi; T. N. Nguyen","Iowa State University, Ames; Iowa State University, Ames; Iowa State University, Ames; Iowa State University, Ames; Iowa State University, Ames","IEEE Transactions on Software Engineering","","2012","38","5","1008","1026","Recent research results suggest a need for code clone management. In this paper, we introduce JSync, a novel clone management tool. JSync provides two main functions to support developers in being aware of the clone relation among code fragments as software systems evolve and in making consistent changes as they create or modify cloned code. JSync represents source code and clones as (sub)trees in Abstract Syntax Trees, measures code similarity based on structural characteristic vectors, and describes code changes as tree editing scripts. The key techniques of JSync include the algorithms to compute tree editing scripts, to detect and update code clones and their groups, to analyze the changes of cloned code to validate their consistency, and to recommend relevant clone synchronization and merging. Our empirical study on several real-world systems shows that JSync is efficient and accurate in clone detection and updating, and provides the correct detection of the defects resulting from inconsistent changes to clones and the correct recommendations for change propagation across cloned code.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6007141","Clone management;clone consistency analysis;clone synchronization;clone merging","Cloning;Feature extraction;Software systems;Synchronization;Vegetation;Merging;Databases","Java;program compilers","evolving software;code clone management;JSync;clone management tool;code fragments;software systems;source code;abstract syntax trees;structural characteristic vectors;tree editing scripts;change propagation","","31","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Emulation of Software Faults: A Field Data Study and a Practical Approach","J. A. Duraes; H. S. Madeira","Instituto Superior de Engeharia de Coimbra, Rua Pedro Nunes—Quinta da Nora, 3030-199 Coimbra, Portugal; Departamento de Endenharia Informa´tica, University of Coimbra, Polo II—Pinhal de Marrocos, 3030-290 Coimbra, Portugal","IEEE Transactions on Software Engineering","","2006","32","11","849","867","The injection of faults has been widely used to evaluate fault tolerance mechanisms and to assess the impact of faults in computer systems. However, the injection of software faults is not as well understood as other classes of faults (e.g., hardware faults). In this paper, we analyze how software faults can be injected (emulated) in a source-code independent manner. We specifically address important emulation requirements such as fault representativeness and emulation accuracy. We start with the analysis of an extensive collection of real software faults. We observed that a large percentage of faults falls into well-defined classes and can be characterized in a very precise way, allowing accurate emulation of software faults through a small set of emulation operators. A new software fault injection technique (G-SWFIT) based on emulation operators derived from the field study is proposed. This technique consists of finding key programming structures at the machine code-level where high-level software faults can be emulated. The fault-emulation accuracy of this technique is shown. This work also includes a study on the key aspects that may impact the technique accuracy. The portability of the technique is also discussed and it is shown that a high degree of portability can be achieved","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.113","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015509","Fault injection;software faults;software reliability.","Emulation;Software reliability;Software measurement;Fault tolerant systems;Hardware;Programming;Computer industry;Software systems;Computer bugs;Software testing","program testing;software fault tolerance;software performance evaluation;software portability","software fault injection technique;field data study;fault tolerance mechanism;computer system;emulation accuracy;programming structure;machine code-level;G-SWFIT;fault representation;programming structures","","119","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Behavioral Testing of Refactoring Engines","G. Soares; R. Gheyi; T. Massoni","Federal University of Campina Grande, Campina Grande; Federal University of Campina Grande, Campina Grande; Federal University of Campina Grande, Campina Grande","IEEE Transactions on Software Engineering","","2013","39","2","147","162","Refactoring is a transformation that preserves the external behavior of a program and improves its internal quality. Usually, compilation errors and behavioral changes are avoided by preconditions determined for each refactoring transformation. However, to formally define these preconditions and transfer them to program checks is a rather complex task. In practice, refactoring engine developers commonly implement refactorings in an ad hoc manner since no guidelines are available for evaluating the correctness of refactoring implementations. As a result, even mainstream refactoring engines contain critical bugs. We present a technique to test Java refactoring engines. It automates test input generation by using a Java program generator that exhaustively generates programs for a given scope of Java declarations. The refactoring under test is applied to each generated program. The technique uses SafeRefactor, a tool for detecting behavioral changes, as an oracle to evaluate the correctness of these transformations. Finally, the technique classifies the failing transformations by the kind of behavioral change or compilation error introduced by them. We have evaluated this technique by testing 29 refactorings in Eclipse JDT, NetBeans, and the JastAdd Refactoring Tools. We analyzed 153,444 transformations, and identified 57 bugs related to compilation errors, and 63 bugs related to behavioral changes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6175911","Refactoring;automated testing;program generation","Java;Metals;Engines;Computer bugs;Testing;Automatic programming;Unified modeling language","automatic programming;Java;program testing","automated behavioral testing;compilation errors;refactoring transformation;refactoring engine developers;Java refactoring engines;Java program generator;SafeRefactor;JastAdd refactoring tools","","42","","49","","","","","","IEEE","IEEE Journals & Magazines"
"On The Detection of Test Smells: A Metrics-Based Approach for General Fixture and Eager Test","B. Van Rompaey; B. Du Bois; S. Demeyer; M. Rieger","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","800","817","As a fine-grained defect detection technique, unit testing introduces a strong dependency on the structure of the code. Accordingly, test coevolution forms an additional burden on the software developer which can be tempered by writing tests in a manner that makes them easier to change. Fortunately, we are able to concretely express what a good test is by exploiting the specific principles underlying unit testing. Analogous to the concept of code smells, violations of these principles are termed test smells. In this paper, we clarify the structural deficiencies encapsulated in test smells by formalizing core test concepts and their characteristics. To support the detection of two such test smells, General Fixture and Eager Test, we propose a set of metrics defined in terms of unit test concepts. We compare their detection effectiveness using manual inspection and through a comparison with human reviewing. Although the latter is the traditional means for test quality assurance, our results indicate it is not a reliable means for test smell detection. This work thus stresses the need for a more reliable detection mechanism and provides an initial contribution through the validation of test smell metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70745","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359471","Test design;Quality assurance;Maintainability;Test design;Quality assurance;Maintainability","Fixtures;Software testing;System testing;Automatic testing;Quality assurance;Costs;Guidelines;Computer Society;Writing;Inspection","program testing;software metrics","test smell detection;metrics-based approach;general fixture;eager test;fine-grained defect detection technique;code structure;software developer;manual inspection;human review;test quality assurance;reliable detection mechanism","","54","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Achieving efficiency and portability in systems software: a case study on POSIX-compliant multithreaded programs","Y. Shinjo; C. Pu","Dept. of Comput. Sci., Tsukuba Univ., Ibaraki, Japan; NA","IEEE Transactions on Software Engineering","","2005","31","9","785","800","Portable (standards-compliant) systems software is usually associated with unavoidable overhead from the standards-prescribed interface. For example, consider the POSIX Threads standard facility for using thread-specific data (TSD) to implement multithreaded code. The first TSD reference must be preceded by pthread/spl I.bar/getspecific( ), typically implemented as a function or macro with 40-50 instructions. This paper proposes a method that uses the runtime specialization'facility of the Tempo program specializer to convert such unavoidable source code into simple memory references of one or two instructions for execution. Consequently, the source code remains standard compliant and the executed code's performance is similar to direct global variable access. Measurements show significant performance gains over a range of code sizes. A random number generator (10 lines of C) shows a speedup of 4.8 times on a SPARC and 2.2 times on a Pentium. A time converter (2,800 lines) was sped up by 14 and 22 percent, respectively, and a parallel genetic algorithm system (14,000 lines) was sped up by 13 and 5 percent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.98","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514446","Index Terms- Performance;portability;threads;software libraries;concurrent programming;runtime specialization;thread-specific data.","System software;Computer aided software engineering;Software libraries;Code standards;Runtime;Packaging;Assembly;Programming profession;Gain measurement","Unix;multi-threading;systems software;software portability;software metrics;software libraries","software portability;systems software;POSIX-compliant multithreaded programs;standards-prescribed interface;thread-specific data;Tempo program specializer;random number generator;SPARC;parallel genetic algorithm system","","3","","34","","","","","","IEEE","IEEE Journals & Magazines"
"The SATIN Component System-A Metamodel for Engineering Adaptable Mobile Systems","S. Zachariadis; C. Mascolo; W. Emmerich","NA; NA; IEEE Computer Society","IEEE Transactions on Software Engineering","","2006","32","11","910","927","Mobile computing devices, such as personal digital assistants and mobile phones, are becoming increasingly popular, smaller, and more capable. We argue that mobile systems should be able to adapt to changing requirements and execution environments. Adaptation requires the ability-to reconfigure the deployed code base on a mobile device. Such reconfiguration is considerably simplified if mobile applications are component-oriented rather than monolithic blocks of code. We present the SATIN (system adaptation targeting integrated networks) component metamodel, a lightweight local component metamodel that offers the flexible use of logical mobility primitives to reconfigure the software system by dynamically transferring code. The metamodel is implemented in the SATIN middleware system, a component-based mobile computing middleware that uses the mobility primitives defined in the metamodel to reconfigure both itself and applications that it hosts. We demonstrate the suitability of SATIN in terms of lightweightedness, flexibility, and reusability for the creation of adaptable mobile systems by using it to implement, port, and evaluate a number of existing and new applications, including an active network platform developed for satellite communication at the European space agency. These applications exhibit different aspects of adaptation and demonstrate the flexibility of the approach and the advantages gained","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.115","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015513","Distributed objects;components;containers;mobile systems;middleware;pervasive computing;mobile code.","Systems engineering and theory;Application software;Mobile computing;Personal digital assistants;Middleware;Bandwidth;Computer networks;Mobile handsets;Software systems;Bluetooth","distributed object management;middleware;mobile computing;mobility management (mobile radio);object-oriented programming","SATIN component metamodel system;engineering adaptable mobile system;personal digital assistant;system adaptation targeting integrated network;software system;middleware system;European space agency","","19","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Grammar Recovery from Parse Trees and Metrics-Guided Grammar Refactoring","N. A. Kraft; E. B. Duffy; B. A. Malloy","University of Alabama, Tuscaloosa; Clemson University, Clemson; Clemson University, Clemson","IEEE Transactions on Software Engineering","","2009","35","6","780","794","Many software development tools that assist with tasks such as testing and maintenance are specific to a particular development language and require a parser for that language. Because a grammar is required to develop a parser, construction of these software development tools is dependent upon the availability of a grammar for the development language. However, a grammar is not always available for a language and, in these cases, acquiring a grammar is the most difficult, costly, and time-consuming phase of tool construction. In this paper, we describe a methodology for grammar recovery from a hard-coded parser. Our methodology is comprised of manual instrumentation of the parser, a technique for automatic grammar recovery from parse trees, and a semi-automatic metrics-guided approach to refactoring an iterative grammar to obtain a recursive grammar. We present the results of a case study in which we recover and refactor a grammar from version 4.0.0 of the GNU C++ parser and then refactor the recovered grammar using our metrics-guided approach. Additionally, we present an evaluation of the recovered and refactored grammar by comparing it to the ISO C++98 grammar.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.65","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5278661","Grammar;grammar recovery;grammar refactoring;grammar metrics;parse tree.","Programming;Software testing;Instruments;Software maintenance;Java;Computer Society;Manuals;Iterative methods;ISO;Debugging","C++ language;grammars;software maintenance;software metrics;system recovery","grammar recovery;parse trees;metrics-guided grammar refactoring;software development tools;hard-coded parser;iterative grammar;recursive grammar;GNU C++ parser;ISO C++98 grammar","","5","","49","","","","","","IEEE","IEEE Journals & Magazines"
"A Framework for Evaluating the Results of the SZZ Approach for Identifying Bug-Introducing Changes","D. A. da Costa; S. McIntosh; W. Shang; U. Kulesza; R. Coelho; A. E. Hassan","Department of Informatics and Applied Mathematics (DIMAp), Federal University of Rio Grande do Norte, Natal-RN, Brazil; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Department of Informatics and Applied Mathematics (DIMAp), Federal University of Rio Grande do Norte, Natal-RN, Brazil; Department of Informatics and Applied Mathematics (DIMAp), Federal University of Rio Grande do Norte, Natal-RN, Brazil; Software Analysis and Intelligence Lab (SAIL), School of Computing, Queen’s University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2017","43","7","641","657","The approach proposed by Silwerski, Zimmermann, and Zeller (SZZ) for identifying bug-introducing changes is at the foundation of several research areas within the software engineering discipline. Despite the foundational role of SZZ, little effort has been made to evaluate its results. Such an evaluation is a challenging task because the ground truth is not readily available. By acknowledging such challenges, we propose a framework to evaluate the results of alternative SZZ implementations. The framework evaluates the following criteria: (1) the earliest bug appearance, (2) the future impact of changes, and (3) the realism of bug introduction. We use the proposed framework to evaluate five SZZ implementations using data from ten open source projects. We find that previously proposed improvements to SZZ tend to inflate the number of incorrectly identified bug-introducing changes. We also find that a single bug-introducing change may be blamed for introducing hundreds of future bugs. Furthermore, we find that SZZ implementations report that at least 46 percent of the bugs are caused by bug-introducing changes that are years apart from one another. Such results suggest that current SZZ implementations still lack mechanisms to accurately identify bug-introducing changes. Our proposed framework provides a systematic mean for evaluating the data that is generated by a given SZZ implementation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2616306","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7588121","SZZ;evaluation framework;bug detection;software repository mining","Computer bugs;Software engineering;Electronic mail;Software;Manuals;History;Systematics","program debugging","SZZ approach;bug-introducing change identification;Silwerski-Zimmermann-Zeller approach;ground truth;open source projects;data evaluation","","4","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Response to ""Comments on factors that impact the implementation of a systems development methodology""","T. L. Roberts; M. L. Gibson; R. K. Rainer; K. T. Fields","University of Central Florida; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","3","282","286","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.910864","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=910864","","Instruments;Telephony;Laboratories;Writing;Statistical analysis;Input variables;Performance analysis;Prototypes;Technological innovation","","","","3","","24","","","","","","IEEE","IEEE Journals & Magazines"
"SMT-Based Bounded Model Checking for Embedded ANSI-C Software","L. Cordeiro; B. Fischer; J. Marques-Silva","Federal University of Amazonas, Brazil; University of Southampton, Southampton; University College Dublin, Dublin","IEEE Transactions on Software Engineering","","2012","38","4","957","974","Propositional bounded model checking has been applied successfully to verify embedded software, but remains limited by increasing propositional formula sizes and the loss of high-level information during the translation preventing potential optimizations to reduce the state space to be explored. These limitations can be overcome by encoding high-level information in theories richer than propositional logic and using SMT solvers for the generated verification conditions. Here, we propose the application of different background theories and SMT solvers to the verification of embedded software written in ANSI-C in order to improve scalability and precision in a completely automatic way. We have modified and extended the encodings from previous SMT-based bounded model checkers to provide more accurate support for variables of finite bit width, bit-vector operations, arrays, structures, unions, and pointers. We have integrated the CVC3, Boolector, and Z3 solvers with the CBMC front-end and evaluated them using both standard software model checking benchmarks and typical embedded software applications from telecommunications, control systems, and medical devices. The experiments show that our ESBMC model checker can analyze larger problems than existing tools and substantially reduce the verification time.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928354","Software engineering;formal methods;verification;model checking","Encoding;Embedded software;Safety;Space exploration;Optimization;Electronic mail","embedded systems;formal verification","SMT based bounded model checking;embedded ANSI-C software;embedded software verification;SMT solvers;model checkers;software model checking benchmarks","","52","","64","","","","","","IEEE","IEEE Journals & Magazines"
"The Design Space of Bug Fixes and How Developers Navigate It","E. Murphy-Hill; T. Zimmermann; C. Bird; N. Nagappan","Department of Computer Science, North Carolina State University, Raleigh, NC; Microsoft Research, Redmond, WA; Microsoft Research, Redmond, WA; Microsoft Research, Redmond, WA","IEEE Transactions on Software Engineering","","2015","41","1","65","81","When software engineers fix bugs, they may have several options as to how to fix those bugs. Which fix they choose has many implications, both for practitioners and researchers: What is the risk of introducing other bugs during the fix? Is the bug fix in the same code that caused the bug? Is the change fixing the cause or just covering a symptom? In this paper, we investigate alternative fixes to bugs and present an empirical study of how engineers make design choices about how to fix bugs. We start with a motivating case study of the Pex4Fun environment. Then, based on qualitative interviews with 40 engineers working on a variety of products, data from six bug triage meetings, and a survey filled out by 326 Microsoft engineers and 37 developers from other companies, we found a number of factors, many of them non-technical, that influence how bugs are fixed, such as how close to release the software is. We also discuss implications for research and practice, including how to make bug prediction and localization more accurate.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2357438","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6901259","Design concepts;human factors in software design;maintainability","Interviews;Computer bugs;Software;Encoding;Navigation;Protocols;Buildings","program debugging;software engineering","bug fix design space;software engineers;design choices;Pex4Fun environment;qualitative interviews;bug triage meetings;Microsoft engineers;bug prediction;bug localization","","12","","36","","","","","","IEEE","IEEE Journals & Magazines"
"A hierarchical model for object-oriented design quality assessment","J. Bansiya; C. G. Davis","Dept. of Math. & Comput. Sci., California State Univ., Hayward, CA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","1","4","17","The paper describes an improved hierarchical model for the assessment of high-level design quality attributes in object-oriented designs. In this model, structural and behavioral design properties of classes, objects, and their relationships are evaluated using a suite of object-oriented design metrics. This model relates design properties such as encapsulation, modularity, coupling, and cohesion to high-level quality attributes such as reusability, flexibility, and complexity using empirical and anecdotal information. The relationship or links from design properties to quality attributes are weighted in accordance with their influence and importance. The model is validated by using empirical and expert opinion to compare with the model results on several large commercial object-oriented systems. A key attribute of the model is that it can be easily modified to include different relationships and weights, thus providing a practical quality assessment tool adaptable to a variety of demands.","0098-5589;1939-3520;2326-3881","","10.1109/32.979986","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979986","","Object oriented modeling;Quality assessment","object-oriented programming;software metrics;software quality;data encapsulation","hierarchical model;object-oriented design quality assessment;high-level design quality attributes;behavioral design properties;object-oriented design metrics;design properties;encapsulation;modularity;coupling;cohesion;high-level quality attributes;reusability;quality attributes;expert opinion;commercial object-oriented systems;quality assessment tool;product metrics","","339","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating the Effects of Architectural Documentation: A Case Study of a Large Scale Open Source Project","R. Kazman; D. Goldenson; I. Monarch; W. Nichols; G. Valetto","Software Engineering Institute, Pittsburgh, PA; Software Engineering Institute, Pittsburgh, PA; NA; Software Engineering Institute, Pittsburgh, PA; Distributed Adaptive Systems research unit at Fondazione Bruno Kessler, Italy","IEEE Transactions on Software Engineering","","2016","42","3","220","260","Sustaining large open source development efforts requires recruiting new participants; however, a lack of architectural documentation might inhibit new participants since large amounts of project knowledge are unavailable to newcomers. We present the results of a multitrait, multimethod analysis of the effects of introducing architectural documentation into a substantial open source project-the Hadoop Distributed File System (HDFS). HDFS had only minimal architectural documentation, and we wanted to discover whether the putative benefits of architectural documentation could be observed over time. To do this, we created and publicized an architecture document and then monitored its usage and effects on the project. The results were somewhat ambiguous: by some measures the architecture documentation appeared to effect the project but not by others. Perhaps of equal importance is our discovery that the project maintained, in its Web-accessible JIRA archive of software issues and fixes, enough architectural discussion to support architectural thinking and reasoning. This “emergent” architecture documentation served an important purpose in recording core project members' architectural concerns and resolutions. However, this emergent architecture documentation did not serve all project members equally well; it appears that those on the periphery of the project-newcomers and adopters-still require explicit architecture documentation, as we will show.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2465387","Department of Defense; Carnegie Mellon University; Software Engineering Institute; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7230299","software architecture,;open source software,;documentation;Software architecture;open source software;documentation","Documentation;Computer architecture;Social network services;Electronic mail;Measurement;Open source software","distributed databases;public domain software;software architecture","large-scale substantial open source project;architectural documentation;project knowledge;multitrait-multimethod analysis;Hadoop distributed file system;HDFS;Web-accessible JIRA archive;architectural thinking;architectural reasoning;project members","","5","","77","","","","","","IEEE","IEEE Journals & Magazines"
"A testbed for configuration management policy programming","A. van der Hoek; A. Carzaniga; D. Heimbigner; A. L. Wolf","Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","1","79","99","Even though the number and variety of available configuration management systems has grown rapidly in the past few years, the need for new configuration management systems still remains. Driving this need are the emergence of situations requiring highly specialized solutions, the demand for management of artifacts other than traditional source code and the exploration of entirely new research questions in configuration management. Complicating the picture is the trend toward organizational structures that involve personnel working at physically separate sites. We have developed a testbed to support the rapid development of configuration management systems. The testbed separates configuration management repositories (i.e., the stores for versions of artifacts) from configuration management policies (i.e., the procedures, according to which the versions are manipulated) by providing a generic model of a distributed repository and an associated programmatic interface. Specific configuration management policies are programmed as unique extensions to the generic interface, while the underlying distributed repository is reused across different policies. The authors describe the repository model and its interface and present their experience in using a prototype of the testbed, called NUCM, to implement a variety of configuration management systems.","0098-5589;1939-3520;2326-3881","","10.1109/32.979990","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979990","","Testing","bibliographies;configuration management;application program interfaces;naming services","configuration management policy programming testbed;configuration management systems;highly specialized solutions;organizational structures;personnel;physically separate sites;configuration management repositories;generic model;distributed repository;programmatic interface;NUCM;distributed configuration management;version control","","12","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing the applicability of fault-proneness models across object-oriented software projects","L. C. Briand; W. L. Melo; J. Wust","Software Quality Eng. Lab., Carleton Univ., Ottawa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2002","28","7","706","720","A number of papers have investigated the relationships between design metrics and the detection of faults in object-oriented software. Several of these studies have shown that such models can be accurate in predicting faulty classes within one particular software product. In practice, however, prediction models are built on certain products to be used on subsequent software development projects. How accurate can these models be, considering the inevitable differences that may exist across projects and systems? Organizations typically learn and change. From a more general standpoint, can we obtain any evidence that such models are economically viable tools to focus validation and verification effort? This paper attempts to answer these questions by devising a general but tailorable cost-benefit model and by using fault and design data collected on two mid-size Java systems developed in the same environment. Another contribution of the paper is the use of a novel exploratory analysis technique - MARS (multivariate adaptive regression splines) to build such fault-proneness models, whose functional form is a-priori unknown. The results indicate that a model built on one system can be accurately used to rank classes within another system according to their fault proneness. The downside, however, is that, because of system differences, the predicted fault probabilities are not representative of the system predicted. However, our cost-benefit model demonstrates that the MARS fault-proneness model is potentially viable, from an economical standpoint. The linear model is not nearly as good, thus suggesting a more complex model is required.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1019484","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1019484","","Object oriented modeling;Predictive models;Mars;Economic forecasting;Environmental economics;Java;Accuracy;Logistics;Computer Society;Fault detection","object-oriented programming;project management;software development management;software metrics;software reliability;cost-benefit analysis;splines (mathematics);statistical analysis","fault-proneness model applicability assessment;object-oriented software projects;software design metrics;fault detection;faulty class prediction;software development projects;organizational learning;organizational change;economically viable tools;software validation;software verification;cost-benefit model;mid-size Java systems;exploratory analysis technique;MARS;multivariate adaptive regression splines;a-priori unknown functional form;system class ranking;fault probabilities;empirical validation;cross-validation","","119","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Eliminating Path Redundancy via Postconditioned Symbolic Execution","Q. Yi; Z. Yang; S. Guo; C. Wang; J. Liu; C. Zhao","National Engineering Research Center for Fundamental Software, Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Western Michigan University, Kalamazoo, MI; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA; Department of Computer Science, University of Southern California, Los Angeles, CA; Key Laboratory of Network Assessment Technology and Beijing Key Laboratory of Network Security Technology, Institute of Information Engineering and University of Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing, China; National Engineering Research Center for Fundamental Software and State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Software Engineering","","2018","44","1","25","43","Symbolic execution is emerging as a powerful technique for generating test inputs systematically to achieve exhaustive path coverage of a bounded depth. However, its practical use is often limited by path explosion because the number of paths of a program can be exponential in the number of branch conditions encountered during the execution. To mitigate the path explosion problem, we propose a new redundancy removal method called postconditioned symbolic execution. At each branching location, in addition to determine whether a particular branch is feasible as in traditional symbolic execution, our approach checks whether the branch is subsumed by previous explorations. This is enabled by summarizing previously explored paths by weakest precondition computations. Postconditioned symbolic execution can identify path suffixes shared by multiple runs and eliminate them during test generation when they are redundant. Pruning away such redundant paths can lead to a potentially exponential reduction in the number of explored paths. Since the new approach is computationally expensive, we also propose several heuristics to reduce its cost. We have implemented our method in the symbolic execution engine KLEE [1] and conducted experiments on a large set of programs from the GNU Coreutils suite. Our results confirm that redundancy due to common path suffix is both abundant and widespread in real-world applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2659751","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7835264","Symbolic execution;testing and debugging;testing tools","Concrete;Explosions;Input variables;Redundancy;Software;Syntactics;Testing","program diagnostics;program testing","common path suffix;postconditioned symbolic execution;exhaustive path coverage;branch conditions;path explosion problem;redundancy removal method;branching location;traditional symbolic execution;path suffixes;path redundancy elimination","","","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Goal-Directed Reasoning for Specification-Based Data Structure Repair","B. Demsky; M. C. Rinard","NA; NA","IEEE Transactions on Software Engineering","","2006","32","12","931","951","Software errors and hardware failures can cause data structures in running programs to violate key data structure consistency properties. As a result of this violation, the program may produce unacceptable results or even fail. We present a new data structure repair system. This system accepts a specification of data structure consistency properties stated in terms of an abstract set-and relation-based model of the data structures in the running program. It then automatically generates a repair algorithm that, during the execution of the program, detects and repairs any violations of these constraints. The goal is to enable the program to continue to execute acceptably in the face of otherwise crippling data structure corruption errors. We have applied our system to repair inconsistent data structures in five applications: CTAS (an air traffic control system), AbiWord (an open source word processing program), Freeciv (an interactive multiplayer game), a parallel x86 emulator, and a simplified Linux file system. Our results indicate that the generated repair algorithms can effectively repair inconsistent data structures in these applications to enable the applications to continue to operate successfully in cases where the original application would have failed. Without repair, all of the applications fail","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.122","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016571","Testing and debugging;language constructs and features.","Data structures;Application software;Software systems;Error correction codes;Hardware;Face detection;Air traffic control;Text processing;Linux;File systems","data structures;formal specification;inference mechanisms;system recovery","goal-directed reasoning;specification-based data structure repair algorithm;software error;hardware failure;air traffic control system;open source word processing program;interactive multiplayer game;parallel x86 emulator;simplified Linux file system;CTAS;AbiWord;Freeciv","","19","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Global-Aware Recommendations for Repairing Violations in Exception Handling","E. A. Barbosa; A. Garcia","Federal University of Rio Grande do Norte, Caicó - RN, Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro - RJ, Brazil","IEEE Transactions on Software Engineering","","2018","44","9","855","873","Empirical evidence suggests exception handling is not reliably implemented. Most faults in exception handling are related to global exceptions violating the intended exception handling design. However, repairing these violations is a cumbersome and error-prone task. It requires knowing the intended design and understanding how the source code violates it. It also requires changing the source code to make it compliant with the intended design. But changing the exception handling code is a difficult task, since changes in exception handling requires changing different parts of a program. Currently, there is still no solution to assist the repair of this type of violations. To bridge this gap, we present RAVEN, a heuristic strategy aware of the global context of exceptions that produces recommendations of how violations in exception handling may be repaired. This strategy takes advantage of explicit specifications of the intended design, although their availability is not mandatory. Our results revealed RAVEN provides recommendations able to repair violations in 69 percent of the cases when policy specifications are not available and in 97 percent of the cases when specifications are available. Thus, development teams may benefit from RAVEN, even when exception handling design decisions are not documented in their projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2716925","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7953550","Exception handling;recommender heuristic;software repair","Source coding;Robustness;Runtime;Software development management;Software reliability","exception handling;recommender systems;software fault tolerance;software quality","intended exception handling design;source code;repair violations;exception handling design decisions;global-aware recommendations;RAVEN strategy","","1","","51","","","","","","IEEE","IEEE Journals & Magazines"
"DeMIMA: A Multilayered Approach for Design Pattern Identification","Y. Guéhéneuc; G. Antoniol","University Montreal, Montreal; Ecole Polytechnique de Montreal, Montreal","IEEE Transactions on Software Engineering","","2008","34","5","667","684","Design patterns are important in object-oriented programming because they offer design motifs, elegant solutions to recurrent design problems, which improve the quality of software systems. Design motifs facilitate system maintenance by helping to understand design and implementation. However, after implementation, design motifs are spread throughout the source code and are thus not directly available to maintainers. We present DeMIMA, an approach to identify semi-automatically micro-architectures that are similar to design motifs in source code and to ensure the traceability of these micro-architectures between implementation and design. DeMIMA consists of three layers: two layers to recover an abstract model of the source code, including binary class relationships, and a third layer to identify design patterns in the abstract model. We apply DeMIMA to five open-source systems and, on average, we observe 34% precision for the considered 12 design motifs. Through the use of explanation-based constraint programming, DeMIMA ensures 100% recall on the five systems. We also apply DeMIMA on 33 industrial components.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.48","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4564471","Object-Oriented Programming;Patterns;Object-Oriented Programming;Patterns","Microarchitecture;Object oriented modeling;Object oriented programming;Software systems;Concrete;Open source software;Industrial relations;Scattering;Documentation;Writing","object-oriented programming;public domain software;software maintenance;software quality","multilayered approach;DeMIMA;design pattern identification;object-oriented programming;software systems quality;facilitate system maintenance;source code;open-source systems","","82","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Software Effort, Quality, and Cycle Time: A Study of CMM Level 5 Projects","M. Agrawal; K. Chari","NA; NA","IEEE Transactions on Software Engineering","","2007","33","3","145","156","The Capability Maturity Model (CMM) has become a popular methodology for improving software development processes with the goal of developing high-quality software within budget and planned cycle time. Prior research literature, while not exclusively focusing on CMM level 5 projects, has identified a host of factors as determinants of software development effort, quality, and cycle time. In this study, we focus exclusively on CMM level 5 projects from multiple organizations to study the impacts of highly mature processes on effort, quality, and cycle time. Using a linear regression model based on data collected from 37 CMM level 5 projects of four organizations, we find that high levels of process maturity, as indicated by CMM level 5 rating, reduce the effects of most factors that were previously believed to impact software development effort, quality, and cycle time. The only factor found to be significant in determining effort, cycle time, and quality was software size. On the average, the developed models predicted effort and cycle time around 12 percent and defects to about 49 percent of the actuals, across organizations. Overall, the results in this paper indicate that some of the biggest rewards from high levels of process maturity come from the reduction in variance of software development outcomes that were caused by factors other than software size","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084133","Cost estimation;time estimation;software quality;productivity.","Software quality;Coordinate measuring machines;Programming;Costs;Capability maturity model;Object oriented modeling;Productivity;Best practices;ISO standards;Six sigma","Capability Maturity Model;regression analysis;software cost estimation;software development management;software quality","software development effort;software quality;software cycle time estimation;CMM level 5 project;Capability Maturity Model;software development process improvement;organization productivity;linear regression model;software cost estimation","","92","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying Extract Method Refactoring Opportunities Based on Functional Relevance","S. Charalampidou; A. Ampatzoglou; A. Chatzigeorgiou; A. Gkortzis; P. Avgeriou","Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands; Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands; Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands; Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands","IEEE Transactions on Software Engineering","","2017","43","10","954","974","`Extract Method' is considered one of the most frequently applied and beneficial refactorings, since the corresponding Long Method smell is among the most common and persistent ones. Although Long Method is conceptually related to the implementation of diverse functionalities within a method, until now, this relationship has not been utilized while identifying refactoring opportunities. In this paper we introduce an approach (accompanied by a tool) that aims at identifying source code chunks that collaborate to provide a specific functionality, and propose their extraction as separate methods. The accuracy of the proposed approach has been empirically validated both in an industrial and an open-source setting. In the former case, the approach was capable of identifying functionally related statements within two industrial long methods (approx. 500 LoC each), with a recall rate of 93 percent. In the latter case, based on a comparative study on open-source data, our approach ranks better compared to two well-known techniques of the literature. To assist software engineers in the prioritization of the suggested refactoring opportunities the approach ranks them based on an estimate of their fitness for extraction. The provided ranking has been validated in both settings and proved to be strongly correlated with experts' opinion.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2645572","ITEA2; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7801138","Design tools and techniques;object-oriented programming;metrics/measurement","Measurement;Open source software;Mathematics;Data mining;Computer science;Syntactics","public domain software;software maintenance","open-source data;functional relevance;beneficial refactorings;diverse functionalities;specific functionality;open-source setting;functionally related statements;industrial long methods;refactoring opportunities;Extract Method;Long Method smell;source code chunks","","2","","44","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"An experimental investigation of formality in UML-based development","L. C. Briand; Y. Labiche; M. Di Penta; H. Yan-Bondoc","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2005","31","10","833","849","The object constraint language (OCL) was introduced as part of the Unified Modeling Language (UML). Its main purpose is to make UML models more precise and unambiguous by providing a constraint language describing constraints that the UML diagrams alone do not convey, including class invariants, operation contracts, and statechart guard conditions. There is an ongoing debate regarding the usefulness of using OCL in UML-based development, questioning whether the additional effort and formality is worth the benefit. It is argued that natural language may be sufficient, and using OCL may not bring any tangible benefits. This debate is in fact similar to the discussion about the effectiveness of formal methods in software engineering, but in a much more specific context. This paper presents the results of two controlled experiments that investigate the impact of using OCL on three software engineering activities using UML analysis models: detection of model defects through inspections, comprehension of the system logic and functionality, and impact analysis of changes. The results show that, once past an initial learning curve, significant benefits can be obtained by using OCL in combination with UML analysis diagrams to form a precise UML analysis model. But, this result is however conditioned on providing substantial, thorough training to the experiment participants.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.105","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542066","Index Terms- Comprehension of software models;software engineering experimentation;UML;OCL.","Unified modeling language;Software engineering;Logic;Programming;Computer Society;Contracts;Natural languages;Inspection;Context","Unified Modeling Language;object-oriented programming;object-oriented languages;constraint handling;natural languages;formal specification;formal verification","UML-based development;Unified Modeling Language;object constraint language;statechart guard condition;system logic;system functionality;UML analysis model","","69","","29","","","","","","IEEE","IEEE Journals & Magazines"
"The Scent of a Smell: An Extensive Comparison Between Textual and Structural Smells","F. Palomba; A. Panichella; A. Zaidman; R. Oliveto; A. De Lucia","TU Delft, Delft, The Netherlands; SnT Centre—University of Luxembourg, Esch-sur-Alzette, Luxembourg; TU Delft, Delft, The Netherlands; University of Molise, Campobasso, Italy; University of Salerno, Fisciano, Italy","IEEE Transactions on Software Engineering","","2018","44","10","977","1000","Code smells are symptoms of poor design or implementation choices that have a negative effect on several aspects of software maintenance and evolution, such as program comprehension or change- and fault-proneness. This is why researchers have spent a lot of effort on devising methods that help developers to automatically detect them in source code. Almost all the techniques presented in literature are based on the analysis of structural properties extracted from source code, although alternative sources of information (e.g., textual analysis) for code smell detection have also been recently investigated. Nevertheless, some studies have indicated that code smells detected by existing tools based on the analysis of structural properties are generally ignored (and thus not refactored) by the developers. In this paper, we aim at understanding whether code smells detected using textual analysis are perceived and refactored by developers in the same or different way than code smells detected through structural analysis. To this aim, we set up two different experiments. We have first carried out a software repository mining study to analyze how developers act on textually or structurally detected code smells. Subsequently, we have conducted a user study with industrial developers and quality experts in order to qualitatively analyze how they perceive code smells identified using the two different sources of information. Results indicate that textually detected code smells are easier to identify and for this reason they are considered easier to refactor with respect to code smells detected using structural properties. On the other hand, the latter are often perceived as more severe, but more difficult to exactly identify and remove.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2752171","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8038053","Code smells;empirical study;mining software repositories","Tools;Data mining;Software systems;Detectors;Maintenance engineering;Large scale integration","data mining;software maintenance;software quality;source code (software)","source code;textually detected code smells;structurally detected code smells;software maintenance;software repository mining;qualitatively analyze","","4","","111","","","","","","IEEE","IEEE Journals & Magazines"
"The guardian model and primitives for exception handling in distributed systems","R. Miller; A. Tripathi","IBM Corp., Rochester, MN, USA; NA","IEEE Transactions on Software Engineering","","2004","30","12","1008","1022","This work presents an abstraction called guardian for exception handling in distributed and concurrent systems that use coordinated exception handling. This model addresses two fundamental problems with distributed exception handling in a group of asynchronous processes. The first is to perform recovery when multiple exceptions are concurrently signaled. The second is to determine the correct context in which a process should execute its exception handling actions. Several schemes have been proposed in the past to address these problems. These are based on structuring a distributed program as atomic actions based on conversations or transactions and resolving multiple concurrent exceptions into a single one. The guardian in a distributed program represents the abstraction of a global exception handler, which encapsulates rules for handling concurrent exceptions and directing each process to the semantically correct context for executing its recovery actions. Its programming primitives and the underlying distributed execution model are presented here. In contrast to the existing approaches, this model is more basic and can be used to implement or enhance the existing schemes. Using several examples we illustrate the capabilities of this model. Finally, its advantages and limitations are discussed in contrast to existing approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.106","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377194","Index Terms- Concurrent programming;distributed programming;fault tolerance.","Computer errors;Testing;Fault tolerance;Software systems;Error correction;Fault tolerant systems;Fault detection;Signal resolution;Application software;Process control","exception handling;concurrency control;parallel programming;software fault tolerance;system recovery","concurrent programming;distributed programming;fault tolerance;exception handling;system recovery;distributed execution model","","13","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Realism in assessment of effort estimation uncertainty: it matters how you ask","M. Jorgensen","Simula Res. Lab., Oslo Univ., Norway","IEEE Transactions on Software Engineering","","2004","30","4","209","217","Traditionally, software professionals are requested to provide minimum-maximum intervals to indicate the uncertainty of their effort estimates. We claim that the traditional request is not optimal and leads to overoptimistic views about the level of estimation uncertainty. Instead, we propose that it is better to frame the request for uncertainty assessment: ""How likely is it that the actual effort will be more than/less than X?"" Our claim is based on the results of a previously reported-experiment and field studies in two companies. The two software companies were instructed to apply the traditional and our alternative framing on random samples of their projects. In total, we collected information about 47 projects applying the traditional-framing and 23 projects applying the alternative framing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274041","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274041","","Uncertainty;Amplitude shift keying;Project management;Risk management;Psychology;Contingency management;Costs;Humans","software cost estimation;risk management;software houses;project management;professional aspects","software professionals;software companies;traditional-framing project;alternative framing project;software cost estimation;risk assessment;software psychology","","28","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Integrating Software Models and Platform Models for Performance Analysis","V. Cortellessa; P. Pierini; D. Rossi","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","6","385","401","System performance is a key factor to take into account throughout the software life cycle of modern computer systems, mostly due to their typical characteristics such as distributed deployment, code mobility, and platform heterogeneity. An open challenge in this direction is to integrate the performance validation as a transparent and efficient activity in the system development process. Several methodologies have been proposed to automate the transformation of software/hardware models into performance models. In this paper, we do not take a transformational approach; rather, we present a framework to integrate a software model with a platform model in order to build a performance model. Performance indices are obtained from simulation of the resulting performance model. Our framework provides a library of predefined resource models, model annotation and integration procedures, and simulation support that makes the performance analysis a much easier activity. We present the results obtained from two different industrial case studies that show the maturity and the stability of our approach","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1014","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181708","Software performance;software model;platform model;UML;simulation.","Software performance;Performance analysis;Embedded software;Hardware;Unified modeling language;Petri nets;Software systems;Distributed computing;Software libraries;Analytical models","integrated software;software performance evaluation;software prototyping;Unified Modeling Language","software model integration;platform model integration;software performance analysis;software life cycle;modern computer system;distributed deployment;code mobility;system development process;software transformation;hardware transformation;predefined resource model;Unified Modeling Language","","12","","32","","","","","","IEEE","IEEE Journals & Magazines"
"DECAF: A Platform-Neutral Whole-System Dynamic Binary Analysis Platform","A. Henderson; L. K. Yan; X. Hu; A. Prakash; H. Yin; S. McCamant","Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; Department of RIG, Rome Laboratory, Rome, NY; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY; Department of Computer Science & Engineering, University of Minnesota (Twin Cities), Minneapolis, MN","IEEE Transactions on Software Engineering","","2017","43","2","164","184","Dynamic binary analysis is a prevalent and indispensable technique in program analysis. While several dynamic binary analysis tools and frameworks have been proposed, all suffer from one or more of: prohibitive performance degradation, a semantic gap between the analysis code and the program being analyzed, architecture/OS specificity, being user-mode only, and lacking APIs. We present DECAF, a virtual machine based, multi-target, whole-system dynamic binary analysis framework built on top of QEMU. DECAF provides Just-In-Time Virtual Machine Introspection and a plugin architecture with a simple-to-use event-driven programming interface. DECAF implements a new instruction-level taint tracking engine at bit granularity, which exercises fine control over the QEMU Tiny Code Generator (TCG) intermediate representation to accomplish on-the-fly optimizations while ensuring that the taint propagation is sound and highly precise. We perform a formal analysis of DECAF's taint propagation rules to verify that most instructions introduce neither false positives nor false negatives. We also present three platform-neutral plugins-Instruction Tracer, Keylogger Detector, and API Tracer, to demonstrate the ease of use and effectiveness of DECAF in writing cross-platform and system-wide analysis tools. Implementation of DECAF consists of 9,550 lines of C++ code and 10,270 lines of C code and we evaluate DECAF using CPU2006 SPEC benchmarks and show average overhead of 605 percent for system wide tainting and 12 percent for VMI.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2589242","US National Science Foundation; US National Science Foundation; McAfee Inc.; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7506264","Dynamic binary analysis;dynamic taint analysis;virtual machine introspection","Instruments;Virtual machining;Kernel;Semantics;Computer architecture;Registers;Context","application program interfaces;C++ language;program compilers;program diagnostics;software architecture;software performance evaluation;software portability;software tools;source code (software);virtual machines","DECAF;platform-neutral whole-system dynamic binary analysis platform;program analysis;dynamic binary analysis tools;prohibitive performance degradation;code analysis;architecture-OS specificity;virtual machine based multitarget whole-system dynamic binary analysis;QEMU;just-in-time virtual machine introspection;plug-in architecture;event-driven programming interface;instruction-level taint tracking engine;QEMU tiny code generator;TCG;on-the-fly optimizations;taint propagation;formal analysis;platform-neutral plugins;instruction tracer;keylogger detector;API tracer;cross-platform analysis tools;system- wide analysis tools;C++ code;C code;CPU2006 SPEC benchmarks;system wide tainting","","3","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical validation of object-oriented metrics on open source software for fault prediction","T. Gyimothy; R. Ferenc; I. Siket","Dept. of Software Eng., Univ. of Szeged, Hungary; Dept. of Software Eng., Univ. of Szeged, Hungary; Dept. of Software Eng., Univ. of Szeged, Hungary","IEEE Transactions on Software Engineering","","2005","31","10","897","910","Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.112","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542070","Index Terms- Fact extraction;metrics validation;reverse engineering;open source software;fault-proneness detection;Mozilla;Bugzilla;C++;compiler wrapping;Columbus.","Open source software;Quality management;Software development management;Computer industry;Fault detection;Electronic mail;Computer bugs;Object oriented databases;Learning systems;Software systems","software metrics;object-oriented programming;object-oriented methods;public domain software;formal verification;fault diagnosis;software fault tolerance;software quality;database management systems","object-oriented metrics;open source software;fault prediction;software quality;software reliability;fault-proneness detection;open source Web;Mozilla;Bugzilla bug database","","392","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Functional paleontology: the evolution of user-visible system services","A. I. Anton; C. Potts","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA","IEEE Transactions on Software Engineering","","2003","29","2","151","166","It has long been accepted that requirements analysis should precede architectural design and implementation, but in software evolution and reverse engineering this concern with black-box analysis of function has necessarily been de-emphasized in favor of code-based analysis and designer-oriented interpretation. In this paper, we redress this balance by describing ""functional paleontology,"" an approach to analyzing the evolution of user-visible features or services independent of architecture and design intent. We classify the benefits and burdens of interpersonal communication services into core and peripheral categories and investigate the telephony services available to domestic subscribers over a 50-year period. We report that services were introduced in discrete bursts, each of which emphasized different benefits and burdens. We discuss the general patterns of functional evolution that this ""fossil record"" illustrates and conclude by discussing their implications for forward engineering of software products.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178053","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178053","","Programming;Reverse engineering;Computer architecture;Telephony;Control systems;Text processing;Jet engines;Buildings;Software systems;Gold","reverse engineering;software metrics;formal specification;software tools","functional paleontology;user-visible system services;requirements analysis;reverse engineering;software evolution;black-box analysis;code-based analysis;designer-oriented interpretation;user-visible features;interpersonal communication services;telephony services;discrete bursts;functional evolution;forward engineering;requirements engineering","","17","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Synthesis and Dynamic Analysis of Tradeoff Spaces for Object-Relational Mapping","H. Bagheri; C. Tang; K. Sullivan","Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE; Department of Computer Sciences, University of Virginia, Charlottesville, VA 22903; Department of Computer Sciences, University of Virginia, Charlottesville, VA 22903","IEEE Transactions on Software Engineering","","2017","43","2","145","163","Producing software systems that achieve acceptable tradeoffs among multiple non-functional properties remains a significant engineering problem. We propose an approach to solving this problem that combines synthesis of spaces of design alternatives from logical specifications and dynamic analysis of each point in the resulting spaces. We hypothesize that this approach has potential to help engineers understand important tradeoffs among dynamically measurable properties of system components at meaningful scales within reach of existing synthesis tools. To test this hypothesis, we developed tools to enable, and we conducted, a set of experiments in the domain of relational databases for object-oriented data models. For each of several data models, we used our approach to empirically test the accuracy of a published suite of metrics to predict tradeoffs based on the static schema structure alone. The results show that exhaustive synthesis and analysis provides a superior view of the tradeoff spaces for such designs. This work creates a path forward toward systems that achieve significantly better tradeoffs among important system properties.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2587646","National Science Foundation; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7506009","Specification-driven synthesis;tradespace analysis;ORM;static analysis;dynamic analysis;relational logic","Data models;Load modeling;Object oriented modeling;Semantics;Relational databases;Measurement","data models;formal specification;object-oriented methods;program diagnostics","object-relational mapping;automated tradeoff space synthesis;dynamic tradeoff space analysis;software systems;nonfunctional properties;logical specifications;dynamic analysis;dynamically measurable properties;relational databases;object-oriented data models;static schema structure;system properties","","2","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Data Scientists in Software Teams: State of the Art and Challenges","M. Kim; T. Zimmermann; R. DeLine; A. Begel","University of California, Los Angeles, CA; One Microsoft Way, Redmond, WA; One Microsoft Way, Redmond, WA; One Microsoft Way, Redmond, WA","IEEE Transactions on Software Engineering","","2018","44","11","1024","1038","The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams, e.g., Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists, and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2754374","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8046093","Data science;development roles;software engineering;industry","Data science;Tools;Sociology;Statistics;Software;Best practices;Interviews","information management;software engineering","professional data scientists;software teams;Microsoft;software engineering context","","1","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A PVS-Simulink Integrated Environment for Model-Based Analysis of Cyber-Physical Systems","C. Bernardeschi; A. Domenici; P. Masci","Department of Information Engineering, University of Pisa, PI, Italy; Department of Information Engineering, University of Pisa, PI, Italy; HASLab/INESC TEC and Universidade do Minho, Braga, Portugal","IEEE Transactions on Software Engineering","","2018","44","6","512","533","This paper presents a methodology, with supporting tool, for formal modeling and analysis of software components in cyber-physical systems. Using our approach, developers can integrate a simulation of logic-based specifications of software components and Simulink models of continuous processes. The integrated simulation is useful to validate the characteristics of discrete system components early in the development process. The same logic-based specifications can also be formally verified using the Prototype Verification System (PVS), to gain additional confidence that the software design complies with specific safety requirements. Modeling patterns are defined for generating the logic-based specifications from the more familiar automata-based formalism. The ultimate aim of this work is to facilitate the introduction of formal verification technologies in the software development process of cyber-physical systems, which typically requires the integrated use of different formalisms and tools. A case study from the medical domain is used to illustrate the approach. A PVS model of a pacemaker is interfaced with a Simulink model of the human heart. The overall cyber-physical system is co-simulated to validate design requirements through exploration of relevant test scenarios. Formal verification with the PVS theorem prover is demonstrated for the pacemaker model for specific safety aspects of the pacemaker design.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2694423","North Portugal Regional Operational Programme; PORTUGAL 2020 Partnership Agreement, and through the European Regional Development Fund (ERDF); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7900400","Real-time and embedded systems;modeling techniques;specification;formal methods","Software packages;Automata;Analytical models;Cyber-physical systems;Mathematical model;Data models","cyber-physical systems;formal specification;program testing;program verification;software development management;software tools;theorem proving","formal modeling;software components;cyber-physical system;Simulink model;discrete system components;logic-based specifications;Prototype Verification System;formal verification technologies;software development process;PVS model;pacemaker model;PVS-Simulink integrated environment;software design;formal verification","","","","76","","","","","","IEEE","IEEE Journals & Magazines"
"Automatically Recommending Peer Reviewers in Modern Code Review","M. B. Zanjani; H. Kagdi; C. Bird","Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, Kansas; Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, Kansas; Microsoft Research, Redmond, WA","IEEE Transactions on Software Engineering","","2016","42","6","530","543","Code review is an important part of the software development process. Recently, many open source projects have begun practicing code review through “modern” tools such as GitHub pull-requests and Gerrit. Many commercial software companies use similar tools for code review internally. These tools enable the owner of a source code change to request individuals to participate in the review, i.e., reviewers. However, this task comes with a challenge. Prior work has shown that the benefits of code review are dependent upon the expertise of the reviewers involved. Thus, a common problem faced by authors of source code changes is that of identifying the best reviewers for their source code change. To address this problem, we present an approach, namely cHRev, to automatically recommend reviewers who are best suited to participate in a given review, based on their historical contributions as demonstrated in their prior reviews. We evaluate the effectiveness of cHRev on three open source systems as well as a commercial codebase at Microsoft and compare it to the state of the art in reviewer recommendation. We show that by leveraging the specific information in previously completed reviews (i.e.,quantification of review comments and their recency), we are able to improve dramatically on the performance of prior approaches, which (limitedly) operate on generic review information (i.e., reviewers of similar source code file and path names) or source coderepository data. We also present the insights into why our approach cHRev outperforms the existing approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2500238","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7328331","Modern code review;reviewer recommendation;code change;Gerrit;Modern code review;reviewer recommendation;code change;Gerrit","History;Electronic mail;Birds;Inspection;Androids;Humanoid robots;Software","software engineering;software reviews","peer reviewer recommendation;code review;software development process;open source projects;GitHub pull-requests tool;Gerrit tool;source code change;cHRev approach;commercial codebase","","12","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Test Synthesis from UML Models of Distributed Software","S. Pickin; C. Jard; T. Jeron; J. Jezequel; Y. Le Traon","Departamento de Ingenieria Telematica, Universidad Carlos III de Madrid, Madrid, Spain; ENS Cachan, Campus de Ker Lann, Bruz, France; IRISA/INRIA, Campus Universitaire de Beaulieu, Rennes Cedex, France; IRISA, Campus Universitaire de Beaulieu, Rennes Cedex, France; ENST Bretagne, 2, rue de la Chataigneraie, 35576 Cesson Sevigne Cedex, France","IEEE Transactions on Software Engineering","","2007","33","4","252","269","The object-oriented software development process is increasingly used for the construction of complex distributed systems. In this context, behavior models have long been recognized as the basis for systematic approaches to requirements capture, specification, design, simulation, code generation, testing, and verification. Two complementary approaches for modeling behavior have proven useful in practice: interaction-based modeling (e.g., UML sequence diagrams) and state-based modeling (e.g., UML statecharts). Building on formal V&V techniques, in this article we present a method and a tool for automated synthesis of test cases from scenarios and a state-based design model of the application, remaining entirely within the UML framework. The underlying ""on the fly"" test synthesis algorithms are based on the input/output labeled transition system formalism, which is particularly appropriate for modeling applications involving asynchronous communication. The method is eminently compatible with classical OO development processes since it can be used to synthesize test cases from the scenarios used in early development stages to model global interactions between actors and components, instead of these test cases being derived manually. We illustrate the system test synthesis process using an air traffic control software example","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123327","Formal methods;testing tools;object-oriented design methods.","Software testing;Unified modeling language;Object oriented modeling;System testing;Context modeling;Control system synthesis;Programming;Buildings;Automatic testing;Asynchronous communication","distributed processing;formal specification;object-oriented programming;program testing;program verification;Unified Modeling Language","object-oriented software development;formal specification;interaction-based modeling;UML sequence diagram;state-based modeling;formal verification;labeled transition system;asynchronous communication;air traffic control;test synthesis;distributed software","","24","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic and Automatic Feedback-Based Threshold Adaptation for Code Smell Detection","H. Liu; Q. Liu; Z. Niu; Y. Liu","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Software Engineering","","2016","42","6","544","558","Most code smell detection tools expose thresholds to engineers for customization because code smell detection is essentially subjective and application specific. Another reason why engineers should customize these thresholds is that they have different working schedules and different requirements on software quality. They have their own unique need on precision and recall in smell detection. This unique need should be fulfilled by adjusting thresholds of smell detection tools. However, it is difficult for software engineers, especially inexperienced ones, to adjust often contradicting and related thresholds manually. One of the possible reasons is that engineers do not know the exact quantitative relation between threshold values and performance, e.g., precision. In this paper, we propose an approach to adapting thresholds automatically and dynamically. Engineers set a target precision manually according to their working schedules and quality requirements. With feedback from engineers, the proposed approach then automatically searches for a threshold setting to maximize recall while having precision close to the target precision. The proposed approach has been evaluated on open-source applications. Evaluation results suggest that the proposed approach is effective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2503740","National Natural Science Foundation of China; Program for New Century Excellent Talents in University; Beijing Higher Education Young Elite Teacher Project; National Natural Science Foundation of China; National Strategic Basic Research Program (“973” Program); Ministry of Science and Technology of China; The 111 Project of Beijing Institute of Technology; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7337457","Software Refactoring;Code Smells;Feedback Control;Smell Identification;Software refactoring;code smells;feedback control;smell identification","Software;Detection algorithms;Cloning;Genetic algorithms;Schedules;Algorithm design and analysis;Measurement","software maintenance;software quality","feedback-based threshold adaptation;code smell detection;software quality;software engineering;open-source applications","","5","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Round-Up: Runtime Verification of Quasi Linearizability for Concurrent Data Structures","L. Zhang; A. Chattopadhyay; C. Wang","Bradley Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University (Virginia Tech), Blacksburg, VA; Bradley Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University (Virginia Tech), Blacksburg, VA; Bradley Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University (Virginia Tech), Blacksburg, VA","IEEE Transactions on Software Engineering","","2015","41","12","1202","1216","We propose a new method for runtime checking of a relaxed consistency property called quasi linearizability for concurrent data structures.Quasi linearizability generalizes the standard notion of linearizability by introducing nondeterminism into the parallel computations quantitatively and then exploiting such nondeterminism to improve the runtime performance. However, ensuring the quantitative aspects of this correctness condition in the low-level code of the concurrent data structure implementation is a difficult task.Our runtime verification method is the first fully automated method for checking quasi linearizability in the C/C++ code of concurrent data structures. It guarantees that all the reported quasi linearizability violations manifested by the concurrent executions are real violations. We have implemented our method in a software tool based on the LLVM compiler and a systematic concurrency testing tool called Inspect. Our experimental evaluation shows that the new method is effective in detecting quasi linearizability violations in the source code implementations of concurrent data structures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2467371","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7192659","Runtime verification;linearizability;serializability;atomicity;relaxed consistency;systematic concurrency testing;partial order reduction;Runtime verification;linearizability;serializability;atomicity;relaxed consistency;systematic concurrency testing;partial order reduction","History;Data structures;Legal aspects;Runtime;Concurrent computing","C++ language;data structures;program compilers;software tools","source code;Inspect;systematic concurrency testing tool;LLVM compiler;software tool;concurrent executions;quasi linearizability violations;C/C++ code;fully automated method;runtime verification method;low-level code;correctness condition;nondeterminism;relaxed consistency property;runtime checking;concurrent data structures","","1","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Task Environment Complexity, Global Team Dispersion, Process Capabilities, and Coordination in Software Development","G. Lee; J. A. Espinosa; W. H. DeLone","American University, Washington; American University, Washington; American University, Washington","IEEE Transactions on Software Engineering","","2013","39","12","1753","1771","Software development teams are increasingly global. Team members are separated by multiple boundaries such as geographic location, time zone, culture, and organization, presenting substantial coordination challenges. Global software development becomes even more challenging when user requirements change dynamically. However, little empirical research has investigated how team dispersion across multiple boundaries and user requirements dynamism, which collectively increase task environment complexity, influence team coordination and software development success in the global context. Further, we have a limited understanding of how software process capabilities such as rigor, standardization, agility, and customizability mitigate the negative effects of global team dispersion and user requirements dynamism. To address these important issues, we test a set of relevant hypotheses using field survey data obtained from both project managers and stakeholders. Our results show that global team dispersion and user requirements dynamism have a negative effect on coordination effectiveness. We find that the negative effect of global team dispersion on coordination effectiveness decreases as process standardization increases and that the negative effect of user requirements dynamism on coordination effectiveness decreases as process agility increases. We find that coordination effectiveness has a positive effect on global software development success in terms of both process and product aspects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6583162","Global boundaries;global software development;user requirements dynamism;software process capability;task environment complexity;team coordination;team dispersion","User centered design;Complexity theory;Dispersion;Global communication;Software development;Process capability;Globalization","software development management;team working","task environment complexity;global team dispersion;process capabilities;coordination;software development teams;geographic location;time zone;culture;organization;global software development;user requirements dynamism;rigor capability;standardization capability;agility capability;customizability capability;coordination effectiveness;process aspect;product aspect","","9","","101","","","","","","IEEE","IEEE Journals & Magazines"
"EDZL Schedulability Analysis in Real-Time Multicore Scheduling","J. Lee; I. Shin","The University of Michigan, Ann Arbor; KAIST, Daejeon","IEEE Transactions on Software Engineering","","2013","39","7","910","916","In real-time systems, correctness depends not only on functionality but also on timeliness. A great number of scheduling theories have been developed for verification of the temporal correctness of jobs (software) in such systems. Among them, the Earliest Deadline first until Zero-Laxity (EDZL) scheduling algorithm has received growing attention thanks to its effectiveness in multicore real-time scheduling. However, the true potential of EDZL has not yet been fully exploited in its schedulability analysis as the state-of-the-art EDZL analysis techniques involve considerable pessimism. In this paper, we propose a new EDZL multicore schedulability test. We first introduce an interesting observation that suggests an insight toward pessimism reduction in the schedulability analysis of EDZL. We then incorporate it into a well-known existing Earliest Deadline First (EDF) schedulability test, resulting in a new EDZL schedulability test. We demonstrate that the proposed EDZL test not only has lower time complexity than existing EDZL schedulability tests, but also significantly improves the schedulability of EDZL by up to 36.6 percent compared to the best existing EDZL schedulability tests.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.75","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6374195","Earliest Deadline first until Zero-Laxity (EDZL);real-time scheduling;schedulability analysis;multicore platform;real-time systems","Real-time systems;Silicon;Scheduling algorithms;Scheduling;Exponential distribution;Time factors;Aerospace electronics","multiprocessing systems;processor scheduling;real-time systems","EDZL schedulability analysis;real-time multicore scheduling;correctness;earliest deadline first until zero-laxity scheduling algorithm;time complexity","","8","","22","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Review of Theory Use in Software Engineering Experiments","J. E. Hannay; D. I. K. Sjoberg; T. Dyba","Simula Research Laboratory, Department of Software Engineering, Lysaker, Norway; Simula Research Laboratory, Department of Software Engineering, Lysaker, Norway; Simula Research Laboratory and with SINTEF ICT, Department of Software Engineering, Safety, and Security, Trondheim, Norway","IEEE Transactions on Software Engineering","","2007","33","2","87","107","Empirically based theories are generally perceived as foundational to science. However, in many disciplines, the nature, role and even the necessity of theories remain matters for debate, particularly in young or practical disciplines such as software engineering. This article reports a systematic review of the explicit use of theory in a comprehensive set of 103 articles reporting experiments, from of a total of 5,453 articles published in major software engineering journals and conferences in the decade 1993-2002. Of the 103 articles, 24 use a total of 40 theories in various ways to explain the cause-effect relationship(s) under investigation. The majority of these use theory in the experimental design to justify research questions and hypotheses, some use theory to provide post hoc explanations of their results, and a few test or modify theory. A third of the theories are proposed by authors of the reviewed articles. The interdisciplinary nature of the theories used is greater than that of research in software engineering in general. We found that theory use and awareness of theoretical issues are present, but that theory-driven research is, as yet, not a major issue in empirical software engineering. Several articles comment explicitly on the lack of relevant theory. We call for an increased awareness of the potential benefits of involving theory, when feasible. To support software engineering researchers who wish to use theory, we show which of the reviewed articles on which topics use which theories for what purposes, as well as details of the theories' characteristics","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052585","Theory;experiments;research methodology;empirical software engineering.","Software engineering;Design for experiments;Testing;Computer industry;Programming","software engineering","software engineering;cause-effect relationship","","78","","182","","","","","","IEEE","IEEE Journals & Magazines"
"Confirming configurations in EFSM testing","A. Petrenko; S. Boroday; R. Groz","Centre de Recherche Informatique de Montreal, Que., Canada; Centre de Recherche Informatique de Montreal, Que., Canada; NA","IEEE Transactions on Software Engineering","","2004","30","1","29","42","We investigate the problem of configuration verification for the extended FSM (EFSM) model. This is an extension of the FSM state identification problem. Specifically, given a configuration (""state vector"") and an arbitrary set of configurations, determine an input sequence such that the EFSM in the given configuration produces an output sequence different from that of the configurations in the given set or at least in a maximal proper subset. Such a sequence can be used in a test case to confirm the destination configuration of a particular EFSM transition. We demonstrate that this problem could be reduced to the EFSM traversal problem, so that the existing methods and tools developed in the context of model checking become applicable. We introduce notions of EFSM projections and products and, based on these notions, we develop a theoretical framework for determining configuration-confirming sequences. The proposed approach is illustrated on a realistic example.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265734","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265734","","System testing;Acoustic testing;Tail;Power system modeling;Fault detection;Context modeling;Design methodology;Automata;Formal specifications;Standards development","conformance testing;formal verification;finite state machines;specification languages;program testing","configuration verification;extended FSM model;finite state machine;FSM state identification;EFSM traversal problem;model checking;configuration-confirming sequence;formal method;functional testing;conformance testing;model based testing","","94","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Supporting Process Model Validation through Natural Language Generation","H. Leopold; J. Mendling; A. Polyvyanyy","WU Vienna, Austria.; WU Vienna, Austria.; Queensland University of Technology, Brisbane, Australia.","IEEE Transactions on Software Engineering","","2014","40","8","818","840","The design and development of process-aware information systems is often supported by specifying requirements as business process models. Although this approach is generally accepted as an effective strategy, it remains a fundamental challenge to adequately validate these models given the diverging skill set of domain experts and system analysts. As domain experts often do not feel confident in judging the correctness and completeness of process models that system analysts create, the validation often has to regress to a discourse using natural language. In order to support such a discourse appropriately, so-called verbalization techniques have been defined for different types of conceptual models. However, there is currently no sophisticated technique available that is capable of generating natural-looking text from process models. In this paper, we address this research gap and propose a technique for generating natural language texts from business process models. A comparison with manually created process descriptions demonstrates that the generated texts are superior in terms of completeness, structure, and linguistic complexity. An evaluation with users further demonstrates that the texts are very understandable and effectively allow the reader to infer the process model semantics. Hence, the generated texts represent a useful input for process model validation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2327044","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6823180","Business process model validation;natural language text generation;verbalization","Unified modeling language;Natural languages;Business;Analytical models;Adaptation models;Context modeling;Context","information systems;natural language processing","process model validation;linguistic complexity;structure complexity;completeness complexity;natural language text generation;natural-looking text generation;verbalization techniques;process model completeness;process model correctness;business process models;process-aware information systems;natural language generation","","26","","110","","","","","","IEEE","IEEE Journals & Magazines"
"Determining the Proper Number and Price of Software Licenses","M. Murtojarvi; J. Jarvinen; M. Johnsson; T. Leipala; O. S. Nevalainen","NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","5","305","315","Software houses sell their products by transferring usage licenses of various software components to the customers. Depending on the kind of software, there are several different license types that allow controlled access of services. The two most popular types are the fixed license, which gives access rights for an identified workstation, and the floating license, which restricts the number of simultaneous users to a certain bound. The latter of these types is advantageous when the users do not demand full-time services and occasional lack of access is bearable. The problem of deciding the number of floating licenses is studied in the present paper. Based on the expected usage profile of the software, we calculate the minimal number of licenses that guarantees that the customers get service better than a given lower bound. The problem is studied by using certain queuing models, known as the Erlang toss system, the Erlang delay system, and the Engset model. None of these analytic models consider, however, the transient period that we analyze by means of simulation and by the so-called modified offered load approximation. We also give simple formulas presenting how the number of software licenses needed to keep the probability of nonaccess below a given blocking level grows as a function of the offered load, which is the proportion of the time used in the case that all requests were successful. Results of the study may be used for setting license prices and for determining the proper number of licenses.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1003","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160969","Software release management and delivery;queuing theory;mathematical modeling;simulation.","Licenses;Workstations;Queueing analysis;Software packages;Packaging;Permission;Delay systems;Transient analysis;Analytical models;Mathematical model","contracts;industrial property;queueing theory;software houses;software management","Engset model;Erlang delay system;Erlang toss system;queuing model;software release management;software license","","8","","16","","","","","","IEEE","IEEE Journals & Magazines"
"An experimental comparison of usage-based and checklist-based reading","T. Thelin; P. Runeson; C. Wohlin","Dept. of Commun. Syst., Lund Univ., Sweden; Dept. of Commun. Syst., Lund Univ., Sweden; NA","IEEE Transactions on Software Engineering","","2003","29","8","687","704","Software quality can be defined as the customers' perception of how a system works. Inspection is a method to monitor and control the quality throughout the development cycle. Reading techniques applied to inspections help reviewers to stay focused on the important parts of an artifact when inspecting. However, many reading techniques focus on finding as many faults as possible, regardless of their importance. Usage-based reading helps reviewers to focus on the most important parts of a software artifact from a user's point of view. We present an experiment, which compares usage-based and checklist-based reading. The results show that reviewers applying usage-based reading are more efficient and effective in detecting the most critical faults from a user's point of view than reviewers using checklist-based reading. Usage-based reading may be preferable for software organizations that utilize or start utilizing use cases in their software development.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223644","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223644","","Inspection;Programming;Computer industry;Monitoring;Fault detection;Bioreactors;Phase estimation;Books;Software engineering;Software testing","software quality;software reviews;inspection;program verification;program testing;software reliability","software quality;software inspection;software development cycle;reading technique;usage-based reading;checklist-based reading;fault detection;software organization;empirical study;software review;controlled experiment;use cases;software validation","","56","","52","","","","","","IEEE","IEEE Journals & Magazines"
"The “Physics” of Notations: Toward a Scientific Basis for Constructing Visual Notations in Software Engineering","D. Moody","University of Twente , Enschede","IEEE Transactions on Software Engineering","","2009","35","6","756","779","Visual notations form an integral part of the language of software engineering (SE). Yet historically, SE researchers and notation designers have ignored or undervalued issues of visual representation. In evaluating and comparing notations, details of visual syntax are rarely discussed. In designing notations, the majority of effort is spent on semantics, with graphical conventions largely an afterthought. Typically, no design rationale, scientific or otherwise, is provided for visual representation choices. While SE has developed mature methods for evaluating and designing semantics, it lacks equivalent methods for visual syntax. This paper defines a set of principles for designing cognitively effective visual notations: ones that are optimized for human communication and problem solving. Together these form a design theory, called the Physics of Notations as it focuses on the physical (perceptual) properties of notations rather than their logical (semantic) properties. The principles were synthesized from theory and empirical evidence from a wide range of fields and rest on an explicit theory of how visual notations communicate. They can be used to evaluate, compare, and improve existing visual notations as well as to construct new ones. The paper identifies serious design flaws in some of the leading SE notations, together with practical suggestions for improving them. It also showcases some examples of visual notation design excellence from SE and other fields.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5353439","Modeling;analysis;diagrams;communication;visualization;visual syntax;concrete syntax.","Software engineering;Humans;Visualization;Design optimization;Problem-solving;Physics;Concrete;Flowcharts;Unified modeling language;Computer industry","software engineering","visual notations;software engineering;visual representation;physics of notations;design flaws","","416","","152","","","","","","IEEE","IEEE Journals & Magazines"
"Verification and Trade-Off Analysis of Security Properties in UML System Models","G. Georg; K. Anastasakis; B. Bordbar; S. H. Houmb; I. Ray; M. Toahchoodee","Colorado State University, Fort Collins, CO; University of Birmingham, Birmingham, UK; University of Birmingham, Birmingham, UK; Telenor GBDR, Trondheim, Norway; Colorado State University, Fort Collins, CO; Colorado State University, Fort Collins, CO","IEEE Transactions on Software Engineering","","2010","36","3","338","356","Designing secure systems is a nontrivial task. Incomplete or faulty designs can cause security mechanisms to be incorrectly incorporated in a system, allowing them to be bypassed and resulting in a security breach. We advocate the use of the Aspect-Oriented Risk-Driven Development (AORDD) methodology for developing secure systems. This methodology begins with designers defining system assets, identifying potential attacks against them, and evaluating system risks. When a risk is unacceptable, designers must mitigate the associated threat by incorporating security mechanisms methodically into the system design. Designers next formally evaluate the resulting design to ensure that the threat has been mitigated, while still allowing development to meet other project constraints. In this paper, we focus on the AORDD analysis, which consists of: (1) a formal security evaluation and (2) a trade-off analysis that enables system designers to position alternative security solutions against each other. The formal security evaluation uses the Alloy Analyzer to provide assurance that an incorporated security mechanism performs as expected and makes the system resilient to previously identified attacks. The trade-off analysis uses a Bayesian Belief Network topology to allow equally effective security mechanisms to be compared against system security requirements and other factors such as time-to-market and budget constraints.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5432225","Aspect-oriented modeling (AOM);Bayesian belief network (BBN);security analysis;trade-off analysis.","Unified modeling language;Protection;Standards development;ISO standards;Data security;Design methodology;Bayesian methods;Risk management;Computer security;Computer Society","aspect-oriented programming;belief networks;formal verification;security of data;Unified Modeling Language","trade-off analysis;security properties verification;UML system models;secure systems design;aspect-oriented risk-driven development;AORDD methodology;risk evaluation;formal security evaluation;Alloy Analyzer;Bayesian belief network topology;time-to-market;budget constraints","","11","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Programmer-Friendly Refactoring Errors","E. Murphy-Hill; A. P. Black","North Carolina State University, Raleigh; Portland State University, Portland","IEEE Transactions on Software Engineering","","2012","38","6","1417","1431","Refactoring tools, common to many integrated development environments, can help programmers to restructure their code. These tools sometimes refuse to restructure the programmer's code, instead giving the programmer a textual error message that she must decode if she wishes to understand the reason for the tool's refusal and what corrective action to take. This paper describes a graphical alternative to textual error messages called Refactoring Annotations. It reports on two experiments, one using an integrated development environment and the other using paper mockups, that show that programmers can use Refactoring Annotations to quickly and accurately understand the cause of refactoring errors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.110","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6072219","Refactoring;refactoring errors;usability;programmers;tools","Taxonomy;Programming;Prototypes;Visualization;Java","computer graphics;software maintenance","programmer-friendly refactoring errors;refactoring tools;textual error message;graphical alternative;textual error messages;refactoring annotations;integrated development environment;paper mockups","","4","","22","","","","","","IEEE","IEEE Journals & Magazines"
"MNav: A Markov Model-Based Web Site Navigability Measure","Y. Zhou; H. Leung; P. Winoto","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","869","890","Web site success is significantly associated with navigability, an important attribute of usability that denotes the ease with which users find desired information as they move through a Web site. Navigable Web sites allow users to form a mental model of the type and location of information in the Web site and an expectation of where and to what a particular hyperlink will lead. Existing navigability measures are based mainly on the static hyperlink structure of a Web site. Such measures, however, have two main drawbacks: 1) the effect on navigability of a hyperlink structure cannot be well characterized and 2) the effect on navigability of the navigation aids (such as the ""Back"" button provided by a browser) is ignored. In this paper, we abstract a dynamic Web surfing behavior as a Markov model which synthesizes typical surfing actions. Based on this model, we propose a novel navigability measure MNav. The experimental results show that MNav can be efficiently computed and it provides an effective and useful measurement of Web site navigability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70743","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359469","Measure;Markov chain;Navigability;Measure;Markov chain;Navigability","Navigation;Web pages;Computer Society;Usability;Cognitive science;Internet;Web design","information retrieval;Markov processes;Web sites","usability;information location;hyperlink;MNav;Markov model-based Web site navigability measure","","16","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Class point: an approach for the size estimation of object-oriented systems","G. Costagliola; F. Ferrucci; G. Tortora; G. Vitiello","Dipt. di Matematica e Informatica, Salerno Univ., Italy; Dipt. di Matematica e Informatica, Salerno Univ., Italy; Dipt. di Matematica e Informatica, Salerno Univ., Italy; Dipt. di Matematica e Informatica, Salerno Univ., Italy","IEEE Transactions on Software Engineering","","2005","31","1","52","74","In this paper, we present an FP-like approach, named class point, which was conceived to estimate the size of object-oriented products. In particular, two measures are proposed, which are theoretically validated showing that they satisfy well-known properties necessary for size measures. An initial, empirical validation is also performed, meant to assess the usefulness and effectiveness of the proposed measures to predict the development effort of object-oriented systems. Moreover, a comparative analysis is carried out, taking into account several other size measures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.5","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392720","Index Terms- Object-oriented systems;size measures;Function Point Analysis;theoretical validation;empirical validation;effort prediction model.","Size measurement;Software measurement;Particle measurements;Object oriented modeling;Costs;Performance evaluation;Predictive models;Current measurement;Computer Society;Maintenance","object-oriented programming;statistical analysis;prediction theory;software cost estimation;software metrics;software quality","class point;object-oriented systems;function point analysis","","51","","73","","","","","","IEEE","IEEE Journals & Magazines"
"API-Evolution Support with Diff-CatchUp","Z. Xing; E. Stroulia","NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","818","836","Applications built on reusable component frameworks are subject to two independent, and potentially conflicting, evolution processes. The application evolves in response to the specific requirements and desired qualities of the application's stakeholders. On the other hand, the evolution of the component framework is driven by the need to improve the framework functionality and quality while maintaining its generality. Thus, changes to the component framework frequently change its API on which its client applications rely and, as a result, these applications break. To date, there has been some work aimed at supporting the migration of client applications to newer versions of their underlying frameworks, but it usually requires that the framework developers do additional work for that purpose or that the application developers use the same tools as the framework developers. In this paper, we discuss our approach to tackle the API-evolution problem in the context of reuse-based software development, which automatically recognizes the API changes of the reused framework and proposes plausible replacements to the ""obsolete"" API based on working examples of the framework code base. This approach has been implemented in the Diff-CatchUp tool. We report on two case studies that we have conducted to evaluate the effectiveness of our approach with its Diff-CatchUp prototype.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70747","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359473","D.2.2.eProgrammer workbench;D.2.3Coding Tools and Techniques;D.2.3.aObject-oriented programming;D.2.10.g Object-oriented design methods;D.2.2.eProgrammer workbench;D.2.3Coding Tools and Techniques;D.2.3.aObject-oriented programming;D.2.10.g Object-oriented design methods","Application software;Programming;Software prototyping;Prototypes;Costs;Software engineering;Software tools;Documentation","application program interfaces;object-oriented programming;software maintenance;software prototyping;software quality;software reusability;software tools","API-evolution support;reuse-based software development;Diff-CatchUp tool;reusable component framework evolution;component framework functionality;component framework quality;component framework maintenance;client application migration","","41","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A Tool-Supported Methodology for Validation and Refinement of Early-Stage Domain Models","M. Autili; A. Bertolino; G. De Angelis; D. D. Ruscio; A. D. Sandro","Department of Information Engineering Computer Science and Mathematics University of L’Aquila, Italy; CNR-ISTI of Pisa, Italy; CNR-ISTI of Pisa, Italy; Department of Information Engineering Computer Science and Mathematics University of L’Aquila, Italy; Department of Computer Science, University of Toronto, Toronto, ON, Canada","IEEE Transactions on Software Engineering","","2016","42","1","2","25","Model-driven engineering (MDE) promotes automated model transformations along the entire development process. Guaranteeing the quality of early models is essential for a successful application of MDE techniques and related tool-supported model refinements. Do these models properly reflect the requirements elicited from the owners of the problem domain? Ultimately, this question needs to be asked to the domain experts. The problem is that a gap exists between the respective backgrounds of modeling experts and domain experts. MDE developers cannot show a model to the domain experts and simply ask them whether it is correct with respect to the requirements they had in mind. To facilitate their interaction and make such validation more systematic, we propose a methodology and a tool that derive a set of customizable questionnaires expressed in natural language from each model to be validated. Unexpected answers by domain experts help to identify those portions of the models requiring deeper attention. We illustrate the methodology and the current status of the developed tool MOTHIA, which can handle UML Use Case, Class, and Activity diagrams. We assess MOTHIA effectiveness in reducing the gap between domain and modeling experts, and in detecting modeling faults on the European Project CHOReOS.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2449319","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7132782","Domain Modeling;Early Stage Model;Model Driven Engineering;Model Refinement;Model Validation;Natural Language Questionnaires;Semantic Model Quality;Domain modeling;early stage model;model driven engineering;model refinement;model validation;natural language questionnaires;semantic model quality","Unified modeling language;Semantics;Context modeling;Load modeling;Engines;Biological system modeling;Context","natural language processing;Unified Modeling Language","early-stage domain model;model-driven engineering;automated model transformation;MDE technique;tool-supported model refinement;natural language;MOTHIA;UML;activity diagram","","1","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Iterative reengineering of legacy systems","A. Bianchi; D. Caivano; V. Marengo; G. Visaggio","Dipt. di Informatica, Bari Univ., Italy; Dipt. di Informatica, Bari Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","2003","29","3","225","241","During its life, a legacy system is subjected to many maintenance activities, which cause degradation of the quality of the system: When this degradation exceeds a critical threshold, the legacy system needs to be reengineered. In order to preserve the asset represented by the legacy system, the familiarity with it gained by the system's maintainers and users, and the continuity of execution of current operations during the reengineering process, the system needs to be reengineered gradually. Moreover, each program needs to be reengineered within a short period of time. The paper proposes a reengineering process model, which is applied to an in-use legacy system to confirm that the process satisfies previous requirements and to measure its effectiveness. The reengineered system replaced the legacy one to the satisfaction of all the stakeholders; the reengineering process also had a satisfactory impact on the quality of the system. Finally, this paper contributes to validate the cause-effect relationship between the reengineering process and overcoming the aging symptoms of a software system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183932","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183932","","Degradation;Aging;Computer Society;Software systems;Vehicles;Business process re-engineering;Software maintenance;Software quality;Q factor","systems re-engineering;software maintenance","legacy system;reengineering;degradation;reengineering process model","","38","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Do Crosscutting Concerns Cause Defects?","M. Eaddy; T. Zimmermann; K. D. Sherwood; V. Garg; G. C. Murphy; N. Nagappan; A. V. Aho","Columbia University, NY; University of Calgary, Calgary; University of British Columbia, Vancouver; Columbia University, NY; University of British Columbia, Vancouver; Microsoft Research, Redmond; Columbia University, NY","IEEE Transactions on Software Engineering","","2008","34","4","497","515","There is a growing consensus that crosscutting concerns harm code quality. An example of a crosscutting concern is a functional requirement whose implementation is distributed across multiple software modules. We asked the question, ""How much does the amount that a concern is crosscutting affect the number of defects in a program?"" We conducted three extensive case studies to help answer this question. All three studies revealed a moderate to strong statistically significant correlation between the degree of scattering and the number of defects. This paper describes the experimental framework we developed to conduct the studies, the metrics we adopted and developed to measure the degree of scattering, the studies we performed, the efforts we undertook to remove experimental and other biases, and the results we obtained. In the process, we have formulated a theory that explains why increased scattering might lead to increased defects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4527257","Metrics/Measurement;Validation;Quality analysis and evaluation;Software Construction;Software Quality/SQA;Correlation and regression analysis;User/Machine Systems;Distribution Maintenance and Enhancement;Software Engineering;Metrics/Measurement;Validation;Quality analysis and evaluation;Software Construction;Software Quality/SQA;Correlation and regression analysis;User/Machine Systems;Distribution Maintenance and Enhancement;Software Engineering","Scattering;Open source software;Software testing;Computer Society;Performance evaluation;Statistical analysis;Software engineering;Creep;Inspection;Failure analysis","data mining;software engineering","crosscutting concerns;code quality;software modules;statistically significant correlation","","116","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Variability Analysis of Requirements: Considering Behavioral Differences and Reflecting Stakeholders’ Perspectives","N. Itzik; I. Reinhartz-Berger; Y. Wand","Department of Information Systems, University of Haifa, Haifa, Israel; Department of Information Systems, University of Haifa, Haifa, Israel; Sauder School of Business, University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Software Engineering","","2016","42","7","687","706","Adoption of Software Product Line Engineering (SPLE) to support systematic reuse of software-related artifacts within product families is challenging, time-consuming and error-prone. Analyzing the variability of existing artifacts needs to reflect different perspectives and preferences of stakeholders in order to facilitate decisions in SPLE adoption. Considering that requirements drive many development methods and activities, we introduce an approach to analyze variability of behaviors as presented in functional requirements. The approach, called semantic and ontological variability analysis (SOVA), uses ontological and semantic considerations to automatically analyze differences between initial states (preconditions), external events (triggers) that act on the system, and final states (post-conditions) of behaviors. The approach generates feature diagrams typically used in SPLE to model variability. Those diagrams are organized according to perspective profiles, reflecting the needs and preferences of the potential stakeholders for given tasks. We conducted an empirical study to examine the usefulness of the approach by comparing it to an existing tool which is mainly based on a latent semantic analysis measurement. SOVA appears to create outputs that are more comprehensible in significantly shorter times. These results demonstrate SOVA's potential to allow for flexible, behavior-oriented variability analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2512599","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7366597","Software product line engineering;variability analysis;feature diagrams;requirements specifications;ontology","Stakeholders;Software;Semantics;Feature extraction;Software product lines;Systematics","formal specification;ontologies (artificial intelligence);software product lines","behavior-oriented variability analysis;latent semantic analysis measurement;model variability;feature diagrams;SOVA;semantic-and-ontological variability analysis;SPLE adoption;software product line engineering;requirements variability analysis","","11","","53","","","","","","IEEE","IEEE Journals & Magazines"
"A Static Approach to Prioritizing JUnit Test Cases","H. Mei; D. Hao; L. Zhang; L. Zhang; J. Zhou; G. Rothermel","Peking University, Beijing; Peking University, Beijing; Peking University, Beijing; Peking University, Beijing; Peking University, Beijing; University of Nebraska, Lincoln","IEEE Transactions on Software Engineering","","2012","38","6","1258","1275","Test case prioritization is used in regression testing to schedule the execution order of test cases so as to expose faults earlier in testing. Over the past few years, many test case prioritization techniques have been proposed in the literature. Most of these techniques require data on dynamic execution in the form of code coverage information for test cases. However, the collection of dynamic code coverage information on test cases has several associated drawbacks including cost increases and reduction in prioritization precision. In this paper, we propose an approach to prioritizing test cases in the absence of coverage information that operates on Java programs tested under the JUnit framework-an increasingly popular class of systems. Our approach, JUnit test case Prioritization Techniques operating in the Absence of coverage information (JUPTA), analyzes the static call graphs of JUnit test cases and the program under test to estimate the ability of each test case to achieve code coverage, and then schedules the order of these test cases based on those estimates. To evaluate the effectiveness of JUPTA, we conducted an empirical study on 19 versions of four Java programs ranging from 2K-80K lines of code, and compared several variants of JUPTA with three control techniques, and several other existing dynamic coverage-based test case prioritization techniques, assessing the abilities of the techniques to increase the rate of fault detection of test suites. Our results show that the test suites constructed by JUPTA are more effective than those in random and untreated test orders in terms of fault-detection effectiveness. Although the test suites constructed by dynamic coverage-based techniques retain fault-detection effectiveness advantages, the fault-detection effectiveness of the test suites constructed by JUPTA is close to that of the test suites constructed by those techniques, and the fault-detection effectiveness of the test suites constructed by some of JUPTA's variants is better than that of the test suites constructed by several of those techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.106","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363461","Software testing;regression testing;test case prioritization;JUnit;call graph","Software testing;Regression analysis;Scheduling","Java;program testing;regression analysis;software fault tolerance","static approach;regression testing;test case prioritization techniques;dynamic code coverage information;Java programs;JUnit test case prioritization techniques operating in the absence of coverage information;JUPTA;static call graphs;fault-detection effectiveness;dynamic coverage-based techniques","","47","","44","","","","","","IEEE","IEEE Journals & Magazines"
"A Qualitative Study of Application-Level Caching","J. Mertz; I. Nunes","Instituto de Inform&#x00E1;tica, Universidade Federal do Rio Grande do Sul, Porto Alegre-RS, Brazil; Instituto de Inform&#x00E1;tica, Universidade Federal do Rio Grande do Sul, Porto Alegre-RS, Brazil","IEEE Transactions on Software Engineering","","2017","43","9","798","816","Latency and cost of Internet-based services are encouraging the use of application-level caching to continue satisfying users' demands, and improve the scalability and availability of origin servers. Despite its popularity, this level of caching involves the manual implementation by developers and is typically addressed in an ad-hoc way, given that it depends on specific details of the application. As a result, application-level caching is a time-consuming and error-prone task, becoming a common source of bugs. Furthermore, it forces application developers to reason about a crosscutting concern, which is unrelated to the application business logic. In this paper, we present the results of a qualitative study of how developers handle caching logic in their web applications, which involved the investigation of ten software projects with different characteristics. The study we designed is based on comparative and interactive principles of grounded theory, and the analysis of our data allowed us to extract and understand how developers address cache-related concerns to improve performance and scalability of their web applications. Based on our analysis, we derived guidelines and patterns, which guide developers while designing, implementing and maintaining application-level caching, thus supporting developers in this challenging task that is crucial for enterprise web applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2633992","CNPq; CNPq; CAPES; BRA; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7762909","Application-level caching;qualitative study;pattern;guideline;web application","Databases;Guidelines;Maintenance engineering;Servers;Software;Scalability;HTML","cache storage;Internet;project management;software management","application-level caching;Internet-based services;caching logic;Web applications;software projects","","","","59","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Analyzing data sets with missing data: an empirical evaluation of imputation methods and likelihood-based methods","I. Myrtveit; E. Stensrud; U. H. Olsson","Norwegian Sch. of Manage., Sandvika, Norway; NA; NA","IEEE Transactions on Software Engineering","","2001","27","11","999","1013","Missing data are often encountered in data sets used to construct software effort prediction models. Thus far, the common practice has been to ignore observations with missing data. This may result in biased prediction models. The authors evaluate four missing data techniques (MDTs) in the context of software cost modeling: listwise deletion (LD), mean imputation (MI), similar response pattern imputation (SRPI), and full information maximum likelihood (FIML). We apply the MDTs to an ERP data set, and thereafter construct regression-based prediction models using the resulting data sets. The evaluation suggests that only FIML is appropriate when the data are not missing completely at random (MCAR). Unlike FIML, prediction models constructed on LD, MI and SRPI data sets will be biased unless the data are MCAR. Furthermore, compared to LD, MI and SRPI seem appropriate only if the resulting LD data set is too small to enable the construction of a meaningful regression-based prediction model.","0098-5589;1939-3520;2326-3881","","10.1109/32.965340","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=965340","","Data analysis;Predictive models;Costs;Enterprise resource planning;Maximum likelihood estimation;Software engineering;Information analysis;Context modeling;Software standards;Databases","data analysis;software cost estimation;maximum likelihood estimation;statistical analysis","data set analysis;biased prediction models;missing data techniques;MDTs;software cost modeling;listwise deletion;mean imputation;similar response pattern imputation;information maximum likelihood;ERP data set;regression-based prediction models;FIML;missing completely at random;MCAR;SRPI data sets;regression-based prediction model;LD;MI;software effort prediction models","","107","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Mutable Protection Domains: Adapting System Fault Isolation for Reliability and Efficiency","G. Parmer; R. West","The George Washington University, Washtington, DC; Boston University, Boston","IEEE Transactions on Software Engineering","","2012","38","4","875","888","As software systems are becoming increasingly complex, the likelihood of faults and unexpected behaviors will naturally increase. Today, mobile devices to large-scale servers feature many millions of lines of code. Compile-time checks and offline verification methods are unlikely to capture all system states and control flow interactions of a running system. For this reason, many researchers have developed methods to contain faults at runtime by using software and hardware-based techniques to define protection domains. However, these approaches tend to impose isolation boundaries on software components that are static, and thus remain intact while the system is running. An unfortunate consequence of statically structured protection domains is that they may impose undue overhead on the communication between separate components. This paper proposes a new runtime technique that trades communication cost for fault isolation. We describe Mutable Protection Domains (MPDs) in the context of our Composite operating system. MPD dynamically adapts hardware isolation between interacting software components, depending on observed communication “hot-paths,” with the purpose of maximizing fault isolation where possible. In this sense, MPD naturally tends toward a system of maximal component isolation, while collapsing protection domains where costs are prohibitive. By increasing isolation for low-cost interacting components, MPD limits the scope of impact of future unexpected faults. We demonstrate the utility of MPD using a webserver, and identify different hot-paths for different workloads that dictate adaptations to system structure. Experiments show up to 40 percent improvement in throughput compared to a statically organized system, while maintaining high-fault isolation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.61","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928356","Component-based;operating systems;reliability;fault isolation;performance","Kernel;Reliability;Hardware;Servers;Switches","fault tolerant computing;mobile computing;object-oriented programming;operating systems (computers)","mutable protection domains;system fault isolation;software systems;mobile devices;large-scale servers;compile-time checks;offline verification methods;control flow interactions;hardware-based techniques;software -based techniques;software components;MPD;composite operating system;fault isolation;maximal component isolation;mobile computing","","3","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Engineering dynamic real-time distributed systems: architecture, system description language, and middleware","B. Ravindran","Bradley Dept. of Electr. Eng., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA","IEEE Transactions on Software Engineering","","2002","28","1","30","57","The paper presents an architectural framework and algorithms for engineering dynamic real-time distributed systems using commercial off-the-shelf technologies. In the proposed architecture, a real-time system application is developed in a general-purpose programming language. Further, the architectural-level description of the system such as composition and interconnections of application software and hardware, and the operational requirements of the system such as timeliness and survivability are specified in a system description language. The specification of the system is automatically translated into an intermediate representation (IR) that models the system in a platform-independent manner. The IR is augmented with dynamic measurements of the system by a language runtime system to produce a dynamic system model. The dynamic model is used by resource management middleware strategies to perform resource management that achieves timeliness and survivability requirements. We present two classes of algorithms: predictive and availability-based, for performing resource allocation. To validate the viability of the approach, we use a real-time benchmark application that functionally approximates dynamic real-time command and control systems. The benchmark results illustrate that the middleware is able to achieve the desired timeliness requirements during a number of load situations. Furthermore, availability-based allocation algorithms perform resource allocation less frequently, whereas predictive algorithms give a better steady state performance for the application.","0098-5589;1939-3520;2326-3881","","10.1109/32.979988","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979988","","Real time systems;Resource management;Computer architecture;Application software;Middleware;Prediction algorithms;Availability;Heuristic algorithms;Paper technology;Computer languages","bibliographies;application program interfaces;specification languages;distributed programming;real-time systems;command and control systems;resource allocation","dynamic real-time distributed systems engineering;system description language;architectural framework;commercial off-the-shelf technologies;general-purpose programming language;architectural-level description;application software;operational requirements;intermediate representation;IR;dynamic measurements;language runtime system;dynamic system model;resource management middleware strategies;survivability;timeliness;resource allocation;real-time benchmark application;dynamic real-time command and control systems;availability-based allocation algorithms;predictive algorithms;process control systems","","22","","70","","","","","","IEEE","IEEE Journals & Magazines"
"An Approach to Checking Consistency between UML Class Model and Its Java Implementation","H. M. Chavez; W. Shen; R. B. France; B. A. Mechling; G. Li","Computer Science, Kalamazoo, MI; Computer Science, Kalamazoo, MI; Computer Science, Fort Collins, United States; Computer Science, Kalamazoo, MI; State Key Lab. of Computer Science, Institute of Software, Beijing, China","IEEE Transactions on Software Engineering","","2016","42","4","322","344","Model Driven Engineering (MDE) aims to expedite the software development process by providing support for transforming models to running systems. Many modeling tools provide forward engineering features, which automatically translate a model into a skeletal program that developers must complete. Inconsistencies between a design model and its implementation, however, can arise, particularly when a final implementation is developed dependently on the code from which it was generated. Manually checking that an implementation conforms to its model is a daunting task. Thus, an MDE tool that developers can use to check that implementations conform to their models can significantly improve a developer's productivity. This paper presents a model-based approach for testing whether or not an implementation satisfies the constraints imposed by its design model. Our model-based testing approach aims to efficiently reduce the test input space while supporting branch coverage criteria. To evaluate the approach's ability to uncover inconsistencies, we developed a prototypical tool and applied it to the Eclipse UML2 projects. We were able to uncover inconsistencies between the models and their implementations using the tool.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2488645","National Natural Science Foundation of China; Open Project of Shanghai Key Lab. of Trustworthy Computing; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7294689","Class Diagrams;UML;Java;Model checking;Class diagrams;UML;Java;model checking","Unified modeling language;Java;Software;Object oriented modeling;Testing;Semantics","","","","1","","43","","","","","","IEEE","IEEE Journals & Magazines"
"An Exploratory Study of How Developers Seek, Relate, and Collect Relevant Information during Software Maintenance Tasks","A. J. Ko; B. A. Myers; M. J. Coblenz; H. H. Aung","Human-Computer Interaction Institute, School of Computer Science, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213; Human-Computer Interaction Institute, School of Computer Science, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213; mcoblenz@andrew.cmu.edu; hhaung@gmail.com","IEEE Transactions on Software Engineering","","2006","32","12","971","987","Much of software developers' time is spent understanding unfamiliar code. To better understand how developers gain this understanding and how software development environments might be involved, a study was performed in which developers were given an unfamiliar program and asked to work on two debugging tasks and three enhancement tasks for 70 minutes. The study found that developers interleaved three activities. They began by searching for relevant code both manually and using search tools; however, they based their searches on limited and misrepresentative cues in the code, environment, and executing program, often leading to failed searches. When developers found relevant code, they followed its incoming and outgoing dependencies, often returning to it and navigating its other dependencies; while doing so, however, Eclipse's navigational tools caused significant overhead. Developers collected code and other information that they believed would be necessary to edit, duplicate, or otherwise refer to later by encoding it in the interactive state of Eclipse's package explorer, file tabs, and scroll bars. However, developers lost track of relevant code as these interfaces were used for other tasks, and developers were forced to find it again. These issues caused developers to spend, on average, 35 percent of their time performing the mechanics of navigation within and between source files. These observations suggest a new model of program understanding grounded in theories of information foraging and suggest ideas for tools that help developers seek, relate, and collect information in a more effective and explicit manner","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.116","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016573","Program investigation;program understanding;program comprehension;empirical software engineering;information foraging;information scent.","Software maintenance;Programming;Navigation;Software engineering;Software tools;Performance gain;Debugging;Encoding;Packaging;Bars","program debugging;software maintenance","software maintenance tasks;software development environment;debugging task;search tool;Eclipse package explorer;file tabs;scroll bars;source files;program understanding;information foraging","","214","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Detection of Unsafe Dynamic Component Loadings","T. Kwon; Z. Su","University of California, Davis, Davis; University of California, Davis, Davis","IEEE Transactions on Software Engineering","","2012","38","2","293","313","Dynamic loading of software components (e.g., libraries or modules) is a widely used mechanism for an improved system modularity and flexibility. Correct component resolution is critical for reliable and secure software execution. However, programming mistakes may lead to unintended or even malicious components being resolved and loaded. In particular, dynamic loading can be hijacked by placing an arbitrary file with the specified name in a directory searched before resolving the target component. Although this issue has been known for quite some time, it was not considered serious because exploiting it requires access to the local file system on the vulnerable host. Recently, such vulnerabilities have started to receive considerable attention as their remote exploitation became realistic. It is now important to detect and fix these vulnerabilities. In this paper, we present the first automated technique to detect vulnerable and unsafe dynamic component loadings. Our analysis has two phases: 1) apply dynamic binary instrumentation to collect runtime information on component loading (online phase), and 2) analyze the collected information to detect vulnerable component loadings (offline phase). For evaluation, we implemented our technique to detect vulnerable and unsafe component loadings in popular software on Microsoft Windows and Linux. Our evaluation results show that unsafe component loading is prevalent in software on both OS platforms, and it is more severe on Microsoft Windows. In particular, our tool detected more than 4,000 unsafe component loadings in our evaluation, and some can lead to remote code execution on Microsoft Windows.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.108","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6065738","Unsafe component loading;dynamic analysis.","Loading;Instruments;Image resolution;Operating systems;Linux;Security","Linux;object-oriented programming;operating systems (computers);security of data;software libraries;software reliability;system monitoring","automatic detection;unsafe dynamic component loadings;dynamic loading;software components;software libraries;software modules;system modularity;system flexibility;component resolution;software execution;malicious components;arbitrary file;file system;vulnerable host;remote exploitation;automated technique;vulnerable dynamic component loadings;dynamic binary instrumentation;runtime information;vulnerable component loadings;Microsoft Windows;Linux;unsafe component loading;OS platforms;remote code execution","","2","","50","","","","","","IEEE","IEEE Journals & Magazines"
"MobiPADS: a reflective middleware for context-aware mobile computing","A. T. S. Chan; Siu-Nam Chuang","Dept. of Comput., Hong Kong Polytech. Univ., China; Dept. of Comput., Hong Kong Polytech. Univ., China","IEEE Transactions on Software Engineering","","2003","29","12","1072","1085","Traditionally, middleware technologies, such as CORBA, Java RMI, and Microsoft's DCOM, have provided a set of distributed computing services that essentially abstract the underlying network services to a monolithic ""black box."" In a mobile operating environment, the fundamental assumption of middleware abstracting a unified distributed service for all types of applications operating over a static network infrastructure is no longer valid. In particular, mobile applications are not able to leverage the benefits of adaptive computing to optimize its computation based on current contextual situations. In this paper, we introduce the Mobile Platform for Actively Deployable Service (MobiPADS) system. MobiPADS is designed to support context-aware processing by providing an executing platform to enable active service deployment and reconfiguration of the service composition in response to environments of varying contexts. Unlike most mobile middleware, MobiPADS supports dynamic adaptation at both the middleware and application layers to provide flexible configuration of resources to optimize the operations of mobile applications. Within the MobiPADS system, services (known as mobilets) are configured as chained service objects to provide augmented services to the underlying mobile applications so as to alleviate the adverse conditions of a wireless environment.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265522","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265522","","Middleware;Mobile computing;Distributed computing;Context-aware services;Availability;Java;Personal digital assistants;Computer networks;Error analysis;Bandwidth","middleware;mobile computing;radio links;XML;Internet","MobiPADS;reflective middleware;context-aware mobile computing;middleware technologies;distributed computing services;underlying network services;mobile operating environment;adaptive computing;executing platform;active service deployment;mobile middleware;dynamic adaptation;flexible resources configuration;mobilets;chained service objects;augmented services;wireless environment","","102","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Formal Specification-Based Inspection for Verification of Programs","S. Liu; Y. Chen; F. Nagoya; J. A. McDermid","Hosei University, Koganei-shi; Shanghai Jiaotong University, Shanghai; Aoyama Gakuin University, Tokyo; University of York, York","IEEE Transactions on Software Engineering","","2012","38","5","1100","1122","Software inspection is a static analysis technique that is widely used for defect detection, but which suffers from a lack of rigor. In this paper, we address this problem by taking advantage of formal specification and analysis to support a systematic and rigorous inspection method. The aim of the method is to use inspection to determine whether every functional scenario defined in the specification is implemented correctly by a set of program paths and whether every program path of the program contributes to the implementation of some functional scenario in the specification. The method is comprised of five steps: deriving functional scenarios from the specification, deriving paths from the program, linking scenarios to paths, analyzing paths against the corresponding scenarios, and producing an inspection report, and allows for a systematic and automatic generation of a checklist for inspection. We present an example to show how the method can be used, and describe an experiment to evaluate its performance by comparing it to perspective-based reading (PBR). The result shows that our method may be more effective in detecting function-related defects than PBR but slightly less effective in detecting implementation-related defects. We also describe a prototype tool to demonstrate the supportability of the method, and draw some conclusions about our work.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.102","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6035726","Specification-based program inspection;software inspection;formal specification;program verification","DH-HEMTs;High definition video;Three dimensional displays","formal specification;formal verification","formal specification based inspection;program verification;software inspection;defect detection;automatic generation;systematic generation;perspective based reading;PBR;prototype tool","","13","","80","","","","","","IEEE","IEEE Journals & Magazines"
"Constructing Interaction Test Suites for Highly-Configurable Systems in the Presence of Constraints: A Greedy Approach","M. B. Cohen; M. B. Dwyer; J. Shi","University of Nebraska-Lincoln, Lincoln; University of Nebraska-Lincoln, Lincoln; University of Nebraska-Lincoln, Lincoln","IEEE Transactions on Software Engineering","","2008","34","5","633","650","Researchers have explored the application of combinatorial interaction testing (CIT) methods to construct samples to drive systematic testing of software system configurations. Applying CIT to highly-configurable software systems is complicated by the fact that, in many such systems, there are constraints between specific configuration parameters that render certain combinations invalid. Many CIT algorithms lack a mechanism to avoid these. In recent work, automated constraint solving methods have been combined with search-based CIT construction methods to address the constraint problem with promising results. However, these techniques can incur a non-trivial overhead. In this paper, we build upon our previous work to develop a family of greedy CIT sample generation algorithms that exploit calculations made by modern Boolean satisfiability (SAT) solvers to prune the search space of the CIT problem. We perform a comparative evaluation of the cost-effectiveness of these algorithms on four real-world highly-configurable software systems and on a population of synthetic examples that share the characteristics of those systems. In combination our techniques reduce the cost of CIT in the presence of constraints to 30 percent of the cost of widely-used unconstrained CIT methods without sacrificing the quality of the solutions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4564473","Testing strategies;Testing tools;Testing strategies;Testing tools","System testing;Software systems;Software testing;Costs;Application software;Logic testing;Logic arrays;Production;Cameras;Computer Society","greedy algorithms;program testing","highly-configurable systems;greedy approach;combinatorial interaction testing methods;software system configurations;systematic testing;automated constraint solving methods;Boolean satisfiability","","122","","45","","","","","","IEEE","IEEE Journals & Magazines"
"Efficiency of Vulnerability Disclosure Mechanisms to Disseminate Vulnerability Knowledge","H. Cavusoglu; H. Cavusoglu; S. Raghunathan","NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","3","171","185","Security vulnerabilities in software are one of the primary reasons for security breaches, and an important challenge from knowledge management perspective is to determine how to manage the disclosure of knowledge about those vulnerabilities. The security community has proposed several disclosure mechanisms, such as full vendor, immediate public, and hybrid, and has debated about the merits and demerits of these alternatives. In this paper, we study how vulnerabilities should be disclosed to minimize the social loss. We find that the characteristics of the vulnerability (vulnerability risk before and after disclosure), cost structure of the software user population, and vendor's incentives to develop a patch determine the optimal (responsible) vulnerability disclosure. We show that, unlike some existing vulnerability disclosure mechanisms that fail to motivate the vendor to release its patch, responsible vulnerability disclosure policy always ensures the release of a patch. However, we find that this is not because of the threat of public disclosure, as argued by some security practitioners. In fact, not restricting the vendor with a time constraint can ensure the patch release. This result runs counter to the argument of some that setting a grace period always pushes the vendor to develop a patch. When the vulnerability affects multiple vendors, we show that the responsible disclosure policy cannot ensure that every vendor will release a patch. However, when the optimal policy does elicit a patch from each vendor, we show that the coordinator's grace period in the multiple vendor case falls between the grace periods that it would set individually for the vendors in the single vendor case. This implies that the coordinator does not necessarily increase the grace period to accommodate more vendors. We then extend our base model to analyze the impact of 1) early discovery and 2) an early warning system that provides privileged vulnerability knowledge to selected users before the release of a patch for the vulnerability on responsible vulnerability disclosure. We show that while early discovery always improves the social welfare, an early warning system does not necessarily improve the social welfare","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084135","Information security;software vulnerabilities;disclosure mechanisms;responsible vulnerability disclosure;economic modeling;game theory.","Computer hacking;Information security;Knowledge management;Alarm systems;Internet;Cost function;Time factors;Counting circuits;Game theory","DP industry;knowledge management;security of data;software cost estimation;software development management;software reliability","software security vulnerability risk disclosure mechanism;vulnerability knowledge dissemination;security breach;knowledge management perspective;software cost structure;software vendor incentives;software patch release;social welfare;early discovery system;early warning system;information security;economic modeling","","25","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Providing Architectural Languages and Tools Interoperability through Model Transformation Technologies","I. Malavolta; H. Muccini; P. Pelliccione; D. Tamburri","University of L'Aquila, L'Aquila; University of L'Aquila, L'Aquila; University of L'Aquila, L'Aquila; University of L'Aquila, L'Aquila","IEEE Transactions on Software Engineering","","2010","36","1","119","140","Many architectural languages have been proposed in the last 15 years, each one with the chief aim of becoming the ideal language for specifying software architectures. What is evident nowadays, instead, is that architectural languages are defined by stakeholder concerns. Capturing all such concerns within a single, narrowly focused notation is impossible. At the same time, it is also impractical to define and use a ""universal"" notation, such as UML. As a result, many domain-specific notations for architectural modeling have been proposed, each one focusing on a specific application domain, analysis type, or modeling environment. As a drawback, a proliferation of languages exists, each one with its own specific notation, tools, and domain specificity. No effective interoperability is possible to date. Therefore, if a software architect has to model a concern not supported by his own language/tool, he has to manually transform (and, eventually, keep aligned) the available architectural specification into the required language/tool. This paper presents DUALLy, an automated framework that allows architectural languages and tools interoperability. Given a number of architectural languages and tools, they can all interoperate thanks to automated model transformation techniques. DUALLy is implemented as an Eclipse plugin. Putting it in practice, we apply the DUALLy approach to the Darwin/FSP ADL and to a UML2.0 profile for software architectures. By making use of an industrial complex system, we transform a UML software architecture specification in Darwin/FSP, make some verifications by using LTSA, and reflect changes required by the verifications back to the UML specification.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5204094","Software architectures;interoperability;domain-specific architectures;design tools and techniques;model transformations.","Software architecture;Unified modeling language;Computer architecture;Application software;Software tools;Computer industry;Architecture description languages;LAN interconnection;Domain specific languages;Software systems","open systems;software architecture;Unified Modeling Language","architectural languages;tools interoperability;automated model transformation techniques;Eclipse plugin;UML2.0 profile;DUALLy approach;Darwin-FSP ADL;industrial complex system;UML software architecture specification","","36","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Extracting Interactions in Component-Based Systems","T. Parsons; A. Mos; M. Trofin; T. Gschwind; J. Murphy","University College Dublin, Dublin; INRIA Rhone Alpes; Microsoft Corp., Redmond; IBM, Switzerland; University College Dublin, Dublin","IEEE Transactions on Software Engineering","","2008","34","6","783","799","Monitoring, analysing and understanding component based enterprise software systems are challenging tasks. These tasks are essential in solving and preventing performance and quality problems. Obtaining component level interactions which show the relationships between different software entities is a necessary prerequisite for such efforts. This paper focuses on component based Java applications, currently widely used by industry. They pose specific challenges while raising interesting opportunities for component level interaction extraction tools. We present a range of representative approaches for dynamically obtaining and using component interactions. For each approach we detail the needs it addresses, and the technical requirements for building an implementation of the approach. We also take a critical look at the different available implementations of the various techniques presented. We give performance and functional considerations and contrast them against each other by outlining their relative advantages and disadvantages. Based on this data, developers and system integrators can better understand the current state-of-the-art and the implications of choosing or implementing different dynamic interaction extraction techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4599581","Metrics/Measurement;Performance measures;Testing and Debugging;Distributed debugging;Monitors;Tracing;Distribution;Maintenance;and Enhancement;Restructuring;reverse engineering;and reengineering;Metrics/Measurement;Performance measures;Testing and Debugging;Distributed debugging;Monitors;Tracing;Distribution;Maintenance;and Enhancement;Restructuring;reverse engineering;and reengineering","Utility programs;Java;Runtime;Application software;Reverse engineering;Software systems;Industrial relations;Data mining;Containers;Communication system software","business data processing;Java;object-oriented programming","component based enterprise software systems;software entities;Java applications;component interactions","","7","","56","","","","","","IEEE","IEEE Journals & Magazines"
"An Extensible Meta-Model for Program Analysis","D. Strein; R. Lincke; J. Lundberg; W. Löwe","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","9","592","607","Software maintenance tools for program analysis and refactoring rely on a metamodel capturing the relevant properties of programs. However, what is considered relevant may change when the tools are extended with new analyses, refactorings, and new programming languages. This paper proposes a language independent metamodel and an architecture to construct instances thereof, which is extensible for new analyses, refactorings, and new front-ends of programming languages. Due to the loose coupling between analysis, refactoring, and front-end components, new components can be added independently and reuse existing ones. Two maintenance tools implementing the metamodel and the architecture, VIZZANALYZER and X-DEVELOP, serve as proof of concept.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70710","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288193","","Information analysis;Independent component analysis;Data mining;Computer languages;Computer architecture;Software systems;Software maintenance;Costs;Software tools;Computer interfaces","program diagnostics;software maintenance;software tools","extensible metamodel;program analysis;software maintenance tools;program refactoring;language independent metamodel","","8","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Checking inside the black box: regression testing by comparing value spectra","T. Xie; D. Notkin","Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA","IEEE Transactions on Software Engineering","","2005","31","10","869","883","Comparing behaviors of program versions has become an important task in software maintenance and regression testing. Black-box program outputs have been used to characterize program behaviors and they are compared over program versions in traditional regression testing. Program spectra have recently been proposed to characterize a program's behavior inside the black box. Comparing program spectra of program versions offers insights into the internal behavioral differences between versions. In this paper, we present a new class of program spectra, value spectra, that enriches the existing program spectra family. We compare the value spectra of a program's old version and new version to detect internal behavioral deviations in the new version. We use a deviation-propagation call tree to present the deviation details. Based on the deviation-propagation call tree, we propose two heuristics to locate deviation roots, which are program locations that trigger the behavioral deviations. We also use path spectra (previously proposed program spectra) to approximate the program states in value spectra. We then similarly compare path spectra to detect behavioral deviations and locate deviation roots in the new version. We have conducted an experiment on eight C programs to evaluate our spectra-comparison approach. The results show that both value-spectra-comparison and path-spectra-comparison approaches can effectively expose program behavioral differences between program versions even when their program outputs are the same, and our value-spectra-comparison approach reports deviation roots with high accuracy for most programs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.107","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542068","Index Terms- Program spectra;regression testing;software testing;empirical studies;software maintenance.","Software testing;Software maintenance;Fault diagnosis;Propagation losses;Program processors;Optimizing compilers","program testing;software maintenance;formal verification;regression analysis","regression testing;program version;software maintenance;black-box program;program behavior;program spectra;deviation-propagation call tree;C program;value-spectra-comparison;path-spectra-comparison","","23","","36","","","","","","IEEE","IEEE Journals & Magazines"
"A new architecture for transformation-based generators","T. J. Biggerstaff","SoftwareGenerators, Austin, TX, USA","IEEE Transactions on Software Engineering","","2004","30","12","1036","1054","A challenge of many transformation-based generators is that they are trying to achieve three mutually antagonistic goals simultaneously: 1) deeply factored operators and operands to gain the combinatorial programming leverage provided by composition, 2) high-performance code in the generated program, and 3) small (i.e., practical) generation search spaces. The anticipatory optimization generator (AOG) has been built to explore architectures and strategies that address this challenge. The fundamental principle underlying all of AOG's strategies is to solve separate, narrow, and specialized generation problems by strategies that are narrowly tailored to specific problems rather than a single, universal strategy aimed at all problems. A second fundamental notion is the preservation and use of domain-specific information as a way to gain extra leverage on generation problems. This paper focuses on two specific mechanisms: 1) Localization: the generation and merging of implicit control structures and 2) Tag-directed transformations: a new control structure for transformation-based optimization that allows differing kinds of retained domain knowledge (e.g., optimization knowledge) to be anticipated, affixed to the component parts in the reuse library, and triggered when the time is right for its use.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377196","Index Terms- Domain-specific architectures;image processing;inference engines;logic programming;optimization;partial evaluation;pattern matching;program synthesis;reusable software;search;program transformations.","Pattern matching;Automatic programming;Merging;Software libraries;Computer architecture;Image processing;Engines;Logic programming;Software reusability;Refining","partial evaluation (compilers);program control structures;optimising compilers;software reusability;automatic programming","program transformation;domain specific architecture;logic programming;anticipatory optimization generator;program control structure;software reuse;partial evaluation;program synthesis;tag-directed transformation","","2","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Using hammock graphs to structure programs","F. Zhang; E. H. D'Hollander","Platform Comput. Inc., Markham, Ont., Canada; NA","IEEE Transactions on Software Engineering","","2004","30","4","231","245","Advanced computer architectures rely mainly on compiler optimizations for parallelization, vectorization, and pipelining. Efficient-code generation is based on a control dependence analysis to find the basic blocks and to determine the regions of control. However, unstructured branch statements, such as jumps and goto's, render the control flow analysis difficult, time-consuming, and result in poor code generation. Branches are part of many programming languages and occur in legacy and maintenance code as well as in assembler, intermediate languages, and byte code. A simple and effective technique is presented to convert unstructured branches into hammock graph control structures. Using three basic transformations, an equivalent program is obtained in which all control statements have a well-defined scope. In the interest of predication and branch prediction, the number of control variables has been minimized, thereby allowing a limited code replication. The correctness of the transformations has been proven using an axiomatic proof rule system. With respect to previous work, the algorithm is simpler and the branch conditions are less complex, making the program more readable and the code generation more efficient. Additionally, hammock graphs define single entry single exit regions and therefore allow localized optimizations. The restructuring method has been implemented into the parallelizing compiler FPT and allows to extract parallelism in unstructured programs. The use of hammock graph transformations in other application areas such as vectorization, decompilation, and assembly program restructuring is also demonstrated.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274043","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274043","","Optimizing compilers;Assembly;Program processors;Parallel processing;Flow graphs;Computer architecture;Pipeline processing;Computer languages;Application software;Parallel programming","structured programming;optimising compilers;program verification;parallelising compilers;program control structures;parallel architectures","hammock graph control structures;structured programming;computer architecture;compiler optimizations;efficient-code generation;branch prediction;program correctness;proof rule system;parallelizing compiler;program transformation;parallel processing;software verification;program verification","","25","","38","","","","","","IEEE","IEEE Journals & Magazines"
"A framework for model-based design of agent-oriented software","Haiping Xu; S. M. Shatz","Dept. of Comput. Sci., Illinois Univ., Chicago, IL, USA; Dept. of Comput. Sci., Illinois Univ., Chicago, IL, USA","IEEE Transactions on Software Engineering","","2003","29","1","15","30","Agents are becoming one of the most important topics in distributed and autonomous decentralized systems, and there are increasing attempts to use agent technologies to develop large-scale commercial and industrial software systems. The complexity of such systems suggests a pressing need for system modeling techniques to support reliable, maintainable, and extensible design. G-nets are a type of Petri net defined to support system modeling in terms of a set of independent and loosely-coupled modules. In this paper, we customize the basic G-net model to define a so-called ""agent-based G-net"" that can serve as a generic model for agent design. Then, to progress from an agent-based design model to an agent-oriented model, new mechanisms to support inheritance modeling are introduced. To illustrate our formal modeling technique for multiagent systems, an example of an agent family in electronic commerce is provided. Finally, we demonstrate how we can use model checking to verify some key behavioral properties of our agent model. This is facilitated by the use of an existing Petri net tool.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166586","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166586","","Software design;Multiagent systems;Object oriented modeling;Large-scale systems;Software systems;Electronic commerce;Software engineering;Application software;Artificial intelligence;Formal languages","object-oriented programming;multi-agent systems;distributed processing;Petri nets;inheritance","model-based design framework;agent-oriented software;distributed systems;autonomous decentralized systems;large-scale commercial software systems;large-scale industrial software systems;reliable design;maintainable design;extensible design;Petri net;independent loosely-coupled modules;agent-based G-net;agent-oriented model;inheritance modeling;multiagent systems;electronic commerce;e-commerce","","22","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Online Reliability Prediction via Motifs-Based Dynamic Bayesian Networks for Service-Oriented Systems","H. Wang; L. Wang; Q. Yu; Z. Zheng; A. Bouguettaya; M. R. Lyu","School of Computer Science and Engineering and Key Laboratory of Computer Network and Information Integration, Southeast University, SIPAILOU 2, Nanjing, China; School of Computer Science and Engineering and Key Laboratory of Computer Network and Information Integration, Southeast University, SIPAILOU 2, Nanjing, China; College of Computing and Information Sciences, Rochester Institute of Technology, 102 Lomb Memorial Drive, Rochester, NY; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Information Technologies, The University of Sydney, NSW, Australia; Department of Computer Science and Engineering, Shenzhen Research Institute, The Chinese University of Hong Kong, Shatin, Hong Kong, China","IEEE Transactions on Software Engineering","","2017","43","6","556","579","A service-oriented System of Systems (SoS) considers a system as a service and constructs a robust and value-added SoS by outsourcing external component systems through service composition techniques. Online reliability prediction for the component systems for the purpose of assuring the overall Quality of Service (QoS) is often a major challenge in coping with a loosely coupled SoS operating under dynamic and uncertain running environments. It is also a prerequisite for guaranteeing runtime QoS of a SoS through optimal service selection for reliable system construction. We propose a novel online reliability time series prediction approach for the component systems in a service-oriented SoS. We utilize Probabilistic Graphical Models (PGMs) to yield near-future, time series predictions. We assess the approach via invocation records collected from widely used real Web services. Experimental results have confirmed the effectiveness of the approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2615615","NSFC; Novel Software Technology and Industrialization and Wireless Communications Technology; Australian Research Council’s; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7585067","Online reliability prediction;time series;service-oriented computing;system of systems","Time series analysis;Quality of service;Web services;Throughput;Software reliability;Time factors","belief networks;service-oriented architecture","online reliability prediction;motifs-based dynamic Bayesian networks;service-oriented system of systems;service composition;component systems;quality of service;QoS;uncertain running environments;optimal service selection;reliable system construction;online reliability time series prediction;service-oriented SoS;probabilistic graphical models;PGM;time series predictions;invocation records;real Web services","","4","","70","","","","","","IEEE","IEEE Journals & Magazines"
"Exception handling in workflow management systems","C. Hagen; G. Alonso","Credit Suisse, Zurich, Switzerland; NA","IEEE Transactions on Software Engineering","","2000","26","10","943","958","Fault tolerance is a key requirement in process support systems (PSS), a class of distributed computing middleware encompassing applications such as workflow management systems and process centered software engineering environments. A PSS controls the flow of work between programs and users in networked environments based on a ""metaprogram"" (the process). The resulting applications are characterized by a high degree of distribution and a high degree of heterogeneity (properties that make fault tolerance both highly desirable and difficult to achieve). We present a solution for implementing more reliable processes by using exception handling, as it is used in programming languages, and atomicity, as it is known from the transaction concept in database management systems. We describe the mechanism incorporating both transactions and exceptions and present a validation technique allowing to assess the correctness of process specifications.","0098-5589;1939-3520;2326-3881","","10.1109/32.879818","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=879818","","Workflow management software;Fault tolerant systems;Fault tolerance;Application software;Runtime environment;Process design;Computer Society;Distributed computing;Middleware;Software engineering","exception handling;workflow management software;software fault tolerance;client-server systems;database management systems","exception handling;workflow management systems;fault tolerance;process support systems;distributed computing middleware;process centered software engineering;metaprogram;programming languages;atomicity;transaction concept;database management systems;validation technique;process specifications","","160","","","","","","","","IEEE","IEEE Journals & Magazines"
"Enhanced Modeling and Solution of Layered Queueing Networks","G. Franks; T. Al-Omari; M. Woodside; O. Das; S. Derisavi","Carleton University, Ottawa; IBM, Toronto; Carleton University , Ottawa; Ryerson University, Toronto; IBM, Toronto","IEEE Transactions on Software Engineering","","2009","35","2","148","161","Layered queues are a canonical form of extended queueing network for systems with nested multiple resource possession, in which successive depths of nesting define the layers. The model has been applied to most modern distributed systems, which use different kinds of client-server and master-slave relationships, and scales up well. The layered queueing network (LQN) model is described here in a unified fashion, including its many more extensions to match the semantics of sophisticated practical distributed and parallel systems. These include efficient representation of replicated services, parallel and quorum execution, and dependability analysis under failure and reconfiguration. The full LQN model is defined here and its solver is described. A substantial case study to an air traffic control system shows errors (compared to simulation) of a few percent. The LQN model is compared to other models and solutions, and is shown to cover all their features.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.74","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4620121","Performance;Modeling and prediction;Queuing theory;Performance;Modeling and prediction;Queuing theory","Network servers;Application software;File servers;Master-slave;Failure analysis;Air traffic control;Error correction;Computational modeling;Queueing analysis;Distributed computing","client-server systems;parallel processing;queueing theory","layered queueing network;canonical form;nested multiple resource possession;distributed system;client-server system;master-slave system;semantics matching;parallel system;replicated service;quorum execution;dependability analysis;air traffic control system","","70","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting maintenance performance using object-oriented design complexity metrics","R. K. Bandi; V. K. Vaishnavi; D. E. Turk","Quantitative Methods & Inf. Syst. Dept., Indian Inst. of Manage., Bangalore, India; NA; NA","IEEE Transactions on Software Engineering","","2003","29","1","77","87","The Object-Oriented (OO) paradigm has become increasingly popular in recent years. Researchers agree that, although maintenance may turn out to be easier for OO systems, it is unlikely that the maintenance burden will completely disappear. One approach to controlling software maintenance costs is the utilization of software metrics during the development phase, to help identify potential problem areas. Many new metrics have been proposed for OO systems, but only a few of them have been validated. The purpose of this research is to empirically explore the validation of three existing OO design complexity metrics and, specifically, to assess their ability to predict maintenance time. This research reports the results of validating three metrics, Interaction Level (IL), Interface Size (IS), and Operation Argument Complexity (OAC). A controlled experiment was conducted to investigate the effect of design complexity (as measured by the above metrics) on maintenance time. Each of the three metrics by itself was found to be useful in the experiment in predicting maintenance performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166590","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166590","","Software maintenance;Costs;Software metrics;Time measurement;Object oriented programming;Data encapsulation;Software systems;Object oriented modeling;Message passing","software maintenance;software metrics;object-oriented programming","object-oriented design complexity metrics;maintenance performance;software maintenance costs;software metrics;design complexity metrics;maintenance time;Interaction Level;Operation Argument Complexity;Interface Size","","68","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Toolkit design for interactive structured graphics","B. B. Bederson; J. Grosjean; J. Meyer","Comput. Sci. Dept., Maryland Univ., College Park, MD, USA; Comput. Sci. Dept., Maryland Univ., College Park, MD, USA; Comput. Sci. Dept., Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","2004","30","8","535","546","Here, we analyze toolkit designs for building graphical applications with rich user interfaces, comparing polylithic and monolithic toolkit-based solutions. Polylithic toolkits encourage extension by composition and follow a design philosophy similar to 3D scene graphs supported by toolkits including JavaSD and Openlnventor. Monolithic toolkits, on the other hand, encourage extension by inheritance, and are more akin to 2D graphical user interface toolkits such as Swing or MFC. We describe Jazz (a polylithic toolkit) and Piccolo (a monolithic toolkit), each of which we built to support interactive 2D structured graphics applications in general, and zoomable user interface applications in particular. We examine the trade offs of each approach in terms of performance, memory requirements, and programmability. We conclude that a polylithic approach is most suitable for toolkit builders, visual design software where code is automatically generated, and application builders where there is much customization of the toolkit. Correspondingly, we find that monolithic approaches appear to be best for application builders where there is not much customization of the toolkit.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316870","Index Terms- Monolithic toolkits;polylithic toolkits;object-oriented design;composition;inheritance;Zoomable User Interfaces (ZUIs);animation;structured graphics;Graphical User Interfaces (GUIs);Pad++;Jazz;Piccolo.","Graphics;User interfaces;Application software;Graphical user interfaces;Displays;Shape;Buildings;Layout;Java;Software design","graphical user interfaces;authoring systems;object-oriented programming;computer animation","toolkit design;interactive structured graphics;polylithic toolkit;3D scene graph;JavaSD;Openlnventor;monolithic toolkit;2D graphical user interface toolkit;Swing;MFC;Jazz;Piccolo;interactive 2D structured graphics application;zoomable user interface application;visual design software;object-oriented design;Pad++","","105","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Solving the Class Responsibility Assignment Problem in Object-Oriented Analysis with Multi-Objective Genetic Algorithms","M. Bowman; L. C. Briand; Y. Labiche","Carleton University, Ottawa; Simula Research Laboratory, Lysaker and University of Oslo, Norway; Carleton University, Ottawa","IEEE Transactions on Software Engineering","","2010","36","6","817","837","In the context of object-oriented analysis and design (OOAD), class responsibility assignment is not an easy skill to acquire. Though there are many methodologies for assigning responsibilities to classes, they all rely on human judgment and decision making. Our objective is to provide decision-making support to reassign methods and attributes to classes in a class diagram. Our solution is based on a multi-objective genetic algorithm (MOGA) and uses class coupling and cohesion measurement for defining fitness functions. Our MOGA takes as input a class diagram to be optimized and suggests possible improvements to it. The choice of a MOGA stems from the fact that there are typically many evaluation criteria that cannot be easily combined into one objective, and several alternative solutions are acceptable for a given OO domain model. Using a carefully selected case study, this paper investigates the application of our proposed MOGA to the class responsibility assignment problem, in the context of object-oriented analysis and domain class models. Our results suggest that the MOGA can help correct suboptimal class responsibility assignment decisions and perform far better than simpler alternative heuristics such as hill climbing and a single-objective GA.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5530324","Object-oriented analysis and design;class responsibility assignment;UML;genetic algorithm.","Algorithm design and analysis;Genetic algorithms;Object oriented modeling;Context modeling;Unified modeling language;Decision making;Humans;Software quality;Genetic engineering;Laboratories","decision making;genetic algorithms;object-oriented methods","class responsibility assignment problem;object-oriented analysis;multiobjective genetic algorithm;object-oriented design;decision making support;class coupling;cohesion measurement;class diagram;object-oriented domain model;domain class model;hill climbing;single-objective genetic algorithm","","42","","40","","","","","","IEEE","IEEE Journals & Magazines"
"First, Debug the Test Oracle","X. Guo; M. Zhou; X. Song; M. Gu; J. Sun","School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, Portland State University, Portland, OR; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China","IEEE Transactions on Software Engineering","","2015","41","10","986","1000","Opposing to the oracle assumption, a trustworthy test oracle is not always available in real practice. Since manually written oracles and human judgements are still widely used, testers and programmers are in fact facing a high risk of erroneous test oracles. However, test oracle errors can bring much confusion thus causing extra time consumption in the debugging process. As substantiated by our experiment on the Siemens Test Suite, automatic fault localization algorithms suffer severely from erroneous test oracles, which impede them from reducing debugging time to the full extent. This paper proposes a simple but effective approach to debug the test oracle. Based on the observation that test cases covering similar lines of code usually generate similar results, we are able to identify suspicious test cases that are differently judged by the test oracle from their neighbors. To validate the effectiveness of our approach, experiments are conducted on both the Siemens Test Suite and grep. The results show that averagely over 75 percent of the highlighted test cases are actually test oracle errors. Moreover, performance of fault localization algorithms recovered remarkably with the debugged oracles.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2425392","NSFC Program; National Key Technologies R&D Program; Postdoctoral Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7091939","test oracle;debugging;spectrum-based fault localization;Test oracle;debugging;spectrum-based fault localization","Accuracy;Debugging;Measurement;Manuals;Error analysis;Software;Algorithm design and analysis","fault diagnosis;program debugging;program testing","test oracle debugging;oracle error testing;Siemens Test Suite;automatic fault localization algorithm","","","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Automating Live Update for Generic Server Programs","C. Giuffrida; C. Iorgulescu; G. Tamburrelli; A. S. Tanenbaum","Vrije Universiteit Amsterdam, De Boelelaan, Amsterdam, Netherlands; École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Vrije Universiteit Amsterdam, De Boelelaan, Amsterdam, Netherlands; Vrije Universiteit Amsterdam, De Boelelaan, Amsterdam, Netherlands","IEEE Transactions on Software Engineering","","2017","43","3","207","225","The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage the adoption of live update. This paper presents<italic>Mutable Checkpoint-Restart</italic>(<italic>MCR</italic>), a new live update solution for generic (multiprocess and multithreaded) server programs written in C. Compared to prior solutions, MCR can support arbitrary software updates and automate most of the common live update operations. The key idea is to allow the running version to safely reach a quiescent state and then allow the new version to restart as similarly to a fresh program initialization as possible, relying on existing code paths to automatically restore the old program threads and reinitialize a relevant portion of the program data structures. To transfer the remaining data structures, MCR relies on a combination of precise and conservative garbage collection techniques to trace all the global pointers and apply the required state transformations on the fly. Experimental results on popular server programs (<italic>Apache httpd</italic>,<italic>nginx</italic>,<italic>OpenSSH</italic>and<italic>vsftpd</italic>) confirm that our techniques can effectively automate problems previously deemed difficult at the cost of negligible performance overhead (2 percent on average) and moderate memory overhead (3.9<inline-formula><tex-math notation=""LaTeX"">$\times$</tex-math><alternatives><inline-graphic xlink:href=""giuffrida-ieq1-2584066.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>on average, without optimizations).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2584066","European Research Council; ERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7497481","Live update;DSU;checkpoint-restart;quiescence detection;record-replay;garbage collection","Servers;Data structures;Convergence;Software;Manuals;System recovery;Buildings","","","","2","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Exploring Mobile End User Development: Existing Use and Design Factors","A. Namoun; A. Daskalopoulou; N. Mehandjiev; Z. Xun","Islamic University of Madinah, Medina, Saudi Arabia; University of Manchester, Booth Street West, Manchester, United Kingdom; University of Manchester, Booth Street West, Manchester, United Kingdom; University of Manchester, Booth Street West, Manchester, United Kingdom","IEEE Transactions on Software Engineering","","2016","42","10","960","976","Mobile devices are everywhere, and the scope of their use is growing from simple calling and texting through Internet browsing to more technical activities such as creating message processing filters and connecting different apps. However, building tools which provide effective support for such advanced technical use of mobile devices by non-programmers (mobile end user development or mEUD) requires thorough understanding of user needs and motivations, including factors which can impact user intentions regarding mEUD activities. We propose a model linking these mEUD factors with mobile users' attitudes towards, and intent of doing mEUD, and discuss a number of implications for supporting mEUD. Our research process is user-centered, and we formulate a number of hypotheses by fusing results from an exploratory survey which gathers facts about mEUD motivations and activities, and from a focus group study, which delivers deeper understanding of particular mEUD practices and issues. We then test the hypothesized relationships through a follow-up enquiry mixing quantitative and qualitative techniques, leading to the creation of a preliminary mEUD model. Altogether we have involved 275 mobile users in our research. Our contribution links seven mEUD factors with mEUD intentions and attitudes, and highlights a number of implications for mEUD support.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2532873","Manchester Business School; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7416245","Human factors in software design;mobile environments;models and principles;requirements/specifications","Mobile communication;Mobile handsets;Mashups;Games;Context;Electronic mail","human factors;mobile computing;software engineering;user centred design","mobile end user development;mEUD;user attitude;user-centered design;mobile device;software development","","4","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Feature Identification: An Epidemiological Metaphor","G. Antoniol; Y. -. Gueheneuc","NA; NA","IEEE Transactions on Software Engineering","","2006","32","9","627","641","Feature identification is a technique to identify the source code constructs activated when exercising one of the features of a program. We propose new statistical analyses of static and dynamic data to accurately identify features in large multithreaded object-oriented programs. We draw inspiration from epidemiology to improve previous approaches to feature identification and develop an epidemiological metaphor. We build our metaphor on our previous approach to feature identification, in which we use processor emulation, knowledge-based filtering, probabilistic ranking, and metamodeling. We carry out three case studies to assess the usefulness of our metaphor, using the ""save a bookmark"" feature of Web browsers as an illustration. In the first case study, we compare our approach with three previous approaches (a naive approach, a concept analysis-based approach, and our previous probabilistic approach) in identifying the feature in MOZILLA, a large, real-life, multithreaded object-oriented program. In the second case study, we compare the implementation of the feature in the FIREFOX and MOZILLA Web browsers. In the third case study, we identify the same feature in two more Web browsers, Chimera (in C) and ICEBrowser (in Java), and another feature in JHOTDRAW and XFIG, to highlight the generalizability of our metaphor","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707664","Program understanding;dynamic analysis;static analysis;feature identification;epidemiology;Firefox and Mozilla Web browsers.","Microarchitecture;Statistical analysis;Emulation;Filtering;Uniform resource locators;Metamodeling;Java;Software maintenance;Data structures;Documentation","multi-threading;object-oriented programming;program diagnostics;software maintenance;statistical analysis","feature identification;epidemiological metaphor;source code identification;statistical analysis;multithreaded object-oriented program;processor emulation;knowledge-based filtering;probabilistic ranking;metamodeling;Web browser;naive approach;concept analysis-based approach;MOZILLA;FIREFOX;Chimera;ICEBrowser;JHOTDRAW;XFIG;program understanding;dynamic program analysis;static program analysis;software maintenance;legacy software","","45","","45","","","","","","IEEE","IEEE Journals & Magazines"
"Reliable Effects Screening: A Distributed Continuous Quality Assurance Process for Monitoring Performance Degradation in Evolving Software Systems","C. Yilmaz; A. Porter; A. S. Krishna; A. M. Memon; D. C. Schmidt; A. S. Gokhale; B. Natarajan","IBM T.J. Watson Research Center, Hawthorne, NY; Department of Computer Science, University of Maryland, College Park, MD; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN; Department of Computer Science, University of Maryland, College Park, MD; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN; Symantec, Bhau Patil Marg, Pune 411 020, India","IEEE Transactions on Software Engineering","","2007","33","2","124","141","Developers of highly configurable performance-intensive software systems often use in-house performance-oriented ""regression testing"" to ensure that their modifications do not adversely affect their software's performance across its large configuration space. Unfortunately, time and resource constraints can limit in-house testing to a relatively small number of possible configurations, followed by unreliable extrapolation from these results to the entire configuration space. As a result, many performance bottlenecks escape detection until systems are fielded. In our earlier work, we improved the situation outlined above by developing an initial quality assurance process called ""main effects screening"". This process 1) executes formally designed experiments to identify an appropriate subset of configurations on which to base the performance-oriented regression testing, 2) executes benchmarks on this subset whenever the software changes, and 3) provides tool support for executing these actions on in-the-field and in-house computing resources. Our initial process had several limitations, however, since it was manually configured (which was tedious and error-prone) and relied on strong and untested assumptions for its accuracy (which made its use unacceptably risky in practice). This paper presents a new quality assurance process called ""reliable effects screening"" that provides three significant improvements to our earlier work. First, it allows developers to economically verify key assumptions during process execution. Second, it integrates several model-driven engineering tools to make process configuration and execution much easier and less error prone. Third, we evaluate this process via several feasibility studies of three large, widely used performance-intensive software frameworks. Our results indicate that reliable effects screening can detect performance degradation in large-scale systems more reliably and with significantly less resources than conventional techniques","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052587","Distributed continuous quality assurance;performance-o-ri-ented regression testing;design-of-experiments theory.","Quality assurance;Monitoring;Degradation;Software systems;Performance evaluation;Software performance;Software testing;System testing;Time factors;Extrapolation","program testing;software performance evaluation;software quality;software reliability","reliable effects screening;distributed continuous quality assurance process;performance degradation monitoring;evolving software systems;performance intensive software systems;regression testing;software performance;in house testing;performance bottlenecks;main effects screening;configuration subset;software benchmarks;tool support;process configuration;process execution","","5","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Technology for testing nondeterministic client/server database applications","Gwan-Hwan Hwang; Sheng-Jen Chang; Huey-Der Chu","Dept. of Inf. & Comput. Educ., Nat. Taiwan Normal Univ., Taipei, Taiwan; NA; NA","IEEE Transactions on Software Engineering","","2004","30","1","59","77","The execution of a client/server application involving database access requires a sequence of database transaction events (or, T-events), called a transaction sequence (or, T-sequence). A client/server database application may have nondeterministic behavior in that multiple executions thereof with the same input may produce different T-sequences. We present a framework for testing all possible T-sequences of a client/server database application. We first show how to define a T-sequence in order to provide sufficient information to detect race conditions between T-events. Second, we design algorithms to change the outcomes of race conditions in order to derive race variants, which are prefixes of other T-sequences. Third, we develop a prefix-based replay technique for race variants derived from T-sequences. We prove that our framework can derive all the possible T-sequences in cases where every execution of the application terminates. A formal proof and an analysis of the proposed framework are given. We describe a prototype implementation of the framework and present experimental results obtained from it.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265736","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265736","","Transaction databases;Relational databases;Algorithm design and analysis;Change detection algorithms;Prototypes;System testing;Database systems;Monitoring;Data structures;Performance evaluation","client-server systems;distributed databases;formal verification;program testing;reachability analysis;parallel programming","client-server database;database transaction event;T-events;transaction sequence;T-sequence;reachability testing;database management system;concurrent programming","","7","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Conservative Bounds for the pfd of a 1-out-of-2 Software-Based System Based on an Assessor's Subjective Probability of ""Not Worse Than Independence""","B. Littlewood; A. Povyakalo","City University, London, London; City University, London, London","IEEE Transactions on Software Engineering","","2013","39","12","1641","1653","We consider the problem of assessing the reliability of a 1-out-of-2 software-based system, in which failures of the two channels cannot be assumed to be independent with certainty. An informal approach to this problem assesses the channel probabilities of failure on demand (pfds) conservatively, and then multiplies these together in the hope that the conservatism will be sufficient to overcome any possible dependence between the channel failures. Our intention here is to place this kind of reasoning on a formal footing. We introduce a notion of ""not worse than independence""' and assume that an assessor has a prior belief about this, expressed as a probability. We obtain a conservative prior system pfd, and show how a conservative posterior system pfd can be obtained following the observation of a number of demands without system failure. We present some illustrative numerical examples, discuss some of the difficulties involved in this way of reasoning, and suggest some avenues of future research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6564279","System reliability;software fault tolerance;1-out-of-2 system;dependent failures;subjective probability","Phase frequency detector;Cognition;Software reliability;Fault tolerant systems;Software reliability;Reliability engineering;Failure analysis","probability;software reliability","conservative bounds;1-out-of-2 software-based system reliability;assessor subjective probability;channel probabilities of failure on demand;formal footing;not worse than independence;conservative posterior system PFD","","3","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Multi-Objective Quality-Driven Service Selection—A Fully Polynomial Time Approximation Scheme","I. Trummer; B. Faltings; W. Binder","Artificial Intelligence Laboratory , École Polytechnique Fédérale de Lausanne, Switzerland; Artificial Intelligence Laboratory , École Polytechnique Fédérale de Lausanne, Switzerland; Faculty of Informatics, University of Lugano, Via Giuseppe Buffi 13, Switzerland","IEEE Transactions on Software Engineering","","2014","40","2","167","191","The goal of multi-objective quality-driven service selection (QDSS) is to find service selections for a workflow whose quality-of-service (QoS) values are Pareto-optimal. We consider multiple QoS attributes such as response time, cost, and reliability. A selection is Pareto-optimal if no other selection has better QoS values for some attributes and at least equivalent values for all others. Exact algorithms have been proposed that find all Pareto-optimal selections. They suffer however from exponential complexity. Randomized algorithms scale well but do not offer any formal guarantees on result precision. We present the first approximation scheme for QDSS. It aims at the sweet spot between exact and randomized algorithms: It combines polynomial complexity with formal result precision guarantees. A parameter allows to seamlessly trade result precision against efficiency. We formally analyze complexity and precision guarantees and experimentally compare our algorithm against exact and randomized approaches. Comparing with exact algorithms, our approximation scheme allows to reduce optimization time from hours to seconds. Its approximation error remains below 1.4 percent while randomized algorithms come close to the theoretical maximum.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.61","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6687160","Quality-driven service selection;multi-objective optimization;approximation algorithms","Quality of service;Approximation methods;Approximation algorithms;Polynomials;Complexity theory;Optimization;Motion pictures","Pareto optimisation;polynomial approximation;quality of service;software quality","approximation error;optimization time;polynomial complexity;randomized algorithms;exponential complexity;Pareto optimal selections;least equivalent values;QoS values;Pareto optimal;quality-of-service;QDSS;fully polynomial time approximation scheme;multiobjective quality driven service selection","","22","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Design and implementation of a fine-grained software inspection tool","P. Anderson; T. Reps; T. Teitelbaum","GrammaTech Inc., Ithaca, NY, USA; GrammaTech Inc., Ithaca, NY, USA; GrammaTech Inc., Ithaca, NY, USA","IEEE Transactions on Software Engineering","","2003","29","8","721","733","Although software inspection has led to improvements in software quality, many software systems continue to be deployed with unacceptable numbers of errors, even when software inspection is part of the development process. The difficulty of manually verifying that the software under inspection conforms to the rules is partly to blame. We describe the design and implementation of a tool designed to help alleviate this problem. The tool provides mechanisms for fine-grained inspection of software by exposing the results of sophisticated whole-program static analysis to the inspector. The tool computes many static-semantic representations of the program, including an accurate call graph and dependence graph. A whole-program pointer analysis is used to make sure that the representation is precise with respect to aliases induced by pointer usage. Views on the dependence graph and related representations are supported. Queries on the dependence graph allow an inspector to answer detailed questions about the semantics of the program. Facilities for openness and extensibility permit the tool to be integrated with many software development processes. The main challenge of the approach is to provide facilities to navigate and manage the enormous complexity of the dependence graph.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223646","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223646","","Software tools;Inspection;Software quality;Navigation;Software systems;Software engineering;Best practices;NASA;Performance analysis;Filters","program slicing;program verification;software quality;inspection;software tools;data flow graphs;software metrics","fine-grained software inspection tool;software quality;program understanding;program slicing;program chopping;software model checking;program static analysis;static-semantic representation;call graph;program pointer analysis;dependence graph;software complexity","","36","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Automatically Detecting and Tracking Inconsistencies in Software Design Models","A. Egyed","Johannes Kepler University, Linz","IEEE Transactions on Software Engineering","","2011","37","2","188","204","Software models typically contain many inconsistencies and consistency checkers help engineers find them. Even if engineers are willing to tolerate inconsistencies, they are better off knowing about their existence to avoid follow-on errors and unnecessary rework. However, current approaches do not detect or track inconsistencies fast enough. This paper presents an automated approach for detecting and tracking inconsistencies in real time (while the model changes). Engineers only need to define consistency rules-in any language-and our approach automatically identifies how model changes affect these consistency rules. It does this by observing the behavior of consistency rules to understand how they affect the model. The approach is quick, correct, scalable, fully automated, and easy to use as it does not require any special skills from the engineers using it. We evaluated the approach on 34 models with model sizes of up to 162,237 model elements and 24 types of consistency rules. Our empirical evaluation shows that our approach requires only 1.4 ms to reevaluate the consistency of the model after a change (on average); its performance is not noticeably affected by the model size and common consistency rules but only by the number of consistency rules, at the expense of a quite acceptable, linearly increasing memory consumption.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5432227","Design tools and techniques;design.","Software design;Feedback;Design engineering;Maintenance engineering;Best practices;Software engineering;Programming profession","formal verification;software maintenance","consistency rules;automatic inconsistency tracking;automatic inconsistency detection;empirical evaluation;memory consumption;software design model;consistency checkers","","46","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Using software architecture for code testing","H. Muccini; P. Inverardi; A. Bertolino","Dipt. di informatica, Universita dell'Aquila, L'Aquila, Italy; Dipt. di informatica, Universita dell'Aquila, L'Aquila, Italy; NA","IEEE Transactions on Software Engineering","","2004","30","3","160","171","Our research deals with the use of software architecture (SA) as a reference model for testing the conformance of an implemented system with respect to its architectural specification. We exploit the specification of SA dynamics to identify useful schemes of interactions between system components and to select test classes corresponding to relevant architectural behaviors. The SA dynamics is modeled by labeled transition systems (LTSs). The approach consists of deriving suitable LTS abstractions called ALTSs. ALTSs offer specific views of SA dynamics by concentrating on relevant features and abstracting away from uninteresting ones. Intuitively, deriving an adequate set of test classes entails deriving a set of paths that appropriately cover the ALTS. Next, a relation between these abstract SA tests and more concrete, executable tests needs to be established so that the architectural tests derived can be refined into code-level tests. We use the TRMCS case study to illustrate our hands-on experience. We discuss the insights gained and highlight some issues, problems, and solutions of general interest in architecture-based testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271170","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271170","","Software architecture;Software testing;System testing;Concrete;Connectors;Debugging;Software engineering;Application software;Performance evaluation;Software design","software architecture;program testing;formal specification;conformance testing","software architecture;architectural specification;code testing;labeled transition system;test classes;test strategies;software engineering","","63","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Exploiting purity for atomicity","C. Flanagan; S. N. Freund; S. Qadeer","Dept. of Comput. Sci., California Univ., Santa Cruz, CA, USA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","4","275","291","Multithreaded programs often exhibit erroneous behavior because of unintended interactions between concurrent threads. This paper focuses on the noninterference property of atomicity. A procedure is atomic if, for every execution, there is an equivalent serial execution in which the actions of the atomic procedure are not interleaved with actions of other threads. This key property makes atomic procedures amenable to sequential reasoning techniques, which significantly facilitates subsequent validation activities such as code inspection and testing. Several existing tools verify atomicity by using commutativity of actions to show that every execution reduces to a corresponding serial execution. However, experiments with these tools have highlighted a number of interesting procedures that, while intuitively atomic, are not reducible. In this paper, we exploit the notion of pure code blocks to verify the atomicity of such irreducible procedures. If a pure block terminates normally, then its evaluation does not change the program state and, hence, these evaluation steps can be removed from the program trace before reduction. We develop a static typed-based analysis for atomicity based on this insight, and we illustrate this analysis on a number of interesting examples that could not be verified using earlier tools based purely on reduction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435350","Index Terms- Atomicity;purity;reduction;concurrent programs.","Inspection;Interleaved codes;Sequential analysis;Commutation;Delay;Interference;Protection;Programming;Code standards","multi-threading;reasoning about programs;program testing;software tools;program diagnostics","multithreaded programs;noninterference atomicity property;sequential reasoning techniques;pure code blocks;static typed-based analysis;program trace;concurrent programs","","12","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Toward formalizing domain modeling semantics in language syntax","J. Evermann; Y. Wand","Sch. of Inf. Manage., Victoria Univ. of Wellington, New Zealand; NA","IEEE Transactions on Software Engineering","","2005","31","1","21","37","Information systems are situated in and are representations of some business or organizational domain. Hence, understanding the application domain is critical to the success of information systems development. To support domain understanding, the application domain is represented in conceptual models. The correctness of conceptual models can affect the development outcome and prevent costly rework during later development stages. This paper proposes a method to restrict the syntax of a modeling language to ensure that only possible configurations of a domain can be modeled, thus increasing the likelihood of creating correct domain models. The proposed method, based on domain ontologies, captures relationships among domain elements via constraints on the language metamodel, thus restricting the set of statements about the domain that can be generated with the language. In effect, this method creates domain specific modeling languages from more generic ones. The method is demonstrated using the Unified Modeling Language (UML). Specifically, it is applied to the subset of UML dealing with object behavior and its applicability is demonstrated on a specific modeling example.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392718","Index Terms- Analysis;methodologies;specification;object-oriented design methods;design concepts;CASE;ontology.","Object oriented modeling;Context modeling;Unified modeling language;Software design;Application software;Ontologies;Design methodology;Information systems;Computer aided software engineering;Software systems","Unified Modeling Language;formal specification;computer aided software engineering;object-oriented methods;information systems;programming language semantics;ontologies (artificial intelligence)","information systems development;conceptual models;domain ontologies;language metamodel;Unified Modeling Language;UML;domain modeling semantics;language syntax;object-oriented design methods;CASE","","45","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Security Requirements Engineering: A Framework for Representation and Analysis","C. Haley; R. Laney; J. Moffett; B. Nuseibeh","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","133","153","This paper presents a framework for security requirements elicitation and analysis. The framework is based on constructing a context for the system, representing security requirements as constraints, and developing satisfaction arguments for the security requirements. The system context is described using a problem-oriented notation, then is validated against the security requirements through construction of a satisfaction argument. The satisfaction argument consists of two parts: a formal argument that the system can meet its security requirements and a structured informal argument supporting the assumptions expressed in the formal argument. The construction of the satisfaction argument may fail, revealing either that the security requirement cannot be satisfied in the context or that the context does not contain sufficient information to develop the argument. In this case, designers and architects are asked to provide additional design information to resolve the problems. We evaluate the framework by applying it to a security requirements analysis within an air traffic control technology evaluation project.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70754","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4359475","Software/Software Engineering;Requirements/Specifications;Security;Software/Software Engineering;Requirements/Specifications;Security","Information security;Computer security;Data security;Application software;Computer Society;Air traffic control;Statistics;Software engineering;Internet;Credit cards","security of data;software engineering","security requirements engineering;air traffic control technology;software security","","182","","82","","","","","","IEEE","IEEE Journals & Magazines"
"Reviving Sequential Program Birthmarking for Multithreaded Software Plagiarism Detection","Z. Tian; T. Liu; Q. Zheng; E. Zhuang; M. Fan; Z. Yang","Department of Computer Science and Technology, Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Computer Science and Technology, Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), Xi'an Jiaotong University, Xi'an, China; Department of Computer Science and Technology, Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), Xi'an Jiaotong University, Xi'an, China; Department of Computer Science, Western Michigan University, Kalamazoo, MI","IEEE Transactions on Software Engineering","","2018","44","5","491","511","As multithreaded programs become increasingly popular, plagiarism of multithreaded programs starts to plague the software industry. Although there has been tremendous progress on software plagiarism detection technology, existing dynamic birthmark approaches are applicable only to sequential programs, due to the fact that thread scheduling nondeterminism severely perturbs birthmark generation and comparison. We propose a framework called TOB (Thread-oblivious dynamic Birthmark) that revives existing techniques so they can be applied to detect plagiarism of multithreaded programs. This is achieved by thread-oblivious algorithms that shield the influence of thread schedules on executions. We have implemented a set of tools collectively called TOB-PD (TOB based Plagiarism Detection tool) by applying TOB to three existing representative dynamic birthmarks, including SCSSB (System Call Short Sequence Birthmark), DYKIS (DYnamic Key Instruction Sequence birthmark) and JB (an API based birthmark for Java). Our experiments conducted on large number of binary programs show that our approach exhibits strong resilience against state-of-the-art semantics-preserving code obfuscation techniques. Comparisons against the three existing tools SCSSB, DYKIS and JB show that the new framework is effective for plagiarism detection of multithreaded programs. The tools, the benchmarks and the experimental results are all publicly available.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2688383","National Key Research and Development Program of China; National Science Foundation of China; Fok Ying-Tong Education Foundation; Ministry of Education Innovation Research Team; Science and Technology Project in Shaanxi Province of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7888597","Software plagiarism detection;multithreaded program;software birthmark;thread-oblivious birthmark","Plagiarism;Instruction sets;Computer science;Dynamic scheduling;Indexes;Electronic mail","computer crime;fraud;multi-threading;scheduling","sequential program birthmarking;multithreaded programs;software industry;software plagiarism detection technology;dynamic birthmark approaches;thread scheduling nondeterminism;birthmark generation;Thread-oblivious dynamic Birthmark;thread-oblivious algorithms;thread schedules;Plagiarism Detection tool;System Call Short Sequence Birthmark;DYnamic Key Instruction Sequence birthmark;API based birthmark;binary programs;multithreaded software","","1","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling development effort in object-oriented systems using design properties","L. C. Briand; J. Wust","Syst. & Comput. Eng. Dept., Carleton Univ., Ottawa, Ont., Canada; NA","IEEE Transactions on Software Engineering","","2001","27","11","963","986","In the context of software cost estimation, system size is widely taken as a main driver of system development effort. However, other structural design properties, such as coupling, cohesion, and complexity, have been suggested as additional cost factors. Using effort data from an object-oriented development project, we empirically investigate the relationship between class size and the development effort for a class and what additional impact structural properties such as class coupling have on effort. The paper proposes a practical, repeatable, and accurate analysis procedure to investigate relationships between structural properties and development effort. Results indicate that fairly accurate predictions of class effort can be made based on simple measures of the class interface size alone (mean MREs below 30 percent). Effort predictions at the system level are even more accurate as, using Bootstrapping, the estimated 95 percent confidence interval for MREs is 3 to 23 percent. But, more sophisticated coupling and cohesion measures do not help to improve these predictions to a degree that would be practically significant. However, the use of hybrid models combining Poisson regression and CART regression trees clearly improves the accuracy of the models as compared to using Poisson regression alone.","0098-5589;1939-3520;2326-3881","","10.1109/32.965338","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=965338","","Object oriented modeling;Size measurement;Costs;Regression tree analysis;Time measurement;Predictive models;Context modeling;Software systems;Regression analysis;Buildings","software cost estimation;object-oriented programming;project management","development effort modeling;object-oriented systems;design properties;software cost estimation;system size;system development effort;structural design properties;cost factors;object-oriented development project;class size;impact structural properties;class coupling;accurate analysis procedure;structural properties;development effort;Poisson regression;regression trees;cost prediction models;design measures;regression analysis;class effort;Bootstrapping;MREs;CART regression trees;empirical validation","","62","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Empirical analysis of CK metrics for object-oriented design complexity: implications for software defects","R. Subramanyam; M. S. Krishnan","Univ. of Michigan Bus. Sch., USA; Univ. of Michigan Bus. Sch., USA","IEEE Transactions on Software Engineering","","2003","29","4","297","310","To produce high quality object-oriented (OO) applications, a strong emphasis on design aspects, especially during the early phases of software development, is necessary. Design metrics play an important role in helping developers understand design aspects of software and, hence, improve software quality and developer productivity. In this paper, we provide empirical evidence supporting the role of OO design complexity metrics, specifically a subset of the Chidamber and Kemerer (1991, 1994) suite (CK metrics), in determining software defects. Our results, based on industry data from software developed in two popular programming languages used in OO development, indicate that, even after controlling for the size of the software, these metrics are significantly associated with defects. In addition, we find that the effects of these metrics on defects vary across the samples from two programming languages-C++ and Java. We believe that these results have significant implications for designing high-quality software products using the OO approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1191795","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1191795","","Software design;Software quality;Application software;Programming;Productivity;Computer industry;Industrial control;Computer languages;Size control;Java","object-oriented programming;software metrics;program debugging","CK metric analysis;object-oriented design complexity;software defects;software development;software quality;OO design complexity metrics;software defect determination;C++;Java","","268","","41","","","","","","IEEE","IEEE Journals & Magazines"
"You Are the Only Possible Oracle: Effective Test Selection for End Users of Interactive Machine Learning Systems","A. Groce; T. Kulesza; C. Zhang; S. Shamasunder; M. Burnett; W. Wong; S. Stumpf; S. Das; A. Shinsel; F. Bice; K. McIntosh","School of Electrical Engineering and Computer Science, Oregon State University, Corvallis; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis; Centre for HCI Design, School of Informatics, City University London, London, United Kingdom; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis","IEEE Transactions on Software Engineering","","2014","40","3","307","323","How do you test a program when only a single user, with no expertise in software testing, is able to determine if the program is performing correctly? Such programs are common today in the form of machine-learned classifiers. We consider the problem of testing this common kind of machine-generated program when the only oracle is an end user: e.g., only you can determine if your email is properly filed. We present test selection methods that provide very good failure rates even for small test suites, and show that these methods work in both large-scale random experiments using a “gold standard” and in studies with real users. Our methods are inexpensive and largely algorithm-independent. Key to our methods is an exploitation of properties of classifiers that is not possible in traditional software testing. Our results suggest that it is plausible for time-pressured end users to interactively detect failures-even very hard-to-find failures-without wading through a large number of successful (and thus less useful) tests. We additionally show that some methods are able to find the arguably most difficult-to-detect faults of classifiers: cases where machine learning algorithms have high confidence in an incorrect result.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6682887","Machine learning;end-user testing;test suite size","Testing;Software;Training;Training data;Electronic mail;Software algorithms;Machine learning algorithms","interactive systems;learning (artificial intelligence);program testing","effective test selection;interactive machine learning systems;end users;program testing;software testing;machine learned classifiers;machine generated program;email;hard-to-find failures;interactive failure detection","","9","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Measuring and modeling usage and reliability for statistical Web testing","C. Kallepalli; J. Tian","Iris Labs Inc, Plano, TX, USA; NA","IEEE Transactions on Software Engineering","","2001","27","11","1023","1036","Statistical testing and reliability analysis can be used effectively to assure quality for Web applications. To support this strategy, we extract Web usage and failure information from existing Web logs. The usage information is used to build models for statistical Web testing. The related failure information is used to measure the reliability of Web applications and the potential effectiveness of statistical Web testing. We applied this approach to analyze some actual Web logs. The results demonstrated the viability and effectiveness of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/32.965342","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=965342","","Testing;Quality assurance;Application software;Statistical analysis;Data mining;Sea measurements;Software systems;Information analysis;Failure analysis;Web server","information resources;statistical analysis;Markov processes;system monitoring","statistical Web testing;reliability analysis;Web applications;Web usage;failure information;Web logs;World Wide Web;Markov chain;usage measurement","","99","","18","","","","","","IEEE","IEEE Journals & Magazines"
"Does Software Process Improvement Reduce the Severity of Defects? A Longitudinal Field Study","D. E. Harter; C. F. Kemerer; S. A. Slaughter","Syracuse University, Syracuse; University of Pittsburgh, Pittsburgh and King Abdul Aziz University, Saudi Arabia; Georgia Institute of Technology, Atlanta","IEEE Transactions on Software Engineering","","2012","38","4","810","827","As firms increasingly rely on information systems to perform critical functions, the consequences of software defects can be catastrophic. Although the software engineering literature suggests that software process improvement can help to reduce software defects, the actual evidence is equivocal. For example, improved development processes may only remove the “easier” syntactical defects, while the more critical defects remain. Rigorous empirical analyses of these relationships have been very difficult to conduct due to the difficulties in collecting the appropriate data on real systems from industrial organizations. This field study analyzes a detailed data set consisting of 7,545 software defects that were collected on software projects completed at a major software firm. Our analyses reveal that higher levels of software process improvement significantly reduce the likelihood of high severity defects. In addition, we find that higher levels of process improvement are even more beneficial in reducing severe defects when the system developed is large or complex, but are less beneficial in development when requirements are ambiguous, unclear, or incomplete. Our findings reveal the benefits and limitations of software process improvement for the removal of severe defects and suggest where investments in improving development processes may have their greatest effects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.63","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5928358","Software complexity;defect severity;requirements ambiguity;software process;CMM","Complexity theory;Production;Coordinate measuring machines;Testing;Software quality;Programming","program debugging;software process improvement","software process improvement;longitudinal field study;information systems;software defects;software engineering literature;syntactical defects;critical defects;severe defects","","20","","68","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Lessons-Learned Sessions on Effort Estimation and Uncertainty Assessments","M. Jørgensen; T. M. Gruschke","Simula Research Laboratory and University of Oslo, Norway; KnowIT Objectnet, Oslo","IEEE Transactions on Software Engineering","","2009","35","3","368","383","Inaccurate estimates of software development effort is a frequently reported cause of IT-project failures. We report results from a study that investigated the effect of introducing lessons-learned sessions on estimation accuracy and the assessment of uncertainty. Twenty software professionals were randomly allocated to a Learning group or a Control group and instructed to estimate and complete the same five development tasks. Those in the Learning group but not those in the Control group were instructed to spend at least 30 minutes on identifying, analyzing, and summarizing their effort estimation and uncertainty assessment experience after completing each task. We found that the estimation accuracy and the realism of the uncertainty assessment were not better in the Learning group than in the Control group. A follow-up study with 83 software professionals was completed to better understand this lack of improvement from lessons-learned sessions. The follow-up study found that receiving feedback about other software professionals' estimation performance led to more realistic uncertainty assessments than receiving the same feedback of one's own estimates. Lessons-learned sessions, not only in estimation contexts, have to be carefully designed to avoid wasting resources on learning processes that stimulate rather than reduce learning biases.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.2","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4752843","Cost estimation;process implementation and change;review and evaluation;software psychology.","Uncertainty;Yield estimation;Feedback;Programming;Software performance;Psychology;Software engineering;Computer industry;Acoustic reflection;Databases","feedback;learning systems;software engineering","lessons-learned sessions;effort estimation;uncertainty assessments;software development;IT-project failures;feedback;learning process","","30","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Skoll: A Process and Infrastructure for Distributed Continuous Quality Assurance","A. Porter; C. Yilmaz; A. M. Memon; D. C. Schmidt; B. Natarajan","Department of Computer Science, University of Maryland, A.V. Williams Building, College Park, MD 20742; IBM T.J. Watson Research Center, Hawthorne, NY 10532; Department of Computer Science, University of Maryland, A.V. Williams Building, College Park, MD 20742; Institute for Software Integrated Systems (ISIS), Vanderbilt University, 2015 Terrace Place, Nashville, TN 37203; Symantech India and the Institute for Software Integrated Systems (ISIS), Vanderbilt University, 2015 Terrace Place, Nashville, TN 37202","IEEE Transactions on Software Engineering","","2007","33","8","510","525","Software engineers increasingly emphasize agility and flexibility in their designs and development approaches. They increasingly use distributed development teams, rely on component assembly and deployment rather than green field code writing, rapidly evolve the system through incremental development and frequent updating, and use flexible product designs supporting extensive end-user customization. While agility and flexibility have many benefits, they also create an enormous number of potential system configurations built from rapidly changing component implementations. Since today's quality assurance (QA) techniques do not scale to handle highly configurable systems, we are developing and validating novel software QA processes and tools that leverage the extensive computing resources of user and developer communities in a distributed, continuous manner to improve software quality significantly. This paper provides several contributions to the study of distributed, continuous QA (DCQA). First, it shows the structure and functionality of Skoll, which is an environment that defines a generic around-the-world, around-the-clock QA process and several sophisticated tools that support this process. Second, it describes several novel QA processes built using the Skoll environment. Third, it presents two studies using Skoll: one involving user testing of the Mozilla browser and another involving continuous build, integration, and testing of the ACE+TAO communication software package. The results of our studies suggest that the Skoll environment can manage and control distributed continuous QA processes more effectively than conventional QA processes. For example, our DCQA processes rapidly identified problems that had taken the ACE+TAO developers much longer to find and several of which they had not found. Moreover, the automatic analysis of QA results provided developers information that enabled them to quickly find the root causes of problems","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70719","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267023","Distributed continuous quality assurance;distributed testing;configurable components;testing.","Quality assurance;Software quality;Software testing;Design engineering;Assembly systems;Writing;Product design;Software tools;Distributed computing;Software packages","distributed processing;object-oriented programming;software quality;systems analysis","Skoll;distributed continuous quality assurance;software engineering;distributed development teams;component assembly;component deployment;incremental development;flexible product design;end-user customization;software quality;ACE+TAO communication software package","","20","","37","","","","","","IEEE","IEEE Journals & Magazines"
"A Two-Component Language for Adaptation: Design, Semantics and Program Analysis","P. Degano; G. Ferrari; L. Galletta","Dipartimento di Informatica, Università di Pisa, Pisa, Italia; Dipartimento di Informatica, Università di Pisa, Pisa, Italia; Dipartimento di Informatica, Università di Pisa, Pisa, Italia","IEEE Transactions on Software Engineering","","2016","42","6","505","529","Adaptive systems are designed to modify their behaviour in response to changes of their operational environment. We propose a two-component language for adaptive programming, within the Context-Oriented Programming paradigm. It has a declarative constituent for programming the context and a functional one for computing. We equip our language with a dynamic formal semantics. Since wrong adaptation could severely compromise the correct behaviour of applications and violate their properties, we also introduce a two-phase verification mechanism. It is based on a type and effect system that type-checks programs and computes, as an effect, a sound approximation of their behaviour. The effect is exploited at load time to mechanically verify that programs correctly adapt themselves to all possible running environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2496941","MIUR Prin Project; Università di Pisa PRA project; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7314969","Adaptive Software;Context Oriented Programming;Formal Methods;Datalog;Functional Programming;Semantics;Type Systems;Verification;Adaptive software;context oriented programming;formal methods;datalog;functional programming;semantics;type systems;verification","Context;Programming;Software;Semantics;Standards;Adaptation models;Computer languages","high level languages;program diagnostics;program verification","two-component language;program analysis;adaptive programming;context-oriented programming paradigm;dynamic formal semantics;two-phase verification mechanism;program type-check","","4","","95","","","","","","IEEE","IEEE Journals & Magazines"
"User-centric content negotiation for effective adaptation service in mobile computing","Wai Yip Lum; F. C. M. Lau","Dept. of Comput. Sci. & Inf. Syst., Hong Kong Univ., China; Dept. of Comput. Sci. & Inf. Syst., Hong Kong Univ., China","IEEE Transactions on Software Engineering","","2003","29","12","1100","1111","We address the challenges of building a good content adaptation service for mobile devices and propose a decision engine that is user-centric with QoS awareness, which can automatically negotiate for the appropriate adaptation decision to use in the synthesis of an optimal adapted version. The QoS-sensitive approach complements the lossy nature of the transcoding operations. The decision engine will look for the best trade off among various parameters in order to reduce the loss of quality in various domains. Quantitative methods are suggested to measure the QoS of the content versions in various quality domains. Based on the particular user perception and other contextual information on the client capability, the network connection, and the requested content, the proposed negotiation algorithm will determine a content version with a good aggregate score. We have built a prototype document adaptation system for PDF documents to demonstrate the viability of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265524","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265524","","Mobile computing;Quality of service;Search engines;Wireless networks;Computer Society;Transcoding;Aggregates;Software prototyping;Prototypes;Context awareness","mobile computing;quality of service;client-server systems;mobile handsets;inference mechanisms","user-centric content negotiation;effective adaptation service;mobile computing;mobile devices;decision engine;QoS awareness;transcoding operations;best trade off;loss of quality;quantitative methods;user perception;contextual information;client capability;network connection;prototype document adaptation system;PDF documents","","37","","23","","","","","","IEEE","IEEE Journals & Magazines"
"GenProg: A Generic Method for Automatic Software Repair","C. Le Goues; T. Nguyen; S. Forrest; W. Weimer","University of Virginia, Charlottesville; University of New Mexico, Albuquerque; University of New Mexico, Albuquerque; University of Virginia, Charlottesville","IEEE Transactions on Software Engineering","","2012","38","1","54","72","This paper describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. GenProg uses an extended form of genetic programming to evolve a program variant that retains required functionality but is not susceptible to a given defect, using existing test suites to encode both the defect and required functionality. Structural differencing algorithms and delta debugging reduce the difference between this variant and the original program to a minimal repair. We describe the algorithm and report experimental results of its success on 16 programs totaling 1.25 M lines of C code and 120K lines of module code, spanning eight classes of defects, in 357 seconds, on average. We analyze the generated repairs qualitatively and quantitatively to demonstrate that the process efficiently produces evolved programs that repair the defect, are not fragile input memorizations, and do not lead to serious degradation in functionality.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.104","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6035728","Automatic programming;corrections;testing and debugging.","Maintenance engineering;Encoding;Computer bugs;Automatic programming;Debugging;Syntactics","formal specification;genetic algorithms;software maintenance","GenProg;automatic software repair;defects repair;legacy programs;formal specifications;program annotations;special coding practices;genetic programming","","182","","75","","","","","","IEEE","IEEE Journals & Magazines"
"Parallel Algorithms for Testing Finite State Machines:Generating UIO Sequences","R. M. Hierons; U. C. Türker","Department of Computer Science, Brunel University London, United Kingdom; Department of Computer Science, Brunel University London, United Kingdom","IEEE Transactions on Software Engineering","","2016","42","11","1077","1091","This paper describes an efficient parallel algorithm that uses many-core GPUs for automatically deriving Unique Input Output sequences (UIOs) from Finite State Machines. The proposed algorithm uses the global scope of the GPU's global memory through coalesced memory access and minimises the transfer between CPU and GPU memory. The results of experiments indicate that the proposed method yields considerably better results compared to a single core UIO construction algorithm. Our algorithm is scalable and when multiple GPUs are added into the system the approach can handle FSMs whose size is larger than the memory available on a single GPU.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2539964","Scientific and Technological Research Council of Turkey; NVIDIA; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7429774","Software engineering/software/program verification;software engineering/testing and debugging;software engineering/test design;finite state machine;unique input output sequence generation;general purpose graphics processing units","Graphics processing units;Testing;Automata;Algorithm design and analysis;Software algorithms;Automation","finite state machines;graphics processing units;input-output programs;multiprocessing systems;parallel algorithms;program testing","parallel algorithms;finite state machine testing;UIO sequence generation;unique input output sequences;many-core GPU;graphics processing unit;coalesced memory access;memory transfer;single core UIO construction algorithm","","4","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Autofolding for Source Code Summarization","J. Fowkes; P. Chanthirasegaran; R. Ranca; M. Allamanis; M. Lapata; C. Sutton","School of Informatics, University of Edinburgh, Edinburgh, UK; School of Informatics, University of Edinburgh, Edinburgh, UK; Tractable, Oval Office, London, UK; School of Informatics, University of Edinburgh, Edinburgh, UK; School of Informatics, University of Edinburgh, Edinburgh, UK; School of Informatics, University of Edinburgh, Edinburgh, UK","IEEE Transactions on Software Engineering","","2017","43","12","1095","1109","Developers spend much of their time reading and browsing source code, raising new opportunities for summarization methods. Indeed, modern code editors provide code folding, which allows one to selectively hide blocks of code. However this is impractical to use as folding decisions must be made manually or based on simple rules. We introduce the autofolding problem, which is to automatically create a code summary by folding less informative code regions. We present a novel solution by formulating the problem as a sequence of AST folding decisions, leveraging a scoped topic model for code tokens. On an annotated set of popular open source projects, we show that our summarizer outperforms simpler baselines, yielding a 28 percent error reduction. Furthermore, we find through a case study that our summarizer is strongly preferred by experienced developers. More broadly, we hope this work will aid program comprehension by turning code folding into a usable and valuable tool.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2664836","Engineering and Physical Sciences Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7843666","Source code summarization, program comprehension, topic modelling","Software development;Natural languages;Source coding;Feature extraction;Complexity theory","public domain software;source code (software)","source code summarization;modern code editors;code folding;autofolding problem;informative code regions;AST folding decisions;code tokens;open source projects","","3","","62","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Nonparametric Analysis of the Order-Statistic Model in Software Reliability","S. P. Wilson; F. J. Samaniego","NA; NA","IEEE Transactions on Software Engineering","","2007","33","3","198","208","In the literature on statistical inference in software reliability, the assumptions of parametric models and random sampling of bugs have been pervasive. We argue that both assumptions are problematic, the first because of robustness concerns and the second due to logical and practical difficulties. These considerations motivate the approach taken in this paper. We propose a nonparametric software reliability model based on the order-statistic paradigm. The objective of the work is to estimate, from data on discovery times observed within a type I censoring framework, both the underlying distribution F from which discovery times are generated and N, the unknown number of bugs in the software. The estimates are used to predict the next time to failure. The approach makes use of Bayesian nonparametric inference methods, in particular, the beta-Stacy process. The proposed methodology is illustrated on both real and simulated data","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084137","Beta-Stacy process;order statistics;reliability;testing strategies;nonparametric statistics;survival analysis.","Software reliability;Computer bugs;Bayesian methods;Parametric statistics;Sampling methods;Robustness;Statistical analysis;Software debugging;Programming;Software testing","Bayes methods;program debugging;program testing;software reliability","order-statistic model;nonparametric software reliability model;statistical inference;random sampling;software bug;type I censoring framework;software failure prediction;Bayesian nonparametric inference method;beta-Stacy process;software testing;survival analysis","","14","","23","","","","","","IEEE","IEEE Journals & Magazines"
"Generating Domain-Specific Visual Language Tools from Abstract Visual Specifications","J. C. Grundy; J. Hosking; K. N. Li; N. M. Ali; J. Huh; R. L. Li","Swinburne University University of Technology, Hawthorn; Australian National University, Canberra; SolNet Solutions Ltd, Wellington; Universiti Putra Malaysia, Kuala Lumpur; University of Auckland, Auckland; Beefand Lamb New Zealand Ltd, Wellington","IEEE Transactions on Software Engineering","","2013","39","4","487","515","Domain-specific visual languages support high-level modeling for a wide range of application domains. However, building tools to support such languages is very challenging. We describe a set of key conceptual requirements for such tools and our approach to addressing these requirements, a set of visual language-based metatools. These support definition of metamodels, visual notations, views, modeling behaviors, design critics, and model transformations and provide a platform to realize target visual modeling tools. Extensions support collaborative work, human-centric tool interaction, and multiplatform deployment. We illustrate application of the metatoolset on tools developed with our approach. We describe tool developer and cognitive evaluations of our platform and our exemplar tools, and summarize key future research directions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6205768","Metatool;domain-specific visual language;software tool;visual specification;model-driven engineering","Visualization;Unified modeling language;Software;Computational modeling;Business;Abstracts;Electronic mail","cognition;formal specification;groupware;simulation languages;software tools;visual languages;visual programming","domain-specific visual language tool generation;abstract visual specifications;high-level modeling;application domains;conceptual requirements;visual language-based metatools;metamodels;visual notations;modeling behaviors;design critics;model transformations;visual modeling tools;collaborative work;human-centric tool interaction;multiplatform deployment;cognitive evaluations","","10","","79","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Abstractions for Contract Validation","G. de Caso; V. Braberman; D. Garbervetsky; S. Uchitel","FCEyN, Universidad de Buenos Aires, Buenos Aires; FCEyN, Universidad de Buenos Aires, Buenos Aires; FCEyN, Universidad de Buenos Aires, Buenos Aires; FCEyN, Universidad de Buenos Aires, Buenos Aires and Imperial College, London","IEEE Transactions on Software Engineering","","2012","38","1","141","162","Pre/postcondition-based specifications are commonplace in a variety of software engineering activities that range from requirements through to design and implementation. The fragmented nature of these specifications can hinder validation as it is difficult to understand if the specifications for the various operations fit together well. In this paper, we propose a novel technique for automatically constructing abstractions in the form of behavior models from pre/postcondition-based specifications. Abstraction techniques have been used successfully for addressing the complexity of formal artifacts in software engineering; however, the focus has been, up to now, on abstractions for verification. Our aim is abstraction for validation and hence, different and novel trade-offs between precision and tractability are required. More specifically, in this paper, we define and study enabledness-preserving abstractions, that is, models in which concrete states are grouped according to the set of operations that they enable. The abstraction results in a finite model that is intuitive to validate and which facilitates tracing back to the specification for debugging. The paper also reports on the application of the approach to two industrial strength protocol specifications in which concerns were identified.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.98","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5639021","Requirements/specifications;validation;automated abstraction.","Validation;Software engineering;Object oriented modeling;Protocols;Buffer storage","formal specification;software engineering","automated abstractions;contract validation;software engineering;behavior models;postcondition based specifications;precondition based specifications;formal artifacts;industrial strength","","16","","38","","","","","","IEEE","IEEE Journals & Magazines"
"An Extensible Framework for Improving a Distributed Software System's Deployment Architecture","S. Malek; N. Medvidovic; M. Mikic-Rakic","George Mason University, Fairfax; University of Southern California, Los Angeles; Google Inc., Santa Monica","IEEE Transactions on Software Engineering","","2012","38","1","73","100","A distributed system's allocation of software components to hardware nodes (i.e., deployment architecture) can have a significant impact on its quality of service (QoS). For a given system, there may be many deployment architectures that provide the same functionality, but with different levels of QoS. The parameters that influence the quality of a system's deployment architecture are often not known before the system's initial deployment and may change at runtime. This means that redeployment of the software system may be necessary to improve the system's QoS properties. This paper presents and evaluates a framework aimed at finding the most appropriate deployment architecture for a distributed software system with respect to multiple, possibly conflicting QoS dimensions. The framework supports formal modeling of the problem and provides a set of tailorable algorithms for improving a system's deployment. We have realized the framework on top of a visual deployment architecture modeling and analysis environment. The framework has been evaluated for precision and execution-time complexity on a large number of simulated distributed system scenarios, as well as in the context of two third-party families of distributed applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.3","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680912","Software architecture;software deployment;quality of service;self-adaptive software.","Software architecture;Quality of service;Distributed processing","computational complexity;distributed processing;object-oriented methods;quality of service;resource allocation","extensible framework;distributed software system;software component allocation;hardware nodes;quality of service;QoS;tailorable algorithms;system deployment;visual deployment architecture modeling environment;visual deployment architecture analysis environment;precision-time complexity;execution-time complexity","","38","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Intent specifications: an approach to building human-centered specifications","N. G. Leveson","Dept. of Aeronaut. & Astronaut., MIT, Cambridge, MA, USA","IEEE Transactions on Software Engineering","","2000","26","1","15","35","This paper examines and proposes an approach to writing software specifications, based on research in systems theory, cognitive psychology and human-machine interaction. The goal is to provide specifications that support human problem solving and the tasks that humans must perform in software development and evolution. A type of specification, called intent specifications, is constructed upon this underlying foundation.","0098-5589;1939-3520;2326-3881","","10.1109/32.825764","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=825764","","Humans;Software systems;Programming;Design engineering;Psychology;Man machine systems;Software maintenance;Software design;Software debugging;Software performance","formal specification;user centred design;user interfaces;human factors","intent specifications;human-centered specifications;systems theory;cognitive psychology;human-machine interaction;human problem solving;software development","","99","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Experience with performance testing of software systems: issues, an approach, and case study","E. J. Weyuker; F. I. Vokolos","AT&T Labs., Florham Park, NJ, USA; NA","IEEE Transactions on Software Engineering","","2000","26","12","1147","1156","An approach to software performance testing is discussed. A case study describing the experience of using this approach for testing the performance of a system used as a gateway in a large industrial client/server transaction processing application is presented.","0098-5589;1939-3520;2326-3881","","10.1109/32.888628","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888628","","Software testing;System testing;Software systems;Computer aided software engineering;Computer architecture;Vehicle crash testing;Software performance;Application software;Computer industry;Computer crashes","software performance evaluation;program testing;transaction processing;client-server systems;network servers","software performance testing;case study;gateway;industrial client/server transaction processing application","","92","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Systematic Testing of Model-Based Code Generators","I. Stuermer; M. Conrad; H. Doerr; P. Pepper","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","9","622","634","Unlike for conventional compilers for imperative programming languages such as C or ADA, no established methods for safeguarding artifacts generated by model-based code generators exist despite progress in the field of formal verification. Several test approaches dominate the engineering practice. This paper describes a general and tool-independent test architecture for code generators used in model-based development. We evaluate the effectiveness of our test approach by means of testing optimizations performed by the TargetLink code generator, a widely accepted and complex development tool used in automotive model-based development.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70708","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288195","Testing and Debugging","System testing;Automatic testing;Program processors;Computer languages;Automotive engineering;Performance evaluation;Formal verification;Computer architecture;Debugging","program compilers;program testing;program verification;software tools","systematic testing;model-based code generators;imperative programming languages;formal verification;tool-independent test architecture;optimization testing;TargetLink code generator;software tools;automotive model-based development","","24","","61","","","","","","IEEE","IEEE Journals & Magazines"
"The role of modeling in the performance testing of e-commerce applications","A. Avritzer; E. J. Weyuker","Siemens Corp. Res., Princeton, NJ, USA; NA","IEEE Transactions on Software Engineering","","2004","30","12","1072","1083","An e-commerce scalability case study is presented in which both traditional performance testing and performance modeling were used to help tune the application for high performance. This involved the creation of a system simulation model as well as the development of an approach for test case generation and execution. We describe our experience using a simulation model to help diagnose production system problems, and discuss ways that the effectiveness of performance testing efforts was improved by its use.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.107","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377198","Index Terms- Software performance testing;software testing;performance modeling;simulation;workload characterization.","System testing;Scalability;Software testing;Aerospace testing;Monitoring;Production systems;Computer architecture;Java;Databases","software performance evaluation;program testing;electronic commerce;resource allocation;Java","software performance testing;e-commerce;test case generation;production system diagnosis;workload characterization;software performance modeling","","17","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Secure execution of Java applets using a remote playground","D. Malkhi; M. K. Reiter","Dept. of Comput. Sci., Hebrew Univ., Jerusalem, Israel; NA","IEEE Transactions on Software Engineering","","2000","26","12","1197","1209","Mobile code presents a number of threats to machines that execute it. We introduce an approach for protecting machines and the resources they hold from mobile code and describe a system based on our approach for protecting host machines from Java 1.1 applets. In our approach, each Java applet downloaded to the protected domain is rerouted to a dedicated machine (or set of machines), the playground, at which it is executed. Prior to execution, the applet is transformed to use the downloading user's Web browser as a graphics terminal for its input and output, and so the user has the illusion that the applet is running on his own machine. In reality, however, mobile code runs only in the sanitized environment of the playground, where user files cannot be mounted and from which only limited network connections are accepted by machines in the protected domain. Our playground thus provides a second level of defense against mobile code that circumvents language-based defenses. This paper presents the design and implementation of a playground for Java 1.1 applets and discusses extensions of it for other forms of mobile code, including Java 1.2.","0098-5589;1939-3520;2326-3881","","10.1109/32.888632","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888632","","Java;Protection;Mobile computing;Computer security;Data security;Computer displays;Graphics;Computer networks;Computer errors;Physics computing","Java;distributed programming;object-oriented programming;security of data;remote procedure calls;online front-ends","secure execution;dowloaded Java applets;remote playground;mobile code;protected domain;applet rerouting;dedicated machine;Web browser;graphics terminal;user files;limited network connections;Java 1.1;Java 1.2;remote method invocation","","10","","32","","","","","","IEEE","IEEE Journals & Magazines"
"The Effects of an ARMOR-based SIFT environment on the performance and dependability of user applications","K. Whisnant; R. K. Iyer; Z. T. Kalbarczyk; P. H. Jones; D. A. Rennels; R. Some","Sun MicroSysterms Inc., San Diego, CA, USA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","4","257","277","Few, distributed software-implemented fault tolerance (SIFT) environments have been experimentally evaluated using substantial applications to show that they protect both themselves and the applications from errors. We present an experimental evaluation of a SIFT environment used to oversee spaceborne applications as part of the Remote Exploration and Experimentation (REE) program at the Jet Propulsion Laboratory. The SIFT environment is built around a set of self-checking ARMOR processes running on different machines that provide error detection and recovery services to themselves and to the REE applications. An evaluation methodology is presented in which over 28,000 errors were injected into both the SIFT processes and two representative REE applications. The experiments were split into three groups of error injections, with each group successively stressing the SIFT error detection and recovery more than the previous group. The results show that the SIFT environment added negligible overhead to the application's execution time during failure-free runs. Correlated failures affecting a SIFT process and application process are possible, but the division of detection and recovery responsibilities in the SIFT environment allows it to recover from these multiple failure scenarios. Only 28 cases were observed in which either the application failed to start or the SIFT environment failed to recognize that the application had completed. Further investigations showed that assertions within the SIFT processes-coupled with object-based incremental checkpointing-were effective in preventing system failures by protecting dynamic data within the SIFT processes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274045","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274045","","Application software;Protection;Computer crashes;Fault tolerance;Propulsion;Space missions;Telescopes;Fault tolerant systems;Availability","system recovery;software fault tolerance;aerospace computing;distributed processing","ARMOR-based SIFT environment;software-implemented fault tolerance;user applications;spaceborne applications;Remote Exploration and Experimentation program;error detection;error recovery;object-based incremental checkpointing;distributed systems","","7","","38","","","","","","IEEE","IEEE Journals & Magazines"
"How Social and Communication Channels Shape and Challenge a Participatory Culture in Software Development","M. Storey; A. Zagalsky; F. F. Filho; L. Singer; D. M. German","University of Victoria, Victoria, BC, Canada; University of Victoria, Victoria, BC, Canada; Universidade Federal do Rio Grande do Norte, Natal, Brazil; University of Victoria, Victoria, BC, Canada; University of Victoria, Victoria, BC, Canada","IEEE Transactions on Software Engineering","","2017","43","2","185","204","Software developers use many different communication tools and channels in their work. The diversity of these tools has dramatically increased over the past decade and developers now have access to a wide range of socially enabled communication channels and social media to support their activities. The availability of such social tools is leading to a participatory culture of software development, where developers want to engage with, learn from, and co-create software with other developers. However, the interplay of these social channels, as well as the opportunities and challenges they may create when used together within this participatory development culture are not yet well understood. In this paper, we report on a large-scale survey conducted with 1,449 GitHub users. We discuss the channels these developers find essential to their work and gain an understanding of the challenges they face using them. Our findings lay the empirical foundation for providing recommendations to developers and tool designers on how to use and improve tools for software developers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2584053","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7498605","Participatory culture;communication;social media;CSCW;software engineering","Software;Communication channels;Media;Collaboration;Electronic mail;Face;Knowledge engineering","cultural aspects;professional communication;social networking (online);software development management;team working","social channels;software development;participatory culture;communication tools;socially enabled communication channels;social media;social tools;GitHub users;software developers","","15","","41","","","","","","IEEE","IEEE Journals & Magazines"
"A model for estimating the size of a formal communication protocol specification and its implementation","R. Lai; Sun-Jen Huang","Dept. of Comput. Sci. & Comput. Eng., La Trobe Univ., Australia; NA","IEEE Transactions on Software Engineering","","2003","29","1","46","62","Good project management is key when developing a software system successfully. To manage a project well, it is important to have the optimal resource allocation which is affected by the size of an implementation. Early software size estimation is essential for good project management. Existing software size models estimate the size of an implementation usually in terms of the number of lines of code. The main drawback of these models is that there is a wide margin of uncertainty as the actual size depends on the type of application and the software development method adopted. To address this drawback, we focus our work on communication protocol, and propose that the size of a formal specification needs to be estimated from an informal specification. This paper presents a two-stage size model for estimating the sizes of a formal communication protocol specification and its implementation, with the model validated using a test data set. The main benefit of this work is that it can give an indication of the likely sizes of both a formal specification and its implementation early at the development stage, giving developers a technique for managing communication software project better.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166588","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166588","","Protocols;Project management;Resource management;Formal specifications;Software systems;Uncertainty;Application software;Programming;Testing;Software development management","specification languages;formal specification;statistical analysis;access protocols","project management;software system;optimal resource allocation;early software size estimation;formal communication protocol specification;software development method;Estelle","","8","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Design and development of multidevice user interfaces through multiple logical descriptions","G. Mori; F. Paterno; C. Santoro","Inf. of Sci. & Inf. Technol., Nat. Res. Council, Pisa, Italy; Inf. of Sci. & Inf. Technol., Nat. Res. Council, Pisa, Italy; Inf. of Sci. & Inf. Technol., Nat. Res. Council, Pisa, Italy","IEEE Transactions on Software Engineering","","2004","30","8","507","520","The increasing availability of new types of interaction platforms raises a number of issues for designers and developers. There is a need for new methods and tools to support development of nomadic applications, which can be accessed through a variety of devices. We present a solution, based on the use of three levels of abstractions, that allows designers to focus on the relevant logical aspects and avoid dealing with a plethora of low-level details. We have defined a number of transformations able to obtain user interfaces from such abstractions, taking into account the available platforms and their interaction modalities while preserving usability. The transformations are supported by an authoring tool, TERESA, which provides designers and developers with various levels of automatic support and several possibilities for tailoring such transformations to their needs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316868","Index Terms- Design tools and techniques;user interfaces;heterogeneous clients;multiplatform user interfaces;authoring environments;abstract user interfaces;user interface design;task models.","User interfaces;Usability;Transcoding;Application software;Personal digital assistants;Mobile handsets;Watches;Displays;Availability;Software tools","user interface management systems;authoring systems","multidevice user interface design;multidevice user interface development;multiple logical description;TERESA authoring tool;heterogeneous client;multiplatform user interface;authoring environment;abstract user interface","","108","","28","","","","","","IEEE","IEEE Journals & Magazines"
"A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection","W. Kessentini; M. Kessentini; H. Sahraoui; S. Bechikh; A. Ouni","Department of Computer Science, University of Montreal, Montreal, Quebec, Canada; Department of Computer Science, University of Michigan, Dearborn, MI; Department of Computer Science, University of Montreal, Montreal, Quebec, Canada; Department of Computer Science, University of Michigan, Dearborn, MI; Department of Computer Science, University of Michigan, Dearborn, MI","IEEE Transactions on Software Engineering","","2014","40","9","841","861","We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different methods are combined in parallel during the optimization process to find a consensus regarding the detection of code-smells. To this end, we used Parallel Evolutionary algorithms (P-EA) where many evolutionary algorithms with different adaptations (fitness functions, solution representations, and change operators) are executed, in a parallel cooperative manner, to solve a common goal which is the detection of code-smells. An empirical evaluation to compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and two code-smells detection techniques that are not based on meta-heuristics search. The statistical analysis of the obtained results provides evidence to support the claim that cooperative P-EA is more efficient and effective than state of the art detection approaches based on a benchmark of nine large open source systems where more than 85 percent of precision and recall scores are obtained on a variety of eight different types of code-smells.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2331057","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6835187","Search-based software engineering;code-smells;software quality;distributed evolutionary algorithms","Measurement;Sociology;Statistics;Evolutionary computation;Detectors;Optimization;Computational modeling","evolutionary computation;public domain software;search problems;software engineering;statistical analysis","cooperative parallel search-based software engineering approach;code-smells detection;distributed optimization problem;optimization process;parallel evolutionary algorithms;P-EA approach;random search;single population-based approaches;statistical analysis;open source systems","","26","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Language Inclusion Checking of Timed Automata with Non-Zenoness","X. Wang; J. Sun; T. Wang; S. Qin","College of Computer Science, Zhejiang University, Hangzhou, P.R. China; Shenzhen University; College of Computer Science, Zhejiang University of Technology, Hangzhou, P.R. China; School of Computing, Teesside University, Middlesbrough, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","11","995","1008","Given a timed automaton P modeling an implementation and a timed automaton S as a specification, the problem of language inclusion checking is to decide whether the language of P is a subset of that of S. It is known to be undecidable. The problem gets more complicated if non-Zenoness is taken into consideration. A run is Zeno if it permits infinitely many actions within finite time. Otherwise it is non-Zeno. Zeno runs might present in both P and S. It is necessary to check whether a run is Zeno or not so as to avoid presenting Zeno runs as counterexamples of language inclusion checking. In this work, we propose a zone-based semi-algorithm for language inclusion checking with non-Zenoness. It is further improved with simulation reduction based on LU-simulation. Though our approach is not guaranteed to terminate, we show that it does in many cases through empirical study. Our approach has been incorporated into the PAT model checker, and applied to multiple systems to show its usefulness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2653778","National Natural Science Foundation of China; Singapore University of Technology and Design; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7819478","Timed automata;language inclusion;non-Zenoness","Automata;Clocks;Safety;Analytical models;Sun;Real-time systems;Semantics","automata theory;formal specification;formal verification","timed automata;language inclusion checking;nonZenoness;finite time;timed automaton modeling;specification;undecidability;zone-based semi-algorithm;simulation reduction;LU-simulation;PAT model checker","","","","40","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Self-Adapting Reliability in Distributed Software Systems","Y. Brun; J. y. Bang; G. Edwards; N. Medvidovic","School of Computer Science, University of Massachusetts, Amherst, MA; Computer Science Department, University of Southern California, Los Angeles, CA; Computer Science Department, University of Southern California, Los Angeles, CA; Computer Science Department, University of Southern California, Los Angeles, CA","IEEE Transactions on Software Engineering","","2015","41","8","764","780","Developing modern distributed software systems is difficult in part because they have little control over the environments in which they execute. For example, hardware and software resources on which these systems rely may fail or become compromised and malicious. Redundancy can help manage such failures and compromises, but when faced with dynamic, unpredictable resources and attackers, the system reliability can still fluctuate greatly. Empowering the system with self-adaptive and self-managing reliability facilities can significantly improve the quality of the software system and reduce reliance on the developer predicting all possible failure conditions. We present iterative redundancy, a novel approach to improving software system reliability by automatically injecting redundancy into the system's deployment. Iterative redundancy self-adapts in three ways: (1) by automatically detecting when the resource reliability drops, (2) by identifying unlucky parts of the computation that happen to deploy on disproportionately many compromised resources, and (3) by not relying on a priori estimates of resource reliability. Further, iterative redundancy is theoretically optimal in its resource use: Given a set of resources, iterative redundancy guarantees to use those resources to produce the most reliable version of that software system possible; likewise, given a desired increase in the system's reliability, iterative redundancy guarantees achieving that reliability using the least resources possible. Iterative redundancy handles even the Byzantine threat model, in which compromised resources collude to attack the system. We evaluate iterative redundancy in three ways. First, we formally prove its self-adaptation, efficiency, and optimality properties. Second, we simulate it at scale using discrete event simulation. Finally, we modify the existing, open-source, volunteer-computing BOINC software system and deploy it on the globally-distributed PlanetLab testbed network to empirically evaluate that iterative redundancy is self-adaptive and more efficient than existing techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2412134","DARPA; IARPA; National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7058381","Redundancy;reliability;fault-tolerance;iterative redundancy;self-adaptation;optimal redundancy;Redundancy;reliability;fault-tolerance;iterative redundancy;self-adaptation;optimal redundancy","Redundancy;Software reliability;Software systems;Computational modeling;Servers;Reliability engineering","discrete event simulation;distributed processing;public domain software;resource allocation;security of data;software quality;software reliability;system recovery","self-adapting reliability;distributed software systems;hardware resources;software resources;failure management;compromise management;dynamic unpredictable resources;system reliability;self-adaptive reliability;self-managing reliability;software system quality;failure condition;iterative redundancy;resource reliability estimate;Byzantine threat model;compromised resource collusion;optimality property;discrete event simulation;open-source volunteer-computing BOINC software system;globally-distributed PlanetLab testbed network","","5","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Applying Formal Methods to a Certifiably Secure Software System","C. Heitmeyer; M. Archer; E. Leonard; J. McLean","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","1","82","98","A major problem in verifying the security of code is that the code's large size makes it much too costly to verify in its entirety. This paper describes a novel and practical approach to verifying the security of code which substantially reduces the cost of verification. In this approach, a compact security model containing only information needed to reason about the security properties of interest is constructed and the security properties are represented formally in terms of the model. To reduce the cost of verification, the code to be verified is partitioned into three categories and only the first category, which is less than 10 percent of the code in our application, requires formal verification. The proof of the other two categories is relatively trivial. Our approach was developed to support a common criteria evaluation of the separation kernel of an embedded software system. This paper describes 1) our techniques and theory for verifying the kernel code and 2) the artifacts produced, that is, a top-level specification (TLS), a formal statement of the security property, a mechanized proof that the TLS satisfies the property, the partitioning of the code, and a demonstration that the code conforms to the TLS. This paper also presents the formal basis for the argument that the kernel code conforms to the TLS and consequently satisfies the security property.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70772","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4407731","Specification;security;verification;security kernels;tools;Formal methods;Software;software verification;Specification;security;verification;security kernels;tools;Formal methods;Software;software verification","Software systems;Information security;Kernel;National security;Data security;Costs;Computer Society;Formal verification;Mechanical factors;Software tools","formal verification;security of data","formal methods;certifiably secure software system;formal verification;common criteria evaluation;embedded software system;kernel code;top-level specification","","27","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Optimized Resource Allocation for Software Release Planning","A. Ngo-The; G. Ruhe","Expert Decisions Inc., Calgary; University of Calgary, Calgary","IEEE Transactions on Software Engineering","","2009","35","1","109","123","Release planning for incremental software development assigns features to releases such that technical, resource, risk and budget constraints are met. Planning of software releases and allocation of resources cannot be handled in isolation. A feature can be offered as part of a release only if all its necessary tasks are done before the given release date. We assume a given pool of human resources with different degrees of productivity to perform different types of tasks. To address the inherent difficulty of this process, we propose a two-phased optimization approach that combines the strength of two existing solution methods. The industrial applicability of the approach is primarily directed towards mature organizations having systematic development and measurement processes in place. The expected practical benefit of the planning method is to provide release plan solutions that achieve a better overall business value (e.g., expressed by the degree of stakeholder satisfaction) by better allocation of resources. Without ignoring the importance of the human expert in this process, the contributions of the paper are seen in making the overall process more objective and the resulting decisions more transparent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.80","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4641940","Planning;Management;Planning;Management","Resource management;Humans;Productivity;Optimization methods;Constraint optimization;Integer linear programming;Genetic programming;Project management;Feedback;Companies","project management;resource allocation;software engineering;software management","optimized resource allocation;software release planning;incremental software development;software project management","","45","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Active learning and effort estimation: Finding the essential content of software effort estimation data","E. Kocaguneli; T. Menzies; J. Keung; D. Cok; R. Madachy","West Virginia University, Morgantown; West Virginia University, Morgantown; The City University of Hong Kong, Hong Kong; Grammatech Inc., Ithaca; Naval Postgraduate School, Monterrey","IEEE Transactions on Software Engineering","","2013","39","8","1040","1053","Background: Do we always need complex methods for software effort estimation (SEE)? Aim: To characterize the essential content of SEE data, i.e., the least number of features and instances required to capture the information within SEE data. If the essential content is very small, then 1) the contained information must be very brief and 2) the value added of complex learning schemes must be minimal. Method: Our QUICK method computes the euclidean distance between rows (instances) and columns (features) of SEE data, then prunes synonyms (similar features) and outliers (distant instances), then assesses the reduced data by comparing predictions from 1) a simple learner using the reduced data and 2) a state-of-the-art learner (CART) using all data. Performance is measured using hold-out experiments and expressed in terms of mean and median MRE, MAR, PRED(25), MBRE, MIBRE, or MMER. Results: For 18 datasets, QUICK pruned 69 to 96 percent of the training data (median = 89 percent). K = 1 nearest neighbor predictions (in the reduced data) performed as well as CART's predictions (using all data). Conclusion: The essential content of some SEE datasets is very small. Complex estimation methods may be overelaborate for such datasets and can be simplified. We offer QUICK as an example of such a simpler SEE method.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6392173","Software cost estimation;active learning;analogy;k-NN","Estimation;Indexes;Labeling;Frequency selective surfaces;Euclidean distance;Complexity theory;Principal component analysis","data handling;learning (artificial intelligence);software cost estimation;statistical analysis","software effort estimation;SEE data content;complex learning scheme;QUICK method;Euclidean distance;CART learner;mean;median;K-nearest neighbor prediction","","31","","50","","","","","","IEEE","IEEE Journals & Magazines"
"The construction of contextual def-use associations for object-oriented systems","A. L. Souter; L. L. Pollock","Dept. of Comput. Sci., Drexel Univ., Philadelphia, PA, USA; NA","IEEE Transactions on Software Engineering","","2003","29","11","1005","1018","This paper describes a program representation and algorithms for realizing a novel structural testing methodology that not only focuses on addressing the complex features of object-oriented languages, but also incorporates the structure of object-oriented software into the approach. The testing methodology is based on the construction of contextual def-use associations, which provide context to each definition and use of an object. Testing based on contextual def-use associations can provide increased test coverage by identifying multiple unique contextual def-use associations for the same context-free association. Such a testing methodology promotes more thorough and focused testing of the manipulation of objects in object-oriented programs. This paper presents a technique for the construction of contextual def-use associations, as well as detailed examples illustrating their construction, an analysis of the cost of constructing contextual def-use associations with this approach, and a description of a prototype testing tool that shows how the theoretical contributions of this work can be useful for structural test coverage.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245302","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245302","","System testing;Computer Society;Software algorithms;Software testing;Automatic testing;Aggregates;Costs;Prototypes;Object oriented programming;Debugging","program testing;object-oriented programming;object-oriented languages;program debugging","program representation;structural testing methodology;object-oriented software;D.3.2.p object-oriented languages;D.2.5.m testing coverage;testing methodology;contextual def-use associations;prototype testing tool","","16","","19","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic adaptation and deployment of distributed components in Hadas","I. Ben-Shaul; O. Holder; B. Lavva","Dept. of Electr. Eng., Technion-Israel Inst. of Technol., Haifa, Israel; NA; NA","IEEE Transactions on Software Engineering","","2001","27","9","769","787","Global network connectivity has enabled accessibility to a wide range of geographically dispersed services. By encapsulating and representing such services as components, they can be effectively composed into sophisticated wide-area applications that otherwise would be much harder to build. This paper presents a component model for encapsulating such services and a composition model for assembling encapsulated and possibly active services into new applications, while preserving the administrative autonomy of sites and individual components. The component model is dynamically and self-adaptable, allowing for the adjustment of structure and behavior of autonomous components to changing or previously unknown contexts in which they need to operate. The composition model includes a set of protocols that enable us to dynamically deploy live components into remote sites and to dynamically reconfigure the deployment scheme through reflective stubs termed Ambassadors. The component and composition models have been fully implemented in Hadas, which also includes a host of tools for the creation, deployment, and composition of autonomous components.","0098-5589;1939-3520;2326-3881","","10.1109/32.950315","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=950315","","Distributed computing;Application software;Intelligent networks;Assembly;Context modeling;Protocols;Systems engineering and theory;Java;Reflection;Large-scale systems","distributed object management;data encapsulation","dynamic adaptation;dynamic deployment;distributed components;Hadas;global network connectivity;geographically dispersed services;wide-area applications;encapsulation;administrative autonomy;component model;composition model;Ambassadors;autonomous components;self adaptive components;distributed systems;Java;negotiation;reflection;mobile objects","","16","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Leveraging user-session data to support Web application testing","S. Elbaum; G. Rothermel; S. Karre; M. Fisher II","Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA","IEEE Transactions on Software Engineering","","2005","31","3","187","202","Web applications are vital components of the global information infrastructure, and it is important to ensure their dependability. Many techniques and tools for validating Web applications have been created, but few of these have addressed the need to test Web application functionality and none have attempted to leverage data gathered in the operation of Web applications to assist with testing. In this paper, we present several techniques for using user session data gathered as users operate Web applications to help test those applications from a functional standpoint. We report results of an experiment comparing these new techniques to existing white-box techniques for creating test cases for Web applications, assessing both the adequacy of the generated test cases and their ability to detect faults on a point-of-sale Web application. Our results show that user session data can be used to produce test suites more effective overall than those produced by the white-box techniques considered; however, the faults detected by the two classes of techniques differ, suggesting that the techniques are complementary.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423991","Index Terms- Software testing;test data generation;Web applications;empirical studies.","Application software;Fault detection;Face detection;System testing;Automatic testing;Computer Society;Software systems;Marketing and sales;Proposals;Medical diagnostic imaging","program testing;Internet","Web application testing;user session data;white-box technique;software testing;test data generation","","113","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Reducing Feedback Delay of Software Development Tools via Continuous Analysis","K. Muşlu; Y. Brun; M. D. Ernst; D. Notkin","Department of Computer Science & Engineering, University of Washington, Seattle, WA; School of Computer Science, University of Massachusetts, Amherst, MA; Department of Computer Science & Engineering, University of Washington, Seattle, WA; Department of Computer Science & Engineering, University of Washington, Seattle, WA","IEEE Transactions on Software Engineering","","2015","41","8","745","763","During software development, the sooner a developer learns how code changes affect program analysis results, the more helpful that analysis is. Manually invoking an analysis may interrupt the developer's workflow or cause a delay before the developer learns the implications of the change. A better approach is continuous analysis tools that always provide up-to-date results. We present Codebase Replication, a technique that eases the implementation of continuous analysis tools by converting an existing offline analysis into an IDE-integrated, continuous tool with two desirable properties: isolation and currency. Codebase Replication creates and keeps in sync a copy of the developer's codebase. The analysis runs on the copy codebase without disturbing the developer and without being disturbed by the developer's changes. We developed Solstice, an open-source, publicly-available Eclipse plug-in that implements Codebase Replication. Solstice has less than 2.5 milliseconds overhead for most common developer actions. We used Solstice to implement four Eclipse-integrated continuous analysis tools based on the offline versions of FindBugs, PMD, data race detection, and unit testing. Each conversion required on average 710 LoC and 20 hours of implementation effort. Case studies indicate that Solstice-based continuous analysis tools are intuitive and easy-to-use.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2417161","NSF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7069257","Continuous analysis;Codebase Replication;Solstice;Continuous analysis;Codebase Replication;Solstice","Delays;Software;Testing;Servers;Synchronization;Electronic mail;Interrupters","program diagnostics;program testing;software tools","feedback delay reduction;software development tools;program analysis;codebase replication;offline analysis;IDE-integrated;Solstice;open-source Eclipse plug-in;Eclipse-integrated continuous analysis tools;FindBugs;PMD;data race detection;unit testing","","3","","80","","","","","","IEEE","IEEE Journals & Magazines"
"A Study of Variability Models and Languages in the Systems Software Domain","T. Berger; S. She; R. Lotufo; A. Wasowski; K. Czarnecki","IT University of Copenhagen, Copenhagen; University of Waterloo, Waterloo; University of Waterloo, Waterloo; IT University of Copenhagen, Copenhagen; University of Waterloo, Waterloo","IEEE Transactions on Software Engineering","","2013","39","12","1611","1640","Variability models represent the common and variable features of products in a product line. Since the introduction of FODA in 1990, several variability modeling languages have been proposed in academia and industry, followed by hundreds of research papers on variability models and modeling. However, little is known about the practical use of such languages. We study the constructs, semantics, usage, and associated tools of two variability modeling languages, Kconfig and CDL, which are independently developed outside academia and used in large and significant software projects. We analyze 128 variability models found in 12 open--source projects using these languages. Our study 1) supports variability modeling research with empirical data on the real-world use of its flagship concepts. However, we 2) also provide requirements for concepts and mechanisms that are not commonly considered in academic techniques, and 3) challenge assumptions about size and complexity of variability models made in academic papers. These results are of interest to researchers working on variability modeling and analysis techniques and to designers of tools, such as feature dependency checkers and interactive product configurators.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6572787","Empirical software engineering;software product lines;variability modeling;feature modeling;configuration;open source","Biological system modeling;Software products;Product line;Analytical models;Computational modeling;Semantics;Computer architecture","public domain software;simulation languages;software engineering","interactive product configurators;feature dependency checkers;variability analysis techniques;open-source projects;software projects;associated language tools;language usage;language semantics;language constructs;CDL language;Kconfig language;variability modeling languages;FODA;systems software domain;variability models","","33","","88","","","","","","IEEE","IEEE Journals & Magazines"
"Static Analysis of Object References in RMI-Based Java Software","M. Sharp; A. Rountev","IEEE Computer Society; NA","IEEE Transactions on Software Engineering","","2006","32","9","664","681","Distributed applications provide numerous advantages related to software performance, reliability, interoperability, and extensibility. This paper focuses on distributed Java programs built with the help of the remote method invocation (RMI) mechanism. We consider points-to analysis for such applications. Points-to analysis determines the objects pointed to by a reference variable or a reference object field. Such information plays a fundamental role as a prerequisite for many other static analyses. We present the first theoretical definition of points-to analysis for RMI-based Java applications, and we present an algorithm for implementing a flow- and context-insensitive points-to analysis for such applications. We also discuss the use of points-to information for corrupting call graph information, for understanding data dependencies due to remote memory locations, and for identifying opportunities for improving the performance of object serialization at remote calls. The work described in this paper solves one key problem for static analysis of RMI programs and provides a starting point for future work on improving the understanding, testing, verification, and performance of RMI-based software","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.93","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707666","RMI;object-oriented software;distributed software;program analysis;points-to analysis;reference analysis;class analysis;call graph construction;side-effect analysis.","Java;Application software;Software performance;Information analysis;Software testing;Algorithm design and analysis;Performance analysis;Computer Society;Middleware;Optimization","application program interfaces;Java;object-oriented programming;program diagnostics;remote procedure calls","static program analysis;object reference;RMI;distributed Java program;remote method invocation;points-to analysis;call graph information;data dependency;remote memory location;object serialization;object-oriented software","","3","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Light-Weight, Inter-Procedural and Callback-Aware Resource Leak Detection for Android Apps","T. Wu; J. Liu; Z. Xu; C. Guo; Y. Zhang; J. Yan; J. Zhang","State Key Laboratory of Computer ScienceInstitute of SoftwareChinese Academy of Sciences; State Key Laboratory of Computer ScienceInstitute of SoftwareChinese Academy of Sciences; NA; NA; NA; State Key Laboratory of Computer ScienceInstitute of SoftwareChinese Academy of Sciences; State Key Laboratory of Computer ScienceInstitute of SoftwareChinese Academy of Sciences","IEEE Transactions on Software Engineering","","2016","42","11","1054","1076","Android devices include many embedded resources such as Camera, Media Player and Sensors. These resources require programmers to explicitly request and release them. Missing release operations might cause serious problems such as performance degradation or system crash. This kind of defects is called resource leak. Despite a large body of existing works on testing and analyzing Android apps, there still remain several challenging problems. In this work, we present Relda2, a light-weight and precise static resource leak detection tool. We first systematically collected a resource table, which includes the resources that the Android reference requires developers release manually. Based on this table, we designed a general approach to automatically detect resource leaks. To make a more precise inter-procedural analysis, we construct a Function Call Graph for each Android application, which handles function calls of user-defined methods and the callbacks invoked by the Android framework at the same time. To evaluate Relda2's effectiveness and practical applicability, we downloaded 103 apps from popular app stores and an open source community, and found 67 real resource leaks, which we have confirmed manually.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2547385","National Basic Research (973); National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7442579","Android apps;resource leak;static analysis;byte-code analysis;inter-procedural analysis","Androids;Humanoid robots;Smart phones;Java;Testing;Leak detection;Computer bugs","Android (operating system);embedded systems;graph theory;program diagnostics;resource allocation","Android application;embedded resources;Relda2;interprocedural analysis;function call graph;static analysis;callback-aware resource leak detection tool","","13","","74","","","","","","IEEE","IEEE Journals & Magazines"
"Frameworks Generate Domain-Specific Languages: A Case Study in the Multimedia Domain","X. Amatriain; P. Arumi","Telefonica Research, Barcelona, Spain; Universitat Pompeu Fabra, Barcelona, Spain","IEEE Transactions on Software Engineering","","2011","37","4","544","558","We present an approach to software framework development that includes the generation of domain-specific languages (DSLs) and pattern languages as goals for the process. Our model is made of three workflows-framework, metamodel, and patterns-and three phases-inception, construction, and formalization. The main conclusion is that when developing a framework, we can produce with minimal overhead-almost as a side effect-a metamodel with an associated DSL and a pattern language. Both outputs will not only help the framework evolve in the right direction, but will also be valuable in themselves. In order to illustrate these ideas, we present a case study in the multimedia domain. For several years, we have been developing a multimedia framework. The process has produced a full-fledged domain-specific metamodel for the multimedia domain, with an associated DSL and a pattern language.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.48","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5441292","Domain-specific architectures;visual programming;life cycle;CASE.","Domain specific languages;DSL;Unified modeling language;Vocabulary;Concrete;Software engineering;Computer aided software engineering;Natural languages;Metamodeling;Best practices","multimedia computing;software engineering;specification languages;visual programming","domain-specific languages;multimedia domain;pattern languages;domain-specific metamodel;associated DSL;visual programming;software framework development","","6","","37","","","","","","IEEE","IEEE Journals & Magazines"
"On the Use of Mutation Faults in Empirical Assessments of Test Case Prioritization Techniques","Hyunsook Do; G. Rothermel","Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE 68588-0115; Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE 68588-0115","IEEE Transactions on Software Engineering","","2006","32","9","733","752","Regression testing is an important activity in the software life cycle, but it can also be very expensive. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of test case prioritization techniques is to increase a test suite's rate of fault detection (how quickly, in a run of its test cases, that test suite can detect faults). Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited primarily to hand-seeded faults, largely due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults and that the use of hand-seeded faults can be problematic for the validity of empirical results focusing on fault detection. We have therefore designed and performed two controlled experiments assessing the ability of prioritization techniques to improve the rate of fault detection of test case prioritization techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. More importantly, a comparison of our results with those collected using hand-seeded faults reveals several implications for researchers performing empirical studies of test case prioritization techniques in particular and testing techniques in general","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.92","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707670","Regression testing;test case prioritization;program mutation;empirical studies.","Genetic mutations;Computer aided software engineering;Fault detection;Software testing;System testing;Life testing;Software maintenance;Performance evaluation;Costs;Software measurement","program diagnostics;program testing","regression testing;software life cycle;test case prioritization technique;fault detection;hand-seeded fault;mutation fault","","98","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Solving the Large Scale Next Release Problem with a Backbone-Based Multilevel Algorithm","J. Xuan; H. Jiang; Z. Ren; Z. Luo","Dalian University of Technology, Dalian; Dalian University of Technology, Dalian; Dalian University of Technology, Dalian; Dalian University of Technology, Dalian","IEEE Transactions on Software Engineering","","2012","38","5","1195","1212","The Next Release Problem (NRP) aims to optimize customer profits and requirements selection for the software releases. The research on the NRP is restricted by the growing scale of requirements. In this paper, we propose a Backbone-based Multilevel Algorithm (BMA) to address the large scale NRP. In contrast to direct solving approaches, the BMA employs multilevel reductions to downgrade the problem scale and multilevel refinements to construct the final optimal set of customers. In both reductions and refinements, the backbone is built to fix the common part of the optimal customers. Since it is intractable to extract the backbone in practice, the approximate backbone is employed for the instance reduction while the soft backbone is proposed to augment the backbone application. In the experiments, to cope with the lack of open large requirements databases, we propose a method to extract instances from open bug repositories. Experimental results on 15 classic instances and 24 realistic instances demonstrate that the BMA can achieve better solutions on the large scale NRP instances than direct solving approaches. Our work provides a reduction approach for solving large scale problems in search-based requirements engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.92","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6261327","The next release problem;backbone;soft backbone;multilevel algorithm;requirements instance generation;search-based requirements engineering","Approximation algorithms;Software;Software algorithms;Algorithm design and analysis;Optimization;Polynomials;Search problems","customer services;formal specification;program debugging;software development management;systems analysis","large scale next release problem;backbone-based multilevel algorithm;customer profit optimization;requirements selection;software releases;BMA;multilevel reductions;optimal customers;instance reduction;soft backbone;open large requirements databases;open bug repositories;large scale NRP instances;search-based requirements engineering;backbone extraction","","33","","69","","","","","","IEEE","IEEE Journals & Magazines"
"A nonpreemptive real-time scheduler with recovery from transient faults and its implementation","D. Mosse; R. Melhem; Sunondo Ghosh","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; NA","IEEE Transactions on Software Engineering","","2003","29","8","752","767","Real-time systems (RTS) are those whose correctness depends on satisfying the required functional as well as the required temporal properties. Due to the criticality of such systems, recovery from faults is an essential part of a RTS. In many systems, such as those supporting space applications, single event upsets (SEUs) are the prevalent type of faults; SEUs are transient faults and affect a single task at a time. We present a scheme to guarantee that the execution of real-time tasks can tolerate SEUs and intermittent faults assuming any queue-based scheduling technique. Three algorithms are presented to solve the problem of adding fault tolerance to a queue of real-time tasks by reserving sufficient slack in a schedule so that recovery can be carried out before the task deadline without compromising guarantees given to other tasks. The first algorithm is a dynamic programming optimal solution, the second is a linear-time heuristic for scheduling dynamic tasks, and the third algorithm comprises extensions to address queues with gaps between tasks (gaps are caused by precedence, resource, or timing constraints). We show through simulations that the heuristics closely approximate the optimal algorithm. Finally, we describe the implementation of the modified admission control algorithm, non-preemptive scheduler, and recovery mechanism in the FT-RT-Mach operating system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1223648","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1223648","","Scheduling algorithm;Single event transient;Dynamic programming;Real time systems;Single event upset;Fault tolerance;Heuristic algorithms;Dynamic scheduling;Timing;Admission control","real-time systems;operating systems (computers);system recovery;fault tolerant computing;scheduling;dynamic programming;computational complexity;queueing theory;graph theory","nonpreemptive real-time scheduler;transient fault recovery;real-time system;single event upset;real-time task execution;intermittent fault;queue-based scheduling technique;dynamic programming;optimal algorithm;linear-time heuristic;dynamic task scheduling;modified admission control algorithm;FT-RT-Mach operating system;fault tolerance;layered graph","","23","","41","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study of speed and communication in globally distributed software development","J. D. Herbsleb; A. Mockus","Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA","IEEE Transactions on Software Engineering","","2003","29","6","481","494","Global software development is rapidly becoming the norm for technology companies. Previous qualitative research suggests that distributed development may increase development cycle time for individual work items (modification requests). We use both data from the source code change management system and survey data to model the extent of delay in a distributed software development organization and explore several possible mechanisms for this delay. One key finding is that distributed work items appear to take about two and one-half times as long to complete as similar items where all the work is colocated. The data strongly suggest a mechanism for the delay, i.e., that distributed work items involve more people than comparable same-site work items, and the number of people involved is strongly related to the calendar time to complete a work item. We replicate the analysis of change data in a different organization with a different product and different sites and confirm our main findings. We also report survey results showing differences between same-site and distributed social networks, testing several hypotheses about characteristics of distributed social networks that may be related to delay. We discuss implications of our findings for practices and collaboration technology that have the potential for dramatically speeding distributed software development.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205177","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205177","","Programming;Collaborative work;Social network services;Frequency;Software development management;Delay effects;Calendars;Data analysis;Testing;Collaborative software","software development management;management of change;social aspects of automation","globally distributed software development;technology companies;development cycle time;source code change management system;survey data;calendar time;distributed social networks;same-site social networks;collaboration technology;speed;communication","","415","","40","","","","","","IEEE","IEEE Journals & Magazines"
"How Reliable Are Systematic Reviews in Empirical Software Engineering?","S. MacDonell; M. Shepperd; B. Kitchenham; E. Mendes","Auckland University of Technology, Auckland; Brunel University, West London; Keele University, Keele; The University of Auckland, Auckland","IEEE Transactions on Software Engineering","","2010","36","5","676","687","BACKGROUND-The systematic review is becoming a more commonly employed research instrument in empirical software engineering. Before undue reliance is placed on the outcomes of such reviews it would seem useful to consider the robustness of the approach in this particular research context. OBJECTIVE-The aim of this study is to assess the reliability of systematic reviews as a research instrument. In particular, we wish to investigate the consistency of process and the stability of outcomes. METHOD-We compare the results of two independent reviews undertaken with a common research question. RESULTS-The two reviews find similar answers to the research question, although the means of arriving at those answers vary. CONCLUSIONS-In addressing a well-bounded research question, groups of researchers with similar domain experience can arrive at the same review outcomes, even though they may do so in different ways. This provides evidence that, in this context at least, the systematic review is a robust research method.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5416726","Empirical software engineering;meta-analysis;systematic review;cost estimation.","Software engineering;Instruments;Robustness;Best practices;Stability;Costs;Mathematics;Computer science","software cost estimation;software reviews","systematic review reliability;empirical software engineering;research instrument;software cost estimation","","29","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Engineering Adaptive Model-Driven User Interfaces","P. A. Akiki; A. K. Bandara; Y. Yu","Department of Computer Science, Notre Dame University—Louaize, Zouk Mosbeh, Lebanon; Computing and Communications Department, The Open University, Walton Hall, Milton Keynes, United Kingdom; Computing and Communications Department, The Open University, Walton Hall, Milton Keynes, United Kingdom","IEEE Transactions on Software Engineering","","2016","42","12","1118","1147","Software applications that are very large-scale, can encompass hundreds of complex user interfaces (UIs). Such applications are commonly sold as feature-bloated off-the-shelf products to be used by people with variable needs in the required features and layout preferences. Although many UI adaptation approaches were proposed, several gaps and limitations including: extensibility and integration in legacy systems, still need to be addressed in the state-of-the-art adaptive UI development systems. This paper presents Role-Based UI Simplification (RBUIS) as a mechanism for increasing usability through adaptive behavior by providing end-users with a minimal feature-set and an optimal layout, based on the context-of-use. RBUIS uses an interpreted runtime model-driven approach based on the Cedar Architecture, and is supported by the integrated development environment (IDE), Cedar Studio. RBUIS was evaluated by integrating it into OFBiz, an open-source ERP system. The integration method was assessed and measured by establishing and applying technical metrics. Afterwards, a usability study was carried out to evaluate whether UIs simplified with RBUIS show an improvement over their initial counterparts. This study leveraged questionnaires, checking task completion times and output quality, and eye-tracking. The results showed that UIs simplified with RBUIS significantly improve end-user efficiency, effectiveness, and perceived usability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2553035","Computing and Communications Department; The Open University; ERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7451279","Design tools and techniques;software architectures;support for adaptation;user interfaces","Adaptation models;Adaptive systems;User interfaces;Usability;Computer architecture","ergonomics;programming environments;software architecture;software maintenance;user interfaces","IDE;integrated development environment;Cedar architecture;adaptive behavior;system usability;RBUIS;role-based UI simplification;legacy system integration;legacy system extensibility;engineering adaptive model-driven user interface","","7","","85","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling software bidding risks","B. Kitchenham; L. M. Pickard; S. Linkman; P. W. Jones","Dept. of Comput. Sci., Keele Univ., UK; Dept. of Comput. Sci., Keele Univ., UK; Dept. of Comput. Sci., Keele Univ., UK; NA","IEEE Transactions on Software Engineering","","2003","29","6","542","554","We discuss a method of developing a software bidding model that allows users to visualize the uncertainty involved in pricing decisions and make appropriate bid/no bid decisions. We present a generic bidding model developed using the modeling method. The model elements were identified after a review of bidding research in software and other industries. We describe the method we developed to validate our model and report the main results of our model validation, including the results of applying the model to four bidding scenarios.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205181","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205181","","Costs;Risk management;Uncertainty;Portfolios;Visualization;Computer industry;Software development management;Project management;Computer Society;Pricing","project management;software development management;software cost estimation;risk management","software bidding risk model;uncertainty visualization;pricing decisions;bid/no bid decisions","","33","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Profiling deployed software: assessing strategies and testing opportunities","S. Elbaum; M. Diep","Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA","IEEE Transactions on Software Engineering","","2005","31","4","312","327","An understanding of how software is employed in the field can yield many opportunities for quality improvements. Profiling released software can provide such an understanding. However, profiling released software is difficult due to the potentially large number of deployed sites that must be profiled, the transparency requirements at a user's site, and the remote data collection and deployment management process. Researchers have recently proposed various approaches to tap into the opportunities offered by profiling deployed systems and overcome those challenges. Initial studies have illustrated the application of these approaches and have shown their feasibility. Still, the proposed approaches, and the tradeoffs between overhead, accuracy, and potential benefits for the testing activity have been barely quantified. This paper aims to overcome those limitations. Our analysis of 1,200 user sessions on a 155 KLOC deployed system substantiates the ability of field data to support test suite improvements, assesses the efficiency of profiling techniques for released software, and the effectiveness of testing efforts that leverage profiled field data.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435352","Index Terms- Profiling;instrumentation;software deployment;testing;empirical studies.","Software testing;Software quality;Instruments;System testing;Computer architecture;Resource management;Reliability engineering;Quality assurance;Information resources;Application software","program testing;software quality;program diagnostics","software testing;deployed software profiling;KLOC deployed system;software quality","","44","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Systematic review and aggregation of empirical studies on elicitation techniques","O. Dieste; N. Juristo","Universidad Polit&#x0E9;cnica de Madrid, Boadilla del Monte; Universidad Polit&#x0E9;cnica de Madrid, Boadilla del Monte","IEEE Transactions on Software Engineering","","2011","37","2","283","304","We have located the results of empirical studies on elicitation techniques and aggregated these results to gather empirically grounded evidence. Our chosen surveying methodology was systematic review, whereas we used an adaptation of comparative analysis for aggregation because meta-analysis techniques could not be applied. The review identified 564 publications from the SCOPUS, IEEEXPLORE, and ACM DL databases, as well as Google. We selected and extracted data from 26 of those publications. The selected publications contain 30 empirical studies. These studies were designed to test 43 elicitation techniques and 50 different response variables. We got 100 separate results from the experiments. The aggregation generated 17 pieces of knowledge about the interviewing, laddering, sorting, and protocol analysis elicitation techniques. We provide a set of guidelines based on the gathered pieces of knowledge.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5416730","Elicitation methods;performance measures;experimentation;systematic literature review.","Protocols;Information analysis;Sensitivity analysis;Databases;Data mining;Sorting;Software measurement","knowledge acquisition;software engineering","empirically grounded evidence;elicitation techniques;surveying methodology;systematic review;SCOPUS;IEEEXPLORE;ACM DL databases;Google","","50","","102","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Employee Participation on the Use of an Electronic Process Guide: A Longitudinal Case Study","T. Dingsoyr; N. B. Moe","NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","212","225","Many software companies disseminate process knowledge through electronic process guides. A common problem with such guides is that they are not used. Through a case study, we investigated how participation in creating an electronic process guide, through process workshops, influenced the use of the guide. We studied developer and project manager usage with respect to three factors: frequency of use, used functionality, and reported advantages and disadvantages. We collected data from three rounds of interviews and 19 months of usage logs in a longitudinal study in a medium-size software company. Employees who participated in process workshops showed a higher degree of usage, used a larger number of functions, and expressed more advantages and disadvantages than those not involved. Our study suggests that employee participation has a long-term positive effect on electronic process guide usage.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70767","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4384504","Organizational management and coordination;Human Factors in Software Design;Software Engineering Process;Process infrastructure;Process implementation and change;Software process models;Software Quality/SQA;Organizational management and coordination;Human Factors in Software Design;Software Engineering Process;Process infrastructure;Process implementation and change;Software process models;Software Quality/SQA","Software quality;Programming;Computer Society;Conferences;Project management;Frequency;Design engineering;Quality management;Engineering management;Human factors","human factors;personnel;software development management","employee participation;electronic process guide;software engineering process","","9","","48","","","","","","IEEE","IEEE Journals & Magazines"
"On the Composability of Design Patterns","H. Zhu; I. Bayley","Oxford Brookes University, Oxford, United Kingdom; Oxford Brookes University, Oxford, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","11","1138","1152","In real applications, design patterns are almost always to be found composed with each other. It is crucial that these compositions be validated. This paper examines the notion of validity, and develops a formal method for proving or disproving it, in a context where composition is performed with formally defined operators on formally specified patterns. In particular, for validity, we require that pattern compositions preserve the features, semantics and soundness of the composed patterns. The application of the theory is demonstrated by a formal analysis of overlap-based pattern compositions and a case study of a real pattern-oriented software design.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2445341","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7123660","Design Patterns;Pattern composition;Composibility;Feature preservation;Semantics preservation;Soundness preservation;Formal methods;Design patterns;pattern composition;composibility;feature preservation;semantics preservation;soundness preservation;formal methods","Semantics;Unified modeling language;Context;Software systems;Software design;Cognition","formal specification;object-oriented methods","design pattern composability;formally specified patterns;formal analysis method;overlap-based pattern compositions;pattern-oriented software design","","7","","70","","","","","","IEEE","IEEE Journals & Magazines"
"Keyword Search for Building Service-Based Systems","Q. He; R. Zhou; X. Zhang; Y. Wang; D. Ye; F. Chen; J. C. Grundy; Y. Yang","State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China; Centre for Applied Informatics, Victoria University, Melbourne, Australia; University of Auckland, Auckland, New Zealand; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; School of Engineering and Information Technology, Federation University Australia, Melbourne, Australia; School of Information Technology, Deakin University, Geelong, Victoria, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia","IEEE Transactions on Software Engineering","","2017","43","7","658","674","With the fast growth of applications of service-oriented architecture (SOA) in software engineering, there has been a rapid increase in demand for building service-based systems (SBSs) by composing existing Web services. Finding appropriate component services to compose is a key step in the SBS engineering process. Existing approaches require that system engineers have detailed knowledge of SOA techniques which is often too demanding. To address this issue, we propose Keyword Search for Service-based Systems (KS3), a novel approach that integrates and automates the system planning, service discovery and service selection operations for building SBSs based on keyword search. KS3 assists system engineers without detailed knowledge of SOA techniques in searching for component services to build SBSs by typing a few keywords that represent the tasks of the SBSs with quality constraints and optimisation goals for system quality, e.g., reliability, throughput and cost. KS3 offers a new paradigm for SBS engineering that can significantly save the time and effort during the system engineering process. We conducted large-scale experiments using two real-world Web service datasets to demonstrate the practicality, effectiveness and efficiency of KS3.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2624293","Australian Research Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7731135","Service-based system;keyword search;service composition;web service;quality of service;cloud computing","Service-oriented architecture;Data models;Keyword search;Buildings;Planning;Libraries","service-oriented architecture;Web services","keyword search;service-based system building;service-oriented architecture;software engineering;Web services;SBS engineering process;SOA techniques;KS3","","","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Interaction Models and Automated Control under Partial Observable Environments","D. Ciolek; V. Braberman; N. D’Ippolito; N. Piterman; S. Uchitel","Departamento de Computación, Universidad de Buenos Aires, Argentina; Departamento de Computación, Universidad de Buenos Aires, Argentina; Department of Computing, Imperial College, London, United Kingdom; Department of Computer Science, University of Leicester, Leicester, United Kingdom; Department of Computing, Imperial College, London, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","1","19","33","The problem of automatically constructing a software component such that when executed in a given environment satisfies a goal, is recurrent in software engineering. Controller synthesis is a field which fits into this vision. In this paper we study controller synthesis for partially observable LTS models. We exploit the link between partially observable control and non-determinism and show that, unlike fully observable LTS or Kripke structure control problems, in this setting the existence of a solution depends on the interaction model between the controller-to-be and its environment. We identify two interaction models, namely Interface Automata and Weak Interface Automata, define appropriate control problems and describe synthesis algorithms for each of them.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2564959","ERC; PBM-FIMBSE; ANPCYT PICT; ANPCYT PICT; ANPCYT PICT; UBACYT; UBACYT; CONICET PIP; MEALS; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7466810","LTS;controller synthesis;imperfect-information games","Servers;Maintenance engineering;Automata;Context;Observability;Uncertainty","automata theory;software engineering","interaction models;automated control;software engineering;controller synthesis;partially observable LTS models;partially observable control;weak interface automata;labelled transition systems","","2","","35","","","","","","IEEE","IEEE Journals & Magazines"
"A unified scheme of some Nonhomogenous Poisson process models for software reliability estimation","Chin-Yu Huang; M. R. Lyu; Sy-Yen Kuo","Dept. of Electr. Eng., Nat. Taiwan Univ., Taipei, Taiwan; NA; NA","IEEE Transactions on Software Engineering","","2003","29","3","261","269","In this paper, we describe how several existing software reliability growth models based on Nonhomogeneous Poisson processes (NHPPs) can be comprehensively derived by applying the concept of weighted arithmetic, weighted geometric, or weighted harmonic mean. Furthermore, based on these three weighted means, we thus propose a more general NHPP model from the quasi arithmetic viewpoint. In addition to the above three means, we formulate a more general transformation that includes a parametric family of power transformations. Under this general framework, we verify the existing NHPP models and derive several new NHPP models. We show that these approaches cover a number of well-known models under different conditions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183936","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183936","","Software reliability;Solid modeling;Arithmetic;Software quality;Programming;Reliability engineering;Software measurement;Statistical distributions;Earth Observing System;Power system harmonics","software reliability;stochastic processes","software reliability;nonhomogeneous Poisson processes;weighted arithmetic;weighted geometric;weighted harmonic mean;weighted means;mean value function","","126","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Session Reliability of Web Systems under Heavy-Tailed Workloads: An Approach Based on Design and Analysis of Experiments","N. Janevski; K. Goseva-Popstojanova","West Virginia University, Morgantown; West Virginia University, Morgantown","IEEE Transactions on Software Engineering","","2013","39","8","1157","1178","While workload characterization and performance of web systems have been studied extensively, reliability has received much less attention. In this paper, we propose a framework for session reliability modeling which integrates the user view represented by the session layer and the system view represented by the service layer. A unique characteristic of the session layer is that, in addition to the user navigation patterns, it incorporates the session length in number of requests and allows us to account for heavy-tailed workloads shown to exist in real web systems. The service layer is focused on the request reliability as it is observed at the service provider side. It considers the multifier web server architecture and the way components interact in serving each request. Within this framework, we develop a session reliability model and solve it using simulation. Instead of the traditional one-factor-at-a-time sensitivity analysis, we use statistical design and analysis of experiments, which allow us to identify the factors and interactions that have statistically significant effect on session reliability. Our findings indicate that session reliability, which accounts for the distribution of failed requests within sessions, provides better representation of the user perceived quality than the request-based reliability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.3","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6409359","Reliability;statistical methods;modeling and prediction;simulation;web servers;Internet applications","Software reliability;Availability;Navigation;Web servers;Reliability engineering;Analytical models","design of experiments;Internet;sensitivity analysis;user interfaces","session reliability;Web system;design-of-experiment;analysis-of-experiment;workload characterization;user view;session layer;system view;service layer;user navigation pattern;heavy-tailed workload;request reliability;multifier Web server architecture;sensitivity analysis;user perceived quality;request-based reliability","","2","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Engineering Trustworthy Self-Adaptive Software with Dynamic Assurance Cases","R. Calinescu; D. Weyns; S. Gerasimou; M. U. Iftikhar; I. Habli; T. Kelly","Department of Computer Science, University of York, York, United Kingdom; Department of Computer Science, Katholieke Universiteit Leuven, Leuven, Belgium; Department of Computer Science, University of York, York, United Kingdom; Department of Computer Science, Linnaeus University, Växjö, Sweden; Department of Computer Science, University of York, York, United Kingdom; Department of Computer Science, University of York, York, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","11","1039","1069","Building on concepts drawn from control theory, self-adaptive software handles environmental and internal uncertainties by dynamically adjusting its architecture and parameters in response to events such as workload changes and component failures. Self-adaptive software is increasingly expected to meet strict functional and non-functional requirements in applications from areas as diverse as manufacturing, healthcare and finance. To address this need, we introduce a methodology for the systematic ENgineering of TRUstworthy Self-adaptive sofTware (ENTRUST). ENTRUST uses a combination of (1) design-time and runtime modelling and verification, and (2) industry-adopted assurance processes to develop trustworthy self-adaptive software and assurance cases arguing the suitability of the software for its intended application. To evaluate the effectiveness of our methodology, we present a tool-supported instance of ENTRUST and its use to develop proof-of-concept self-adaptive software for embedded and service-based systems from the oceanic monitoring and e-finance domains, respectively. The experimental results show that ENTRUST can be used to engineer self-adaptive software systems in different application domains and to generate dynamic assurance cases for these systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2738640","Defence Science and Technology Laboratory; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8008800","Self-adaptive software systems;software engineering methodology;assurance evidence;assurance cases","Software systems;Control systems;Runtime;Monitoring;Computer architecture;Adaptive systems","embedded systems;formal verification;software architecture;trusted computing","self-adaptive software handles;nonfunctional requirements;ENTRUST;industry-adopted assurance processes;engineering trustworthy;control theory;engineering of trustworthy self-adaptive software systems;service-based systems;oceanic monitoring;e-finance domains;embedded software;software architecture","","5","","132","","","","","","IEEE","IEEE Journals & Magazines"
"Bayesian Network Models for Web Effort Prediction: A Comparative Study","E. Mendes; N. Mosley","The University of Auckland, Auckland; MetriQ Limited, Auckland","IEEE Transactions on Software Engineering","","2008","34","6","723","737","The objective of this paper is to compare, using a cross-company dataset, several Bayesian network (BN) models for Web effort estimation. Eight BNs were built; four automatically using Hugin and PowerSoft tools with two training sets, each with 130 Web projects from the Tukutuku database; four using a causal graph elicited by a domain expert, with parameters automatically fit using the same training sets used in the automated elicitation (hybrid models). Their accuracy was measured using two validation sets, each containing data on 65 projects, and point estimates. As a benchmark, the BN-based estimates were also compared to estimates obtained using manual stepwise regression (MSWR), case-based reasoning (CBR), mean- and median-based effort models. MSWR presented significantly better predictions than any of the BN models built herein, and in addition was the only technique to provide significantly superior predictions to a median-based effort model. This paper investigated data-driven and hybrid BN models using project data from the Tukutuku database. Our results suggest that the use of simpler models, such as the median effort, can outperform more complex models, such as BNs. In addition, MSWR seemed to be the only effective technique for Web effort estimation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4589218","Process measurement;Cost estimation;Planning;Time estimation;Process measurement;Cost estimation;Planning;Time estimation","Bayesian methods;Predictive models;Databases;Costs;Project management;Software engineering;Uncertainty;Acoustical engineering;Resource management;Programming","Bayes methods;case-based reasoning;graph theory;Internet;project management;regression analysis;software development management","Bayesian network models;Web effort prediction;cross-company dataset;Hugin tool;PowerSoft tool;Tukutuku database;causal graph;automated elicitation;hybrid models;manual stepwise regression;case-based reasoning;mean-based effort models;median-based effort models","","49","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Measuring the maintainability of a communication protocol based on its formal specification","Sun-Jen Huang; R. Lai","Dept. of Inf. Manage., Nat. Taiwan Univ. of Sci. & Technol., Taipei, Taiwan; NA","IEEE Transactions on Software Engineering","","2003","29","4","327","344","It is difficult to measure the maintainability of a software system early in the development life cycle from its requirement descriptions written in a natural language because informal specifications cannot be analyzed. With the uses of formal description techniques (FDTs) in the communication protocol area since the mid-1980s, avenues have been opened up for a system to be analyzed early in the specification phase. Quantitative measures on its maintainability can then be extracted from such a formal specification, so that we can develop easily maintainable communication software systems and further reduce the increasingly high cost of software maintenance. To date, there is hardly any work done on measuring the maintainability of a system early in its specification phase. This paper presents a method for measuring the maintainability of a communication by using maintainability metrics derived from its formal specification written in Estelle. The methodology for building the Estelle maintainability metrics hierarchy is presented. We have also developed an automated tool, called PSAMS, to automate the calculation of the maintainability indices. We also found that there is a significant correlation between the specification metrics proposed and the widely adopted implementation metrics, thus demonstrating that our proposed metrics are a reliable means of measuring the maintainability of a communication protocol early in the specification phase.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1191797","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1191797","","Protocols;Formal specifications;Software maintenance;Software measurement;Software systems;Costs;Phase measurement;Software quality;Software metrics;Natural languages","software maintenance;software metrics;formal specification;protocols;specification languages;software tools","communication protocol maintainability measurement;formal specification;development life cycle;natural language;informal specifications;formal description techniques;FDT;quantitative measures;maintainable communication software systems;software maintenance cost reduction;Estelle maintainability metrics hierarchy;PSAMS;automated tool;communication protocol","","4","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Maturing Software Engineering Knowledge through Classifications: A Case Study on Unit Testing Techniques","S. Vegas; N. Juristo; V. R. Basili","Universidad Politecnica de Madrid, Madrid; Universidad Politecnica de Madrid, Madrid; University of Maryland, College Park and Fraunhofer Center for Experimental Software Engineering, Maryland","IEEE Transactions on Software Engineering","","2009","35","4","551","565","Classification makes a significant contribution to advancing knowledge in both science and engineering. It is a way of investigating the relationships between the objects to be classified and identifies gaps in knowledge. Classification in engineering also has a practical application; it supports object selection. They can help mature software engineering knowledge, as classifications constitute an organized structure of knowledge items. Till date, there have been few attempts at classifying in software engineering. In this research, we examine how useful classifications in software engineering are for advancing knowledge by trying to classify testing techniques. The paper presents a preliminary classification of a set of unit testing techniques. To obtain this classification, we enacted a generic process for developing useful software engineering classifications. The proposed classification has been proven useful for maturing knowledge about testing techniques, and therefore, SE, as it helps to: 1) provide a systematic description of the techniques, 2) understand testing techniques by studying the relationships among techniques (measured in terms of differences and similarities), 3) identify potentially useful techniques that do not yet exist by analyzing gaps in the classification, and 4) support practitioners in testing technique selection by matching technique characteristics to project characteristics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4775907","Classification;software engineering;software testing;test design techniques;testing techniques;unit testing techniques.","Software engineering;Software testing;Diseases;Knowledge engineering;System testing;Chemical elements;Application software;Software design;Chemistry;Civil engineering","program testing;software engineering","software engineering knowledge;unit testing techniques;engineering classification;matching technique characteristic;project characteristic;software testing","","23","","31","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Analysis of the Impact of Software Vulnerability Announcements on Firm Stock Price","R. Telang; S. Wattal","NA; NA","IEEE Transactions on Software Engineering","","2007","33","8","544","557","Security defects in software cost millions of dollars to firms in terms of downtime, disruptions, and confidentiality breaches. However, the economic implications of these defects for software vendors are not well understood. Lack of legal liability and the presence of switching costs and network externalities may protect software vendors from incurring significant costs in the event of a vulnerability announcement, unlike such industries as auto and pharmaceuticals, which have been known to suffer significant loss in market value in the event of a defect announcement. Although research in software economics has studied firms' incentives to improve overall quality, there have not been any studies which show that software vendors have an incentive to invest in building more secure software. The objectives of this paper are twofold. 1) We examine how a software vendor's market value changes when a vulnerability is announced. 2) We examine how firm and vulnerability characteristics mediate the change in the market value of a vendor. We collect data from leading national newspapers and industry sources, such as the Computer Emergency Response Team (CERT), by searching for reports on published software vulnerabilities. We show that vulnerability announcements lead to a negative and significant change in a software vendor's market value. In our sample, on average, a vendor loses around 0.6 percent value in stock price when a vulnerability is reported. We find that a software vendor loses more market share if the market is competitive or if the vendor is small. To provide further insight, we use the information content of the disclosure announcement to classify vulnerabilities into various types. We find that the change in stock price is more negative if the vendor fails to provide a patch at the time of disclosure. Also, more severe flaws have a significantly greater impact. Our analysis provides many interesting implications for software vendors as well as policy makers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70712","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267025","Information security;software vulnerability;quality;event-study;patching;software vendors.","IEEE news;Software quality;Costs;Security;Computer industry;Internet;Law;Legal factors;Protection;Pharmaceuticals","","","","71","","48","","","","","","IEEE","IEEE Journals & Magazines"
"An Analysis and Survey of the Development of Mutation Testing","Y. Jia; M. Harman","University College London, London; University College London, London","IEEE Transactions on Software Engineering","","2011","37","5","649","678","Mutation Testing is a fault-based software testing technique that has been widely studied for over three decades. The literature on Mutation Testing has contributed a set of approaches, tools, developments, and empirical results. This paper provides a comprehensive analysis and survey of Mutation Testing. The paper also presents the results of several development trend analyses. These analyses provide evidence that Mutation Testing techniques and tools are reaching a state of maturity and applicability, while the topic of Mutation Testing itself is the subject of increasing interest.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.62","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5487526","Mutation testing;survey.","Genetic mutations;Software testing;Fault detection;History;Books;Programming profession;Computer languages;Java;Educational institutions;Automata","fault diagnosis;program testing","mutation testing development;fault-based software testing technique;empirical results;comprehensive analysis;development trend analysis;mutation testing technique;mutation testing tool","","423","","264","","","","","","IEEE","IEEE Journals & Magazines"
"POEMS: end-to-end performance design of large parallel adaptive computational systems","V. S. Adve; R. Bagrodia; J. C. Browne; E. Deelman; A. Dube; E. N. Houstis; J. R. Rice; R. Sakellariou; D. J. Sundaram-Stukel; P. J. Teller; M. K. Vernon","Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","11","1027","1048","The POEMS project is creating an environment for end-to-end performance modeling of complex parallel and distributed systems, spanning the domains of application software, runtime and operating system software, and hardware architecture. Toward this end, the POEMS framework supports composition of component models from these different domains into an end-to-end system model. This composition can be specified using a generalized graph model of a parallel system, together with interface specifications that carry information about component behaviors and evaluation methods. The POEMS Specification Language compiler will generate an end-to-end system model automatically from such a specification. The components of the target system may be modeled using different modeling paradigms and at various levels of detail. Therefore, evaluation of a POEMS end-to-end system model may require a variety of evaluation tools including specialized equation solvers, queuing network solvers, and discrete event simulators. A single application representation based on static and dynamic task graphs serves as a common workload representation for all these modeling approaches. Sophisticated parallelizing compiler techniques allow this representation to be generated automatically for a given parallel program. POEMS includes a library of predefined analytical and simulation component models of the different domains and a knowledge base that describes performance properties of widely used algorithms. The paper provides an overview of the POEMS methodology and illustrates several of its key components. The modeling capabilities are demonstrated by predicting the performance of alternative configurations of Sweep3D, a benchmark for evaluating wavefront application technologies and high-performance, parallel architectures.","0098-5589;1939-3520;2326-3881","","10.1109/32.881716","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=881716","","Adaptive systems;Concurrent computing;Application software;Runtime environment;Operating systems;Software performance;Software systems;Hardware;Computer architecture;Specification languages","performance evaluation;parallel programming;parallel architectures;message passing;parallelising compilers;adaptive systems;object-oriented programming;formal specification;software libraries","modeling paradigms;end-to-end performance design;large parallel adaptive computational systems;POEMS project;end-to-end performance modeling;distributed systems;application software;operating system software;hardware architecture;component models;end-to-end system model;generalized graph model;parallel system;interface specifications;component behaviors;evaluation methods;POEMS Specification Language compiler;specialized equation solvers;queuing network solvers;discrete event simulators;application representation;dynamic task graphs;common workload representation;parallelizing compiler techniques;parallel program;simulation component models;knowledge base;performance properties;POEMS methodology;Sweep3D;benchmark;wavefront application technologies;high performance parallel architectures","","40","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Static Fault Localization in Model Transformations","L. Burgueño; J. Troya; M. Wimmer; A. Vallecillo","Dept. Lenguajes y Ciencias de la Computación, Universidad de Málaga, Bulevar Louis Pasteur, 35, Malaga, Spain; Vienna University of Technology, Business Informatics Group, Vienna, Austria; Vienna University of Technology, Business Informatics Group, Vienna, Austria; Dept. Lenguajes y Ciencias de la Computación, Universidad de Málaga, Bulevar Louis Pasteur, 35, Malaga, Spain","IEEE Transactions on Software Engineering","","2015","41","5","490","506","As the complexity of model transformations grows, there is an increasing need to count on methods, mechanisms, and tools for checking their correctness, i.e., the alignment between specifications and implementations. In this paper we present a light-weight and static approach for locating the faulty rules in model transformations, based on matching functions that automatically establish these alignments using the metamodel footprints, i.e., the metamodel elements used. The approach is implemented for the combination of Tracts and ATL, both residing in the Eclipse Modeling Framework, and is supported by the corresponding toolkit. An evaluation discussing the accuracy and the limitations of the approach is also provided. Furthermore, we identify the kinds of transformations which are most suitable for validation with the proposed approach and use mutation techniques to evaluate its effectiveness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2375201","Spanish Project; Austrian Research Promotion Agency; EC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6967841","Model transformation;transformation testing;model alignment;Model transformation;transformation testing;model alignment","Contracts;Context;Context modeling;Navigation;Testing;Complexity theory;Analytical models","fault tolerant computing;Unified Modeling Language","static fault localization;model transformations;faulty rules;matching functions;metamodel footprints;metamodel elements;Eclipse modeling framework;mutation techniques","","18","","67","","","","","","IEEE","IEEE Journals & Magazines"
"Toward a reference process for developing wireless Internet services","A. Ocampo; D. Boggio; J. Munch; G. Palladino","Fraunhofer Inst. for Exp. Software Eng., Kaiserslautern, Germany; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","12","1122","1134","Wireless Internet services such as mobile Web applications promise an enormous market potential. The field is characterized by extreme time-to-market pressure and insufficient knowledge about development procedures and technical constraints. This results in insufficient guidance for project managers and software developers on selecting appropriate development processes, techniques, methods, and tools. In addition, there is an enormous lack of knowledge about the effects (such as effort consumption, defect injection) of such technologies that hinders the transfer of innovative technologies into practice. This article describes an initial reference process by summarizing essential technologies for the development of wireless Internet services and experience with these technologies on the levels of life cycle processes, engineering processes, and managerial processes. The reference process is based on a comprehensive literature survey and the execution of development projects for wireless Internet services. The goal of the article is to provide domain-specific guidance for project managers and software developers with accompanying lessons learned from the past.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265526","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265526","","Web and internet services;Application software;Project management;Software development management;Time to market;Technology management;Engineering management;Space technology;Software engineering;Programming","Internet;project management;software management;mobile computing;user centred design;telecommunication services;mobile radio","reference process model;wireless Internet services;mobile Web applications;extreme time-to-market pressure;project managers;software developers;essential technologies;domain-specific guidance;commerce;software process modeling;process elicitation","","9","","56","","","","","","IEEE","IEEE Journals & Magazines"
"A Machine Learning Approach to Software Requirements Prioritization","A. Perini; A. Susi; P. Avesani","Fondazione Bruno Kessler. CIT - IRST, Trento; Fondazione Bruno Kessler. CIT - IRST, Trento; Fondazione Bruno Kessler. CIT - IRST, Trento","IEEE Transactions on Software Engineering","","2013","39","4","445","461","Deciding which, among a set of requirements, are to be considered first and in which order is a strategic process in software development. This task is commonly referred to as requirements prioritization. This paper describes a requirements prioritization method called Case-Based Ranking (CBRank), which combines project's stakeholders preferences with requirements ordering approximations computed through machine learning techniques, bringing promising advantages. First, the human effort to input preference information can be reduced, while preserving the accuracy of the final ranking estimates. Second, domain knowledge encoded as partial order relations defined over the requirement attributes can be exploited, thus supporting an adaptive elicitation process. The techniques CBRank rests on and the associated prioritization process are detailed. Empirical evaluations of properties of CBRank are performed on simulated data and compared with a state-of-the-art prioritization method, providing evidence of the method ability to support the management of the tradeoff between elicitation effort and ranking accuracy and to exploit domain knowledge. A case study on a real software project complements these experimental measurements. Finally, a positioning of CBRank with respect to state-of-the-art requirements prioritization methods is proposed, together with a discussion of benefits and limits of the method.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6249686","Requirements management;requirements prioritization;machine learning","Approximation methods;Accuracy;Software;Humans;Data models;Boosting","formal specification;learning (artificial intelligence);project management;software development management","machine learning approach;software requirements prioritization;strategic process;software development;case-based ranking;CBRank;project stakeholder preference;requirements ordering approximations;domain knowledge;partial order relations;requirement attributes;adaptive elicitation process;elicitation effort;ranking accuracy;software project","","44","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Carving and Replaying Differential Unit Test Cases from System Test Cases","S. Elbaum; H. N. Chin; M. B. Dwyer; M. Jorde","University of Nebraska, Lincoln; University of Nebraska, Lincoln; University of Nebraska, Lincoln; University of Nebraska, Lincoln","IEEE Transactions on Software Engineering","","2009","35","1","29","45","Unit test cases are focused and efficient. System tests are effective at exercising complex usage patterns. Differential unit tests (DUT) are a hybrid of unit and system tests that exploits their strengths. They are generated by carving the system components, while executing a system test case, that influence the behavior of the target unit, and then re-assembling those components so that the unit can be exercised as it was by the system test. In this paper we show that DUTs retain some of the advantages of unit tests, can be automatically generated, and have the potential for revealing faults related to intricate system executions. We present a framework for carving and replaying DUTs that accounts for a wide variety of strategies and tradeoffs, we implement an automated instance of the framework with several techniques to mitigate test cost and enhance flexibility and robustness, and we empirically assess the efficacy of carving and replaying DUTs on three software artifacts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.103","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4711061","Testing strategies;Test execution;Test design;Testing strategies;Test execution;Test design","System testing;Software testing;Automatic testing;Costs;Robustness;Packaging;Debugging;Systems engineering and theory;Software systems;Humans","automatic test software;program testing","differential unit test cases;system test cases;complex usage patterns;system components;software artifacts","","19","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Measuring Code Quality to Improve Specification Mining","C. Le Goues; W. Weimer","University of Virginia, Charlottesville; University of Virginia, Charlottesville","IEEE Transactions on Software Engineering","","2012","38","1","175","190","Formal specifications can help with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. However, they are difficult to write manually, and automatic mining techniques suffer from 90-99 percent false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process and using information from code that is more likely to be correct, as well as code that is less likely to be correct. When used as a preprocessing step for an existing specification miner, our technique identifies which input is most indicative of correct program behavior, which allows off-the-shelf techniques to learn the same number of specifications using only 45 percent of their original input. As a novel inference technique, our approach has few false positives in practice (63 percent when balancing precision and recall, 3 percent when focused on precision), while still finding useful specifications (e.g., those that find many bugs) on over 1.5 million lines of code.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.5","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680914","Specification mining;machine learning;software engineering;code metrics;program understanding.","Software measurement;Refactoring;Data mining;Maintenance engineering;Cloning;Optimization","data mining;formal specification;program debugging;program testing;software quality","specification mining;formal specifications;program testing;optimization;refactoring;documentation;debugging;repair;automatic mining techniques;temporal-property miner;code quality metrics;software engineering process","","11","","61","","","","","","IEEE","IEEE Journals & Magazines"
"Automatically checking an implementation against its formal specification","S. Antoy; D. Hamlet","Dept. of Comput. Sci., Portland State Univ., OR, USA; NA","IEEE Transactions on Software Engineering","","2000","26","1","55","69","We propose checking the execution of an abstract data type's imperative implementation against its algebraic specification. An explicit mapping from implementation states to abstract values is added to the imperative code. The form of specification allows mechanical checking of desirable properties such as consistency and completeness, particularly when operations are added incrementally to the data type. During unit testing, the specification serves as a test oracle. Any variance between computed and specified values is automatically detected. When the module is made part of some application, the checking can he removed, or may remain in place for further validating the implementation. The specification, executed by rewriting, can be thought of as itself an implementation with maximum design diversity, and the validation as a form of multiversion-programming comparison.","0098-5589;1939-3520;2326-3881","","10.1109/32.825766","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=825766","","Formal specifications;Application software;Java;Mechanical factors;Software testing;Computer languages;Software engineering;Software maintenance;Equations;Software prototyping","algebraic specification;program verification;abstract data types;program testing;object-oriented programming","implementation checking;formal specification;abstract data type;imperative implementation;algebraic specification;imperative code;unit testing;rewriting;multiversion programming;object oriented program testing","","45","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Aspectual Feature Modules","S. Apel; T. Leich; G. Saake","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","162","180","Two programming paradigms are gaining attention in the overlapping fields of software product lines (SPLs) and incremental software development (ISD). Feature-oriented programming (FOP) aims at large-scale compositional programming and feature modularity in SPLs using ISD. Aspect-oriented programming (AOP) focuses on the modularization of crosscutting concerns in complex software. Although feature modules, the main abstraction mechanisms of FOP, perform well in implementing large-scale software building blocks, they are incapable of modularizing certain kinds of crosscutting concerns. This weakness is exactly the strength of aspects, the main abstraction mechanisms of AOP. We contribute a systematic evaluation and comparison of FOP and AOP. It reveals that aspects and feature modules are complementary techniques. Consequently, we propose the symbiosis of FOP and AOP and aspectual feature modules (AFMs), a programming technique that integrates feature modules and aspects. We provide a set of tools that support implementing AFMs on top of Java and C++. We apply AFMs to a nontrivial case study demonstrating their practical applicability and to justify our design choices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70770","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4407729","Design Tools and Techniques;Design;Language Constructs and Features;Design Tools and Techniques;Design;Language Constructs and Features","Programming;Large-scale systems;Collaborative software;Computer languages;Computer Society;Software performance;Symbiosis;Java;Software design;Software engineering","object-oriented programming;software reusability","software product line;incremental software development;feature-oriented programming;large-scale compositional programming;aspect-oriented programming;crosscutting concern;large-scale software building block;systematic evaluation;aspectual feature module;Java;C++","","57","","120","","","","","","IEEE","IEEE Journals & Magazines"
"CACheck: Detecting and Repairing Cell Arrays in Spreadsheets","W. Dou; C. Xu; S. C. Cheung; J. Wei","State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Kowloon, Hong Kong, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Software Engineering","","2017","43","3","226","251","Spreadsheets are widely used by end users for numerical computation in their business. Spreadsheet cells whose computation is subject to the same semantics are often clustered in a row or column as a cell array. When a spreadsheet evolves, the cells in a cell array can degenerate due to ad hoc modifications. Such degenerated cell arrays no longer keep cells prescribing the same computational semantics, and are said to exhibit ambiguous computation smells. We propose CACheck, a novel technique that automatically detects and repairs smelly cell arrays by recovering their intended computational semantics. Our empirical study on the EUSES and Enron corpora finds that such smelly cell arrays are common. Our study also suggests that CACheck is useful for detecting and repairing real spreadsheet problems caused by smelly cell arrays. Compared with our previous work AmCheck, CACheck detects smelly cell arrays with higher precision and recall rate.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2584059","Beijing Natural Science Foundation; National Key Research and Development Plan; Research Grants Council; General Research Fund; National Natural Science Foundation; Collaborative Innovation Center of Novel Software Technology and Industrialization of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7498607","Spreadsheet;cell array;ambiguous computation smell","Semantics;Maintenance engineering;Software;Computer science;Nonhomogeneous media;Electronic mail;Business","software engineering;spreadsheet programs","spreadsheets;CACheck;numerical computation;ad hoc modifications;EUSES corpora;Enron corpora;smelly cell arrays","","6","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Using Declarative Specification to Improve the Understanding, Extensibility, and Comparison of Model-Inference Algorithms","I. Beschastnikh; Y. Brun; J. Abrahamson; M. D. Ernst; A. Krishnamurthy","Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; School of Computer Science, University of Massachusetts, Amherst, MA; Facebook Inc., Seattle, WA; Computer Science &amp; Engineering, University of Washington, Seattle, WA; Computer Science &amp; Engineering, University of Washington, Seattle, WA","IEEE Transactions on Software Engineering","","2015","41","4","408","428","It is a staple development practice to log system behavior. Numerous powerful model-inference algorithms have been proposed to aid developers in log analysis and system understanding. Unfortunately, existing algorithms are typically declared procedurally, making them difficult to understand, extend, and compare. This paper presents InvariMint, an approach to specify model-inference algorithms declaratively. We applied the InvariMint declarative approach to two model-inference algorithms. The evaluation results illustrate that InvariMint (1) leads to new fundamental insights and better understanding of existing algorithms, (2) simplifies creation of new algorithms, including hybrids that combine or extend existing algorithms, and (3) makes it easy to compare and contrast previously published algorithms. InvariMint's declarative approach can outperform procedural implementations. For example, on a log of 50,000 events, InvariMint's declarative implementation of the kTails algorithm completes in 12 seconds, while a procedural implementation completes in 18 minutes. We also found that InvariMint's declarative version of the Synoptic algorithm can be over 170 times faster than the procedural implementation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2369047","NSERC; Google; Microsoft Research via a SEIF; DARPA; NSF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6951474","Model inference;API mining;specification mining;process mining;declarative specification;inference understanding;inference extensibility;inference comparison;InvariMint;kTails;synoptic;Model inference;API mining;specification mining;process mining;declarative specification;inference understanding;inference extensibility;inference comparison;InvariMint;kTails;synoptic","Inference algorithms;Postal services;Electronic mail;Algorithm design and analysis;Software algorithms;Educational institutions;Approximation algorithms","formal specification;inference mechanisms;system monitoring","log system behavior analysis;model inference algorithm specification;system understanding;InvariMint declarative specification approach;kTails algorithm;synoptic algorithm","","12","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Scalable Differential Analysis of Process Algebra Models","M. Tribastone; S. Gilmore; J. Hillston","The University of Edinburgh, Edinburgh; The University of Edinburgh, Edinburgh; The University of Edinburgh, Edinburgh","IEEE Transactions on Software Engineering","","2012","38","1","205","219","The exact performance analysis of large-scale software systems with discrete-state approaches is difficult because of the well-known problem of state-space explosion. This paper considers this problem with regard to the stochastic process algebra PEPA, presenting a deterministic approximation to the underlying Markov chain model based on ordinary differential equations. The accuracy of the approximation is assessed by means of a substantial case study of a distributed multithreaded application.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.82","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5567115","Modeling and prediction;ordinary differential equations;Markov processes.","Mathematical model;Semantics;Computational modeling;Approximation methods;Numerical models;Stochastic processes;Markov methods","differential equations;Markov processes;multi-threading;process algebra;software engineering","scalable differential analysis;large-scale software systems;discrete-state approach;stochastic process algebra;PEPA;Markov chain model;ordinary differential equations;distributed multithreaded application","","44","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Models of parallel applications with large computation and I/O requirements","E. Rosti; G. Serazzi; E. Smirni; M. S. Squillante","Dipt. di Sci. dell'Informazione, Milan Univ., Italy; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","3","286","307","A fundamental understanding of the interplay between computation and I/O activities in parallel applications that manipulate huge amounts of data is critical to achieving good application performance, as well as correctly characterizing the workloads of large-scale high-performance parallel systems. We present a formal model of the behavior of CPU and I/O interactions in scientific applications, from which we derive various formulas that characterize application performance. Our model captures the I/O and CPU activity at different levels of granularity, where results from the model are shown to be in excellent agreement with measurement data from a set of I/O-intensive applications. Using the formulas from our model, which explicitly take I/O activity into account, we also present examples of possible applications of the model.","0098-5589;1939-3520;2326-3881","","10.1109/32.991321","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=991321","","Computer applications;Concurrent computing","parallel programming;software performance evaluation;input-output programs","parallel programs;workload characterization;performance modeling;application performance;CPU;scientific applications;granularity;computation requirements;input output requirements","","23","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic detection and diagnosis of faults in generated code for procedure calls","M. W. Bailey; J. W. Davidson","Dept. of Comput. Sci., Hamilton Coll., Clinton, NY, USA; NA","IEEE Transactions on Software Engineering","","2003","29","11","1031","1042","In this paper, we present a compiler testing technique that closes the gap between existing compiler implementations and correct compilers. Using formal specifications of procedure-calling conventions, we have built a target-sensitive test suite generator that builds test cases for a specific aspect of compiler code generators: the procedure-calling sequence generator. By exercising compilers with these specification-derived target-specific test suites, our automated testing tool has exposed bugs in every compiler tested on the MIPS and one compiler on the SPARC. These compilers include some that have been in heavy use for many years. Once a fault has been detected, the system can often suggest the nature of the problem. The testing system is an invaluable tool for detecting, isolating, and correcting faults in today's compilers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245304","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245304","","Fault detection;Fault diagnosis;Automatic testing;Automata;Computer bugs;Robustness;Program processors;Computer languages;Computer science;Educational institutions","program compilers;program debugging;program testing;compiler generators","procedure calls;compiler testing;compiler implementations;formal specifications;target-sensitive test suite generator;procedure-calling sequence generator;SPARC;compiler debugging","","2","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Pair Programming and Software Defects--A Large, Industrial Case Study","E. di Bella; I. Fronza; N. Phaphoom; A. Sillitti; G. Succi; J. Vlasenko","University of Genova, Genova; Free University of Bolzano - Bozen, Bolzano; Free University of Bolzano - Bozen, Bolzano; Free University of Bolzano - Bozen, Bolzano; Free University of Bozen - Bolzano, Bolzano; Free University of Bolzano - Bozen, Bolzano","IEEE Transactions on Software Engineering","","2013","39","7","930","953","In the last decade, there has been increasing interest in pair programming (PP). However, despite the existing work, there is still a lack of substantial evidence of the effects of PP in industrial environments. To address this issue, we have analyzed the work of a team of 17 industrial developers for 14 months. The team is part of the IT department of a large Italian manufacturing company; it adopts a customized version of extreme programming (XP). We have investigated the effects of PP on software quality in five different scenarios. The results show that PP appears to provide a perceivable but small effect on the reduction of defects in these settings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6331491","Pair programming;software defects;case study","Programming;Software","configuration management;manufacturing industries;production engineering computing;software fault tolerance;software prototyping;software quality;team working","pair programming;software defects;industrial case study;IT department;large Italian manufacturing company;extreme programming;customized version;software quality","","32","","94","","","","","","IEEE","IEEE Journals & Magazines"
"Bounding cache-related preemption delay for real-time systems","Chang-Gun Lee; Kwangpo Lee; Joosun Hahn; Yang-Min Seo; Sang Lyul Min; Rhan Ha; Seongsoo Hong; Chang Yun Park; Minsuk Lee; Chong Sang Kim","Dept. of Comput. Sci., Illinois Univ., Urbana, IL, USA; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","9","805","826","Cache memory is used in almost all computer systems today to bridge the ever increasing speed gap between the processor and main memory. However, its use in multitasking computer systems introduces additional preemption delay due to the reloading of memory blocks that are replaced during preemption. This cache-related preemption delay poses a serious problem in realtime computing systems where predictability is of utmost importance. We propose an enhanced technique for analyzing and thus bounding the cache-related preemption delay in fixed-priority preemptive scheduling focusing on instruction caching. The proposed technique improves upon previous techniques in two important ways. First, the technique takes into account the relationship between a preempted task and the set of tasks that execute during the preemption when calculating the cache-related preemption delay. Second, the technique considers the phasing of tasks to eliminate many infeasible task interactions. These two features are expressed as constraints of a linear programming problem whose solution gives a guaranteed upper bound on the cache-related preemption delay. This paper also compares the proposed technique with previous techniques using randomly generated task sets. The results show that the improvement on the worst-case response time prediction by the proposed technique over previous techniques ranges between 5 percent and 18 percent depending on the cache refill time when the task set utilization is 0.6. The results also show that as the cache refill time increases, the improvement increases, which indicates that accurate prediction of cache-related preemption delay by the proposed technique becomes increasingly important if the current trend of widening speed gap between the processor and main memory continues.","0098-5589;1939-3520;2326-3881","","10.1109/32.950317","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=950317","","Delay systems;Real time systems;Cache memory;Bridges;Multitasking;Added delay;Processor scheduling;Linear programming;Upper bound;Delay effects","scheduling;linear programming;cache storage;real-time systems;multiprogramming","cache-related preemption delay;real-time systems;cache memory;multitasking computer systems;realtime computing systems;fixed-priority preemptive scheduling;instruction caching;linear programming problem;worst-case response time prediction;cache refill time;schedulability analysis","","40","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Ranking significance of software components based on use relations","K. Inoue; R. Yokomori; T. Yamamoto; M. Matsushita; S. Kusumoto","Dept. of Comput. Sci., Osaka Univ., Japan; Dept. of Comput. Sci., Osaka Univ., Japan; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","3","213","225","Collections of already developed programs are important resources for efficient development of reliable software systems. In this paper, we propose a novel graph-representation model of a software component library (repository), called component rank model. This is based on analyzing actual usage relations of the components and propagating the significance through the usage relations. Using the component rank model, we have developed a Java class retrieval system named SPARS-J and applied SPARS-J to various collections of Java files. The result shows that SPARS-J gives a higher rank to components that are used more frequently. As a result, software engineers looking for a component have a better chance of finding it quickly. SPARS-J has been used by two companies, and has produced promising results.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423993","Index Terms- Component rank;graph representation model;reuse models;program analysis;reusable libraries.","Software libraries;Java;Software quality;Internet;Application software;Companies;Software systems;Programming;Information retrieval;Information analysis","Java;program diagnostics;software reusability;software libraries","graph-representation model;software component library;component rank model;Java class retrieval system;software reusability;program analysis","","71","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Discovering Architectures from Running Systems","B. Schmerl; J. Aldrich; D. Garlan; R. Kazman; Hong Yan","NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","7","454","466","One of the challenging problems for software developers is guaranteeing that a system as built is consistent with its architectural design. In this paper, we describe a technique that uses runtime observations about an executing system to construct an architectural view of the system. In this technique, we develop mappings that exploit regularities in system implementation and architectural style. These mappings describe how low-level system events can be interpreted as more abstract architectural operations and are formally defined using colored Petri nets. In this paper, we describe a system, called DiscoTect, that uses these mappings and we introduce the DiscoSTEP mapping language and its formal definition. Two case studies showing the application of DiscoTect suggest that the tool is practical to apply to legacy systems and can dynamically verify conformance to a preexisting architectural specification","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677532","Software architecture discovery;reverse engineering;architecture design tools and analyses.","Runtime;Computer architecture;Software architecture;Monitoring;Petri nets;Application software;Reverse engineering;Software systems;Connectors;Databases","formal specification;formal verification;Petri nets;reverse engineering;software architecture;software maintenance","software architecture discovery;reverse engineering;architecture design tool;runtime observation;formal specification;colored Petri net;legacy system;formal verification","","50","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Generating Complete Controllable Test Suites for Distributed Testing","R. M. Hierons","Department of Computer Science, Brunel University, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","3","279","293","A test suite is m-complete for finite state machine (FSM) M if it distinguishes between M and all faulty FSMs with m states or fewer. While there are several algorithms that generate m-complete test suites, they cannot be directly used in distributed testing since there can be additional controllability and observability problems. Indeed, previous results show that there is no general method for generating an m-complete test suite for distributed testing and so the focus has been on conditions under which this is possible. This paper takes a different approach, which is to generate what we call cm-complete test suites: controllable test suites that distinguish an FSM N with no more than m states from M if this is possible in controllable testing. Thus, under the hypothesis that the system under test has no more than m states, a cm-complete test suite achieves as much as is possible given the restriction that testing should be controllable. We show how the problem of generating a cm-complete test suite can be mapped to the problem of generating an m-complete test suite for a partial FSM. Thus, standard test suite generation methods can be adapted for use in distributed testing.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2364035","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6930767","Software engineering/software/program verification;software engineering/testing and debugging;systems and software;distributed testing;test suite generation;checking experiment;Software engineering/software/program verification;software engineering/testing and debugging;systems and software;distributed testing;test suite generation;checking experiment","Testing;Controllability;Ports (Computers);Protocols;Automata;Computer architecture;Observability","distributed processing;finite state machines;program debugging;program testing","complete controllable test suite generation;distributed testing;m-complete test suite;finite state machine;faulty FSM;controllability problem;observability problem;c<sub>m</sub>-complete test suites;controllable testing;system under test;partial-FSM;standard test suite generation methods","","7","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Reliability and validity in comparative studies of software prediction models","I. Myrtveit; E. Stensrud; M. Shepperd","Norwegian Sch. of Manage. BI, Sandvika, Norway; NA; NA","IEEE Transactions on Software Engineering","","2005","31","5","380","391","Empirical studies on software prediction models do not converge with respect to the question ""which prediction model is best?"" The reason for this lack of convergence is poorly understood. In this simulation study, we have examined a frequently used research procedure comprising three main ingredients: a single data sample, an accuracy indicator, and cross validation. Typically, these empirical studies compare a machine learning model with a regression model. In our study, we use simulation and compare a machine learning and a regression model. The results suggest that it is the research procedure itself that is unreliable. This lack of reliability may strongly contribute to the lack of convergence. Our findings thus cast some doubt on the conclusions of any study of competing software prediction models that used this research procedure as a basis of model comparison. Thus, we need to develop more reliable research procedures before we can have confidence in the conclusions of comparative studies of software prediction models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.58","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1438374","Index Terms- Software metrics;cost estimation;cross-validation;empirical methods;arbitrary function approximators;machine learning;estimation by analogy;regression analysis;simulation;reliability;validity;accuracy indicators.","Predictive models;Machine learning;Convergence;Mathematical model;Regression analysis;Artificial neural networks;Cost function;Analytical models;Programming;Maximum likelihood estimation","software reliability;software cost estimation;software metrics;program verification;convergence;regression analysis;function approximation;learning (artificial intelligence)","software reliability;software validity;software prediction model;convergence;data sample;accuracy indicator;cross validation;machine learning model;regression model;simulation;software metrics;cost estimation;empirical method;arbitrary function approximators;analogy estimation","","116","","44","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis and visualization of predicate dependence on formal parameters and global variables","D. Binkley; M. Harman","Loyola Coll., Baltimore, MD, USA; NA","IEEE Transactions on Software Engineering","","2004","30","11","715","735","Empirical data concerning the qualitative and quantitative nature of program dependence is presented for a set of 20 programs ranging from 600 lines of code to 167,000 lines of code. The sources of dependence considered are global variables and formal parameters and the targets considered are a program's predicate nodes. The results show that as the number of formal parameters available to a predicate increases, there is a decrease in the proportion of these formal parameters which are depended upon by the predicate. No such correlation was found for global variables. Results from theoretical and actual computation time analysis indicate that the computation of dependence information is practical, suggesting that the analysis may be beneficial to several application areas. The paper also presents results concerning correlations that provide strong evidence that the global and formal dependence sources are independent of one another and that the numbers of globals and formals are independent of the size of the procedure that contains them. Finally, two visualization techniques for displaying dependence information are introduced. Illustrations show how these visualizations and predicate dependence analysis can assist in activities such as testing, comprehension, and evolution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.78","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359767","Index Terms- Dependence analysis;program slicing;program comprehension;software maintenance.","Humans;Information analysis;Independent component analysis;Data visualization;Software maintenance;Software engineering;Application software;Automatic testing;Logic;Software testing","program testing;program slicing;software maintenance;formal specification;program visualisation","predicate dependence visualization;formal parameter;global variable;program dependence;program predicate node","","19","","79","","","","","","IEEE","IEEE Journals & Magazines"
"The impact of UML documentation on software maintenance: an experimental evaluation","E. Arisholm; L. C. Briand; S. E. Hove; Y. Labiche","Dept. of Software Eng., Simula Res. Lab., Norway; Dept. of Software Eng., Simula Res. Lab., Norway; Dept. of Software Eng., Simula Res. Lab., Norway; NA","IEEE Transactions on Software Engineering","","2006","32","6","365","381","The Unified Modeling Language (UML) is becoming the de facto standard for software analysis and design modeling. However, there is still significant resistance to model-driven development in many software organizations because it is perceived to be expensive and not necessarily cost-effective. Hence, it is important to investigate the benefits obtained from modeling. As a first step in this direction, this paper reports on controlled experiments, spanning two locations, that investigate the impact of UML documentation on software maintenance. Results show that, for complex tasks and past a certain learning curve, the availability of UML documentation may result in significant improvements in the functional correctness of changes as well as the quality of their design. However, there does not seem to be any saving of time. For simpler tasks, the time needed to update the UML documentation may be substantial compared with the potential benefits, thus motivating the need for UML tools with better support for software maintenance","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650213","Maintenance;UML;experiment.","Unified modeling language;Documentation;Software maintenance;Object oriented modeling;Programming;Software standards;Software design;Software systems;Costs;Design engineering","software maintenance;system documentation;Unified Modeling Language","UML documentation;software maintenance;Unified Modeling Language;software analysis;design modeling;model-driven development;software organization","","71","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Tool-Supported Refactoring of Existing Object-Oriented Code into Aspects","D. Binkley; M. Ceccato; M. Harman; F. Ricca; P. Tonella","Loyola College in Maryland, 4501 North Charles Street, Baltimore, MD 21210-2699; ITC-irst, Via Sommarive 18, 38050 Povo (Trento), Italy; King’s College London, Strand, London, WC2R 2LS, UK; ITC-irst, Via Sommarive 18, 38050 Povo (Trento), Italy; ITC-irst, Via Sommarive 18, 38050 Povo (Trento), Italy","IEEE Transactions on Software Engineering","","2006","32","9","698","717","Aspect-oriented programming (AOP) provides mechanisms for the separation of crosscutting concerns - functionalities scattered through the system and tangled with the base code. Existing systems are a natural testbed for the AOP approach since they often contain several crosscutting concerns which could not be modularized using traditional programming constructs. This paper presents an automated approach to the problem of migrating systems developed according to the object-oriented programming (OOP) paradigm into aspect-oriented programming (AOP). A simple set of six refactorings has been defined to transform OOP to AOP and has been implemented in the AOP-migrator tool, an Eclipse plug-in. A set of enabling transformations from OOP to OOP complement the initial set of refactorings. The paper presents the results of four case studies, which use the approach to migrate selected crosscutting concerns from medium-sized Java programs (in the range of 10K to 40K lines of code) into equivalent programs in AspectJ. The case study results show the feasibility of the migration and indicate the importance of the enabling transformations as a preprocessing step","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.95","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707668","Aspect-oriented software development;refactoring;program transformation.","Object oriented programming;Scattering;System testing;Automatic programming;Java;Contracts;Software engineering;Automatic control;Costs;Humans","Java;object-oriented programming;program compilers;software quality;software tools","aspect-oriented programming;crosscutting concern;migrating system;object-oriented programming;Java program;AspectJ;tool-supported refactoring;program transformation","","39","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Reporting Usability Defects: A Systematic Literature Review","N. S. M. Yusop; J. Grundy; R. Vasa","Faculty of Computer and Mathematical Science, Universiti Teknologi MARA, Malaysia; Faculty of Science, Engineering and Built Environment, Deakin University, AUD, Australia; Faculty of Science, Engineering and Built Environment, Deakin University, AUD, Australia","IEEE Transactions on Software Engineering","","2017","43","9","848","867","Usability defects can be found either by formal usability evaluation methods or indirectly during system testing or usage. No matter how they are discovered, these defects must be tracked and reported. However, empirical studies indicate that usability defects are often not clearly and fully described. This study aims to identify the state of the art in reporting of usability defects in the software engineering and usability engineering literature. We conducted a systematic literature review of usability defect reporting drawing from both the usability and software engineering literature from January 2000 until March 2016. As a result, a total of 57 studies were identified, in which we classified the studies into three categories: reporting usability defect information, analysing usability defect data and key challenges. Out of these, 20 were software engineering studies and 37 were usability studies. The results of this systematic literature review show that usability defect reporting processes suffer from a number of limitations, including: mixed data, inconsistency of terms and values of usability defect data, and insufficient attributes to classify usability defects. We make a number of recommendations to improve usability defect reporting and management in software engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2638427","Ministry of Higher Education Malaysia, Universiti Teknologi MARA (UiTM); ARC Discovery projects scheme, the Deakin Software Technology Innovation Lab; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7779159","Systematic review;test management;user interface;usability testing;usability defect reporting","Usability;Systematics;Software engineering;Testing;Human computer interaction;Bibliographies","formal verification;pattern classification;product design;program testing","formal usability evaluation;system testing;software engineering;usability defect reporting processes;mixed data;usability defect data;usability defect classification","","3","","87","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Contract-checking wrappers for C++ classes","S. H. Edwards; M. Sitaraman; B. W. Weide; E. Hollingsworth","Comput. Sci. Dept., Virginia Tech., Blacksburg, VA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","11","794","810","Two kinds of interface contract violations can occur in component-based software: A client component can fail to satisfy a requirement of a component it is using, or a component implementation can fail to fulfill its obligations to the client. The traditional approach to detecting and reporting such violations is to embed assertion checks into component source code, with compile-time control over whether they are enabled. This works well for the original component developers, but it fails to meet the needs of component clients who do not have access to source code for such components. A wrapper-based approach, in which contract checking is not hard-coded into the underlying component but is ""layered"" on top of it, offers several relative advantages. It is practical and effective for C++ classes. Checking code can be distributed in binary form along with the underlying component, it can be installed or removed without requiring recompilation of either the underlying component or the client code, it can be selectively enabled or disabled by the component client on a per-component basis, and it does not require the client to have access to any special tools (which might have been used by the component developer) to support wrapper installation and control. Experimental evidence indicates that wrappers in C++ impose-modest additional overhead compared to inlining assertion checks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.80","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359771","Index Terms- Assertion checkers;binary components;design by contract;preconditions;postconditions;class invariants;coding techniques;debugging aids;specification.","Contracts;Runtime;Debugging;Testing;Computer Society;Software engineering;Modular construction;Software systems;Assembly systems;Investments","C++ language;object-oriented programming;data encapsulation;program debugging;formal specification;formal verification","contract-checking wrapper;C++ class;component-based software;component source code;wrapper-based approach;assertion checker;binary component;design by contract;class invariant","","11","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Toward a Formalism for Conservative Claims about the Dependability of Software-Based Systems","P. Bishop; R. Bloomfield; B. Littlewood; A. Povyakalo; D. Wright","City University, London and Adelard LLP, London; City University, London and Adelard LLP, London; City University, London; City University, London; City University, London","IEEE Transactions on Software Engineering","","2011","37","5","708","717","In recent work, we have argued for a formal treatment of confidence about the claims made in dependability cases for software-based systems. The key idea underlying this work is ""the inevitability of uncertainty"": It is rarely possible to assert that a claim about safety or reliability is true with certainty. Much of this uncertainty is epistemic in nature, so it seems inevitable that expert judgment will continue to play an important role in dependability cases. Here, we consider a simple case where an expert makes a claim about the probability of failure on demand (pfd) of a subsystem of a wider system and is able to express his confidence about that claim probabilistically. An important, but difficult, problem then is how such subsystem (claim, confidence) pairs can be propagated through a dependability case for a wider system, of which the subsystems are components. An informal way forward is to justify, at high confidence, a strong claim, and then, conservatively, only claim something much weaker: ""I'm 99 percent confident that the pfd is less than 10<sup>-5</sup>, so it's reasonable to be 100 percent confident that it is less than 10<sup>-3</sup>."" These conservative pfds of subsystems can then be propagated simply through the dependability case of the wider system. In this paper, we provide formal support for such reasoning.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5492693","Bayesian probability;safety case;software reliability.","Uncertainty;Software reliability;Phase frequency detector;Battery powered vehicles;Software systems;Software safety;Programming;Power engineering computing;Reliability engineering;Power engineering and energy","probability;software fault tolerance;uncertainty handling","software-based system dependability;software-based system safety;software-based system reliability;probability of failure on demand;conservative claims;formal support","","12","","24","","","","","","IEEE","IEEE Journals & Magazines"
"The Awareness Network, To Whom Should I Display My Actions? And, Whose Actions Should I Monitor?","C. R. B. de Souza; D. F. Redmiles","IBM Brazil, S&#x0E3; o Paulo; University of California, Irvine, Irvine","IEEE Transactions on Software Engineering","","2011","37","3","325","340","The concept of awareness plays a pivotal role in research in Computer-Supported Cooperative Work. Recently, software engineering researchers interested in the collaborative nature of software development have explored the implications of this concept in the design of software development tools. A critical aspect of awareness is the associated coordinative work practices of displaying and monitoring actions. This aspect concerns how colleagues monitor one another's actions to understand how these actions impact their own work and how they display their actions in such a way that others can easily monitor them while doing their own work. In this paper, we focus on an additional aspect of awareness: the identification of the social actors who should be monitored and the actors to whom their actions should be displayed. We address this aspect by presenting software developers' work practices based on ethnographic data from three different software development teams. In addition, we illustrate how these work practices are influenced by different factors, including the organizational setting, the age of the project, and the software architecture. We discuss how our results are relevant for both CSCW and software engineering researchers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5710950","Computer-supported cooperative work;organizational management and coordination;programming environments;programming teams;tools.","Software;Programming;Monitoring;Interviews;Collaboration;Servers","groupware;software architecture","awareness network;computer-supported cooperative work;software development;social actor;ethnographic data;organizational setting;project age;software architecture;CSCW","","31","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Heuristic search + local model checking in selective mu-calculus","A. Santone","Res. Centre on Software Technol., Univ. of Sannio, Benevento, Italy","IEEE Transactions on Software Engineering","","2003","29","6","510","523","Many tools for the automatic analysis or verification of finite-state distributed systems are based on construction of the global state graph of the system under consideration. Thus, they often fail because of the state explosion problem: the state space of a distributed system potentially increases exponentially in the number of its parallel components. To overcome this problem, we present a model checking procedure, based on the combination of heuristic searches with ideas taken from local model checking. We use heuristic mechanisms for exploration of the search space in order to avoid construction of the complete state graph.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205179","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205179","","Explosions;State-space methods;Sequential circuits;Costs;Space exploration;Logic design;Protocols;Circuit simulation;Circuit testing;Data structures","formal verification;search problems;heuristic programming;state-space methods;process algebra","automatic analysis;verification;finite-state distributed systems;global state graph;state explosion problem;state space;parallel components;local model checking;heuristic search;search space;state graph;selective mu-calculus","","15","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Effective Software Merging in the Presence of Object-Oriented Refactorings","D. Dig; K. Manzoor; R. E. Johnson; T. N. Nguyen","University of Illinois at Urbana-Champaign, Urbana; University of Illinois at Urbana-Champaign, Urbana; University of Illinois at Urbana-Champaign, Urbana; Iowa State University, Ames","IEEE Transactions on Software Engineering","","2008","34","3","321","335","Current text based Software Configuration Management (SCM) systems have trouble with refactorings. Refactorings result in global changes which lead to merge conflicts. A refactoring-aware SCM system reduces merge conflicts. This paper describes MolhadoRef, a refactoring-aware SCM system and the merge algorithm at its core. MolhadoRef records change operations (refactorings and edits) used to produce one version, and replays them when merging versions. Since refactorings are change operations with well defined semantics, MolhadoRef treats them intelligently. A case study and a controlled experiment show that MolhadoRef automatically solves more merge conflicts than CVS while resulting in fewer merge errors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4509441","Configuration Management;Restructuring;reverse engineering;and reengineering;Version control;Configuration Management;Restructuring;reverse engineering;and reengineering;Version control","Merging;Programming profession;Automatic control;Java;Computer Society;Error correction;Control systems;Runtime","object-oriented programming;software engineering","software merging;object-oriented refactorings;software configuration management;refactoring-aware SCM system;MolhadoRef records","","29","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System","B. C. D. Anda; D. I. K. Sjøberg; A. Mockus","University of Oslo, Oslo; Simula Research Laboratory, Lysaker; Avaya Labs Research, Basking Ridge","IEEE Transactions on Software Engineering","","2009","35","3","407","429","The scientific study of a phenomenon requires it to be reproducible. Mature engineering industries are recognized by projects and products that are, to some extent, reproducible. Yet, reproducibility in software engineering (SE) has not been investigated thoroughly, despite the fact that lack of reproducibility has both practical and scientific consequences. We report a longitudinal multiple-case study of variations and reproducibility in software development, from bidding to deployment, on the basis of the same requirement specification. In a call for tender to 81 companies, 35 responded. Four of them developed the system independently. The firm price, planned schedule, and planned development process, had, respectively, ldquolow,rdquo ldquolow,rdquo and ldquomediumrdquo reproducibilities. The contractor's costs, actual lead time, and schedule overrun of the projects had, respectively, ldquomedium,rdquo ldquohigh,rdquo and ldquolowrdquo reproducibilities. The quality dimensions of the delivered products, reliability, usability, and maintainability had, respectively, ldquolow,rdquo ""high,rdquo and ldquolowrdquo reproducibilities. Moreover, variability for predictable reasons is also included in the notion of reproducibility. We found that the observed outcome of the four development projects matched our expectations, which were formulated partially on the basis of SE folklore. Nevertheless, achieving more reproducibility in SE remains a great challenge for SE research, education, and industry.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4693714","Software engineering life cycle;software quality;software project success;software process;multiple-case study.;General;Life cycle;Software Quality/SQA;Multiple-case study","Reproducibility of results;Software engineering;Programming;Job shop scheduling;Software quality;Computer industry;Costs;Software measurement;Systems engineering and theory;Usability","software quality","software engineering;mature engineering industries;software development;software quality;software process","","39","","94","","","","","","IEEE","IEEE Journals & Magazines"
"Disaggregating and calibrating the CASE tool variable in COCOMO II","Jongmoon Baik; B. Boehm; B. M. Steece","Software & Syst. Eng. Res. Lab., Motorola Inc., Schaumburg, IL, USA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","11","1009","1022","CASE (computer aided software engineering) tools are believed to have played a critical role in improving software productivity and quality by assisting tasks in software development processes since the 1970s. Several parametric software cost models adopt ""use of software tools"" as one of the environmental factors that affects software development productivity. Several software cost models assess the productivity impacts of CASE tools based only on breadth of tool coverage without considering other productivity dimensions such as degree of integration, tool maturity, and user support. This paper provides an extended set of tool rating scales based on the completeness of tool coverage, the degree of tool integration, and tool maturity/user support. Those scales are used to refine the way in which CASE tools are effectively evaluated within COCOMO (constructive cost model) II. In order to find the best fit of weighting values for the extended set of tool rating scales in the extended research model, a Bayesian approach is adopted to combine two sources of (expert-judged and data-determined) information to increase prediction accuracy. The extended model using the three TOOL rating scales is validated by using the cross-validation methodologies, data splitting, and bootstrapping. This approach can be used to disaggregate other parameters that have significant impacts on software development productivity and to calibrate the best-fit weight values based on data-determined and expert-judged distributions. It results in an increase in the prediction accuracy in software parametric cost estimation models and an improvement in insights on software productivity investments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049401","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049401","","Computer aided software engineering;Productivity;Software tools;Costs;Programming;Predictive models;Accuracy;Software quality;Environmental factors;Bayesian methods","computer aided software engineering;software tools;software cost estimation;software quality","COCOMO II;CASE tool variable;software productivity;software quality;parametric software cost models;user support;tool maturity;tool rating scales;Constructive Cost Model II;Bayesian approach;prediction accuracy;data splitting;cross-validation methodologies;bootstrapping;best-fit weight values;expert-judged distributions;data-determined distributions","","28","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Comments on ""Formal methods application: an empirical tale of software development""","D. M. Berry; W. F. Tichy","Sch. of Comput. Sci., Waterloo Univ., Ont., Canada; NA","IEEE Transactions on Software Engineering","","2003","29","6","567","571","We comment on the experimental design and the result of the paper mentioned in the title. Our purpose is to show interested readers examples of what can go wrong with experiments in software research and how to avoid the attending problems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1205183","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1205183","","Application software;Design for experiments;Software tools;Books;Psychology;Data structures;Software engineering;Programming profession;Veins;Problem-solving","software engineering","experimental design;software development;software engineering;formal methods;software research","","16","","5","","","","","","IEEE","IEEE Journals & Magazines"
"A UML-based pattern specification technique","R. B. France; D. -. Kim; Sudipto Ghosh; E. Song","Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA; Dept. of Comput. Sci., Colorado State Univ., Fort Collins, CO, USA","IEEE Transactions on Software Engineering","","2004","30","3","193","206","Informally described design patterns are useful for communicating proven solutions for recurring design problems to developers, but they cannot be used as compliance points against which solutions that claim to conform to the patterns are checked. Pattern specification languages that utilize mathematical notation provide the needed formality, but often at the expense of usability. We present a rigorous and practical technique for specifying pattern solutions expressed in the unified modeling language (UML). The specification technique paves the way for the development of tools that support rigorous application of design patterns to UML design models. The technique has been used to create specifications of solutions for several popular design patterns. We illustrate the use of the technique by specifying observer and visitor pattern solutions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271174","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271174","","Unified modeling language;Object oriented modeling;Specification languages;Computer Society;Usability;Context modeling","specification languages;formal specification;formal verification;object-oriented methods","UML;pattern specification technique;design pattern;pattern specification languages;unified modeling language;object-oriented models","","126","","20","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting the location and number of faults in large software systems","T. J. Ostrand; E. J. Weyuker; R. M. Bell","AT&T Labs., Florham Park, NJ, USA; AT&T Labs., Florham Park, NJ, USA; AT&T Labs., Florham Park, NJ, USA","IEEE Transactions on Software Engineering","","2005","31","4","340","355","Advance knowledge of which files in the next release of a large software system are most likely to contain the largest numbers of faults can be a very valuable asset. To accomplish this, a negative binomial regression model has been developed and used to predict the expected number of faults in each file of the next release of a system. The predictions are based on the code of the file in the current release, and fault and modification history of the file from previous releases. The model has been applied to two large industrial systems, one with a history of 17 consecutive quarterly releases over 4 years, and the other with nine releases over 2 years. The predictions were quite accurate: for each release of the two systems, the 20 percent of the files with the highest predicted number of faults contained between 71 percent and 92 percent of the faults that were actually detected, with the overall average being 83 percent. The same model was also used to predict which files of the first system were likely to have the highest fault densities (faults per KLOC). In this case, the 20 percent of the files with the highest predicted fault densities contained an average of 62 percent of the system's detected faults. However, the identified files contained a much smaller percentage of the code mass than the files selected to maximize the numbers of faults. The model was also used to make predictions from a much smaller input set that only contained fault data from integration testing and later. The prediction was again very accurate, identifying files that contained from 71 percent to 93 percent of the faults, with the average being 84 percent. Finally, a highly simplified version of the predictor selected files containing, on average, 73 percent and 74 percent of the faults for the two systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435354","Index Terms- Software faults;fault-prone;prediction;regression model;empirical study;software testing.","Software systems;Predictive models;Fault diagnosis;System testing;History;Fault detection;Software testing;Personnel;Resource management;Sorting","program testing;software fault tolerance;binomial distribution;regression analysis","large software systems;binomial regression model;software fault prediction;software testing","","280","","17","","","","","","IEEE","IEEE Journals & Magazines"
"Techniques to tackle state explosion in global predicate detection","S. Alagar; S. Venkatesan","Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; NA","IEEE Transactions on Software Engineering","","2001","27","8","704","714","Global predicate detection, which is an important problem in testing and debugging distributed programs, is very hard due to the combinatorial explosion of the global state space. The paper presents several techniques to tackle the state explosion problem in detecting whether an arbitrary predicate /spl Phi/ is true at some consistent global state of a distributed system. We present space efficient online algorithms for detecting /spl Phi/. We then improve the performance of our algorithms, both in space and time, by increasing the granularity of the execution step from an event to a sequence of events in each process.","0098-5589;1939-3520;2326-3881","","10.1109/32.940566","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=940566","","Explosions;Testing;Polynomials;Debugging;Computer Society;State-space methods;Lattices;Safety;Message passing","distributed programming;program testing;program debugging;computational complexity","state explosion;global predicate detection;distributed program testing;debugging;combinatorial explosion;global state space;arbitrary predicate;consistent global state;distributed system;space efficient online algorithms;execution step","","21","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A Model-Based Approach to Families of Embedded Domain-Specific Languages","J. Sanchez Cuadrado; J. G. Molina","University of Murcia, Murcia; University of Murcia, Murcia","IEEE Transactions on Software Engineering","","2009","35","6","825","840","With the emergence of model-driven engineering (MDE), the creation of domain-specific languages (DSLs) is becoming a fundamental part of language engineering. The development cost of a DSL should be modest compared to the cost of developing a general-purpose programming language. Reducing the implementation effort and providing reuse techniques are key aspects for DSL approaches to be really effective. In this paper, we present an approach to build embedded domain-specific languages applying the principles of model-driven engineering. On the basis of this approach, we will tackle reuse of DSLs by defining families of DSLs, addressing reuse both from the DSL developer and user point of views. A family of DSLs will be built up by composing several DSLs, so we will propose composition mechanisms for the abstract syntax, concrete syntax, and model transformation levels of a DSL's definition. Finally, we contribute a software framework to support our approach, and we illustrate the paper with a case study to demonstrate its practical applicability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4782971","Domain-specific languages;model-driven development;families of DSLs;DSL composition.","Domain specific languages;DSL;Model driven engineering;Object oriented modeling;Costs;Computer languages;Concrete;Metamodeling;Proposals;Software engineering","computational linguistics;programming languages;software engineering","embedded domain-specific languages;model-driven engineering;programming language;abstract syntax;concrete syntax;model transformation levels","","16","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Problem Oriented Software Engineering: Solving the Package Router Control Problem","J. Hall; L. Rapanotti; M. Jackson","NA; NA; NA","IEEE Transactions on Software Engineering","","2008","34","2","226","241","Problem orientation is gaining interest as a way of approaching the development of software intensive systems, and yet, a significant example that explores its use is missing from the literature. In this paper, we present the basic elements of Problem Oriented Software Engineering (POSE), which aims at bringing both nonformal and formal aspects of software development together in a single framework. We provide an example of a detailed and systematic POSE development of a software problem: that of designing the controller for a package router. The problem is drawn from the literature, but the analysis presented here is new. The aim of the example is twofold: to illustrate the main aspects of POSE and how it supports software engineering design and to demonstrate how a nontrivial problem can be dealt with by the approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70769","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4384506","Software Engineering;Requirements/Specifications;Methodologies;Software Engineering;Requirements/Specifications;Methodologies","Software engineering;Software packages;Packaging;Programming;Control systems;Software design;Hardware;Humans;Computer Society;Software systems","software engineering","problem oriented software engineering;package router control problem;software development intensive systems;software engineering design","","39","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Coverage-Aware Test Database Reduction","J. Tuya; C. d. l. Riva; M. J. Suárez-Cabal; R. Blanco","Dpto. Informática, University of Oviedo, Campus Universitario de Gijón, Gijón, Spain; Dpto. Informática, University of Oviedo, Campus Universitario de Gijón, Gijón, Spain; Dpto. Informática, University of Oviedo, Campus Universitario de Gijón, Gijón, Spain; Dpto. Informática, University of Oviedo, Campus Universitario de Gijón, Gijón, Spain","IEEE Transactions on Software Engineering","","2016","42","10","941","959","Functional testing of applications that process the information stored in databases often requires a careful design of the test database. The larger the test database, the more difficult it is to develop and maintain tests as well as to load and reset the test data. This paper presents an approach to reduce a database with respect to a set of SQL queries and a coverage criterion. The reduction procedures search the rows in the initial database that contribute to the coverage in order to find a representative subset that satisfies the same coverage as the initial database. The approach is automated and efficiently executed against large databases and complex queries. The evaluation is carried out over two real life applications and a well-known database benchmark. The results show a very large degree of reduction as well as scalability in relation to the size of the initial database and the time needed to perform the reduction.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2519032","Spanish Ministry of Economy and Competitiveness; Principality of Asturias; ERDF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7384760","Test database reduction;test coverage of code;test design;testing tools","Databases;Production;Minimization;Fault detection;Benchmark testing;Scalability","database management systems;design;information storage;program testing;software tools;SQL","coverage-aware test database reduction;functional testing;information storage;SQL queries;coverage criterion;database benchmark;test design;testing tools","","2","","105","","","","","","IEEE","IEEE Journals & Magazines"
"iTree: Efficiently Discovering High-Coverage Configurations Using Interaction Trees","C. Song; A. Porter; J. S. Foster","Fraunhofer USA Center for Experimental Software Engineering, 5825 University Research Court, Suite 1300, College Park; Department of Computer Science, University of Maryland, A.V. Williams Building, College Park; Department of Computer Science, University of Maryland, A.V. Williams Building, College Park","IEEE Transactions on Software Engineering","","2014","40","3","251","265","Modern software systems are increasingly configurable. While this has many benefits, it also makes some software engineering tasks,such as software testing, much harder. This is because, in theory,unique errors could be hiding in any configuration, and, therefore,every configuration may need to undergo expensive testing. As this is generally infeasible, developers need cost-effective technique for selecting which specific configurations they will test. One popular selection approach is combinatorial interaction testing (CIT), where the developer selects a strength t and then computes a covering array (a set of configurations) in which all t-way combinations of configuration option settings appear at least once. In prior work, we demonstrated several limitations of the CIT approach. In particular, we found that a given system's effective configuration space - the minimal set of configurations needed to achieve a specific goal - could comprise only a tiny subset of the system's full configuration space. We also found that effective configuration space may not be well approximated by t-way covering arrays. Based on these insights we have developed an algorithm called interaction tree discovery (iTree). iTree is an iterative learning algorithm that efficiently searches for a small set of configurations that closely approximates a system's effective configuration space. On each iteration iTree tests the system on a small sample of carefully chosen configurations, monitors the system's behaviors, and then applies machine learning techniques to discover which combinations of option settings are potentially responsible for any newly observed behaviors. This information is used in the next iteration to pick a new sample of configurations that are likely to reveal further new behaviors. In prior work, we presented an initial version of iTree and performed an initial evaluation with promising results. This paper presents an improved iTree algorithm in greater detail. The key improvements are based on our use of composite proto-interactions - a construct that improves iTree's ability to correctly learn key configuration option combinations, which in turn significantly improves iTree's running time, without sacrificing effectiveness. Finally, the paper presents a detailed evaluation of the improved iTree algorithm by comparing the coverage it achieves versus that of covering arrays and randomly generated configuration sets, including a significantly expanded scalability evaluation with the ~ 1M-LOC MySQL. Our results strongly suggest that the improved iTree algorithm is highly scalable and can identify a high-coverage test set of configurations more effectively than existing methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6671585","Empirical software engineering;software configurations;software testing and analysis","Testing;Arrays;Software algorithms;Software engineering;Machine learning algorithms;Software systems;Algorithm design and analysis","iterative methods;learning (artificial intelligence);program testing;software engineering","iTree;efficiently discovering high-coverage configurations;interaction trees;software systems;software engineering;software testing;cost-effective technique;combinatorial interaction testing;CIT;interaction tree discovery;iterative learning algorithm;machine learning techniques;iTree algorithm","","5","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Proofs from Tests","N. E. Beckman; A. V. Nori; S. K. Rajamani; R. J. Simmons; S. D. Tetali; A. V. Thakur","Carnegie Mellon University, Pittsburgh; Microsoft Research India, Bangalore; Microsoft Research India, Bangalore; Carnegie Mellon University, Pittsburgh; University of California, Los Angeles, Los Angeles; University of Wisconsin-Madison, Madison","IEEE Transactions on Software Engineering","","2010","36","4","495","508","We present an algorithm DASH to check if a program P satisfies a safety property φ. The unique feature of this algorithm is that it uses only test generation operations, and it refines and maintains a sound program abstraction as a consequence of failed test generation operations. Thus, each iteration of the algorithm is inexpensive, and can be implemented without any global may-alias information. In particular, we introduce a new refinement operator WP<sub>α</sub>that uses only the alias information obtained by symbolically executing a test to refine abstractions in a sound manner. We present a full exposition of the DASH algorithm and its theoretical properties. We have implemented DASH in a tool called YOGI that plugs into Microsoft's Static Driver Verifier framework. We have used this framework to run YOGI on 69 Windows Vista drivers with 85 properties and find that YOGI scales much better than SLAM, the current engine driving Microsoft's Static Driver Verifier.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5444886","Software model checking;directed testing;abstraction refinement.","Acoustic testing;Safety;Performance evaluation;Iterative algorithms;Plugs;Simultaneous localization and mapping;Engines;Instruments;Automatic testing;Algorithm design and analysis","operating systems (computers);program testing;program verification","DASH algorithm;test generation operations;YOGI tool;Microsoft static driver verifier framework;Windows Vista drivers;software model checking","","22","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Stressing Search with Scenarios for Flexible Solutions to Real-Time Task Allocation Problems","P. Emberson; I. Bate","University of York, York; University of York, York","IEEE Transactions on Software Engineering","","2010","36","5","704","718","One of the most important properties of a good software engineering process and of the design of the software it produces is robustness to changing requirements. Scenario-based analysis is a popular method for improving the flexibility of software architectures. This paper demonstrates a search-based technique for automating scenario-based analysis in the software architecture deployment view. Specifically, a novel parallel simulated annealing search algorithm is applied to the real-time task allocation problem to find baseline solutions which require a minimal number of changes in order to meet the requirements of potential upgrade scenarios. Another simulated annealing-based search is used for finding a solution that is similar to an existing baseline when new requirements arise. Solutions generated using a variety of scenarios are judged by how well they respond to different system requirements changes. The evaluation is performed on a set of problems with a controlled set of different characteristics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.58","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5262947","Maintainability;extensibility;heuristics;search;scheduling;scenarios.","Software architecture;Computer architecture;Hardware;Real time systems;Software engineering;Software design;Simulated annealing;System testing;Robustness;Performance evaluation","parallel algorithms;real-time systems;search problems;simulated annealing;software architecture;task analysis","real-time task allocation problem;search stress;software engineering process;software design;scenario-based analysis;software architectures;search-based technique;parallel simulated annealing search algorithm","","11","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Discovering Neglected Conditions in Software by Mining Dependence Graphs","R. Chang; A. Podgurski; J. Yang","Case Western Reserve University, Cleveland; Case Western Reserve University, Cleveland; Case Western Reserve University, Cleveland","IEEE Transactions on Software Engineering","","2008","34","5","579","596","Neglected conditions are an important but difficult-to-find class of software defects. This paper presents a novel approach to revealing neglected conditions that integrates static program analysis and advanced data mining techniques to discover implicit conditional rules in a code base and to discover rule violations that indicate neglected conditions. The approach requires the user to indicate minimal constraints on the context of the rules to be sought, rather than specific rule templates. To permit this generality, rules are modeled as graph minors of enhanced procedure dependence graphs (EPDGs), in which control and data dependence edges are augmented by edges representing shared data dependences. A heuristic maximal frequent subgraph mining algorithm is used to extract candidate rules from EPDGs, and a heuristic graph matching algorithm is used to identify rule violations. We also report the results of an empirical study in which the approach was applied to four open source projects (openssl, make, procmail, amaya). These results indicate that the approach is effective and reasonably efficient.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4492791","Methods for SQA and V&;V;Pre- and post-conditions;Methods for SQA and V&V;Pre- and post-conditions","Data mining;Computer bugs;Buffer overflow;Databases;Computer Society;Data analysis;Heuristic algorithms;Open source software;Operating systems;Linux","data mining;graph theory;program diagnostics","dependence graph mining;neglected conditions;software defects;static program analysis;data mining techniques;enhanced procedure dependence graphs;heuristic maximal frequent subgraph mining algorithm","","27","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Enhancing an Application Server to Support Available Components","A. I. Kistijantoro; G. Morgan; S. K. Shrivastava; M. C. Little","Bandung Institute of Technology, Bandung; University of Newcastle, Newcastle; University of Newcastle, Newcastle; University of Newcastle, Newcastle","IEEE Transactions on Software Engineering","","2008","34","4","531","545","Three-tier middleware architecture is commonly used for hosting enterprise-distributed applications. Typically, the application is decomposed into three layers: front end, middle tier, and back end. Front end (""Web server"") is responsible for handling user interactions and acts as a client of the middle tier, while back end provides storage facilities for applications. Middle tier (""application server"") is usually the place where all computations are performed. One of the benefits of this architecture is that it allows flexible management of a cluster of computers for performance and scalability; further, availability measures, such as replication, can be introduced in each tier in an application-specific manner. However, incorporation of availability measures in a multitier system poses challenging system design problems of integrating open, nonproprietary solutions to transparent failover, exactly once execution of client requests, nonblocking transaction processing, and an ability to work with clusters. This paper describes how replication for availability can be incorporated within the middle and back-end tiers, meeting all these challenges. This paper develops an approach that requires enhancements to the middle tier only for supporting replication of both the middleware back-end tiers. The design, implementation, and performance evaluation of such a middle-tier-based replication scheme for multidatabase transactions on a widely deployed open source application server (JBoss) are presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4528966","Software engineering for Internet projects;Distributed objects;Software engineering for Internet projects;Distributed objects","Middleware;Availability;Application software;Computer architecture;Java;Scalability;Fault tolerance;Transaction databases;Containers;Distributed computing","client-server systems;middleware","middleware architecture;Web server;user interaction;back-end tier;middle tier;replication database;multidatabase transaction;open source application server","","5","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Timed state space analysis of real-time preemptive systems","G. Bucci; A. Fedeli; L. Sassoli; E. Vicario","Dipt. di Sistemi e Informatica, Florence Univ., Firenze, Italy; Dipt. di Sistemi e Informatica, Florence Univ., Firenze, Italy; Dipt. di Sistemi e Informatica, Florence Univ., Firenze, Italy; Dipt. di Sistemi e Informatica, Florence Univ., Firenze, Italy","IEEE Transactions on Software Engineering","","2004","30","2","97","111","A modeling notation is introduced which extends time Petri nets with an additional mechanism of resource assignment making the progress of timed transitions be dependent on the availability of a set of preemptable resources. The resulting notation, which we call preemptive time Petri nets, permits natural description of complex real-time systems running under preemptive scheduling, with periodic, sporadic, and one-shot processes, with nondeterministic execution times, with semaphore synchronizations and precedence relations deriving from internal task sequentialization and from interprocess communication, running on multiple processors. A state space analysis technique is presented which supports the validation of preemptive time Petri net models, combining tight schedulability analysis with exhaustive verification of the correctness of logical sequencing. The analysis technique partitions the state space in equivalence classes in which timing constraints are represented in the form of difference bounds matrixes. This permits it to maintain a polynomial complexity in the representation and derivation of state classes, but it does not tightly encompass the constraints deriving from preemptive behavior, thus producing an enlarged representation of the state space. False behaviors deriving from the approximation can be cleaned-up through an algorithm which provides a necessary and sufficient condition for the feasibility of a behavior along with a tight estimation of its timing profile.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265815","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265815","","State-space methods;Real time systems;Processor scheduling;Petri nets;Timing;Clocks;Sufficient conditions;Availability;Polynomials;Approximation algorithms","Petri nets;real-time systems;state-space methods;computational complexity;processor scheduling;equivalence classes","state space analysis technique;real-time system;preemptive time Petri nets model;difference bounds matrices;nondeterministic time parameter;polynomial complexity;interprocess communication","","56","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Exemplar: A Source Code Search Engine for Finding Highly Relevant Applications","C. McMillan; M. Grechanik; D. Poshyvanyk; C. Fu; Q. Xie","College of William and Mary, Williamsburg; Accenture Technology Labs, Chicago; College of William and Mary, Williamsburg; Accenture Technology Labs, Chicago; Accenture Technology Labs, Chicago","IEEE Transactions on Software Engineering","","2012","38","5","1069","1087","A fundamental problem of finding software applications that are highly relevant to development tasks is the mismatch between the high-level intent reflected in the descriptions of these tasks and low-level implementation details of applications. To reduce this mismatch we created an approach called EXEcutable exaMPLes ARchive (Exemplar) for finding highly relevant software projects from large archives of applications. After a programmer enters a natural-language query that contains high-level concepts (e.g., MIME, datasets), Exemplar retrieves applications that implement these concepts. Exemplar ranks applications in three ways. First, we consider the descriptions of applications. Second, we examine the Application Programming Interface (API) calls used by applications. Third, we analyze the dataflow among those API calls. We performed two case studies (with professional and student developers) to evaluate how these three rankings contribute to the quality of the search results from Exemplar. The results of our studies show that the combined ranking of application descriptions and API documents yields the most-relevant search results. We released Exemplar and our case study data to the public.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.84","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5989838","Source code search engines;information retrieval;concept location;open source software;mining software repositories;software reuse","Search engines;Engines;Software;Java;Cryptography;Vocabulary;Data mining","application program interfaces;data flow analysis;document handling;natural language processing;project management;query processing;software management;software reusability;system documentation","source code search engine;software application;development task;executable examples archive;Exemplar;software project;natural-language query;application programming interface;dataflow;API call;search quality;application description ranking;API document;software reuse","","34","","52","","","","","","IEEE","IEEE Journals & Magazines"
"The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs","C. Le Goues; N. Holtschulte; E. K. Smith; Y. Brun; P. Devanbu; S. Forrest; W. Weimer","School of Computer Science, Carnegie Mellon University, Pittsburgh, PA; Department of Computer Science at the University of New Mexico, Albuquerque, NM; College of Information and Computer Science, University of Massachusetts at Amherst, MA; College of Information and Computer Science, University of Massachusetts at Amherst, MA; Department of Computer Science, University of California at Davis, Davis, CA; Department of Computer Science at the University of New Mexico, Albuquerque, NM; Department of Computer Science, University of Virginia, Charlottesville, VA","IEEE Transactions on Software Engineering","","2015","41","12","1236","1256","The field of automated software repair lacks a set of common benchmark problems. Although benchmark sets are used widely throughout computer science, existing benchmarks are not easily adapted to the problem of automatic defect repair, which has several special requirements. Most important of these is the need for benchmark programs with reproducible, important defects and a deterministic method for assessing if those defects have been repaired. This article details the need for a new set of benchmarks, outlines requirements, and then presents two datasets, ManyBugs and IntroClass, consisting between them of 1,183 defects in 15 C programs. Each dataset is designed to support the comparative evaluation of automatic repair algorithms asking a variety of experimental questions. The datasets have empirically defined guarantees of reproducibility and benchmark quality, and each study object is categorized to facilitate qualitative evaluation and comparisons by category of bug or program. The article presents baseline experimental results on both datasets for three existing repair methods, GenProg, AE, and TrpAutoRepair, to reduce the burden on researchers who adopt these datasets for their own comparative evaluations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2454513","AFOSR; US Defense Advanced Research Projects Agency (DARPA); US Department of Energy (DOE); US National Science Foundation (NSF); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7153570","Automated program repair;benchmark;subject defect;reproducibility;ManyBugs;IntroClass;Automated program repair;benchmark;subject defect;reproducibility;ManyBugs;IntroClass","Maintenance engineering;Benchmark testing;Computer bugs;Software systems;Electronic mail","benchmark testing;C language;program debugging;software maintenance;software performance evaluation;software quality","TrpAutoRepair;GenProg;qualitative evaluation;benchmark quality;reproducibility;automatic repair algorithms;defects assessment;deterministic method;benchmark programs;automatic defect repair;computer science;benchmark sets;benchmark problems;automated software repair;C programs;IntroClass benchmarks;ManyBugs benchmarks","","46","","80","","","","","","IEEE","IEEE Journals & Magazines"
"Zebu: A Language-Based Approach for Network Protocol Message Processing","L. Burgy; L. Reveillere; J. Lawall; G. Muller","Princeton University, Princeton; University of Bordeaux, LaBRI, Talence; DIKU, University of Copenhagen, Paris; INRIA-Regal, LIP6, Paris","IEEE Transactions on Software Engineering","","2011","37","4","575","591","A network application communicates with other applications according to a set of rules known as a protocol. This communication is managed by the part of the application known as the protocol-handling layer, which enables the manipulation of protocol messages. The protocol-handling layer is a critical component of a network application since it represents the interface between the application and the outside world. It must thus satisfy two constraints: It must be efficient to be able to treat a large number of messages and it must be robust to face various attacks targeting the application itself or the underlying platform. Despite these constraints, the development process of this layer still remains rudimentary and requires a high level of expertise. It includes translating the protocol specification written in a high-level formalism such as ABNF toward low-level code such as C. The gap between these abstraction levels can entail many errors. This paper proposes a new language-based approach to developing protocol-handling layers, to improve their robustness without compromising their performance. Our approach is based on the use of a domain-specific language, Zebu, to specify the protocol-handling layer of network applications that use textual HTTP-like application protocols. The Zebu syntax is very close to that of ABNF, facilitating the adoption of Zebu by domain experts. By annotating the original ABNF specification of a protocol, the Zebu user can dedicate the protocol-handling layer to the needs of a given application. The Zebu compiler first checks the annotated specification for inconsistencies, and then generates a protocol-handling layer according to the annotations. This protocol-handling layer is made up of a set of data structures that represent a message, a parser that fills in these data structures, and various stub functions to access these data structures or drive the parsing of a message.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5487528","Network protocols;message parsing;message composing;domain-specific languages.","Electronic mail;Data structures;Robustness;Domain specific languages;Access protocols;Streaming media;IP networks;Network servers;Web server;Computer bugs","computational linguistics;formal specification;Internet;program compilers;protocols;specification languages;telecommunication computing","Zebu syntax;language-based approach;network protocol message processing;protocol-handling layer;protocol specification;ABNF;textual HTTP-like application protocols;Zebu compiler;data structures;message parsing;Internet era;domain-specific languages","","8","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Steering of Model-Based Test Oracles to Admit Real Program Behaviors","G. Gay; S. Rayadurgam; M. P. E. Heimdahl","Department of Computer Science & Engineering, University of South Carolina, Columbia, SC; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN","IEEE Transactions on Software Engineering","","2017","43","6","531","555","The test oracle-a judge of the correctness of the system under test (SUT)-is a major component of the testing process. Specifying test oracles is challenging for some domains, such as real-time embedded systems, where small changes in timing or sensory input may cause large behavioral differences. Models of such systems, often built for analysis and simulation, are appealing for reuse as test oracles. These models, however, typically represent an idealized system, abstracting away certain issues such as non-deterministic timing behavior and sensor noise. Thus, even with the same inputs, the model's behavior may fail to match an acceptable behavior of the SUT, leading to many false positives reported by the test oracle. We propose an automated steering framework that can adjust the behavior of the model to better match the behavior of the SUT to reduce the rate of false positives. This model steering is limited by a set of constraints (defining the differences in behavior that are acceptable) and is based on a search process attempting to minimize a dissimilarity metric. This framework allows non-deterministic, but bounded, behavioral differences, while preventing future mismatches by guiding the oracle-within limits-to match the execution of the SUT. Results show that steering significantly increases SUT-oracle conformance with minimal masking of real faults and, thus, has significant potential for reducing false positives and, consequently, testing and debugging costs while improving the quality of the testing process.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2615311","US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7583721","Software testing;test oracles;model-based testing;model-based development;verification","Testing;Analytical models;Computational modeling;Software;Delays;Hardware;Pacemakers","program debugging;program testing;program verification","automated steering;model-based test oracles;real program behaviors;system under test correctness;SUT correctness;model steering;search process;fault masking;debugging costs;testing process quality","","3","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Identification, Impact, and Refactoring of Smells in Pipe-Like Web Mashups","K. T. Stolee; S. Elbaum","Iowa State University, Ames; University of Nebraska-Lincoln, Lincoln","IEEE Transactions on Software Engineering","","2013","39","12","1654","1679","With the emergence of tools to support visual mashup creation, tens of thousands of users have started to access, manipulate, and compose data from web sources. We have observed, however, that mashups created by these users tend to suffer from deficiencies that propagate as mashups are reused, which happens frequently. To address these deficiencies, we would like to bring some of the benefits of software engineering techniques to the end users creating these programs. In this work, we focus on identifying code smells indicative of the deficiencies we observed in web mashups programmed in the popular Yahoo! Pipes environment. Through an empirical study, we explore the impact of those smells on the preferences of 61 users, and observe that a significant majority of users prefer mashups without smells. We then introduce refactorings targeting those smells. These refactorings reduce the complexity of the mashup programs, increase their abstraction, update broken data sources and dated components, and standardize their structures to fit the community development patterns. Our assessment of a sample of over 8,000 mashups shows that smells are present in 81 percent of them and that the proposed refactorings can reduce the number of smelly mashups to 16 percent, illustrating the potential of refactoring to support the thousands of end-users programming mashups. Further, we explore how the smells and refactorings can apply to other end-user programming domains to show the generalizability of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6589568","End-user software engineering;end-user programming;web mashups;refactoring;code smells;empirical studies","Mashups;Visualization;Factoring;Generators;Programming","Internet;software maintenance","smell identification;smell impact;smell refactoring;pipe-like Web mashups;visual mashup creation;Web sources;software engineering techniques;Yahoo! Pipes environment;end-users programming mashups","","11","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Using Dependency Structures for Prioritization of Functional Test Suites","S. Haidry; T. Miller","University of Melbourne, Parkville; University of Melbourne, Parkville","IEEE Transactions on Software Engineering","","2013","39","2","258","275","Test case prioritization is the process of ordering the execution of test cases to achieve a certain goal, such as increasing the rate of fault detection. Increasing the rate of fault detection can provide earlier feedback to system developers, improving fault fixing activity and, ultimately, software delivery. Many existing test case prioritization techniques consider that tests can be run in any order. However, due to functional dependencies that may exist between some test cases-that is, one test case must be executed before another-this is often not the case. In this paper, we present a family of test case prioritization techniques that use the dependency information from a test suite to prioritize that test suite. The nature of the techniques preserves the dependencies in the test ordering. The hypothesis of this work is that dependencies between tests are representative of interactions in the system under test, and executing complex interactions earlier is likely to increase the fault detection rate, compared to arbitrary test orderings. Empirical evaluations on six systems built toward industry use demonstrate that these techniques increase the rate of fault detection compared to the rates achieved by the untreated order, random orders, and test suites ordered using existing ""coarse-grained” techniques based on function coverage.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6189361","Software engineering;testing and debugging;test execution","Fault detection;Digital signal processing;Schedules;Software;Complexity theory;Testing;Weight measurement","program debugging;program testing;software fault tolerance;software process improvement","dependency structures;functional test suite prioritization;test case execution ordering;fault detection rate;system developers;fault fixing improvement;software delivery;functional dependencies;test case prioritization technique;dependency information;system under test;software debugging","","17","","31","","","","","","IEEE","IEEE Journals & Magazines"
"State-Density Functions over DBM Domains in the Analysis of Non-Markovian Models","L. Carnevali; L. Grassi; E. Vicario","Università di Firenze, Firenze; Università di Firenze, Firenze; Università di Firenze, Firenze","IEEE Transactions on Software Engineering","","2009","35","2","178","194","Quantitative evaluation of models with generally-distributed transitions requires analysis of non-Markovian processes that may be not isomorphic to their underlying untimed models and may include any number of concurrent non-exponential timers. The analysis of stochastic Time Petri Nets copes with the problem by covering the state space with stochastic-classes, which extend Difference Bounds Matrices (DBM) with a state probability density function. We show that the state-density function accepts a continuous piecewise representation over a partition in DBM-shaped sub-domains. We then develop a closed-form symbolic calculus of state-density functions assuming that model transitions have expolynomial distributions. The calculus shows that within each sub-domain the state-density function is a multivariate expolynomial function and makes explicit how this form evolves through subsequent transitions. This enables an efficient implementation of the analysis process and provides the formal basis that supports introduction of an approximate analysis based on Bernstein Polynomials. The approximation attacks practical and theoretical limits in the applicability of stochastic state-classes, and devises a new approach to the analysis of non Markovian models, relying on approximations in the state space rather than in the structure of the model.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.101","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4711059","Software Engineering;Tools;Validation;Software and System Safety;Software/Program Verification;Formal methods;Reliability;Automata;Parallelism and concurrency;Approximation;Markov processes;Renewal theory;Stochastic processes;Software Engineering;Tools;Validation;Software and System Safety;Software/Program Verification;Formal methods;Reliability;Automata;Parallelism and concurrency;Approximation;Markov processes;Renewal theory;Stochastic processes","Stochastic processes;Function approximation;State-space methods;Petri nets;Density functional theory;Calculus;Polynomials;Software safety;Timing;Encoding","calculus;concurrency control;Petri nets;polynomial approximation;polynomial matrices;program verification;state-space methods;statistical distributions;stochastic processes","difference bound matrix domain;nonMarkovian model analysis;concurrent nonexponential timer;stochastic time Petri net;state space;state probability density function;continuous piecewise representation;closed-form symbolic calculus;expolynomial distribution;multivariate expolynomial function;approximate analysis;Bernstein polynomial;quantitative evaluation;untimed model;timed software verification","","17","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Test Case-Aware Combinatorial Interaction Testing","C. Yilmaz","Sabanci University, Istanbul","IEEE Transactions on Software Engineering","","2013","39","5","684","706","The configuration spaces of modern software systems are too large to test exhaustively. Combinatorial interaction testing (CIT) approaches, such as covering arrays, systematically sample the configuration space and test only the selected configurations by using a battery of test cases. Traditional covering arrays, while taking system-wide interoption constraints into account, do not provide a systematic way of handling test case-specific interoption constraints. The basic justification for t-way covering arrays is that they can cost effectively exercise all system behaviors caused by the settings of t or fewer options. In this paper, we hypothesize, however, that in the presence of test case-specific interoption constraints, many such behaviors may not be tested due to masking effects caused by the overlooked test case-specific constraints. For example, if a test case refuses to run in a configuration due to an unsatisfied test case-specific constraint, none of the valid option setting combinations appearing in the configuration will be tested by that test case. To account for test case-specific constraints, we introduce a new combinatorial object, called a test case-aware covering array. A t-way test case-aware covering array is not just a set of configurations, as is the case in traditional covering arrays, but a set of configurations, each of which is associated with a set of test cases such that all test case-specific constraints are satisfied and that, for each test case, each valid combination of option settings for every combination of t options appears at least once in the set of configurations that the test case is associated with. We furthermore present three algorithms to compute test case-aware covering arrays. Two of the algorithms aim to minimize the number of configurations required (one is fast, but produces larger arrays, the other is slower, but produces smaller arrays), whereas the remaining algorithm aims to minimize the number of test runs required. The results of our empirical studies conducted on two widely used highly configurable software systems suggest that test case-specific constraints do exist in practice, that traditional covering arrays suffer from masking effects caused by ignorance of such constraints, and that test case-aware covering arrays are better than other approaches in handling test case-specific constraints, thus avoiding masking effects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.65","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311411","Software quality assurance;combinatorial interaction testing;covering arrays","Testing;Servers;Software algorithms;Software systems;Systematics;Computational modeling","program testing;software quality","test case-aware combinatorial interaction testing;software system;CIT approach;interoption constraint;masking effect;t-way test case-aware covering array;highly configurable software system","","7","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Power-Law Distributions of Component Size in General Software Systems","L. Hatton","CISM, University, United Kingdom","IEEE Transactions on Software Engineering","","2009","35","4","566","572","This paper begins by modeling general software systems using concepts from statistical mechanics which provide a framework for linking microscopic and macroscopic features of any complex system. This analysis provides a way of linking two features of particular interest in software systems: first the microscopic distribution of defects within components and second the macroscopic distribution of component sizes in a typical system. The former has been studied extensively, but the latter much less so. This paper shows that subject to an external constraint that the total number of defects is fixed in an equilibrium system, commonly used defect models for individual components directly imply that the distribution of component sizes in such a system will obey a power-law Pareto distribution. The paper continues by analyzing a large number of mature systems of different total sizes, different implementation languages, and very different application areas, and demonstrates that the component sizes do indeed appear to obey the predicted power-law distribution. Some possible implications of this are explored.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.105","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4711063","Defects;macroscopic system behavior;component size distribution;Pareto.;General systems theory;Software science;Design concepts;Reliability;availability;and serviceability","Software systems;Power system modeling;Joining processes;Microscopy;Thermodynamics;Temperature distribution;Predictive models;Assembly systems;Software standards;Equations","Pareto distribution;software engineering;statistical mechanics","general software systems modeling;statistical mechanics;complex system;defects distribution;component sizes distribution;power-law Pareto distribution","","14","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Reducing Unauthorized Modification of Digital Objects","P. C. Van Oorschot; G. Wurster","Carleton University, Ottawa; Carleton University, Ottawa","IEEE Transactions on Software Engineering","","2012","38","1","191","204","We consider the problem of malicious modification of digital objects. We present a protection mechanism designed to protect against unauthorized replacement or modification of digital objects while still allowing authorized updates transparently. We use digital signatures without requiring any centralized public key infrastructure. To explore the viability of our proposal, we apply the approach to file-system binaries, implementing a prototype in Linux which protects operating system and application binaries on disk. To test the prototype and related kernel modifications, we show that it protects against various rootkits currently available while incurring minimal overhead costs. The general approach can be used to restrict updates to general digital objects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680916","Protection mechanisms;software release management and delivery;system integration and implementation;access controls;file organization;operating systems.","Public key;Digital signatures;Access controls;Malware;File organization;Operating systems","authorisation;digital signatures;file organisation;industrial property;operating system kernels","unauthorized modification reduction;malicious modification problem;unauthorized digital object replacement;digital signatures;file-system binaries;Linux;operating system protection;kernel modification;overhead cost minimisation","","","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Model-Transformation Design Patterns","K. Lano; S. Kolahdouz-Rahimi","Department of Informatics, King’s College London, London WC2R 2LS, United Kingdom; Department of Informatics, King’s College London, London WC2R 2LS, United Kingdom","IEEE Transactions on Software Engineering","","2014","40","12","1224","1259","This paper defines a catalogue of patterns for the specification and design of model transformations, and provides a systematic scheme and classification of these patterns, together with pattern application examples in leading model transformation languages such as ATL, QVT, GrGen.NET, and others. We consider patterns for improving transformation modularization and efficiency and for reducing data storage requirements. We define a metamodel-based formalization of model transformation design patterns, and measurement-based techniques to guide the selection of patterns. We also provide an evaluation of the effectiveness of transformation patterns on a range of different case studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2354344","EPSRC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6891324","Model transformations;design patterns;model-driven development","Unified modeling language;Software development;Systematics;Analytical models;Semantics;Complexity theory;Syntactics","formal specification;object-oriented programming;pattern classification","metamodel-based formalization;data storage;transformation modularization improvement;GrGen.NET;QVT;ATL;model transformation languages;pattern application;pattern classification;systematic scheme;model-transformation design patterns","","20","","72","","","","","","IEEE","IEEE Journals & Magazines"
"A Flexible Infrastructure for Multilevel Language Engineering","C. Atkinson; M. Gutheil; B. Kennel","University of Mannheim, Mannheim; itemis AG, Bonn; University of Mannheim, Mannheim","IEEE Transactions on Software Engineering","","2009","35","6","742","755","Although domain-specific modeling tools have come a long way since the modern era of model-driven development started in the early 1990s and now offer an impressive range of features, there is still significant room for enhancing the flexibility they offer to end users and for combining the advantages of domain-specific and general-purpose languages. To do this, however, it is necessary to enhance the way in which the current generation of tools view metamodeling and support the representation of the multiple, ?ontological? classification levels that often exist in subject domains. State-of-the-art tools essentially allow users to describe the abstract and concrete syntaxes of a language in the form of metamodels and to make statements in that language in the form of models. These statements typically convey information in terms of types and instances in the domain (e.g., the classes and objects of UML), but not in terms of types of types (i.e., domain metaclasses), and types of types of types, and so on, across multiple classification levels. In essence, therefore, while they provide rich support for ?linguistic? metamodeling, the current generation of tools provides little if any built-in support for modeling ?ontological? classification across more than one type/instance level in the subject domain. In this paper, we describe a prototype implementation of a new kind of modeling infrastructure that, by providing built-in support for multiple ontological as well as linguistic classification levels, offers various advantages over existing language engineering approaches and tools. These include the ability to view a single model from the perspective of both a general-purpose and a domain-specific modeling language, the ability to define constraints across multiple ontological classification levels, and the ability to tie the rendering of model elements to ontological as well as linguistic types over multiple classification levels. After first outlining the key conceptual ingredients of this new infrastructure and presenting the main elements of our current realization, we show these benefits through two small examples.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4907005","Language engineering;metamodeling;multilevel modeling.","Unified modeling language;Ontologies;Metamodeling;Concrete;Design engineering;Software tools;Prototypes;DSL;Production facilities;Software engineering","ontologies (artificial intelligence);software tools;Unified Modeling Language","multilevel language engineering;model-driven development;linguistic metamodeling;ontological classification levels;domain-specific modeling language;UML","","33","","37","","","","","","IEEE","IEEE Journals & Magazines"
"EgoSpaces: facilitating rapid development of context-aware mobile applications","C. Julien; G. -. Roman","Dept. of Electr. & Comput. Eng., Texas Univ., Austin, TX, USA; NA","IEEE Transactions on Software Engineering","","2006","32","5","281","298","Today's mobile applications require constant adaptation to their changing environments, or contexts. Technological advances have increased the pervasiveness of mobile computing devices such as laptops, handhelds, and embedded sensors. The sheer amount of context information available for adaptation places a heightened burden on application developers as they must manage and utilize vast amounts of data from diverse sources. Facilitating programming in this data-rich environment requires a middleware that provides context information to applications in an abstract form. In this paper, we demonstrate the feasibility of such a middleware that allows programmers to focus on high-level interactions among programs and to employ declarative abstract context specifications in settings that exhibit transient interactions with opportunistically encountered components. We also discuss the novel context-aware abstractions the middleware provides and the programming knowledge necessary to write applications using it. Finally, we provide examples demonstrating the infrastructure's ability to support differing tasks from a wide variety of application domains","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642677","Context-awareness;middleware;mobile ad hoc networks;programming abstraction.","Application software;Mobile computing;Middleware;Pervasive computing;Network topology;Mobile communication;Context modeling;Computer Society;Embedded computing;Handheld computers","ad hoc networks;distributed programming;formal specification;interactive programming;middleware;mobile computing","EgoSpaces;context-aware mobile applications;mobile computing devices;data-rich environment;middleware;context information;context-aware abstractions;transient interactions","","72","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Knowledge-based automation of a design method for concurrent systems","K. L. Mills; H. Gomaa","Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA; NA","IEEE Transactions on Software Engineering","","2002","28","3","228","255","This paper describes a knowledge-based approach to automate a software design method for concurrent systems. The approach uses multiple paradigms to represent knowledge embedded in the design method. Semantic data modeling provides the means to represent concepts from a behavioral modeling technique, called Concurrent Object-Based Real-time Analysis (COBRA), which defines system behavior using data/control flow diagrams. Entity-relationship modeling is used to represent a design metamodel based on a design method, called COncurrent Design Approach for Real-Time Systems (CODARTS), which represents concurrent designs as software architecture diagrams, task behavior specifications and module specifications. Production rules provide the mechanism for codifying a set of CODARTS heuristics that can generate concurrent designs based on semantic concepts included in COBRA behavioral models and on entities and relationships included in CODARTS design metamodels. Together, the semantic data model, the entity-relationship model, and the production rules, when encoded using an expert system shell, compose CODA, an automated designer's assistant. CODA is applied to generate 10 concurrent designs for four real-time problems. The paper reports the degree of automation achieved by CODA. The paper also evaluates the quality of generated designs by comparing the similarity between designs produced by CODA and human designs reported in the literature for the same problems. In addition, it compares CODA with four other approaches used to automate software design methods.","0098-5589;1939-3520;2326-3881","","10.1109/32.991319","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=991319","","Design automation;Design methodology;Software design;Real time systems;Automatic control;Software architecture;Data models;Production systems;Expert systems;Humans","real-time systems;distributed object management;diagrams;entity-relationship modelling;computer aided software engineering;expert system shells;software tools;software architecture","knowledge-based software engineering;software design;concurrent systems;knowledge representation;Concurrent Object-Based Real-time Analysis;expert-system shell;CODA;automated reasoning;automated software engineering;COBRA;dataflow diagrams;entity-relationship modeling;metamodel;Concurrent Design Approach for Real-Time Systems;CODARTS;software architecture diagrams;task behavior specifications;module specifications;production rules;semantic data model","","11","","50","","","","","","IEEE","IEEE Journals & Magazines"
"A Taxonomy and Mapping of Computer-Based Critiquing Tools","N. M. Ali; J. Hosking; J. Grundy","Universiti Putra Malaysia, Selangor; Australian National University, Canberra; Swinburne University, Melbourne","IEEE Transactions on Software Engineering","","2013","39","11","1494","1520","Critics have emerged in recent times as a specific tool feature to support users in computer-mediated tasks. These computer-supported critics provide proactive guidelines or suggestions for improvement to designs, code, and other digital artifacts. The concept of a critic has been adopted in various domains, including medical, programming, software engineering, design sketching, and others. Critics have been shown to be an effective mechanism for providing feedback to users. We propose a new critic taxonomy based on extensive review of the critic literature. The groups and elements of our critic taxonomy are presented and explained collectively with examples, including the mapping of 13 existing critic tools, predominantly for software engineering and programming education tasks to the taxonomy. We believe this critic taxonomy will assist others in identifying, categorizing, developing, and deploying computer-supported critics in a range of domains.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6570472","Design critics;critiquing systems;critic taxonomy;software tool support;survey","Taxonomy;Software;Recommender systems;Programming;Software engineering;Unified modeling language;Java","computer science education;programming;software engineering","computer-based critiquing tool taxonomy;computer-based critiquing tool mapping;computer-mediated tasks;computer-supported critics;digital artifacts;critic taxonomy;software engineering;programming education tasks","","4","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Mathematical assessment of object-oriented design quality","A. Chatzigeorgiou","Dept. of Appl. Informatics, Universij y of Macedonia, Thessaloniki, Greece","IEEE Transactions on Software Engineering","","2003","29","11","1050","1053","A method of link analysis employed for retrieving information from the Web is extended in order to evaluate one aspect of quality in an object-oriented model. The principal eigenvectors of matrices derived from the adjacency matrix of a modified class diagram are used to identify and quantity heavily loaded portions of an object-oriented design that deviate from the principle of distributed responsibilities.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245306","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245306","","Object oriented modeling;Algorithm design and analysis;Information analysis;Information retrieval;Scalability;Software maintenance;Guidelines;Programming profession;Performance analysis;Software metrics","eigenvalues and eigenfunctions;software metrics;object-oriented programming;information retrieval","object-oriented design quality;mathematical assessment;information retrieval;Web;eigenvectors;matrices;modified class diagram;software metrics","","5","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating capture-recapture models with two inspectors","K. El Emam; O. Laitenberger","Inst. for Inf. Technol., Nat. Res. Council of Canada, Ottawa, Ont., Canada; NA","IEEE Transactions on Software Engineering","","2001","27","9","851","864","Capture-recapture (CR) models have been proposed as an objective method for controlling software inspections. CR models were originally developed to estimate the size of animal populations. In software, they have been used to estimate the number of defects in an inspected artifact. This estimate can be another source of information for deciding whether the artifact requires a reinspection to ensure that a minimal inspection effectiveness level has been attained. Little evaluative research has been performed thus far on the utility of CR models for inspections with two inspectors. We report on an extensive Monte Carlo simulation that evaluated capture-recapture models suitable for two inspectors assuming a code inspections context. We evaluate the relative error of the CR estimates as well as the accuracy of the reinspection decision made using the CR model. Our results indicate that the most appropriate capture-recapture model for two inspectors is an estimator that allows for inspectors with different capabilities. This model always produces an estimate (i.e., does not fail), has a predictable behavior (i.e., works well when its assumptions are met), will have a relatively high decision accuracy, and will perform better than the default decision of no reinspections. Furthermore, we identify the conditions under which this estimator will perform best.","0098-5589;1939-3520;2326-3881","","10.1109/32.950319","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=950319","","Inspection;Chromium;Animals;Software quality;Software engineering;Process control;Computer Society;Information resources;Performance evaluation;Context modeling","software quality;program testing;reviews","capture-recapture models;inspectors;software inspections;reinspection;Monte Carlo simulation;predictable behavior;fault estimation;quality control","","33","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Comparing uniform and flexible policies for software maintenance and replacement","Y. Tan; V. S. Mookerjee","Bus. Sch., Washington Univ., Seattle, WA, USA; NA","IEEE Transactions on Software Engineering","","2005","31","3","238","255","The importance of software maintenance in managing the life-cycle costs of a system cannot be overemphasized. Beyond a point, however, it is better to replace a system rather than maintain it. We derive model and operating policy that reduces the sum of maintenance and replacement costs in the useful life of a software system. The main goal is to compare uniform (occurring at fixed time intervals) versus flexible (occurring at varying, planned time intervals) polices for maintenance and replacement. The model draws from the empirical works of earlier researchers to consider 1) inclusion of user requests for maintenance, 2) scale economies in software maintenance, 3) efficiencies derived from replacing old software technology with new software technology, and 4) the impact of software reuse on replacement and maintenance. Results from our model show that the traditional practice of maintaining or replacing a software system at uniform time intervals may not be optimal. We also find that an increase in software reuse leads to more frequent replacement, but the number of maintenance activities is not significantly impacted.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423995","Index Terms- Software maintenance and replacement;cost models;optimal scheduling.","Software maintenance;Software systems;Application software;Cost function;Environmental economics;Optimal scheduling;Computer industry;Software performance;Personnel;Java","software reusability;software maintenance;software cost estimation","software maintenance;software replacement;software reuse;optimal scheduling;software cost estimation","","26","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Performance Modeling and Evaluation of Distributed Component-Based Systems Using Queueing Petri Nets","S. Kounev","IEEE Computer Society","IEEE Transactions on Software Engineering","","2006","32","7","486","502","Performance models are used increasingly throughout the phases of the software engineering lifecycle of distributed component-based systems. However, as systems grow in size and complexity, building models that accurately capture the different aspects of their behavior becomes a more and more challenging task. In this paper, we present a novel case study of a realistic distributed component-based system, showing how queueing Petri net models can be exploited as a powerful performance prediction tool in the software engineering process. A detailed system model is built in a step-by-step fashion, validated, and then used to evaluate the system performance and scalability. Along with the case study, a practical performance modeling methodology is presented which helps to construct models that accurately reflect the system performance and scalability characteristics. Taking advantage of the modeling power and expressiveness of queueing Petri nets, our approach makes it possible to model the system at a higher degree of accuracy, providing a number of important benefits","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677534","Performance modeling and prediction;software verification;performance evaluation;distributed systems.","Petri nets;Power system modeling;Predictive models;Application software;Distributed control;Quality of service;Software engineering;System performance;Scalability;Performance analysis","distributed processing;formal specification;object-oriented programming;Petri nets;program verification;queueing theory;software performance evaluation","software engineering lifecycle;distributed component-based system;queueing Petri net model;performance prediction tool;software verification;software performance evaluation","","81","","31","","","","","","IEEE","IEEE Journals & Magazines"
"A General Software Defect-Proneness Prediction Framework","Q. Song; Z. Jia; M. Shepperd; S. Ying; J. Liu","Xi'an Jiaotong University, Xi'an; Xi'an Jiaotong University, Xi'an; Brunel University, Uxbridge; Wuhan University, Wuhan; Wuhan University, Wuhan","IEEE Transactions on Software Engineering","","2011","37","3","356","370","BACKGROUND - Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5611551","Software defect prediction;software defect-proneness prediction;machine learning;scheme evaluation.","Software;Training data;Predictive models;Buildings;Data models;Prediction algorithms;Training","learning (artificial intelligence);software fault tolerance;software performance evaluation","software defect proneness prediction framework;scheme evaluation;competing learning schemes;defect predictor","","107","","44","","","","","","IEEE","IEEE Journals & Magazines"
"An empirical investigation of the key factors for success in software process improvement","T. Dyba","Software Eng. Dept., SINTEF ICT, Trondheim, Norway","IEEE Transactions on Software Engineering","","2005","31","5","410","424","Understanding how to implement software process improvement (SPI) successfully is arguably the most challenging issue facing the SPI field today. The SPI literature contains many case studies of successful companies and descriptions of their SPI programs. However, the research efforts to date are limited and inconclusive and without adequate theoretical and psychometric justification. This paper extends and integrates models from prior research by performing an empirical investigation of the key factors for success in SPI. A quantitative survey of 120 software organizations was designed to test the conceptual model and hypotheses of the study. The results indicate that success depends critically on six organizational factors, which explained more than 50 percent of the variance in the outcome variable. The main contribution of the paper is to increase the understanding of the influence of organizational issues by empirically showing that they are at least as important as technology for succeeding with SPI and, thus, to provide researchers and practitioners with important new insights regarding the critical factors of success in SPI.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1438376","Index Terms- Empirical software engineering;software process improvement;critical success factors;organizational issues;survey research.","Quality management;Psychology;Software engineering;Particle measurements;Computer Society;Software design;Psychometric testing;Software testing;Paper technology;Computer industry","software process improvement;organisational aspects;software quality","software process improvement;empirical software engineering;critical success factor;organizational issues;survey research;psychometric justification;conceptual model","","111","","98","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Web software reliability based on workload and failure data extracted from server logs","J. Tian; S. Rudraraju; Zhao Li","Dept. of Comput. Sci., Southern Methodist Univ., Dallas, TX, USA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","11","754","769","We characterize usage and problems for Web applications, evaluate their reliability, and examine the potential for reliability improvement. Based on the characteristics of Web applications and the overall Web environment, we classify Web problems and focus on the subset of source content problems. Using information about Web accesses, we derive various measurements that can characterize Web site workload at different levels of granularity and from different perspectives. These workload measurements, together with failure information extracted from recorded errors, are used to evaluate the operational reliability for source contents at a given Web site and the potential for reliability improvement. We applied this approach to the Web sites www.seas.smu.edu and www.kde.org. The results demonstrated the viability and effectiveness of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359769","Index Terms- World Wide Web (WWW) and Internet;Web applications and Web server logs;quality and reliability;reliability modeling;workload measurement","Software reliability;Data mining;Application software;World Wide Web;Web server;Software systems;Software measurement;Computer Society;Internet;Web sites","Internet;Web sites;software reliability;content management;information retrieval;software quality","Web software reliability;failure data extraction;Web server log;source content problem;Web access;Web site workload;Internet;software quality","","47","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Using SCL to specify and check design intent in source code","D. Hou; H. J. Hoover","Avra Software Lab. Inc., Edmonton, Alta., Canada; NA","IEEE Transactions on Software Engineering","","2006","32","6","404","423","Software developers often fail to respect the intentions of designers due to missing or ignored documentation of design intent. SCL (Structural Constraint Language) addresses this problem by enabling designers to formalize and confirm compliance with design intent. The designer expresses his intent as constraints on the program model using the SCL language. The SCL conformance checking tool examines developer code to confirm that the code honors these constraints. This paper presents the design of the SCL language and its checker, a set of practical examples of applying SCL, and our experience with using it both in an industrial setting and on open-source software","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650215","Design intent;structural constraints;program analysis;object-oriented software;SCL;FCL.","Open source software;Programming;Context modeling;Application software;Documentation;Object oriented modeling;Computer industry;Roads;Government","formal specification;formal verification;object-oriented programming;public domain software;source coding","SCL language;source code;structural constraint language;program model;SCL conformance checking tool;open-source software","","35","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Semantics-Based Obfuscation-Resilient Binary Code Similarity Comparison with Applications to Software and Algorithm Plagiarism Detection","L. Luo; J. Ming; D. Wu; P. Liu; S. Zhu","College of Information Sciences and Technology, Pennsylvania State University, University Park, PA; Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX; College of Information Sciences and Technology, Pennsylvania State University, University Park, PA; College of Information Sciences and Technology, Pennsylvania State University, University Park, PA; Department of Computer Science and Engineering, Pennsylvania State University, University Park, PA","IEEE Transactions on Software Engineering","","2017","43","12","1157","1177","Existing code similarity comparison methods, whether source or binary code based, are mostly not resilient to obfuscations. Identifying similar or identical code fragments among programs is very important in some applications. For example, one application is to detect illegal code reuse. In the code theft cases, emerging obfuscation techniques have made automated detection increasingly difficult. Another application is to identify cryptographic algorithms which are widely employed by modern malware to circumvent detection, hide network communications, and protect payloads among other purposes. Due to diverse coding styles and high programming flexibility, different implementation of the same algorithm may appear very distinct, causing automatic detection to be very hard, let alone code obfuscations are sometimes applied. In this paper, we propose a binary-oriented, obfuscation-resilient binary code similarity comparison method based on a new concept, longest common subsequence of semantically equivalent basic blocks , which combines rigorous program semantics with longest common subsequence based fuzzy matching. We model the semantics of a basic block by a set of symbolic formulas representing the input-output relations of the block. This way, the semantic equivalence (and similarity) of two blocks can be checked by a theorem prover. We then model the semantic similarity of two paths using the longest common subsequence with basic blocks as elements. This novel combination has resulted in strong resiliency to code obfuscation. We have developed a prototype. The experimental results show that our method can be applied to software plagiarism and algorithm detection, and is effective and practical to analyze real-world software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2655046","US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7823022","Software plagiarism detection;algorithm detection;binary code similarity comparison;obfuscation;symbolic execution;constraint solving","Semantics;Software development;Plagiarism;Binary codes;Software algorithms;Syntactics;Computational modeling","fuzzy set theory;invasive software;program diagnostics;software engineering;theorem proving","code similarity comparison methods;identical code fragments;illegal code reuse;code theft cases;obfuscation techniques;cryptographic algorithms;diverse coding styles;automatic detection;code obfuscations;obfuscation-resilient binary code similarity comparison method;longest common subsequence;semantically equivalent basic blocks;rigorous program semantics;semantic similarity","","3","","81","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Improving cohesion metrics for classes by considering dependent instance variables","Heung Seok Chae; Yong Rae Kwon; Doo Hwan Bae","Dept. of Comput. Sci. & Eng., Pusan Nat. Univ., South Korea; NA; NA","IEEE Transactions on Software Engineering","","2004","30","11","826","832","The existing cohesion metrics for classes do not consider the characteristics of dependent instance variables that are commonly used in a class and, thus, do not properly reflect the cohesiveness of the class. This paper presents an approach for improving the cohesion metrics by considering the characteristics of the dependent instance variables in an object-oriented program.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359773","Index Terms- Cohesion;object-oriented design;metrics/measurement.","Software engineering;Design methodology;Libraries","object-oriented programming;object-oriented methods;software metrics","cohesion metrics;dependent instance variable;object-oriented program;object-oriented design","","18","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Embedding, Evolution, and Validation of Model-Driven Spreadsheets","J. Cunha; J. P. Fernandes; J. Mendes; J. Saraiva","Universidade Nova de Lisboa, Portugal, and HASLab / INESC TEC, Portugal; (rel)ease/Universidade da Beira Interior, Portugal, and HASLab / INESC TEC, Portugal; Universidade do Minho & ESTGF, Instituto Politécnico do Porto, Portugal, and HASLab / INESC TEC, Portugal; Universidade do Minho, Portugal, and HASLab / INESC TEC, Portugal","IEEE Transactions on Software Engineering","","2015","41","3","241","263","This paper proposes and validates a model-driven software engineering technique for spreadsheets. The technique that we envision builds on the embedding of spreadsheet models under a widely used spreadsheet system. This means that we enable the creation and evolution of spreadsheet models under a spreadsheet system. More precisely, we embed ClassSheets, a visual language with a syntax similar to the one offered by common spreadsheets, that was created with the aim of specifying spreadsheets. Our embedding allows models and their conforming instances to be developed under the same environment. In practice, this convenient environment enhances evolution steps at the model level while the corresponding instance is automatically co-evolved. Finally, we have designed and conducted an empirical study with human users in order to assess our technique in production environments. The results of this study are promising and suggest that productivity gains are realizable under our model-driven spreadsheet development setting.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2361141","European Regional Development; FCT; cão para a Ciência e a Tecnologia; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6915751","Spreadsheets;Models;ClassSheets;Embedding;Evolution;Empirical Validation;Spreadsheets;models;ClassSheets;embedding;evolution;empirical validation","Data models;Visualization;Atmospheric modeling;Business;Software;Unified modeling language;Syntactics","software development management;spreadsheet programs;visual languages","model driven software engineering technique;spreadsheet model embedding;spreadsheet system;spreadsheet model evolution;ClassSheets;visual language;syntax;model-driven spreadsheet development;spreadsheet model validation","","6","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Event-based traceability for managing evolutionary change","J. Cleland-Huang; C. K. Chang; M. Christensen","Sch. of Comput. Sci., Telecommun., & Inf. Syst., DePaul Univ., Chicago, IL, USA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","9","796","810","Although the benefits of requirements traceability are widely recognized, the actual practice of maintaining a traceability scheme is not always entirely successful. The traceability infrastructure underlying a software system tends to erode over its lifetime, as time-pressured practitioners fail to consistently maintain links and update impacted artifacts each time a change occurs, even with the support of automated systems. This paper proposes a new method of traceability based upon event-notification and is applicable even in a heterogeneous and globally distributed development environment. Traceable artifacts are no longer tightly coupled but are linked through an event service, which creates an environment in which change is handled more efficiently, and artifacts and their related links are maintained in a restorable state. The method also supports enhanced project management for the process of updating and maintaining the system artifacts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232285","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232285","","Software maintenance;Software systems;Maintenance engineering;Control systems;Computer science;Computer Society;Project management;Software testing;System testing;Software engineering","software maintenance;systems analysis;bibliographies","requirements traceability;traceability scheme;traceability infrastructure;impacted artifacts;event-notification;heterogeneous globally distributed development environment;evolutionary change;software maintenance","","126","","69","","","","","","IEEE","IEEE Journals & Magazines"
"Formal Analysis of the Probability of Interaction Fault Detection Using Random Testing","A. Arcuri; L. Briand","Simula Research Laboratory, Lysaker; Simula Research Laboratory, Lysaker","IEEE Transactions on Software Engineering","","2012","38","5","1088","1099","Modern systems are becoming highly configurable to satisfy the varying needs of customers and users. Software product lines are hence becoming a common trend in software development to reduce cost by enabling systematic, large-scale reuse. However, high levels of configurability entail new challenges. Some faults might be revealed only if a particular combination of features is selected in the delivered products. But testing all combinations is usually not feasible in practice, due to their extremely large numbers. Combinatorial testing is a technique to generate smaller test suites for which all combinations of t features are guaranteed to be tested. In this paper, we present several theorems describing the probability of random testing to detect interaction faults and compare the results to combinatorial testing when there are no constraints among the features that can be part of a product. For example, random testing becomes even more effective as the number of features increases and converges toward equal effectiveness with combinatorial testing. Given that combinatorial testing entails significant computational overhead in the presence of hundreds or thousands of features, the results suggest that there are realistic scenarios in which random testing may outperform combinatorial testing in large systems. Furthermore, in common situations where test budgets are constrained and unlike combinatorial testing, random testing can still provide minimum guarantees on the probability of fault detection at any interaction level. However, when constraints are present among features, then random testing can fare arbitrarily worse than combinatorial testing. As a result, in order to have a practical impact, future research should focus on better understanding the decision process to choose between random testing and combinatorial testing, and improve combinatorial testing in the presence of feature constraints.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.85","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5999671","Combinatorial testing;random testing;interaction testing;theory;constraint;feature diagram;lower bound","Software;Context;Fault detection;Feature extraction;Scalability;Benchmark testing","cost reduction;customer satisfaction;probability;program testing;program verification;random processes;software cost estimation;software fault tolerance","interaction fault detection probability;formal analysis;random testing;customer satisfaction;user satisfaction;software product lines;software development;cost reduction;large-scale reusability;combinatorial testing;computational overhead;feature constraints","","18","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Test Case Generation for Boolean Expressions by Cell Covering","L. Yu; W. Tsai","School of Software and Microelectronics in Peking University, Beijing, China; Beihang University, Beijing, China","IEEE Transactions on Software Engineering","","2018","44","1","70","99","This paper characterizes Boolean expression faults as changes of the topological structures in terms of shrinking and/or expanding regions in K-map. A cell-covering is a set of cells (test cases) in K-map to cover the fault regions such that faults guarantee to be detected. Minimizing cell covering can be formulated as an Integer Linear Programming (ILP) problem. By analyzing the structures of the constraint coefficient matrix, the original problem can be decomposed into sub-programs that can be solved instead of the original problem, and this significantly reduces the time needed for ILP execution. An efficient approximate algorithm with a tight theoretical bound is used to address those complex Boolean expressions by corresponding the cell-covering problem to the set-covering problem. The optimal approach and the approximate approach are combined into a hybrid process to identify test cases based on the fraction analysis on the ILP relaxation. The proposed approach is evaluated by three sets of Boolean expressions and the results are compared with three leading approaches with respect to test sizes, time consumption and fault detection capabilities. For most Boolean expressions encountered, the proposed approach obtains optimal solutions quickly, and produces near-optimal solutions rapidly for those rare and complex expressions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2669184","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7855791","Boolean expression testing;fault characterization;cell-covering problem;approximate algorithms","Fault detection;Approximation algorithms;Optimization;Periodic structures;Algorithm design and analysis;Testing;Software","approximation theory;Boolean functions;computational complexity;fault diagnosis;integer programming;linear programming;set theory","test case generation;Boolean expression faults;topological structures;cell-covering;complex Boolean expressions;set-covering problem;fault detection capabilities;complex expressions;integer linear programming problem;cell covering minimization","","","","51","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Extraction of Heap Reference Properties in Object-Oriented Programs","B. Demsky; M. Rinard","University of California, Irvine, Irvine; Massachusetts Institute of Technology, Cambridge","IEEE Transactions on Software Engineering","","2009","35","3","305","324","We present a new technique for helping developers understand heap referencing properties of object-oriented programs and how the actions of the program affect these properties. Our dynamic analysis uses the aliasing properties of objects to synthesize a set of roles; each role represents an abstract object state intended to be of interest to the developer. We allow the developer to customize the analysis to explore the object states and behavior of the program at multiple different and potentially complementary levels of abstraction. The analysis uses roles as the basis for three abstractions: role transition diagrams, which present the observed transitions between roles and the methods responsible for the transitions; role relationship diagrams, which present the observed referencing relationships between objects playing different roles; and enhanced method interfaces, which present the observed roles of method parameters. Together, these abstractions provide useful information about important object and data structure properties and how the actions of the program affect these properties. We have implemented the role analysis and have used this implementation to explore the behavior of several Java programs. Our experience indicates that, when combined with a powerful graphical user interface, roles are a useful abstraction for helping developers explore and understand the behavior of object-oriented programs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4693716","Program understanding;roles;design recovery.;Requirements/Specifications;Design Tools and Techniques;Software Engineering;Software/Software Engineering;Testing and Debugging","Data structures;Computer Society;Java;Graphical user interfaces;Information analysis;Data mining","data structures;diagrams;graphical user interfaces;Java;object-oriented programming;reverse engineering","automatic extraction;heap reference property;object-oriented program;role transition diagram;enhanced method interface;data structure;Java program;graphical user interface;program understanding;role relationship diagram","","2","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Engineering mobile agent applications via context-dependent coordination","G. Cabri; L. Leonardi; F. Zambonelli","Dipt. di Ingegneria dell'Informazione, Universita di Modena e Reggio Emilia, Italy; Dipt. di Ingegneria dell'Informazione, Universita di Modena e Reggio Emilia, Italy; NA","IEEE Transactions on Software Engineering","","2002","28","11","1039","1055","The design and development of Internet applications requiring dynamic and possibly mobile access to Internet resources can take advantage of an approach based on autonomous mobile agents. However, mobility introduces peculiar issues related to the modeling and management of the agents' coordination activities. This paper introduces context-dependent coordination as a framework for the design and development of Internet applications based on mobile agents, and shows how it can be supported by a proper coordination infrastructure. Context-dependent coordination is centered on the notion of programmable coordination media, as the software abstraction via which an agent in an Internet site can access to local resources and coordinate with local agents. Programmability stems from the fact that the behavior of the media can be fully configured to influence agents' coordination activities. This enables local administrators to configure coordination media so as to enact site-dependent coordination policies, and mobile agents to configure the accessed coordination media to obtain an application-dependent behavior of the media themselves. Several application examples shows that exploiting context-dependent coordination promotes a clear separation of concerns in design and development, and can make applications more modular and easier to be maintained. The MARS system is assumed as an exemplar coordination infrastructure to clarify the concepts expressed and to show their actual implementation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049403","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049403","","Mobile agents;Application software;Web and internet services;Mars;Middleware;Global communication;Communication system control;Design engineering","mobile agents;Internet;mobile computing;knowledge engineering","mobile agent application engineering;context-dependent coordination;Internet applications;dynamic Internet resource access;MARS system;mobile Internet resource access;autonomous mobile agents;modeling;management;programmable coordination media;software abstraction;site-dependent coordination policies","","33","","49","","","","","","IEEE","IEEE Journals & Magazines"
"GK-Tail+ An Efficient Approach to Learn Software Models","L. Mariani; M. Pezzè; M. Santoro","Department of Informatics, Systems and Communication, University of Milano Bicocca, Milano, Italy; Faculty of Informatics, Universitá della Svizzera Italiana, Lugano, Switzerland; Faculty of Informatics, Universitá della Svizzera Italiana, Lugano, Switzerland","IEEE Transactions on Software Engineering","","2017","43","8","715","738","Inferring models of program behavior from execution samples can provide useful information about a system, also in the increasingly common case of systems that evolve and adapt in their lifetime, and without requiring large developers' effort. Techniques for learning models of program behavior from execution traces shall address conflicting challenges of recall, specificity and performance: They shall generate models that comprehensively represent the system behavior (recall) while limiting the amount of illegal behaviors that may be erroneously accepted by the model (specificity), and should infer the models within a reasonable time budget to process industrial scale systems (performance). In our early work, we designed GK-tail, an approach that can infer guarded finite state machines that model the behavior of object-oriented programs in terms of sequences of method calls and constraints on the parameter values. GK-tail addresses well two of the three main challenges, since it infers guarded finite state machines with a high level of recall and specificity, but presents severe limitations in terms of performance that reduce its scalability. In this paper, we present GK-tail+, a new approach to infer guarded finite state machines from execution traces of object-oriented programs. GK-tail+ proposes a new set of inference criteria that represent the core element of the inference process: It largely reduces the inference time of GK-tail while producing guarded finite state machines with a comparable level of recall and specificity. Thus, GK-tail+ advances the preliminary results of GK-tail by addressing all the three main challenges of learning models of program behavior from execution traces.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2623623","The H2020 Learn project; ERC Consolidator Grant 2014 program; ERC; Swiss National Foundation; SNF; ASysT: Automatic System Testing; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7728088","Dynamic model learning;software models;state based models;guarded finite state machines;specification mining","Object oriented modeling;Merging;Analytical models;Adaptation models;Software systems;Limiting","finite state machines;learning (artificial intelligence);object-oriented programming","GK-Tail+;software models;inferring models;program behavior;learning models;system behavior;illegal behaviors;reasonable time budget;industrial scale systems;object-oriented programs;guarded finite state machines;inference criteria;inference process;inference time;execution traces","","4","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Backward Bisimulation in Markov Chain Model Checking","J. Sproston; S. Donatelli","IEEE Computer Society; NA","IEEE Transactions on Software Engineering","","2006","32","8","531","546","Equivalence relations can be used to reduce the state space of a system model, thereby permitting more efficient analysis. We study backward stochastic bisimulation in the context of model checking continuous-time Markov chains against continuous stochastic logic (CSL) properties. While there are simple CSL properties that are not preserved when reducing the state space of a continuous-time Markov chain using backward stochastic bisimulation, we show that the equivalence can nevertheless be used in the verification of a practically significant class of CSL properties. We consider an extension of these results to Markov reward models and continuous stochastic reward logic. Furthermore, we identify the logical properties for which the requirement on the equality of state-labeling sets (normally imposed on state equivalences in a model-checking context) can be omitted from the definition of the equivalence, resulting in a better state-space reduction","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.74","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703385","Markov processes;model checking;temporal logic;verification.","State-space methods;Stochastic processes;Logic;Stochastic systems;Context modeling;Petri nets;Algebra;Biological system modeling;Computer Society;System recovery","bisimulation equivalence;formal verification;Markov processes;set theory;temporal logic","backward stochastic bisimulation equivalence relation;continuous-time Markov chain model checking;system model state space reduction;continuous stochastic reward logic property verification;Markov reward model;state-labeling set equality;temporal logic","","12","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Seer: A Lightweight Online Failure Prediction Approach","B. Ozcelik; C. Yilmaz","freelance software developer; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey","IEEE Transactions on Software Engineering","","2016","42","1","26","46","Online failure prediction approaches aim to predict the manifestation of failures at runtime before the failures actually occur. Existing approaches generally refrain themselves from collecting internal execution data, which can further improve the prediction quality. One reason behind this general trend is the runtime overhead incurred by the measurement instruments that collect the data. Since these approaches are targeted at deployed software systems, excessive runtime overhead is generally undesirable. In this work we conjecture that large cost reductions in collecting internal execution data for online failure prediction may derive from pushing the substantial parts of the data collection work onto the hardware. To test this hypothesis, we present a lightweight online failure prediction approach, called Seer, in which most of the data collection work is performed by fast hardware performance counters. The hardware-collected data is augmented with further data collected by a minimal amount of software instrumentation that is added to the systems software. In our empirical evaluations conducted on three open source projects, Seer performed significantly better than other related approaches in predicting the manifestation of failures.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2442577","Marie Curie International Reintegration; European Community Framework Programme; Scientific and Technological Research Council of Turkey; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7120143","Online failure prediction;hardware performance counters;software quality assurance;software reliability;Online failure prediction;hardware performance counters;software quality assurance;software reliability","Radiation detectors;Hardware;Runtime;Predictive models;Instruments;Software;Indexes","data handling;public domain software;quality assurance;software cost estimation;software quality;software reliability;system recovery","Seer;lightweight online failure prediction;internal execution data collection;prediction quality improvement;measurement instruments;software systems;runtime overhead;cost reduction;fast hardware performance counters;software instrumentation;open source projects;software quality assurance;software reliability","","4","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Engineering a Sound Assertion Semantics for the Verifying Compiler","P. Chalin","Dependable Software Research Group, Concordia University, Montreal","IEEE Transactions on Software Engineering","","2010","36","2","275","287","The Verifying Compiler (VC) project is a core component of the Dependable Systems Evolution Grand Challenge. The VC offers the promise of automatically proving that a program or component is correct, where correctness is defined by program assertions. While several VC prototypes exist, all adopt a semantics for assertions that is unsound. This paper presents a consolidation of VC requirements analysis (RA) activities that, in particular, brought us to ask targeted VC customers what kind of semantics they wanted. Taking into account both practitioners' needs and current technological factors, we offer recovery of soundness through an adjusted definition of assertion validity that matches user expectations and can be implemented practically using current prover technology. For decades, there have been debates concerning the most appropriate semantics for program assertions. Our contribution here is unique in that we have applied fundamental software engineering techniques by asking primary stakeholders what they want and, based on this, proposed a means of efficiently realizing the semantics stakeholders want using standard tools and techniques. We describe how support for the new semantics has been added to ESC/Java2, one of the most fully developed VC prototypes. Case studies demonstrate the effectiveness of the new semantics at uncovering previously indiscernible specification errors.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5262949","Software verification;assertions;programming by contract;logics of programs;requirements engineering.","Acoustical engineering;Virtual colonoscopy;Prototypes;Software prototyping;Java;Logic programming;Runtime;Software engineering;Software standards;Contracts","program compilers;program verification;systems analysis","verifying compiler project;Dependable Systems Evolution Grand Challenge;sound assertion semantics engineering;program correctness proving;VC requirements analysis;software engineering techniques;program assertions;ESC/Java2","","5","","70","","","","","","IEEE","IEEE Journals & Magazines"
"A method for detecting obfuscated calls in malicious binaries","Arun Lakhotia; Eric Uday Kumar; M. Venable","Louisiana State Univ., Baton Rouge, LA, USA; Louisiana State Univ., Baton Rouge, LA, USA; Louisiana State Univ., Baton Rouge, LA, USA","IEEE Transactions on Software Engineering","","2005","31","11","955","968","Information about calls to the operating system (or kernel libraries) made by a binary executable may be used to determine whether the binary is malicious. Being aware of this approach, malicious programmers hide this information by making such calls without using the call instruction. For instance, the call addr instruction may be replaced by two push instructions and a ret instruction, the first push pushes the address of instruction after the ret instruction, and the second push pushes the address addr. The code may be further obfuscated by spreading the three instructions and by splitting each instruction into multiple instructions. This work presents a method to statically detect obfuscated calls in binary code. The idea is to use abstract interpretation to detect where the normal call-ret convention is violated. These violations can be detected by what is called an abstract stack graph. An abstract stack graph is a concise representation of all potential abstract stacks at every point in a program. An abstract stack is used to associate each element in the stack to the instruction that pushes the element. An algorithm for constructing the abstract stack graph is also presented. Methods for using the abstract stack graph are shown to detect eight different obfuscations. The technique is demonstrated by implementing a prototype tool called DOC (detector for obfuscated calls).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.120","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556554","Index Terms- Invasive software (viruses;worms);program analysis;validation;obfuscation;abstract stack.","Kernel;Performance analysis;Laboratories;Operating systems;Libraries;Programming profession;Binary codes;Software prototyping;Prototypes;Detectors","operating system kernels;graph theory;resource allocation;binary codes;security of data;software prototyping","malicious binaries information;operating system;kernel libraries;call instruction;binary code;abstract stack graph;prototype tool DOC;detector for obfuscated calls","","21","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Deriving Bisimulation Relations from Path Extension Based Equivalence Checkers","K. Banerjee; D. Sarkar; C. Mandal","Intel Parallel Computing Lab, Bangalore, Karnataka, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, India","IEEE Transactions on Software Engineering","","2017","43","10","946","953","Constructing bisimulation relations between programs as a means of translation validation has been an active field of study. The problem is in general undecidable. Currently available mechanisms suffer from drawbacks such as non-termination and significant restrictions on the structures of programs to be checked. We have developed a path extension based equivalence checking method as an alternative translation validation technique to alleviate these drawbacks. In this work, path extension based equivalence checking of programs (flowcharts) is leveraged to establish a bisimulation relation between a program and its translated version by constructing the relation from the outputs of the equivalence checker.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2645687","Tata Consultancy Services (TCS); Department of Science and Technology (DST); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7801117","Translation validation;bisimulation relation;equivalence checking;path extension based equivalence checker","Computational modeling;Inference algorithms;Processor scheduling;Computer science;Electronic mail;Optimization;Integrated circuit modeling","bisimulation equivalence;flowcharting;program interpreters;program verification","bisimulation relation;equivalence checker;path extension based equivalence checking method;alternative translation validation technique;program checking;flowcharts","","","","19","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"A survey of software refactoring","T. Mens; T. Tourwe","Univ. de Mons-Hainaut, Mons, Belgium; NA","IEEE Transactions on Software Engineering","","2004","30","2","126","139","We provide an extensive overview of existing research in the field of software refactoring. This research is compared and discussed based on a number of different criteria: the refactoring activities that are supported, the specific techniques and formalisms that are used for supporting these activities, the types of software artifacts that are being refactored, the important issues that need to be taken into account when building refactoring tool support, and the effect of refactoring on the software process. A running example is used to explain and illustrate the main concepts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265817","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265817","","Software quality;Software tools;Software maintenance;Software reusability;Buildings;Programming environments;Reverse engineering;Costs;Spirals;Taxonomy","software reusability;object-oriented programming;bibliographies;software maintenance;programming environments;reverse engineering;software quality","software refactoring;programming environment;reverse engineering;coding tools;coding technique;construction tool","","445","","111","","","","","","IEEE","IEEE Journals & Magazines"
"Forecasting Java Software Evolution Trends Employing Network Models","T. Chaikalis; A. Chatzigeorgiou","Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece","IEEE Transactions on Software Engineering","","2015","41","6","582","602","The evolution of networks representing systems in various domains, including social networks, has been extensively studied enabling the development of growth models which govern their behavior over time. The architecture of software systems can also be naturally represented in the form of networks, whose properties change as software evolves. In this paper we attempt to model several aspects of graphs representing object-oriented software systems as they evolve over a number of versions. The goal is to develop a prediction model by considering global phenomena such as preferential attachment, past evolutionary trends such as the tendency of classes to create fewer relations as they age, as well as domain knowledge in terms of principles that have to be followed in object-oriented design. The derived models can provide insight into the future trends of software and potentially form the basis for eliciting improved or novel laws of software evolution. The forecasting power of the proposed model is evaluated against the actual evolution of 10 open-source projects and the achieved accuracy in the prediction of several network and software properties, which reflect the underlying system design, appears to be promising.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2381249","European Union (European Social Fund—ESF); National Strategic Reference Framework; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6985636","Graphs and networks;Restructuring;reverse engineering;reengineering;Software Architectures;Objectoriented design methods;Graphs and networks;restructuring;reverse engineering;and reengineering;software architectures;object-oriented design methods","Object oriented modeling;Predictive models;Forecasting;Software systems;Analytical models;Market research","graph theory;Java;object-oriented programming;social networking (online);software architecture;software maintenance","Java software evolution trend;social network;software system architecture;object-oriented software system;prediction model;preferential attachment","","10","","96","","","","","","IEEE","IEEE Journals & Magazines"
"The Effect of GoF Design Patterns on Stability: A Case Study","A. Ampatzoglou; A. Chatzigeorgiou; S. Charalampidou; P. Avgeriou","Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands; Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands; Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands","IEEE Transactions on Software Engineering","","2015","41","8","781","802","Stability refers to a software system's resistance to the “ripple effect”, i.e., propagation of changes. In this paper, we investigate the stability of classes that participate in instances/occurrences of GoF design patterns. We examine whether the stability of such classes is affected by (a) the pattern type, (b) the role that the class plays in the pattern, (c) the number of pattern occurrences in which the class participates, and (d) the application domain. To this end, we conducted a case study on about 65.000 Java open-source classes, where we performed change impact analysis on classes that participate in zero, one (single pattern), or more than one (coupled) pattern occurrences. The results suggest that, the application of design patterns can provide the expected “shielding” of certain pattern-participating classes against changes, depending on their role in the pattern. Moreover, classes that participate in coupled pattern occurrences appear to be the least stable. The results can be used for assessing the benefits and liabilities of the use of patterns and for testing and refactoring prioritization, because less stable classes are expected to require more effort while testing, and urge for refactoring activities that would make them more resistant to change propagation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2414917","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7066925","D.2.2 Design Tools and Techniques;D.2.3.a Object-oriented programming,;D.2.8 Metrics/Measurement;Design Tools and Techniques;Object-oriented programming;Metrics/Measurement","Stability analysis;Couplings;Abstracts;Measurement;Production facilities;Open source software","Java;object-oriented programming;public domain software","GoF design pattern;stability;software system resistance;ripple effect;Java open-source class;change impact analysis;coupled pattern occurrence;pattern-participating class;refactoring prioritization;refactoring activity;change propagation","","17","","53","","","","","","IEEE","IEEE Journals & Magazines"
"What Makes a Great Manager of Software Engineers?","E. Kalliamvakou; C. Bird; T. Zimmermann; A. Begel; R. DeLine; D. M. German","Department of Computer Science, University of Victoria, Victoria, BC, Canada; Microsoft Corportation, Redmond, WA; Microsoft Corportation, Redmond, WA; Microsoft Corportation, Redmond, WA; Microsoft Corportation, Redmond, WA; Department of Computer Science, University of Victoria, Victoria, BC, Canada","IEEE Transactions on Software Engineering","","2019","45","1","87","106","Having great managers is as critical to success as having a good team or organization. In general, a great manager is seen as fuelling the team they manage, enabling it to use its full potential. Though software engineering research studies factors that may affect the performance and productivity of software engineers and teams (like tools and skills), it has overlooked the software engineering manager. The software industry's growth and change in the last decades is creating a need for a domain-specific view of management. On the one hand, experts are questioning how the abundant work in management applies to software engineering. On the other hand, practitioners are looking to researchers for evidence-based guidance on how to manage software teams. We conducted a mixed methods empirical study of software engineering management at Microsoft to investigate what manager attributes developers and engineering managers perceive important and why. We present a conceptual framework of manager attributes, and find that technical skills are not the sign of greatness for an engineering manager. Through statistical analysis we identify how engineers and managers relate in their views, and how software engineering differs from other knowledge work groups in its perceptions about what makes great managers. We present strategies for putting the attributes to use, discuss implications for research and practice, and offer avenues for further work.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2768368","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8094304","software engineering management;empirical studies;software companies","Software engineering;Software;Organizations;Knowledge engineering;Productivity;Interviews;Psychology","software development management;software engineering;statistical analysis","software industry;software teams;software engineering management;manager attributes;evidence-based guidance;statistical analysis","","1","","108","","","","","","IEEE","IEEE Journals & Magazines"
"Software Plagiarism Detection with Birthmarks Based on Dynamic Key Instruction Sequences","Z. Tian; Q. Zheng; T. Liu; M. Fan; E. Zhuang; Z. Yang","Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), Department of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), Department of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), Department of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; Ministry of Education Key Lab For Intelligent Networks and Network Security (MOEKLINNS), Department of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; Department of Computer Science, Western Michigan University, Kalamazoo, MI","IEEE Transactions on Software Engineering","","2015","41","12","1217","1235","A software birthmark is a unique characteristic of a program. Thus, comparing the birthmarks between the plaintiff and defendant programs provides an effective approach for software plagiarism detection. However, software birthmark generation faces two main challenges: the absence of source code and various code obfuscation techniques that attempt to hide the characteristics of a program. In this paper, we propose a new type of software birthmark called DYnamic Key Instruction Sequence (DYKIS) that can be extracted from an executable without the need for source code. The plagiarism detection algorithm based on our new birthmarks is resilient to both weak obfuscation techniques such as compiler optimizations and strong obfuscation techniques implemented in tools such as SandMark, Allatori and Upx. We have developed a tool called DYKIS-PD (DYKIS Plagiarism Detection tool) and conducted extensive experiments on large number of binary programs. The tool, the benchmarks and the experimental results are all publicly available.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2454508","National Science Foundation of China; Ministry of Education Innovation Research Team; Key Projects in the National Science and Technology Pillar Program of China; Fundamental Research Funds for the Central Universities; National Science Foundation (NSF); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7153572","Software plagiarism detection;software birthmark;Software plagiarism detection;software birthmark","Software engineering;Plagiarism;Licenses;Heuristic algorithms;Watermarking","fraud;program diagnostics;security of data","DYKIS plagiarism detection tool;DYKIS-PD;Upx;Allatori;SandMark;compiler optimization;code obfuscation;source code;software birthmark;dynamic key instruction sequences;software plagiarism detection","","15","","77","","","","","","IEEE","IEEE Journals & Magazines"
"Priority queues and sorting methods for parallel simulation","M. D. Grammatikakis; S. Liesche","INTRACOM, Peania, Greece; NA","IEEE Transactions on Software Engineering","","2000","26","5","401","422","The authors examine the design, implementation, and experimental analysis of parallel priority queues for device and network simulation. They consider: 1) distributed splay trees using MPI; 2) concurrent heaps using shared memory atomic locks; and 3) a new, more general concurrent data structure based on distributed sorted lists, designed to provide dynamically balanced work allocation and efficient use of shared memory resources. We evaluate performance for all three data structures on a Cray-TSESOO system at KFA-Julich. Our comparisons are based on simulations of single buffers and a 64/spl times/64 packet switch which supports multicasting. In all implementations, PEs monitor traffic at their preassigned input/output ports, while priority queue elements are distributed across the Cray-TBE virtual shared memory. Our experiments with up to 60000 packets and two to 64 PEs indicate that concurrent priority queues perform much better than distributed ones. Both concurrent implementations have comparable performance, while our new data structure uses less memory and has been further optimized. We also consider parallel simulation for symmetric networks by sorting integer conflict functions and implementing a packet indexing scheme. The optimized message passing network simulator can process /spl sim/500 K packet moves in one second, with an efficiency that exceeds /spl sim/50 percent for a few thousand packets on the Cray-T3E with 32 PEs. All developed data structures form a parallel library. Although our concurrent implementations use the Cray-TSE ShMem library, portability can be derived from Open-MP or MP1-2 standard libraries, which will provide support for one-way communication and shared memory lock mechanisms.","0098-5589;1939-3520;2326-3881","","10.1109/32.846298","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846298","","Sorting;Data structures;Libraries;Switches;Queueing analysis;Analytical models;Tree data structures;Resource management;Packet switching;Traffic control","parallel programming;message passing;application program interfaces;queueing theory;sorting;digital simulation;abstract data types;virtual storage;shared memory systems;Cray computers;packet switching;software libraries","priority queues;sorting methods;parallel simulation;parallel priority queues;network simulation;distributed splay trees;MPI;concurrent heaps;shared memory atomic locks;concurrent data structure;distributed sorted lists;dynamically balanced work allocation;shared memory resources;Cray-TSESOO system;single buffers;packet switch;multicasting;preassigned input/output ports;priority queue elements;Cray-TBE virtual shared memory;concurrent priority queues;concurrent implementations;data structure;symmetric networks;integer conflict functions;packet indexing scheme;optimized message passing network simulator;parallel library;Cray-TSE ShMem library;MPI-2 standard libraries;Open-MP;shared memory lock mechanisms","","11","","33","","","","","","IEEE","IEEE Journals & Magazines"
"A controlled experiment for evaluating quality guidelines on the maintainability of object-oriented designs","L. C. Briand; C. Bunse; J. W. Daly","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2001","27","6","513","530","The paper presents a controlled experiment, focusing on the impact of applying quality design principles such as the ones provided by P. Coad and E. Yourdon (1991) on the maintainability of object oriented designs. Results, which repeat the findings of a previous study, strongly suggest that such design principles have a beneficial effect on the maintainability of object oriented designs. It is argued that object oriented designs are sensitive to poor design practices because the cognitive complexity introduced becomes increasingly unmanageable. However, as our ability to generalize these results is limited, they should be considered as preliminary, i.e., it is very likely that they can only be generalized to programmers with little object oriented training and programming experience. Such programmers can, however, be commonly found on maintenance projects. As well as additional research, external replications of this study are required to confirm the results and achieve confidence in these findings.","0098-5589;1939-3520;2326-3881","","10.1109/32.926174","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=926174","","Guidelines;Programming profession;Software maintenance;Productivity;Software quality;Object oriented programming;Software systems;Design methodology;Entropy;Maintenance engineering","object-oriented programming;software maintenance;project management;software quality;software development management;human resource management;human factors","controlled experiment;quality guideline evaluation;object oriented design maintainability;quality design principles;design principles;poor design practices;cognitive complexity;object oriented training;programming experience;maintenance projects","","88","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Automated API Property Inference Techniques","M. P. Robillard; E. Bodden; D. Kawrykow; M. Mezini; T. Ratchford","McGill University, Montréal; Technische Universität Darmstadt, Darmstadt; McGill University, Montréal; Technische Universität Darmstadt, Darmstadt; McGill University, Montréal","IEEE Transactions on Software Engineering","","2013","39","5","613","637","Frameworks and libraries offer reusable and customizable functionality through Application Programming Interfaces (APIs). Correctly using large and sophisticated APIs can represent a challenge due to hidden assumptions and requirements. Numerous approaches have been developed to infer properties of APIs, intended to guide their use by developers. With each approach come new definitions of API properties, new techniques for inferring these properties, and new ways to assess their correctness and usefulness. This paper provides a comprehensive survey of over a decade of research on automated property inference for APIs. Our survey provides a synthesis of this complex technical field along different dimensions of analysis: properties inferred, mining techniques, and empirical results. In particular, we derive a classification and organization of over 60 techniques into five different categories based on the type of API property inferred: unordered usage patterns, sequential usage patterns, behavioral specifications, migration mappings, and general information.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.63","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311409","API property;programming rules;specifications;protocols;interface;data mining;pattern mining;API evolution;API usage pattern","Itemsets;Context;Association rules;Protocols;Programming;Software engineering","application program interfaces;data mining;pattern classification","automated API property inference technique;application programming interfaces;properties inferred;mining techniques;empirical results;technique classification;unordered usage patterns;sequential usage patterns;behavioral specifications;migration mappings;general information","","45","","109","","","","","","IEEE","IEEE Journals & Magazines"
"A UML/MARTE Model Analysis Method for Uncovering Scenarios Leading to Starvation and Deadlocks in Concurrent Systems","M. Shousha; L. Briand; Y. Labiche","Carleton University, Ottawa; Simula Research Laboratory, Lysaker and University of Oslo, Norway; Carleton University, Ottawa","IEEE Transactions on Software Engineering","","2012","38","2","354","374","Concurrency problems such as starvation and deadlocks should be identified early in the design process. As larger, more complex concurrent systems are being developed, this is made increasingly difficult. We propose here a general approach based on the analysis of specialized design models expressed in the Unified Modeling Language (UML) that uses a specifically designed genetic algorithm to detect concurrency problems. Though the current paper addresses deadlocks and starvation, we will show how the approach can be easily tailored to other concurrency issues. Our main motivations are 1) to devise solutions that are applicable in the context of the UML design of concurrent systems without requiring additional modeling and 2) to use a search technique to achieve scalable automation in terms of concurrency problem detection. To achieve the first objective, we show how all relevant concurrency information is extracted from systems' UML models that comply with the UML Modeling and Analysis of Real-Time and Embedded Systems (MARTE) profile. For the second objective, a tailored genetic algorithm is used to search for execution sequences exhibiting deadlock or starvation problems. Scalability in terms of problem detection is achieved by showing that the detection rates of our approach are, in general, high and are not strongly affected by large increases in the size of complex search spaces.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.107","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5661791","Search-based software engineering;MDD;deadlock;starvation;model analysis;concurrent systems;UML;MARTE;genetic algorithms.","Unified modeling language;Concurrent computing;System recovery;Analytical models;Real time systems;Computational modeling;Data mining","concurrency control;embedded systems;genetic algorithms;search problems;software engineering;Unified Modeling Language","UML-MARTE model analysis method;starvation;deadlocks;concurrency problems;design process;complex concurrent systems;specialized design models;Unified Modeling Language;genetic algorithm;UML design;search technique;scalable automation;concurrency problem detection;concurrency information;UML models;UML modeling and analysis;real-time systems;embedded systems;MARTE profile;execution sequences;complex search spaces","","13","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Visualizing software changes","S. G. Eick; T. L. Graves; A. F. Karr; A. Mockus; P. Schuster","Visual Insights, Naperville, IL, USA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","4","396","412","A key problem in software engineering is changing the code. We present a sequence of visualizations and visual metaphors designed to help engineers understand and manage the software change process. The principal metaphors are matrix views, cityscapes, bar and pie charts, data sheets and networks. Linked by selection mechanisms, multiple views are combined to form perspectives that both enable discovery of high-level structure in software change data and allow effective access to details of those data. Use of the views and perspectives is illustrated in two important contexts: understanding software change by exploration of software change data and management of software development. Our approach complements existing visualizations of software structure and software execution.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995435","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995435","","Visualization","program visualisation;systems re-engineering;software development management;software maintenance","software change visualization;software engineering;visual metaphors;software change process management;matrix views;cityscapes;bar charts;pie charts;reengineering;code structure;software maintenance;data sheets;multiple views;software development management","","52","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Schedule of Bad Smell Detection and Resolution: A New Way to Save Effort","H. Liu; Z. Ma; W. Shao; Z. Niu","Beijing Institute of Technology and Ministry of Education, Beijing; Ministry of Education, Beijing; Ministry of Education, Beijing; Beijing Institute of Technology, Beijing","IEEE Transactions on Software Engineering","","2012","38","1","220","235","Bad smells are signs of potential problems in code. Detecting and resolving bad smells, however, remain time-consuming for software engineers despite proposals on bad smell detection and refactoring tools. Numerous bad smells have been recognized, yet the sequences in which the detection and resolution of different kinds of bad smells are performed are rarely discussed because software engineers do not know how to optimize sequences or determine the benefits of an optimal sequence. To this end, we propose a detection and resolution sequence for different kinds of bad smells to simplify their detection and resolution. We highlight the necessity of managing bad smell resolution sequences with a motivating example, and recommend a suitable sequence for commonly occurring bad smells. We evaluate this recommendation on two nontrivial open source applications, and the evaluation results suggest that a significant reduction in effort ranging from 17.64 to 20 percent can be achieved when bad smells are detected and resolved using the proposed sequence.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5680918","Scheme;bad smell;software refactoring;effort;detection;schedule.","Refactoring;Feature extraction;Distance measurement;Scheduling","scheduling;software engineering","bad smell detection;bad smell resolution;software engineers;optimal sequence;resolution sequence;detection sequence;open source applications","","41","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Identifying and Summarizing Systematic Code Changes via Rule Inference","M. Kim; D. Notkin; D. Grossman; G. Wilson","The University of Texas at Austin, Austin; University of Washington, Seattle; University of Washington, Seattle; The University of Texas at Austin, Austin","IEEE Transactions on Software Engineering","","2013","39","1","45","62","Programmers often need to reason about how a program evolved between two or more program versions. Reasoning about program changes is challenging as there is a significant gap between how programmers think about changes and how existing program differencing tools represent such changes. For example, even though modification of a locking protocol is conceptually simple and systematic at a code level, diff extracts scattered text additions and deletions per file. To enable programmers to reason about program differences at a high level, this paper proposes a rule-based program differencing approach that automatically discovers and represents systematic changes as logic rules. To demonstrate the viability of this approach, we instantiated this approach at two different abstraction levels in Java: first at the level of application programming interface (API) names and signatures, and second at the level of code elements (e.g., types, methods, and fields) and structural dependences (e.g., method-calls, field-accesses, and subtyping relationships). The benefit of this approach is demonstrated through its application to several open source projects as well as a focus group study with professional software engineers from a large e-commerce company.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6165314","Software evolution;program differencing;rule learning;logic-based program representation","Systematics;Syntactics;Inference algorithms;Cloning;Software;Semantics;Libraries","application program interfaces;electronic commerce;inference mechanisms;Java;reasoning about programs;software engineering","systematic code;rule inference;reasoning about program;locking protocol;scattered text additions;rule-based program differencing approach;Java;application programming interface;API;professional software engineers;large e-commerce company;structural dependences","","16","","50","","","","","","IEEE","IEEE Journals & Magazines"
"A study of design characteristics in evolving software using stability as a criterion","D. Kelly","Dept. of Math. & Comput. Sci., R. Mil. Coll. of Canada, Kingston, Ont., Canada","IEEE Transactions on Software Engineering","","2006","32","5","315","329","There are many ideas in software design that are considered good practice. However, research is still needed to validate their contributions to software maintenance. This paper presents a method for examining software systems that have been actively maintained and used over the long term and are potential candidates for yielding lessons about design. The method relies on a criterion of stability and a definition of distance to flag design characteristics that have potentially contributed to long-term maintainability. It is demonstrated by application to an example of long-lived scientific software. The results from this demonstration show that the method can provide insight into the relative importance of individual elements of a set of design characteristics for the long-term evolution of software","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642679","Software evolution;scientific software;maintainability;distance metric;software design;stability;software architecture.","Stability criteria;Software systems;Software maintenance;Software design;History;Application software;Software architecture;Information resources;Mathematics;Extraterrestrial measurements","software architecture;software maintenance;software prototyping","software evolution;software design;software maintenance;software stability criterion;scientific software;distance metric;software architecture","","37","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Capsule-Based User Interface Modeling for Large-Scale Applications","D. Milicev; Z. Mijailovic","University of Belgrade, Belgrade; SOL Software, Belgrade","IEEE Transactions on Software Engineering","","2013","39","9","1190","1207","We present a novel approach to modeling and implementing user interfaces (UI) of large business applications. The approach is based on the concept of capsule, a profiled structured class from UML which models a simple UI component or a coherent UI fragment of logically and functionally coupled components or other fragments with a clear interface. Consequently, the same modeling concept of capsule with internal structure can be reapplied recursively at successively lower levels of detail within a model, starting from high architectural modeling levels down to the lowest levels of modeling simple UI components. The interface of capsules is defined in terms of pins, while the functional coupling of capsules is specified declaratively by simply wiring their pins. Pins and wires transport messages between capsules, ensuring strict encapsulation. The approach includes a method for formal coupling of capsules' behavior with the underlying object space that provides proper impedance matching between the UI and the business logic while preserving clear separation of concerns between them. We also briefly describe an implementation of a framework that supports the proposed method, including a rich library of ready-to-use capsules, and report on our experience in applying the approach in large-scale industrial systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6464270","Graphical user interface (GUI);Unified Modeling Language (UML);modeling;model-driven development;software architecture;business applications;data-centric applications;information systems","Unified modeling language;Business;Couplings;Complexity theory;Object oriented modeling;Buildings;User interfaces","business data processing;Unified Modeling Language;user interfaces","profiled structured class;ready-to-use capsule library;object space;business logic;functional capsule coupling;capsule interface;UI component;UI fragment;Unified Modeling Language;UML;capsule concept;business application;capsule-based user interface modeling","","3","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Synthesizing Multithreaded Code from Real-Time Object-Oriented Models via Schedulability-Aware Thread Derivation","S. Kim","Department of Information Communications Engineering, Hankuk University of Foreign Studies, Global Campus, San 89, Mohyun-myun, Cheoin-gu, Yongin-si, South Korea","IEEE Transactions on Software Engineering","","2014","40","4","413","426","One of the major difficulties in developing embedded systems with object-oriented modeling is to translate a designed model into code that satisfies required real-time performance. This paper proposes scenario-based implementation synthesis architecture with timing guarantee (SISAtime) that addresses these difficulties. The problems that SISAtime must solve are: how to synthesize multithreaded-code from a real-time object-oriented model; and how to design supporting development tools and runtime system architecture while ensuring that the scenarios in the system have minimal response times and the code satisfies the given timing constraints with a minimal number of threads. SISAtime provides a new scheduling algorithm which minimizes scenario response times. SISAtime also provides a new thread derivation method that derives tasks and maps tasks to threads while automatically assigning task scheduling attributes. We have fully implemented SISAtime by extending the RoseRT development tool that uses UML 2.0 as a modeling language, and we applied it to an existing industrial private branch exchange system. The performance evaluation results show that the response times, context switches, and the number of threads of the system with SISAtime were reduced by 21.6, 33.2, and 65.2 percent, respectively, compared to the system with the best known existing thread derivation method.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6617637","Multitasking;object-oriented design methods;real-time systems and embedded systems;system integration and implementation","Unified modeling language;Object oriented modeling;Message systems;Timing;Time factors;Real-time systems;Ports (Computers)","embedded systems;multi-threading;object-oriented methods;scheduling;software architecture;software performance evaluation;Unified Modeling Language","multithreaded code synthesis;real-time object-oriented models;schedulability-aware thread derivation;embedded systems;real-time performance evaluation;scenario-based implementation synthesis architecture-with-timing guarantee;SISAtime;development tools;runtime system architecture;timing constraints;scenario response time minimization;thread derivation method;task derivation;task mapping;automatic task scheduling attribute assignment;RoseRT development tool;UML 2.0;modeling language;industrial private branch exchange system;context switches","","4","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Governing Software Process Improvementsin Globally Distributed Product Development","N. Ramasubbu","Joseph M. Katz Graduate School of Business, University of Pittsburgh, 354 Mervis Hall, Pittsburgh","IEEE Transactions on Software Engineering","","2014","40","3","235","250","Continuous software process improvement (SPI) practices have been extensively prescribed to improve performance of software projects. However, SPI implementation mechanisms have received little scholarly attention, especially in the context of distributed software product development. We took an action research approach to study the SPI journey of a large multinational enterprise that adopted a distributed product development strategy. We describe the interventions and action research cycles enacted over a period of five years in collaboration with the firm, which resulted in a custom SPI framework that catered to both the social and technical needs of the firm's distributed teams. Institutionalizing the process maturity framework got stalled initially because the SPI initiatives were perceived by product line managers as a mechanism for exercising wider controls by the firm's top management. The implementation mechanism was subsequently altered to co-opt product line managers, which contributed to a wider adoption of the SPI framework. Insights that emerge from our analysis of the firm's SPI journey pertain to the integration of the technical and social views of software development, preserving process diversity through the use of a multi-tiered, non-blueprint approach to SPI, the linkage between key process areas and project control modes, and the role of SPI in aiding organizational learning.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.58","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6682900","Software process improvement (SPI);distributed teams;software engineering;process control;action research","Software;Product development;Process control;Benchmark testing;ISO standards;Resource management;Quality management","software management;software process improvement;software product lines","software process improvements;globally distributed software product development strategy;software project performance;large multinational enterprise;custom SPI framework;process maturity framework;product line managers;firm top management;multitiered nonblueprint approach;project control modes;organizational learning;action research approach","","11","","93","","","","","","IEEE","IEEE Journals & Magazines"
"Defining and Evaluating a Measure of Open Source Project Survivability","U. Raja; M. J. Tretter","The University of Alabama, Tuscaloosa; Texas A&M University, College Station","IEEE Transactions on Software Engineering","","2012","38","1","163","174","In this paper, we define and validate a new multidimensional measure of Open Source Software (OSS) project survivability, called Project Viability. Project viability has three dimensions: vigor, resilience, and organization. We define each of these dimensions and formulate an index called the Viability Index (VI) to combine all three dimensions. Archival data of projects hosted at SourceForge.net are used for the empirical validation of the measure. An Analysis Sample (n=136) is used to assign weights to each dimension of project viability and to determine a suitable cut-off point for VI. Cross-validation of the measure is performed on a hold-out Validation Sample (n=96). We demonstrate that project viability is a robust and valid measure of OSS project survivability that can be used to predict the failure or survival of an OSS project accurately. It is a tangible measure that can be used by organizations to compare various OSS projects and to make informed decisions regarding investment in the OSS domain.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6127835","Evaluation framework;external validity;open source software;project evaluation;software measurement;software survivability.","Software measurement;Indexes;Maintenance engineering","project management;public domain software;software metrics","open source project survivability;multidimensional measure;open source software project survivability;project viability;vigor;resilience;organization;viability index","","15","","69","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting Delivery Capability in Iterative Software Development","M. Choetkiertikul; H. K. Dam; T. Tran; A. Ghose; J. Grundy","Faculty of Engineering and Information Sciences, School of Computing and Information Technology, University of Wollongong, NSW, Australia; Faculty of Engineering and Information Sciences, School of Computing and Information Technology, University of Wollongong, NSW, Australia; School of Information Technology, Deakin University, Victoria, Australia; Faculty of Engineering and Information Sciences, School of Computing and Information Technology, University of Wollongong, NSW, Australia; School of Information Technology, Deakin University, Victoria, Australia","IEEE Transactions on Software Engineering","","2018","44","6","551","573","Iterative software development has become widely practiced in industry. Since modern software projects require fast, incremental delivery for every iteration of software development, it is essential to monitor the execution of an iteration, and foresee a capability to deliver quality products as the iteration progresses. This paper presents a novel, data-driven approach to providing automated support for project managers and other decision makers in predicting delivery capability for an ongoing iteration. Our approach leverages a history of project iterations and associated issues, and in particular, we extract characteristics of previous iterations and their issues in the form of features. In addition, our approach characterizes an iteration using a novel combination of techniques including feature aggregation statistics, automatic feature learning using the Bag-of-Words approach, and graph-based complexity measures. An extensive evaluation of the technique on five large open source projects demonstrates that our predictive models outperform three common baseline methods in Normalized Mean Absolute Error and are highly accurate in predicting the outcome of an ongoing iteration.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2693989","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7898472","Mining software engineering repositories;empirical software engineering;iterative software development","Software;Feature extraction;Predictive models;Data mining;Complexity theory;Iterative methods;Agile software development","graph theory;learning (artificial intelligence);project management;software development management;software prototyping;software quality","iterative software development;incremental delivery;data-driven approach;project iterations;open source projects;delivery capability;software projects;feature aggregation statistics;automatic feature learning;Bag-of-Words;graph-based complexity;Normalized Mean Absolute Error","","1","","88","","","","","","IEEE","IEEE Journals & Magazines"
"Preventing Defects: The Impact of Requirements Traceability Completeness on Software Quality","P. Rempel; P. Mäder","Software Engineering for Safety-Critical Systems Group, Technische Universität Ilmenau, Ilmenau, Germany; Software Engineering for Safety-Critical Systems Group, Technische Universität Ilmenau, Ilmenau, Germany","IEEE Transactions on Software Engineering","","2017","43","8","777","797","Requirements traceability has long been recognized as an important quality of a well-engineered system. Among stakeholders, traceability is often unpopular due to the unclear benefits. In fact, little evidence exists regarding the expected traceability benefits. There is a need for empirical work that studies the effect of traceability. In this paper, we focus on the four main requirements implementation supporting activities that utilize traceability. For each activity, we propose generalized traceability completeness measures. In a defined process, we selected 24 medium to large-scale open-source projects. For each software project, we quantified the degree to which a studied development activity was enabled by existing traceability with the proposed measures. We analyzed that data in a multi-level Poisson regression analysis. We found that the degree of traceability completeness for three of the studied activities significantly affects software quality, which we quantified as defect rate. Our results provide for the first time empirical evidence that more complete traceability decreases the expected defect rate in the developed software. The strong impact of traceability completeness on the defect rate suggests that traceability is of great practical value for any kind of software development project, even if traceability is not mandated by a standard or regulation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2622264","German Ministry of Education and Research (BMBF); Thüringer Aufbaubank (TAB); ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7723818","Requirements traceability;traceability completeness;traceability metrics;change impact analysis;requirements satisfaction analysis;source code justification analysis;software quality;error proneness;defects;bugs;empirical validation;regression analysis","Software quality;Software systems;Context;Software engineering;Stakeholders;Standards","public domain software;regression analysis;software quality;stochastic processes","software development project;multilevel Poisson regression analysis;software project;large-scale open-source projects;generalized traceability completeness measurement;requirement traceability completeness impact;software quality","","3","","131","","","","","","IEEE","IEEE Journals & Magazines"
"TCTL Inevitability Analysis of Dense-Time Systems: From Theory to Engineering","Farn Wang; Geng-Dian Huang; Fang Yu","IEEE Computer Society; NA; NA","IEEE Transactions on Software Engineering","","2006","32","7","510","526","Inevitability properties in branching temporal logics are of the syntax foralldiamphi, where phi is an arbitrary (timed) CTL (computation tree logic) formula. Such inevitability properties in dense-time logics can be analyzed with the greatest fixpoint calculation. We present algorithms to model-check inevitability properties. We discuss a technique for early decision on greatest fixpoint calculation which has shown promising performance against several benchmarks. We have experimented with various issues which may affect the performance of TCTL inevitability analysis. Specifically, our algorithms come with a parameter for the measurement of time-progress. We report the performance of our implementation with regard to various parameter values and with or without the non-Zeno computation requirement in the evaluation of greatest fixpoints. We have also experimented with safe abstraction techniques for model-checking TCTL inevitability properties. The experiment results help us in deducing rules for setting the parameter for verification performance. Finally, we summarize suggestions for configurations of efficient TCTL inevitability evaluation procedure","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.71","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677536","TCTL;real-time systems;inevitability;non-Zeno;model-checking;greatest fixpoint;abstraction.","Logic;Performance analysis;Safety;Computer Society;Real time systems;Automata;Performance evaluation;Reachability analysis;Algorithm design and analysis;State-space methods","automata theory;formal specification;formal verification;reachability analysis;temporal logic;trees (mathematics)","branching temporal logic;timed computation tree logic;dense-time system;greatest fixpoint calculation;model-checking;TCTL inevitability analysis;nonZeno computation;abstraction technique;formal verification;formal specification","","9","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic QoS Management and Optimization in Service-Based Systems","R. Calinescu; L. Grunske; M. Kwiatkowska; R. Mirandola; G. Tamburrelli","Aston University, Birmingham; Swinburne University of Technology, Swinburne; Oxford University Computing Laboratories, Oxford; Politecnico di Milano, Milano; Politecnico di Milano, Milano","IEEE Transactions on Software Engineering","","2011","37","3","387","409","Service-based systems that are dynamically composed at runtime to provide complex, adaptive functionality are currently one of the main development paradigms in software engineering. However, the Quality of Service (QoS) delivered by these systems remains an important concern, and needs to be managed in an equally adaptive and predictable way. To address this need, we introduce a novel, tool-supported framework for the development of adaptive service-based systems called QoSMOS (QoS Management and Optimization of Service-based systems). QoSMOS can be used to develop service-based systems that achieve their QoS requirements through dynamically adapting to changes in the system state, environment, and workload. QoSMOS service-based systems translate high-level QoS requirements specified by their administrators into probabilistic temporal logic formulae, which are then formally and automatically analyzed to identify and enforce optimal system configurations. The QoSMOS self-adaptation mechanism can handle reliability and performance-related QoS requirements, and can be integrated into newly developed solutions or legacy systems. The effectiveness and scalability of the approach are validated using simulations and a set of experiments based on an implementation of an adaptive service-based system for remote medical assistance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.92","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5611553","Service-oriented software engineering;QoS management;QoS optimization;adaptive systems.","Quality of service;Markov processes;Probabilistic logic;Unified modeling language;Analytical models;Optimization;Scattering","health care;medical computing;optimisation;quality of service;software engineering","QoS management;optimization;service-based systems;software engineering;QoSMOS;remote medical assistance;health care","","130","","102","","","","","","IEEE","IEEE Journals & Magazines"
"Design wizards and visual programming environments for GenVoca generators","D. Batory; Gang Chen; E. Robertson; Tao Wang","Dept. of Comput. Sci., Texas Univ., Austin, TX, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","5","441","452","Domain-specific generators will increasingly rely on graphical languages for declarative specifications of target applications. Such languages will provide front-ends to generators and related tools to produce customized code on demand. Critical to the success of this approach will be domain-specific design wizards, tools that guide users in their selection of components for constructing particular applications. The authors present the P3 ContainerStore graphical language, its generator, and design wizard.","0098-5589;1939-3520;2326-3881","","10.1109/32.846301","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846301","","Programming environments;Data structures;Application software;Java;Containers;DSL;Computer languages;Libraries;EMP radiation effects;Computer Society","application generators;programming environments;visual programming;visual languages;user interfaces","design wizards;visual programming environments;GenVoca generators;domain-specific generators;graphical languages;declarative specifications;target applications;front-ends;customized code;P3 ContainerStore graphical language","","35","","39","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic mining of source code repositories to improve bug finding techniques","C. C. Williams; J. K. Hollingsworth","Maryland Univ., College Park, MD, USA; Maryland Univ., College Park, MD, USA","IEEE Transactions on Software Engineering","","2005","31","6","466","480","We describe a method to use the source code change history of a software project to drive and help to refine the search for bugs. Based on the data retrieved from the source code repository, we implement a static source code checker that searches for a commonly fixed bug and uses information automatically mined from the source code repository to refine its results. By applying our tool, we have identified a total of 178 warnings that are likely bugs in the Apache Web server source code and a total of 546 warnings that are likely bugs in Wine, an open-source implementation of the Windows API. We show that our technique is more effective than the same static analysis that does not use historical data from the source code repository.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.63","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463230","Index Terms- Testing tools;version control;configuration control;debugging aids.","Computer bugs;Web server;Inspection;Detectors;History;Data mining;Information retrieval;Open source software;Debugging;Programming profession","data mining;Internet;application program interfaces;program testing;configuration management;public domain software;program diagnostics;program debugging;file servers","automatic mining;source code repository;bug finding technique;software project;data retrieval;static source code checker;Apache Web server;open-source implementation;Windows API;static analysis;historical data;testing tools;version control;configuration control;debugging aids","","74","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Version Histories for Detecting Code Smells","F. Palomba; G. Bavota; M. D. Penta; R. Oliveto; D. Poshyvanyk; A. De Lucia","University of Salerno, Fisciano, SA, Italy; Free University of Bozen-Bolzano, Bolzano, Italy; University of Sannio, Benevento, Italy; University of Molise, Pesche, IS, Italy; College of William and Mary, Williamsburg, VA; University of Salerno, Fisciano, SA, Italy","IEEE Transactions on Software Engineering","","2015","41","5","462","489","Code smells are symptoms of poor design and implementation choices that may hinder code comprehension, and possibly increase changeand fault-proneness. While most of the detection techniques just rely on structural information, many code smells are intrinsically characterized by how code elements change overtime. In this paper, we propose Historical Information for Smell deTection (HIST), an approach exploiting change history information to detect instances of five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy. We evaluate HIST in two empirical studies. The first, conducted on 20 open source projects, aimed at assessing the accuracy of HIST in detecting instances of the code smells mentioned above. The results indicate that the precision of HIST ranges between 72 and 86 percent, and its recall ranges between 58 and 100 percent. Also, results of the first study indicate that HIST is able to identify code smells that cannot be identified by competitive approaches solely based on code analysis of a single system's snapshot. Then, we conducted a second study aimed at investigating to what extent the code smells detected by HIST (and by competitive code analysis techniques) reflect developers' perception of poor design and implementation choices. We involved 12 developers of four open source projects that recognized more than 75 percent of the code smell instances identified by HIST as actual design/implementation problems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2372760","EU; grants; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6963448","Code smells;mining software repositories;empirical studies;Code smells;mining software repositories;empirical studies","History;Feature extraction;Surgery;Accuracy;Association rules;Detectors","data mining;program compilers;public domain software","code smell detection;historical information for smell detection;divergent change;shotgun surgery;parallel inheritance;blob;feature envy;HIST;code analysis;single system snapshot;open source project;mining version history","","62","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Strategies for software reuse: a principal component analysis of reuse practices","M. A. Rothenberger; K. J. Dooley; U. R. Kulkarni; N. Nada","Sch. of Bus. Adm., Wisconsin Univ., Milwaukee, WI, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","9","825","837","This research investigates the premise that the likelihood of success of software reuse efforts may vary with the reuse strategy employed and, hence, potential reuse adopters must be able to understand reuse strategy alternatives and their implications. We use survey data collected from 71 software development groups to empirically develop a set of six dimensions that describe the practices employed in reuse programs. The study investigates the patterns in which these practices co-occur in the real world, demonstrating that the dimensions cluster into five distinct reuse strategies, each with a different potential for reuse success. The findings provide a means to classify reuse settings and assess their potential for success.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232287","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232287","","Principal component analysis;Software reusability;Software quality;Software systems;Programming;Software maintenance;Writing;Best practices;Costs;Investments","software reusability;principal component analysis;software quality;object-oriented programming;bibliographies","reuse strategy;software process improvement;reuse classification scheme;best practices;software development groups;reuse programs;systematic software reuse","","53","","66","","","","","","IEEE","IEEE Journals & Magazines"
"Structural Complexity and Programmer Team Strategy: An Experimental Test","N. Ramasubbu; C. F. Kemerer; J. Hong","University of Pittsburgh, Pittsburgh; University of Pittsburgh, Pittsburgh and King Abdul Aziz University, Saudi Arabia; Singapore Management University, Singapore","IEEE Transactions on Software Engineering","","2012","38","5","1054","1068","This study develops and empirically tests the idea that the impact of structural complexity on perfective maintenance of object-oriented software is significantly determined by the team strategy of programmers (independent or collaborative). We analyzed two key dimensions of software structure, coupling and cohesion, with respect to the maintenance effort and the perceived ease-of-maintenance by pairs of programmers. Hypotheses based on the distributed cognition and task interdependence theoretical frameworks were tested using data collected from a controlled lab experiment employing professional programmers. The results show a significant interaction effect between coupling, cohesion, and programmer team strategy on both maintenance effort and perceived ease-of-maintenance. Highly cohesive and low-coupled programs required lower maintenance effort and were perceived to be easier to maintain than the low-cohesive programs and high-coupled programs. Further, our results would predict that managers who strategically allocate maintenance tasks to either independent or collaborative programming teams depending on the structural complexity of software could lower their team's maintenance effort by as much as 70 percent over managers who use simple uniform resource allocation policies. These results highlight the importance of achieving congruence between team strategies employed by collaborating programmers and the structural complexity of software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5999673","Object-oriented programming;complexity measures;software quality;software productivity;programming teams;maintenance process;CK metrics;software management","Maintenance engineering;Complexity theory;Couplings;Collaboration;Software;Programming profession","computational complexity;object-oriented programming;resource allocation;software maintenance","programmer team strategy;structural complexity;object-oriented software;perfective maintenance;software structure;perceived ease-of-maintenance;distributed cognition;task interdependence;controlled lab experiment;professional programmers;significant interaction;low-cohesive programs;high-coupled programs;collaborative programming teams;resource allocation policies","","6","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Learning Project Management Decisions: A Case Study with Case-Based Reasoning versus Data Farming","T. Menzies; A. Brady; J. Keung; J. Hihn; S. Williams; O. El-Rawas; P. Green; B. Boehm","West Virginia University, Morgantown; West Virginia University, Morgantown; The City University of Hong Kong, Hong Kong; California Institute of Technology, Pasadena; Indiana University, Bloomington; West Virginia University, Morgantown; West Virginia University, Morgantown; University of Southern California, Los Angeles","IEEE Transactions on Software Engineering","","2013","39","12","1698","1713","Background: Given information on just a few prior projects, how do we learn the best and fewest changes for current projects? Aim: To conduct a case study comparing two ways to recommend project changes. 1) Data farmers use Monte Carlo sampling to survey and summarize the space of possible outcomes. 2) Case-based reasoners (CBR) explore the neighborhood around test instances. Method: We applied a state-of-the data farmer (SEESAW) and a CBR tool ()'V2) to software project data. Results: CBR with )'V2 was more effective than SEESAW's data farming for learning best and recommended project changes, effectively reducing runtime, effort, and defects. Further, CBR with )'V2 was comparably easier to build, maintain, and apply in novel domains, especially on noisy data sets. Conclusion: Use CBR tools like )'V2 when data are scarce or noisy or when project data cannot be expressed in the required form of a data farmer. Future Work: This study applied our own CBR tool to several small data sets. Future work could apply other CBR tools and data farmers to other data (perhaps to explore other goals such as, say, minimizing maintenance effort).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6600685","Search-based software engineering;case-based reasoning;data farming;COCOMO","Data models;Project management;Search methods;Monte Carlo methods;Mathematical model;Software engineering","case-based reasoning;data handling;learning (artificial intelligence);Monte Carlo methods;project management;sampling methods;software management","project management decision learning;case-based reasoning;data farming;Monte Carlo sampling;CBR;SEESAW;software project data","","5","","101","","","","","","IEEE","IEEE Journals & Magazines"
"On Accelerating Source Code Analysis at Massive Scale","G. Upadhyaya; H. Rajan","Department of Computer Science, Iowa State University, Ames, IA; Department of Computer Science, Iowa State University, Ames, IA","IEEE Transactions on Software Engineering","","2018","44","7","669","688","Encouraged by the success of data-driven software engineering (SE) techniques that have found numerous applications e.g., in defect prediction, specification inference, the demand for mining and analyzing source code repositories at scale has significantly increased. However, analyzing source code at scale remains expensive to the extent that data-driven solutions to certain SE problems are beyond our reach today. Extant techniques have focused on leveraging distributed computing to solve this problem, but with a concomitant increase in computational resource needs. This work proposes a technique that reduces the amount of computation performed by the ultra-large-scale source code mining task, especially those that make use of control and data flow analyses. Our key idea is to analyze the mining task to identify and remove the irrelevant portions of the source code, prior to running the mining task. We show a realization of our insight for mining and analyzing massive collections of control flow graphs of source codes. Our evaluation using 16 classical control-/data-flow analyses that are typical components of mining tasks and 7 Million CFGs shows that our technique can achieve on average a 40 percent reduction in the task computation time. Our case studies demonstrates the applicability of our technique to massive scale source code mining tasks.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2828848","US National Science Foundation (NSF); US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8344505","Source code analysis;mining software repositories;ultra-large-scale mining;data-driven software engineering","Task analysis;Data mining;Acceleration;Static analysis;Software engineering;Distributed computing;Software","data flow analysis;data mining;software engineering;source code (software)","control-flow analyses;ultralarge-scale source code mining task;source code repository analysis;massive scale source code mining tasks;task computation time;control flow graphs;data flow analyses;specification inference;defect prediction;data-driven software engineering techniques","","","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Saturation for a General Class of Models","A. S. Miner","IEEE Computer Society","IEEE Transactions on Software Engineering","","2006","32","8","559","570","Implicit techniques for construction and representation of the reachability set of a high-level model have become quite efficient for certain types of models. In particular, previous work developed a ""saturation"" algorithm that exploits asynchronous behavior to efficiently construct the reachability set using multiway decision diagrams, but using a Kronecker product expression to represent each model event. For models whose events do not naturally fall into this category, use of the saturation algorithm requires adjusting the model by combining components or splitting events into subevents until a Kronecker product expression is possible. In practice, this can lead to additional overheads during reachability set construction. This paper presents a new version of the saturation algorithm that works for a general class of models: models whose events are not necessarily expressible as Kronecker products, models containing events with complex priority structures, and models whose state variables have unknown bounds. Experimental results are given for several examples","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.81","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703387","Model checking;stochastic analysis.","Data structures;Explosions;Boolean functions;Computer Society;Stochastic processes;Formal verification;System recovery;Performance analysis;Degradation","decision diagrams;formal verification;Markov processes;reachability analysis;set theory","reachability set construction;multiway decision diagram;Kronecker product expression;model event representation;saturation algorithm;model checking;stochastic analysis","","7","","37","","","","","","IEEE","IEEE Journals & Magazines"
"A Scalable Approach to Exact Model and Commonality Counting for Extended Feature Models","D. Fernandez-Amoros; R. Heradio; J. A. Cerrada; C. Cerrada","Department of Languages and Computer Systems, Spanish Open University (UNED), Madrid, Spain; Department of Software Engineering and Computer Systems, Spanish Open University (UNED), Madrid, Spain; Department of Software Engineering and Computer Systems, Spanish Open University (UNED), Madrid, Spain; Department of Software Engineering and Computer Systems, Spanish Open University (UNED), Madrid, Spain","IEEE Transactions on Software Engineering","","2014","40","9","895","910","A software product line is an engineering approach to efficient development of software product portfolios. Key to the success of the approach is to identify the common and variable features of the products and the interdependencies between them, which are usually modeled using feature models. Implicitly, such models also include valuable information that can be used by economic models to estimate the payoffs of a product line. Unfortunately, as product lines grow, analyzing large feature models manually becomes impracticable. This paper proposes an algorithm to compute the total number of products that a feature model represents and, for each feature, the number of products that implement it. The inference of both parameters is helpful to describe the standardization/parameterization balance of a product line, detect scope flaws, assess the product line incremental development, and improve the accuracy of economic models. The paper reports experimental evidence that our algorithm has better runtime performance than existing alternative approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2331073","Spanish Government; Comunidad de Madrid; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6835200","Feature models;formal methods;economic models;software product lines","Frequency modulation;Computational modeling;Analytical models;Software;Economics;Headphones;Portfolios","software engineering;software product lines","product line incremental development;software product portfolio development;software product line;commonality counting;extended feature models","","4","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Model-based performance prediction in software development: a survey","S. Balsamo; A. Di Marco; P. Inverardi; M. Simeoni","Dipt. di Informatica, Universita di Venezia, Italy; NA; NA; NA","IEEE Transactions on Software Engineering","","2004","30","5","295","310","Over the last decade, a lot of research has been directed toward integrating performance analysis into the software development process. Traditional software development methods focus on software correctness, introducing performance issues later in the development process. This approach does not take into account the fact that performance problems may require considerable changes in design, for example, at the software architecture level, or even worse at the requirement analysis level. Several approaches were proposed in order to address early software performance analysis. Although some of them have been successfully applied, we are still far from seeing performance analysis integrated into ordinary software development. In this paper, we present a comprehensive review of recent research in the field of model-based performance prediction at software development time in order to assess the maturity of the field and point out promising research directions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291833","Software verification;performance modeling and prediction;integrated environments.","Predictive models;Programming;Software performance;Performance analysis;Software systems;Software architecture;Availability;Runtime;System software;Automation","formal verification;software process improvement;software performance evaluation","software development process;software architecture level;requirement analysis level;software performance analysis;model-based performance prediction;software verification","","296","","82","","","","","","IEEE","IEEE Journals & Magazines"
"The structural complexity of software an experimental test","D. P. Darcy; C. F. Kemerer; S. A. Slaughter; J. E. Tomayko","Robert H. Smith Sch. of Bus., Maryland Univ., College Park, MD, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","11","982","995","This research examines the structural complexity of software and, specifically, the potential interaction of the two dominant dimensions of structural complexity, coupling and cohesion. Analysis based on an information processing view of developer cognition results in a theoretically driven model with cohesion as a moderator for a main effect of coupling on effort. An empirical test of the model was devised in a software maintenance context utilizing both procedural and object-oriented tasks, with professional software engineers as participants. The results support the model in that there was a significant interaction effect between coupling and cohesion on effort, even though there was no main effect for either coupling or cohesion. The implication of this result is that, when designing, implementing, and maintaining software to control complexity, both coupling and cohesion should be considered jointly, instead of independently. By providing guidance on structuring software for software professionals and researchers, these results enable software to continue as the solution of choice for a wider range of richer, more complex problems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.130","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556556","Index Terms- Software complexity;software structure;Wood's model of task complexity;coupling;cohesion;experiment;software maintenance;software metrics;cognition;procedural programming;object-oriented programming.","Software testing;Software maintenance;Software quality;Object oriented modeling;Cognition;Software measurement;Productivity;Computer Society;Object oriented programming;Quality management","software maintenance;software metrics;professional aspects;cognition;object-oriented programming","software structural complexity;experimental test;coupling;cohesion;information processing;software maintenance;software professionals;software metrics;cognition;procedural programming;object-oriented programming","","60","","55","","","","","","IEEE","IEEE Journals & Magazines"
"Mutation-Driven Generation of Unit Tests and Oracles","G. Fraser; A. Zeller","Saarland University, Saarbrücken; Saarland University, Saarbrücken","IEEE Transactions on Software Engineering","","2012","38","2","278","292","To assess the quality of test suites, mutation analysis seeds artificial defects (mutations) into programs; a nondetected mutation indicates a weakness in the test suite. We present an automated approach to generate unit tests that detect these mutations for object-oriented classes. This has two advantages: First, the resulting test suite is optimized toward finding defects modeled by mutation operators rather than covering code. Second, the state change caused by mutations induces oracles that precisely detect the mutants. Evaluated on 10 open source libraries, our μtest prototype generates test suites that find significantly more seeded defects than the original manually written test suites.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.93","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6019060","Mutation analysis;test case generation;unit testing;test oracles;assertions;search-based testing.","Testing;Genetic algorithms;Biological cells;Software;Software algorithms;Generators;Libraries","automatic test pattern generation;object-oriented programming;optimisation;program testing","mutation driven generation;unit test;oracle;test suites quality;artificial defects;automated test case generation;object-oriented classes;optimization;mutation operators;open source libraries","","85","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Design and implementation of a VBR continuous media file server","D. Makaroff; G. Neufeld; N. Hutchinson","Sch. of Inf. Technol. & Eng., Ottawa Univ., Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2001","27","1","13","28","We describe the design and implementation of a file server for variable bit rate continuous media. We address the problem of building a server where each stream may have a different bit rate and, more importantly, where the bit rate within a single stream may vary considerably. Such a server has been implemented within a high-speed network environment. The server is designed to be used in a heterogeneous environment and is linearly scalable. A significant aspect of the design of the system is the detailed consideration of the variable bit-rate profile of each data stream in performing admission control for the disk and for the network. This paper describes the system model, the user interface design, implementation details, and performance results based on initial experience with the server.","0098-5589;1939-3520;2326-3881","","10.1109/32.895985","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=895985","","File servers;Network servers;Bit rate;Admission control;User interfaces;Bandwidth;Scalability;Availability;Buildings;High-speed networks","multimedia servers;file servers;variable rate codes;user interfaces","continuous media file server;variable bit rate continuous media;high-speed network;heterogeneous environment;linearly scalable server;data stream;admission control;system model;user interface design;performance results;multimedia server","","2","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Software reuse in product populations","R. van Ommering","Philips Res. Lab., Eindhoven, Netherlands","IEEE Transactions on Software Engineering","","2005","31","7","537","550","Consumer products are becoming increasingly software intensive. The software complexity of individual products grows, while the diversity of products increases and the lead time must decrease. Software reuse is the answer to this, not only within a family but also between families of consumer products. We have devised an approach based upon a software component technology to enable reuse. This paper describes that approach, and it zooms in on two important aspects of component-based development. One aspect concerns the prediction of system properties from properties of components, which we illustrate using thread synchronization as example. The other aspect concerns branching of our software in our configuration management systems, where our analysis leads to the discovery that we may be constantly rewriting our own code and to the definition of the turn-over factor to quantify this. We end this paper with a brief validation of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.84","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492370","Index Terms- Software reuse;software components;component-based software architectures;software product families;software product lines.","TV;Consumer electronics;Consumer products;Software architecture;Home appliances;Computer industry;Electronics industry;Software systems;Application software","software reusability;object-oriented programming;configuration management;software architecture","software reuse;software intensive consumer products;software complexity;software component technology;component-based development;thread synchronization;configuration management system;component-based software architecture;software product family;software product line","","33","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Model Checking Software with First Order Logic Specifications Using AIG Solvers","M. A. Noureddine; F. A. Zaraket","Department of Computer Science, University of Illinois at Urbana Champaign, IL; Department of Electrical and Computer Engineering, American University of Beirut, Beirut, Lebanon","IEEE Transactions on Software Engineering","","2016","42","8","741","763","Static verification techniques leverage Boolean formula satisfiability solvers such as SAT and SMT solvers that operate on conjunctive normal form and first order logic formulae, respectively, to validate programs. They force bounds on variable ranges and execution time and translate the program and its specifications into a Boolean formula. They are limited to programs of relatively low complexity for the following reasons. (1) A small increase in the bounds can cause a large increase in the size of the translated formula. (2) Boolean satisfiability solvers are restricted to using optimizations that apply at the level of the formula. Finally, (3) the Boolean formulae often need to be regenerated with higher bounds to ensure the correctness of the translation. We present a method that uses And-Inverter-Graph (AIG) sequential circuits, and AIG synthesis and verification frameworks to validate programs. An AIG is a Boolean formula with memory elements, logically complete negated conjunction gates, and a hierarchical structure. Encoding the validation problem of a program as an AIG (1) typically provides a more succinct representation than a Boolean formulae encoding with no memory elements, (2) preserves the high-level structure of the program, and (3) enables the use of a number of powerful automated analysis techniques that have no counterparts for other Boolean formulae such as CNF. Our method takes an imperative program with a first order logic specification consisting of a precondition and a postcondition pair, and a bound on the program variable ranges, and produces an AIG with a designated output that is<inline-formula><tex-math notation=""LaTeX"">${true}$</tex-math><alternatives><inline-graphic xlink:href=""zaraket-ieq1-2520468.gif"" xlink:type=""simple"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>when the program violates the specification. Our method uses AIG synthesis reduction techniques to reduce the AIG, and then uses AIG verification techniques to check the satisfiability of the designated output. The results show that our method can validate designs that are not possible with other state of the art techniques, and with bounds that are an order of magnitude larger.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2520468","University Research Board; American University of Beirut; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7389426","Software verification;static analysis;Boolean satisfiability solvers;Hoare triplet","Sequential circuits;Model checking;Software;Encoding;Optimization;Radiation detectors;Interpolation","","","","1","","57","","","","","","IEEE","IEEE Journals & Magazines"
"Tools for the Rapid Prototyping of Provably Correct Ambient Intelligence Applications","A. Coronato; G. De Pietro","National Research Council (CNR), Naples; National Research Council (CNR), Naples","IEEE Transactions on Software Engineering","","2012","38","4","975","991","Ambient Intelligence technologies have not yet been widely adopted in safety critical scenarios. This principally has been due to fact that acceptable degrees of dependability have not been reached for the applications that rely on such technologies. However, the new critical application domains, like Ambient Assisted Living and Smart Hospitals, which are currently emerging, are increasing the need for methodologies and tools that can improve the reliability of the final systems. This paper presents a middleware architecture for safety critical Ambient Intelligence applications which provides the developer with services for runtime verification. It is now possible to continuously monitor and check the running system against correctness properties defined at design time. Moreover, a visual tool which allows the formal design of several of the characteristics of an Ambient Intelligence application and the automatic generation of setting up parameters and code for the middleware infrastructure is also presented.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5963693","Safety critical ambient intelligence systems;middleware infrastructures;designing tools","Calculus;Runtime;Middleware;Ambient intelligence;Monitoring;Biomembranes;Mobile communication","middleware;safety-critical software;software prototyping;ubiquitous computing","rapid prototyping;ambient intelligence applications;safety critical scenarios;new critical application domains;ambient assisted living;smart hospitals;middleware architecture;runtime verification;visual tool;formal design;automatic generation;pervasive computing","","15","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic Software Refactoring via Weighted Clustering in Method-Level Networks","Y. Wang; H. Yu; Z. Zhu; W. Zhang; Y. Zhao","Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China","IEEE Transactions on Software Engineering","","2018","44","3","202","236","In this study, we describe a system-level multiple refactoring algorithm, which can identify the move method, move field, and extract class refactoring opportunities automatically according to the principle of “high cohesion and low coupling.” The algorithm works by merging and splitting related classes to obtain the optimal functionality distribution from the system-level. Furthermore, we present a weighted clustering algorithm for regrouping the entities in a system based on merged method-level networks. Using a series of preprocessing steps and preconditions, the “bad smells” introduced by cohesion and coupling problems can be removed from both the non-inheritance and inheritance hierarchies without changing the code behaviors. We rank the refactoring suggestions based on the anticipated benefits that they bring to the system. Based on comparisons with related research and assessing the refactoring results using quality metrics and empirical evaluation, we show that the proposed approach performs well in different systems and is beneficial from the perspective of the original developers. Finally, an open source tool is implemented to support the proposed approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2679752","National Natural Science Foundation of China; MOE research center for online education, China; Ph.D. Start-up Foundation of Liaoning Province, China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7874207","Clustering analysis;cohesion;coupling;complex network;software refactoring","Couplings;Clustering algorithms;Software algorithms;Measurement;Software systems;Partitioning algorithms","network theory (graphs);pattern clustering;software maintenance;software quality","high cohesion;low coupling;merging classes;splitting related classes;optimal functionality distribution;weighted clustering algorithm;merged method-level networks;preprocessing steps;coupling problems;inheritance hierarchies;refactoring suggestions;refactoring results;automatic software refactoring;system-level multiple refactoring algorithm;class refactoring opportunities;open source tool","","","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Towards Model Checking Android Applications","G. Bai; Q. Ye; Y. Wu; H. Botha; J. Sun; Y. Liu; J. S. Dong; W. Visser","Singapore Institute of Technology, Singapore; National University of Singapore, Singapore; Huawei; Stellenbosch University, Stellenbosch, South Africa; Singapore University of Technology and Design, Singapore; Nanyang Technological University, Singapore; National University of Singapore, Singapore; Stellenbosch University, Stellenbosch, South Africa","IEEE Transactions on Software Engineering","","2018","44","6","595","612","As feature-rich Android applications (apps for short) are increasingly popularized in security-sensitive scenarios, methods to verify their security properties are highly desirable. Existing approaches on verifying Android apps often have limited effectiveness. For instance, static analysis often suffers from a high false-positive rate, whereas approaches based on dynamic testing are limited in coverage. In this work, we propose an alternative approach, which is to apply the software model checking technique to verify Android apps. We have built a general framework named DroidPF upon Java PathFinder (JPF), towards model checking Android apps. In the framework, we craft an executable mock-up Android OS which enables JPF to dynamically explore the concrete state spaces of the tested apps; we construct programs to generate user interaction and environmental input so as to drive the dynamic execution of the apps; and we introduce Android specific reduction techniques to help alleviate the state space explosion. DroidPF focuses on common security vulnerabilities in Android apps including sensitive data leakage involving a non-trivial flow- and context-sensitive taint-style analysis. DroidPF has been evaluated with 131 apps, which include real-world apps, third-party libraries, malware samples and benchmarks for evaluating app analysis techniques like ours. DroidPF precisely identifies nearly all of the previously known security issues and nine previously unreported vulnerabilities/bugs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2697848","Singapore NRF; NRF; National Research Foundation, Prime Ministers Office, Singapore; National Cybersecurity R&D Program; National Cybersecurity R&D Directorate; National University of Singapore and Singapore University of Technology and Design; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7911333","Software model checking;security verification;android application","Androids;Humanoid robots;Model checking;Java;Security;Software;Libraries","Android (operating system);invasive software;Java;mobile computing;program diagnostics;program verification;public domain software","security properties;DroidPF;Android OS;tested apps;Android specific reduction techniques;context-sensitive taint-style analysis;real-world apps;app analysis techniques;feature-rich Android applications;security-sensitive scenarios;model checking;Android apps verification;Java PathFinder;concrete state spaces;user interaction;environmental input;dynamic execution;state space explosion;sensitive data leakage;nontrivial flow-style analysis;third-party libraries;malware samples","","1","","75","","","","","","IEEE","IEEE Journals & Magazines"
"Making CEGAR More Efficient in Software Model Checking","C. Tian; Z. Duan; Z. Duan","ICTT and ISN Laboratory, Xidian University, Xi’an, China; ICTT and ISN Laboratory, Xidian University, Xi’an, China; ICTT and ISN Laboratory, Xidian University, Xi’an, China","IEEE Transactions on Software Engineering","","2014","40","12","1206","1223","Counter-example guided abstraction refinement (CEGAR) is widely used in software model checking. With an abstract model, the state space is largely reduced, however, a counterexample found in such a model that does not satisfy the desired property may not exist in the concrete model. Therefore, how to check whether a reported counterexample is spurious is a key problem in the abstraction-refinement loop. Next, in the case that a spurious counterexample is found, the abstract model needs to be further refined where an NP-hard state separation problem is often involved. Thus, how to refine the abstract model efficiently has attracted a great attention in the past years. In this paper, by re-analyzing spurious counterexamples, a new formal definition of spurious paths is given. Based on it, efficient algorithms for detecting spurious counterexamples are presented. By the new algorithms, when dealing with infinite counterexamples, the finite prefix to be analyzed will be polynomially shorter than the one dealt with by the existing algorithms. Moreover, in practical terms, the new algorithms can naturally be parallelized that enables multi-core processors contributes more in spurious counterexample checking. In addition, a novel refining approach by adding extra Boolean variables to the abstract model is presented. With this approach, not only the NP-hard state separation problem can be avoided, but also a smaller refined abstract model can be obtained. Experimental results show that the new algorithms perform well in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2357442","NSFC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6895263","Model checking;formal verification;abstraction;refinement;CEGAR","Model checking;Software design;Computational modeling;Benchmark testing","multiprocessing systems;parallel algorithms;program verification","Boolean variables;spurious counterexample checking;multicore processors;parallelized algorithms;polynomially-shorter finite prefix;infinite counterexamples;spurious counterexample detection;spurious paths;spurious counterexample reanalysis;NP-hard state separation problem;abstraction-refinement loop;state space reduction;abstract model;counter-example guided abstraction refinement;software model checking;CEGAR","","13","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling the dynamic behavior of hypermedia applications","P. Diaz; I. Aedo; F. Panetsos","Dept. de Inf., Univ. Carlos III, Madrid, Spain; NA; NA","IEEE Transactions on Software Engineering","","2001","27","6","550","572","Hypermedia applications can be defined as collections of interactive and multimedia documents that are organized as a hypertext net. The development of hypermedia applications poses specific problems, such as the need for modeling sophisticated navigational structures, interactive behaviors, and harmonic presentations involving the synchronization of contents. Moreover, the increasing popularity of Internet based systems has put stress on the lack of mechanisms to formally specify security policies when designing hypermedia applications. Traditional design models and methodologies are not suitable for hypermedia applications and the up-to-now developed hypermedia oriented models do not cover the whole set of design needs. In this context, we present Labyrinth, a hypermedia oriented model providing formal elements to describe the static structure and dynamic behavior of this kind of nonlinear, multisensory, and interactive applications.","0098-5589;1939-3520;2326-3881","","10.1109/32.926176","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=926176","","Application software;Navigation;Internet;Stress;Design methodology;Information security;Context modeling;Information geometry;Multimedia systems;Computer languages","bibliographies;hypermedia;interactive systems;user interfaces;formal specification","dynamic behavior modeling;hypermedia applications;interactive documents;multimedia documents;hypertext net;navigational structures;interactive behaviors;harmonic presentations;Internet based systems;security policies;traditional design models;hypermedia oriented models;design needs;Labyrinth;hypermedia oriented model;formal elements;static structure;dynamic behavior;multisensory interactive applications","","20","","56","","","","","","IEEE","IEEE Journals & Magazines"
"Does Socio-Technical Congruence Have an Effect on Software Build Success? A Study of Coordination in a Software Project","I. Kwan; A. Schroter; D. Damian","University of Victoria, Victoria; University of Victoria, Victoria; University of Victoria, Victoria","IEEE Transactions on Software Engineering","","2011","37","3","307","324","Socio-technical congruence is an approach that measures coordination by examining the alignment between the technical dependencies and the social coordination in the project. We conduct a case study of coordination in the IBM Rational Team Concert project, which consists of 151 developers over seven geographically distributed sites, and expect that high congruence leads to a high probability of successful builds. We examine this relationship by applying two congruence measurements: an unweighted congruence measure from previous literature, and a weighted measure that overcomes limitations of the existing measure. We discover that there is a relationship between socio-technical congruence and build success probability, but only for certain build types, and observe that in some situations, higher congruence actually leads to lower build success rates. We also observe that a large proportion of zero-congruence builds are successful, and that socio-technical gaps in successful builds are larger than gaps in failed builds. Analysis of the social and technical aspects in IBM Rational Team Concert allows us to discuss the effects of congruence on build success. Our findings provide implications with respect to the limits of applicability of socio-technical congruence and suggest further improvements of socio-technical congruence to study coordination.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5740929","Empirical software engineering;socio-technical congruence;coordination;awareness;software quality;integration.","Software;Weight measurement;Programming;Software measurement;Collaboration;Software engineering;Context","social aspects of automation;software development management","socio-technical congruence;software build success;software project;social coordination;congruence measurements;unweighted congruence measure;weighted measure","","45","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Data Mining Static Code Attributes to Learn Defect Predictors","T. Menzies; J. Greenwald; A. Frank","Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV; Department of Computer Science, Portland State University, Portland, OR; Department of Computer Science, Portland State University, Portland, OR","IEEE Transactions on Software Engineering","","2007","33","1","2","13","The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of ""McCabes versus Halstead versus lines of code counts"" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256941","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027145","Data mining detect prediction;McCabe;Halstead;artifical intelligence;empirical;naive Bayes.","Data mining;Bayesian methods;Artificial intelligence;Software testing;System testing;Learning systems;Art;Software quality;Software systems;Financial management","data mining;learning (artificial intelligence);program diagnostics;program testing;software quality","data mining;static code attributes;defect predictor learning;McCabes versus Halstead;lines of code counts;resource-bound exploration","","496","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Architecture-Based Reliability Prediction with the Palladio Component Model","F. Brosch; H. Koziolek; B. Buhnova; R. Reussner","FZI Forschungszentrum Informatik, Karlsruhe; ABB Corporate Research, Ladenburg; Masaryk University, Brno; Karlsruhe Institute of Technology (KIT), Karlsruhe","IEEE Transactions on Software Engineering","","2012","38","6","1319","1339","With the increasing importance of reliability in business and industrial software systems, new techniques of architecture-based reliability engineering are becoming an integral part of the development process. These techniques can assist system architects in evaluating the reliability impact of their design decisions. Architecture-based reliability engineering is only effective if the involved reliability models reflect the interaction and usage of software components and their deployment to potentially unreliable hardware. However, existing approaches either neglect individual impact factors on reliability or hard-code them into formal models, which limits their applicability in component-based development processes. This paper introduces a reliability modeling and prediction technique that considers the relevant architectural factors of software systems by explicitly modeling the system usage profile and execution environment and automatically deriving component usage profiles. The technique offers a UML-like modeling notation whose models are automatically transformed into a formal analytical model. Our work builds upon the Palladio Component Model (PCM), employing novel techniques of information propagation and reliability assessment. We validate our technique with sensitivity analyses and simulation in two case studies. The case studies demonstrate effective support of usage profile analysis and architectural configuration ranking, together with the employment of reliability-improving architecture tactics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.94","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6018968","Software architectures;quality analysis and evaluation;reliability;design tools and techniques","Unified modeling language;Software reliability;Markov processes;Phase change materials;Software architecture;Software quality;Design methodology","object-oriented programming;software architecture;software reliability;Unified Modeling Language","architecture based reliability prediction;palladio component model;industrial software system;architecture based reliability engineering;assist system architects;reliability impact;software component;component based development process;reliability modeling;system usage profile;execution environment;component usage profiles;UML like modeling notation;formal analytical model;information propagation;reliability assessment;sensitivity analysis;usage profile analysis;architectural configuration ranking;architecture tactics","","36","","63","","","","","","IEEE","IEEE Journals & Magazines"
"Specifying Dynamic Analyses by Extending Language Semantics","A. Lienhard; T. Girba; O. Nierstrasz","University of Bern, Bern; University of Bern, Bern; University of Bern, Bern","IEEE Transactions on Software Engineering","","2012","38","3","694","706","Dynamic analysis is increasingly attracting attention for debugging, profiling, and program comprehension. Ten to twenty years ago, many dynamic analyses investigated only simple method execution traces. Today, in contrast, many sophisticated dynamic analyses exist, for instance, for detecting memory leaks, analyzing ownership properties, measuring garbage collector performance, or supporting debugging tasks. These analyses depend on complex program instrumentations and analysis models, making it challenging to understand, compare, and reproduce the proposed approaches. While formal specifications and proofs are common in the field of static analysis, most dynamic analyses are specified using informal, textual descriptions. In this paper, we propose a formal framework using operational semantics that allows researchers to precisely specify their dynamic analysis. Our goal is to provide an accessible and reusable basis on which researchers who may not be familiar with rigorous specifications of dynamic analyses can build. By extending the provided semantics, one can concisely specify how runtime events are captured and how this data is transformed to populate the analysis model. Furthermore, our approach provides the foundations to reason about properties of a dynamic analysis.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5740933","Dynamic analysis;formal definitions and theory;tracing;debugging.","Semantics;Runtime;Arrays;Context;Analytical models;Performance analysis;Syntactics","formal specification;program debugging;programming language semantics","dynamic analysis specification;program debugging;memory leaks detection;ownership properties;garbage collector performance;complex program instrumentations;formal specifications;textual descriptions;informal descriptions;language semantics extending;program comprehension;program profiling","","","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Swarm Verification Techniques","G. J. Holzmann; R. Joshi; A. Groce","California Institute of Technology, Pasadena; California Institute of Technology, Pasadena; Oregon State University, Corvallis","IEEE Transactions on Software Engineering","","2011","37","6","845","857","The range of verification problems that can be solved with logic model checking tools has increased significantly in the last few decades. This increase in capability is based on algorithmic advances and new theoretical insights, but it has also benefitted from the steady increase in processing speeds and main memory sizes on standard computers. The steady increase in processing speeds, though, ended when chip-makers started redirecting their efforts to the development of multicore systems. For the near-term future, we can anticipate the appearance of systems with large numbers of CPU cores, but without matching increases in clock-speeds. We will describe a model checking strategy that can allow us to leverage this trend and that allows us to tackle significantly larger problem sizes than before.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.110","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5661793","Software engineering tools and techniques;logic model checking;distributed algorithms;software verification.","Memory management;Formal verification;Search problems;Computational modeling;Multicore processing;Data models;Parallel processing","formal verification;multiprocessing systems","swarm verification techniques;logic model checking tools;multicore system development;CPU cores","","26","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Legal Implementation Readiness Decision-Making","A. K. Massey; P. N. Otto; A. I. Antón","Postdoctoral Fellow at the School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA; Association of Computing Machinery, District of Columbia, 555 13th St. NW, Washington; Professor and Chair of the School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA","IEEE Transactions on Software Engineering","","2015","41","6","545","564","Software systems are increasingly regulated. Software engineers therefore must determine which requirements have met or exceeded their legal obligations and which requirements have not. Requirements that have met or exceeded their legal obligations are legally implementation ready, whereas requirements that have not met or exceeded their legal obligations need further refinement. In this paper, we examine how software engineers make these determinations using a multi-case study with three cases. Each case involves assessment of requirements for an electronic health record system that must comply with the US Health Insurance Portability and Accountability Act (HIPAA) and is measured against the evaluations of HIPAA compliance subject matter experts. Our first case examines how individual graduate-level software engineering students assess whether the requirements met or exceeded their HIPAA obligations. Our second case replicates the findings from our first case using a different set of participants. Our third case examines how graduate-level software engineering students assess requirements using the Wideband Delphi approach to deriving consensus in groups. Our findings suggest that the average graduate-level software engineering student is ill-prepared to write legally compliant software with any confidence and that domain experts are an absolute necessity.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2383374","NSF ITR; NSF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6991569","Legal Implementation Readiness;Regulatory Compliance Software Engineering;Legal Requirements;Requirements Engineering;Legal implementation readiness;regulatory compliance software engineering;legal requirements;requirements engineering","Law;Software;Software engineering;Atmospheric measurements;Particle measurements;Decision making","electronic health records;law;software engineering","legal implementation readiness decision-making;software systems;legal obligations;electronic health record system;US Health Insurance Portability and Accountability Act;HIPAA obligations;requirement assessment;wideband Delphi approach;legally compliant software","","1","","41","","","","","","IEEE","IEEE Journals & Magazines"
"The Effects of Time Constraints on Test Case Prioritization: A Series of Controlled Experiments","H. Do; S. Mirarab; L. Tahvildari; G. Rothermel","North Dakota State University, Fargo; IBM, Vancouver; University of Waterloo, Waterloo; University of Nebraska—Lincoln, Lincoln","IEEE Transactions on Software Engineering","","2010","36","5","593","617","Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. If this is correct, a better understanding of the effects of time constraints could lead to improved prioritization techniques and improved maintenance and testing processes. We therefore conducted a series of experiments to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our first experiment manipulates time constraint levels and shows that time constraints do play a significant role in determining both the cost-effectiveness of prioritization and the relative cost-benefit trade-offs among techniques. Our second experiment replicates the first experiment, controlling for several threats to validity including numbers of faults present, and shows that the results generalize to this wider context. Our third experiment manipulates the number of faults present in programs to examine the effects of faultiness levels on prioritization and shows that faultiness level affects the relative cost-effectiveness of prioritization techniques. Taken together, these results have several implications for test engineers wishing to cost-effectively regression test their software systems. These include suggestions about when and when not to prioritize, what techniques to employ, and how differences in testing processes may relate to prioritization cost--effectiveness.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.58","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5482587","Regression testing;test case prioritization;cost-benefits;Bayesian networks;empirical studies.","Time factors;Software testing;Automatic testing;Maintenance engineering;Programming;System testing;Computer Society;Software systems;Bayesian methods;Software quality","belief networks;program testing;regression analysis;software fault tolerance","time constraints;test case prioritization techniques;regression testing;various software development processes;cost-benefit trade-offs;Bayesian networks","","66","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Exploring the Relationship between Software Process Adaptive Capability and Organisational Performance","P. Clarke; R. V. O’Connor; B. Leavy; M. Yilmaz","School of Computing, Dublin City University, Ireland, and Lero—The Irish Software Research Centre; School of Computing, Dublin City University, Ireland, and Lero—The Irish Software Research Centre; Dublin City University Business School, Ireland; Çanyaka University, Ankara, Turkey","IEEE Transactions on Software Engineering","","2015","41","12","1169","1183","Software development is a complex socio-technical activity, with the result that software development organisations need to establish and maintain robust software development processes. While much debate exists regarding the effectiveness of various software development approaches, no single approach is perfectly suited to all settings and no setting is unchanging. The capability to adapt the software process is therefore essential to sustaining an optimal software process. We designed an exploratory study to concurrently examine software process adaptive capability and organisational performance in 15 software development organisations, finding that companies with greater software process adaptive capability are shown to also experience greater business success. While our exploratory study of the complex relationship between these phenomena is limited in some respects, the findings indicate that software process adaptive capability may be worthy of further integration into software process engineering techniques. Software process adaptive capability may be an important organisational strength when deriving competitive advantage, and those responsible for the creation and evolution of software process models and methodologies may want to focus some of their future efforts in this area.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2467388","Science Foundation Ireland; Irish Software Engineering Research Centre; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7214314","Software engineering;Software engineering process;Software development;Software management;Software engineering;software engineering process;software development;software management","Software engineering;Software development;ISO Standards;IEC Standards;Software management","software process improvement","software process adaptive capability;organisational performance;socio-technical activity;software development organisation;robust software development process;software development approach;optimal software process;software process engineering technique;organisational strength","","17","","97","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of View Histories on Edit Recommendations","S. Lee; S. Kang; S. Kim; M. Staats","Department of Computer Science, KAIST, Daejeon 305-701, Republic of Korea, Guseong-dong, Yuseong-gu; Department of Computer Science, KAIST, Daejeon 305-701, Republic of Korea, Guseong-dong, Yuseong-gu; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; is with the Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg","IEEE Transactions on Software Engineering","","2015","41","3","314","330","Recommendation systems are intended to increase developer productivity by recommending files to edit. These systems mine association rules in software revision histories. However, mining coarse-grained rules using only edit histories produces recommendations with low accuracy, and can only produce recommendations after a developer edits a file. In this work, we explore the use of finer-grained association rules, based on the insight that view histories help characterize the contexts of files to edit. To leverage this additional context and fine-grained association rules, we have developed MI, a recommendation system extending ROSE, an existing edit-based recommendation system. We then conducted a comparative simulation of ROSE and MI using the interaction histories stored in the Eclipse Bugzilla system. The simulation demonstrates that MI predicts the files to edit with significantly higher recommendation accuracy than ROSE (about 63 over 35 percent), and makes recommendations earlier, often before developers begin editing. Our results clearly demonstrate the value of considering both views and edits in systems to recommend files to edit, and results in more accurate, earlier, and more flexible recommendations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2362138","National Research Foundation of Korea; National Research Foundation of Korea; Ministry of Education; Ministry of Science, ICT & Future Planning; Information Technology Research Center; National IT Industry Promotion Agency; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6926851","Programming environments/construction tools;interactive environments;software maintenance;data mining;association rules;programmer interaction histories;Programming environments/construction tools;interactive environments;software maintenance;data mining;association rules;programmer interaction histories","Context;History;Association rules;Software;Accuracy;Predictive models","data mining;interactive programming;recommender systems","association rules mining;software revision histories;coarse grained rules mining;edit histories;finer grained association rules;MI;ROSE;edit-based recommendation system;programmer interaction histories","","9","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Model-Based Test Oracle Generation for Automated Unit Testing of Agent Systems","L. Padgham; Z. Zhang; J. Thangarajah; T. Miller","RMIT University, Melbourne; RMIT University, Melbourne; RMIT University, Melbourne; University of Melbourne, Melbourne","IEEE Transactions on Software Engineering","","2013","39","9","1230","1244","Software testing remains the most widely used approach to verification in industry today, consuming between 30-50 percent of the entire development cost. Test input selection for intelligent agents presents a problem due to the very fact that the agents are intended to operate robustly under conditions which developers did not consider and would therefore be unlikely to test. Using methods to automatically generate and execute tests is one way to provide coverage of many conditions without significantly increasing cost. However, one problem using automatic generation and execution of tests is the oracle problem: How can we automatically decide if observed program behavior is correct with respect to its specification? In this paper, we present a model-based oracle generation method for unit testing belief-desire-intention agents. We develop a fault model based on the features of the core units to capture the types of faults that may be encountered and define how to automatically generate a partial, passive oracle from the agent design models. We evaluate both the fault model and the oracle generation by testing 14 agent systems. Over 400 issues were raised, and these were analyzed to ascertain whether they represented genuine faults or were false positives. We found that over 70 percent of issues raised were indicative of problems in either the design or the code. Of the 19 checks performed by our oracle, faults were found by all but 5 of these checks. We also found that 8 out the 11 fault types identified in our fault model exhibited at least one fault. The evaluation indicates that the fault model is a productive conceptualization of the problems to be expected in agent unit testing and that the oracle is able to find a substantial number of such faults with relatively small overhead in terms of false positives.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6464272","Test oracles;unit testing;BDI agents","Testing;Context;Object oriented modeling;Computational modeling;Fault diagnosis;Arrays;Robustness","fault diagnosis;multi-agent systems;program testing","model-based test oracle generation;agent system automated unit testing;software testing;test input selection;intelligent agents;test automatic generation;test execution;belief-desire-intention agents;fault model;core units;agent design models","","20","","31","","","","","","IEEE","IEEE Journals & Magazines"
"GALE: Geometric Active Learning for Search-Based Software Engineering","J. Krall; T. Menzies; M. Davies","LoadIQ, NV; Computer ScienceNorth Carolina State University; Intelligent Systems Division, NASA Ames Research Center, CA","IEEE Transactions on Software Engineering","","2015","41","10","1001","1018","Multi-objective evolutionary algorithms (MOEAs) help software engineers find novel solutions to complex problems. When automatic tools explore too many options, they are slow to use and hard to comprehend. GALE is a near-linear time MOEA that builds a piecewise approximation to the surface of best solutions along the Pareto frontier. For each piece, GALE mutates solutions towards the better end. In numerous case studies, GALE finds comparable solutions to standard methods (NSGA-II, SPEA2) using far fewer evaluations (e.g. 20 evaluations, not 1,000). GALE is recommended when a model is expensive to evaluate, or when some audience needs to browse and understand how an MOEA has made its conclusions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2432024","US National Science Foundation (NSF); Qatar/West Virginia University; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7105950","Multi-objective optimization;search based software engineering;active learning;Multi-objective optimization;search based software engineering;active learning","Optimization;Software;Computational modeling;Approximation methods;Standards;Biological system modeling;Sociology","approximation theory;computational complexity;evolutionary computation;learning (artificial intelligence);Pareto optimisation;software engineering","GALE;geometric active learning;search-based software engineering;multiobjective evolutionary algorithm;near-linear time MOEA;piecewise approximation;Pareto frontier","","13","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Genetic Algorithms for Randomized Unit Testing","J. H. Andrews; T. Menzies; F. C. H. Li","University of Western Ontario, London, Ont., Canada; West Virginia University, Morgantown, WV, USA; University of Western Ontario, London, Ont., Canada","IEEE Transactions on Software Engineering","","2011","37","1","80","94","Randomized testing is an effective method for testing software units. The thoroughness of randomized unit testing varies widely according to the settings of certain parameters, such as the relative frequencies with which methods are called. In this paper, we describe Nighthawk, a system which uses a genetic algorithm (GA) to find parameters for randomized unit testing that optimize test coverage. Designing GAs is somewhat of a black art. We therefore use a feature subset selection (FSS) tool to assess the size and content of the representations within the GA. Using that tool, we can reduce the size of the representation substantially while still achieving most of the coverage found using the full representation. Our reduced GA achieves almost the same results as the full system, but in only 10 percent of the time. These results suggest that FSS could significantly optimize metaheuristic search-based software engineering tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5704237","Software testing;randomized testing;genetic algorithms;feature subset selection;search-based optimization;testing tools.","Testing;Biological cells;Gallium;Receivers;Software;Java;Optimization","feature extraction;genetic algorithms;program testing;randomised algorithms;search problems;software engineering","genetic algorithm;randomized unit testing;relative frequency;Nighthawk;optimized test coverage;feature subset selection tool;metaheuristic search;software engineering tool;software testing","","33","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Better Debugging via Output Tracing and Callstack-Sensitive Slicing","S. Horwitz; B. Liblit; M. Polishchuk","University of Wisconsin-Madison, Madison; University of Wisconsin-Madison, Madison; Microsoft Corporation, Redmond","IEEE Transactions on Software Engineering","","2010","36","1","7","19","Debugging often involves 1) finding the point of failure (the first statement that produces bad output) and 2) finding and fixing the actual bug. Print statements and debugger break points can help with step 1. Slicing the program back from values used at the point of failure can help with step 2. However, neither approach is ideal: Debuggers and print statements can be clumsy and time-consuming and backward slices can be almost as large as the original program. This paper addresses both problems. We present callstack-sensitive slicing, which reduces slice sizes by leveraging the series of calls active when a program fails. We also show how slice intersections may further reduce slice sizes. We then describe a set of tools that identifies points of failure for programs that produce bad output. Finally, we apply our point-of-failure tools to a suite of buggy programs and evaluate callstack-sensitive slicing and slice intersection as applied to debugging. Callstack-sensitive slicing is very effective: On average, a callstack-sensitive slice is about 0.31 time the size of the corresponding full slice, down to just 0.06 time in the best case. Slice intersection is less impressive, on average, but may sometimes prove useful in practice.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5282499","Static program slicing;callstack-sensitive analysis;points of failure;output tracing and attribution.","Debugging;Programming profession;Computer crashes;Failure analysis;Testing;Linux","program debugging;program slicing;software reliability","callstack-sensitive slicing;program debugging;program slicing;point-of-failure tools;buggy programs;slice intersection","","12","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Locating Software Faults Based on Minimum Debugging Frontier Set","F. Li; Z. Li; W. Huo; X. Feng","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, P.R.China; Department of Computer Science, Purdue University, West Lafayette, IN; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, P.R.China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, P.R.China","IEEE Transactions on Software Engineering","","2017","43","8","760","776","In this article, we propose a novel state-based fault-localization approach. Given an observed failure that is reproducible under the same program input, this new approach uses two main techniques to reduce the state exploration cost. Firstly, the execution trace to be analyzed for the observed failure is successively narrowed by making the set of trace points in each step a cut of the dynamic dependence graph. Such a cut divides the remaining trace into two parts and, based on the sparse symbolic exploration outcome, one part is removed from further exploration. This process continues until reaching where the fault is determined to be. Second, the cut in each step is chosen such that the union of the program states from the members of the cut is of the minimum size among all candidate cuts. The set of statement instances in the chosen cut is called a minimum debugging frontier set (MDFS). To evaluate our approach, we apply it to 16 real bugs from real world programs and compare our fault reports with those generated by state-of-the-art approaches. Results show that the MDFS approach obtains high quality fault reports for these test cases with considerably higher efficiency than previous approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2632122","National Natural Science Foundation of China; National High Technology Research and Development Program of China; National Science Foundation of United States; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7755837","Fault localization;minimum debugging frontier set;sparse symbolic exploration;dynamic dependence graph","Debugging;Computer aided software engineering;Computer bugs;Software;Computer architecture;Computers;Indexes","graph theory;program debugging;software fault tolerance","software faults location;minimum debugging frontier set;state-based fault-localization;state exploration cost;execution trace;trace points;dynamic dependence graph;sparse symbolic exploration;MDFS;program bugs","","2","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Collaborative Model-Driven Software Engineering: A Classification Framework and a Research Map","M. Franzago; D. D. Ruscio; I. Malavolta; H. Muccini","Department of Information Engineering, Computer Science and Mathematics (DISIM), University of L'Aquila, L'Aquila, AQ, Italy; Department of Information Engineering, Computer Science and Mathematics (DISIM), University of L'Aquila, L'Aquila, AQ, Italy; Department of Computer Science, Vrije Universiteit Amsterdam, Amsterdam, North Holland, HV, The Netherlands; Department of Information Engineering, Computer Science and Mathematics (DISIM), University of L'Aquila, L'Aquila, AQ, Italy","IEEE Transactions on Software Engineering","","2018","44","12","1146","1175","Context: Collaborative Model-Driven Software Engineering (MDSE) consists of methods and techniques where multiple stakeholders manage, collaborate, and are aware of each others' work on shared models. Objective: Collaborative MDSE is attracting research efforts from different areas, resulting in a variegated scientific body of knowledge. This study aims at identifying, classifying, and understanding existing collaborative MDSE approaches. Method: We designed and conducted a systematic mapping study. Starting from over 3,000 potentially relevant studies, we applied a rigorous selection procedure resulting in 106 selected papers, further clustered into 48 primary studies along a time span of 19 years. We rigorously defined and applied a classification framework and extracted key information from each selected study for subsequent analysis. Results: Our analysis revealed the following main fidings: (i) there is a growing scientific interest on collaborative MDSE in the last years; (ii) multi-view modeling, validation support, reuse, and branching are more rarely covered with respect to other aspects about collaborative MDSE; (iii) different primary studies focus differently on individual dimensions of collaborative MDSE (i.e., model management, collaboration, and communication); (iv) most approaches are language-specific, with a prominence of UML-based approaches; (v) few approaches support the interplay between synchronous and asynchronous collaboration. Conclusion: This study gives a solid foundation for classifying existing and future approaches for collaborative MDSE. Researchers and practitioners can use our results for identifying existing research/technical gaps to attack, better scoping their own contributions, or understanding existing ones.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2755039","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8047991","Collaborative MDSE;CoMDSE;C-MDSE;model-driven engineering;collaborative software engineering;CoSE;systematic mapping study","Collaboration;Analytical models;Software engineering;Stakeholders;Systematics","formal specification;groupware;Unified Modeling Language","asynchronous collaboration;Collaborative Model-Driven Software Engineering;classification framework;systematic mapping study;synchronous collaboration;primary studies;collaborative MDSE approaches;time 19.0 year","","2","","119","","","","","","IEEE","IEEE Journals & Magazines"
"A Theoretical and Empirical Study of Search-Based Testing: Local, Global, and Hybrid Search","M. Harman; P. McMinn","King's College London, London; University of Sheffield, Sheffield","IEEE Transactions on Software Engineering","","2010","36","2","226","247","Search-based optimization techniques have been applied to structural software test data generation since 1992, with a recent upsurge in interest and activity within this area. However, despite the large number of recent studies on the applicability of different search-based optimization approaches, there has been very little theoretical analysis of the types of testing problem for which these techniques are well suited. There are also few empirical studies that present results for larger programs. This paper presents a theoretical exploration of the most widely studied approach, the global search technique embodied by Genetic Algorithms. It also presents results from a large empirical study that compares the behavior of both global and local search-based optimization on real-world programs. The results of this study reveal that cases exist of test data generation problem that suit each algorithm, thereby suggesting that a hybrid global-local search (a Memetic Algorithm) may be appropriate. The paper presents a Memetic Algorithm along with further empirical results studying its performance.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.71","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5342440","Automated test data generation;search-based testing;search-based software engineering;Evolutionary Testing;Genetic Algorithms;Hill Climbing;schema theory;Royal Road;testing and debugging;testing tools;artificial intelligence;problem solving;control methods;and search;heuristic methods;algorithms;experimentation;measurement;performance;theory.","Software testing;Automatic testing;Genetic algorithms;Costs;Hybrid power systems;Software engineering;Automation;Stress;Debugging;Artificial intelligence","automatic test software;genetic algorithms;program testing;search problems","search based testing;search based optimization techniques;structural software test data generation;genetic algorithms;hybrid global-local search problem;memetic algorithm;real-world programs","","152","","61","","","","","","IEEE","IEEE Journals & Magazines"
"A Flowchart Language for Quantum Programming","M. Ying; Y. Feng","University of Technology, Sydney and Tsinghua University, Beijing; University of Technology, Sydney and Tsinghua University, Beijing","IEEE Transactions on Software Engineering","","2011","37","4","466","485","Several high-level quantum programming languages have been proposed in the previous research. In this paper, we define a low-level flowchart language for quantum programming, which can be used in implementation of high-level quantum languages and in design of quantum compilers. The formal semantics of the flowchart language is given, and the notion of correctness for programs written in this language is introduced. A structured quantum programming theorem is presented, which provides a technique of translating quantum flowchart programs into programs written in a high-level language, namely, a quantum extension of the while-language.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.94","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5611555","Quantum programming;flowchart language;while-language;structured programming.","Quantum computing;Semantics;Quantum mechanics;Programming;Computer languages;Probabilistic logic;Computers","flowcharting;formal languages;program compilers;program interpreters;programming language semantics;quantum computing","high level quantum programming language;low level flowchart language;quantum compiler design;formal semantics;structured quantum programming theorem;quantum flowchart program translation","","8","","34","","","","","","IEEE","IEEE Journals & Magazines"
"Mining version histories to guide software changes","T. Zimmermann; A. Zeller; P. Weissgerber; S. Diehl","Dept. of Comput. Sci., Saarlandes Univ., Saarbrucken, Germany; Dept. of Comput. Sci., Saarlandes Univ., Saarbrucken, Germany; NA; NA","IEEE Transactions on Software Engineering","","2005","31","6","429","445","We apply data mining to version histories in order to guide programmers along related changes: ""Programmers who changed these functions also changed...."" Given a set of existing changes, the mined association rules 1) suggest and predict likely further changes, 2) show up item coupling that is undetectable by program analysis, and 3) can prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict further locations to be changed; the best predictive power is obtained for changes to existing software. In our evaluation based on the history of eight popular open source projects, ROSE's topmost three suggestions contained a correct location with a likelihood of more than 70 percent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.72","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463228","Index Terms- Programming environments/construction tools;distribution;maintenance;enhancement;configuration management;clustering;classification;association rules;data mining.","History;Data mining;Programming profession;Association rules;Books;Computer science;Computer Society;Software prototyping;Open source software;Environmental management","software maintenance;data mining;software prototyping;configuration management;programming environments;software tools","software changes;data mining;program analysis;ROSE prototype;programming environments;construction tools;configuration management;clustering;classification;association rule;software maintenance","","311","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Using Reduced Execution Flow Graph to Identify Library Functions in Binary Code","J. Qiu; X. Su; P. Ma","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Software Engineering","","2016","42","2","187","202","Discontinuity and polymorphism of a library function create two challenges for library function identification, which is a key technique in reverse engineering. A new hybrid representation of dependence graph and control flow graph called Execution Flow Graph (EFG) is introduced to describe the semantics of binary code. Library function identification turns to be a subgraph isomorphism testing problem since the EFG of a library function instance is isomorphic to the sub-EFG of this library function. Subgraph isomorphism detection is time-consuming. Thus, we introduce a new representation called Reduced Execution Flow Graph (REFG) based on EFG to speed up the isomorphism testing. We have proved that EFGs are subgraph isomorphic as long as their corresponding REFGs are subgraph isomorphic. The high efficiency of the REFG approach in subgraph isomorphism detection comes from fewer nodes and edges in REFGs and new lossless filters for excluding the unmatched subgraphs before detection. Experimental results show that precisions of both the EFG and REFG approaches are higher than the state-of-the-art tool and the REFG approach sharply decreases the processing time of the EFG approach with consistent precision and recall.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2470241","National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7210204","Reverse engineering;static analysis;inline function;library function identification;subgraph isomorphism and graph mining;Reverse engineering;static analysis;inline function;library function identification;subgraph isomorphism and graph mining","Libraries;Registers;Binary codes;Testing;Joining processes;Flow graphs;Reverse engineering","binary codes;graph theory;reverse engineering;software libraries","reduced execution flow graph;library function identification;binary code;reverse engineering;hybrid representation;dependence graph;control flow graph;subgraph isomorphism testing problem;library function instance;subgraph isomorphism detection;REFG;lossless filters","","4","","29","","","","","","IEEE","IEEE Journals & Magazines"
"A Multi-Site Joint Replication of a Design Patterns Experiment Using Moderator Variables to Generalize across Contexts","J. L. Krein; L. Prechelt; N. Juristo; A. Nanthaamornphong; J. C. Carver; S. Vegas; C. D. Knutson; K. D. Seppi; D. L. Eggett","Department of Computer Science, Provo, UT; Institut für Informatik, Germany; Computing School, Spain; Department of Information and Communication Technology, Thailand; Department of Computer Science, Tuscaloosa, AL; Computing School, Spain; Department of Computer Science, Provo, UT; Department of Computer Science, Provo, UT; Department of Statistics, Provo, UT","IEEE Transactions on Software Engineering","","2016","42","4","302","321","<bold>Context.</bold>Several empirical studies have explored the benefits of software design patterns, but their collective results are highly inconsistent. Resolving the inconsistencies requires investigating moderators—i.e., variables that cause an effect to differ across contexts.<bold>Objectives.</bold>Replicate a design patterns experiment at multiple sites and identify sufficient moderators to generalize the results across prior studies.<bold>Methods.</bold>We perform a close replication of an experiment investigating the impact (in terms of time and quality) of design patterns (Decorator and Abstract Factory) on software maintenance. The experiment was replicated once previously, with divergent results. We execute our replication at four universities—spanning two continents and three countries—using a new method for performing distributed replications based on closely coordinated, small-scale instances (“joint replication”). We perform two analyses: 1) a<italic>post-hoc</italic>analysis of moderators, based on frequentist and Bayesian statistics; 2) an<italic>a priori</italic>analysis of the original hypotheses, based on frequentist statistics.<bold>Results.</bold>The main effect differs across the previous instances of the experiment and across the sites in our distributed replication. Our analysis of moderators (including developer experience and pattern knowledge) resolves the differences sufficiently to allow for cross-context (and cross-study) conclusions. The final conclusions represent 126 participants from five universities and 12 software companies, spanning two continents and at least four countries.<bold>Conclusions.</bold>The Decorator pattern is found to be preferable to a simpler solution during maintenance, as long as the developer has at least some prior knowledge of the pattern. For Abstract Factory, the simpler solution is found to be mostly equivalent to the pattern solution. Abstract Factory is shown to require a higher level of knowledge and/or experience than Decorator for the pattern to be beneficial.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2488625","Spanish Ministry of Economy and Competitiveness; LLC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7294706","Design patterns;software maintenance;moderator variables;multi-site;joint replication;controlled experiment;Design patterns;software maintenance;moderator variables;multi-site;joint replication;controlled experiment","Production facilities;Design methodology;Training;Context modeling","","","","1","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Enabling reuse-based software development of large-scale systems","R. W. Selby","Northrop Grumman Space Technol., Redondo Beach, CA, USA","IEEE Transactions on Software Engineering","","2005","31","6","495","510","Software reuse enables developers to leverage past accomplishments and facilitates significant improvements in software productivity and quality. Software reuse catalyzes improvements in productivity by avoiding redevelopment and improvements in quality by incorporating components whose reliability has already been established. This study addresses a pivotal research issue that underlies software reuse - what factors characterize successful software reuse in large-scale systems. The research approach is to investigate, analyze, and evaluate software reuse empirically by mining software repositories from a NASA software development environment that actively reuses software. This software environment successfully follows principles of reuse-based software development in order to achieve an average reuse of 32 percent per project, which is the average amount of software either reused or modified from previous systems. We examine the repositories for 25 software systems ranging from 3,000 to 112,000 source lines from this software environment. We analyze four classes of software modules: modules reused without revision, modules reused with slight revision (<25 percent revision), modules reused with major revision (/spl ges/25 percent revision), and newly developed modules. We apply nonparametric statistical models to compare numerous development variables across the 2,954 software modules in the systems. We identify two categories of factors that characterize successful reuse-based software development of large-scale systems: module design factors and module implementation factors. We also evaluate the fault rates of the reused, modified, and newly developed modules. The module design factors that characterize module reuse without revision were (after normalization by size in source lines): few calls to other system modules, many calls to utility functions, few input-output parameters, few reads and writes, and many comments. The module implementation factors that characterize module reuse without revision were small size in source lines and (after normalization by size in source lines): low development effort and many assignment statements. The modules reused without revision had the fewest faults, fewest faults per source line, and lowest fault correction effort. The modules reused with major revision had the highest fault correction effort and highest fault isolation effort as wed as the most changes, most changes per source line, and highest change correction effort. In conclusion, we outline future research directions that build on these software reuse ideas and strategies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463232","Index Terms- Software reuse;software measurement;software metrics;software faults;software changes;mining software repositories;large-scale systems;experimentation;empirical study.","Programming;Large-scale systems;Software quality;Productivity;Computer architecture;Software reusability;NASA;Software systems;Software measurement;Software metrics","software reliability;software reusability;software metrics;software quality;data mining;statistical analysis;large-scale systems;programming environments","software measurement;software metrics;software faults;software change;mining software repository;large-scale systems;empirical study;software productivity;software quality;software reliability;NASA software development environment;Nonparametric statistical model;reuse-based software development;fault correction","","76","","71","","","","","","IEEE","IEEE Journals & Magazines"
"A simulation approach to structure-based software reliability analysis","S. S. Gokhale; Michael Rung-Tsong Lyu","Dept. of Comput. Sci. & Eng., Connecticut Univ., Storrs, CT, USA; NA","IEEE Transactions on Software Engineering","","2005","31","8","643","656","Structure-based techniques enable an analysis of the influence of individual components on the application reliability. In an effort to ensure analytical tractability, prevalent structure-based analysis techniques are based on assumptions which preclude the use of these techniques for reliability analysis during the testing and operational phases. In this paper, we develop simulation procedures to assess the impact of individual components on the reliability of an application in the presence of fault detection and repair strategies that may be employed during testing. We also develop simulation procedures to analyze the application reliability for various operational configurations. We illustrate the potential of simulation procedures using several examples. Based on the results of these examples, we provide novel insights into how testing and repair strategies can be tailored depending on the application structure to achieve the desired reliability in a cost-effective manner. We also discuss how the results could be used to explore alternative operational configurations of a software application taking into consideration the application structure so as to cause minimal interruption in the field.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498770","Index Terms- Application structure;reliability analysis;discrete-event simulation.","Analytical models;Software reliability;Application software;Testing;Runtime;Flow graphs;Computer science;Fault detection;Discrete event simulation;Programming","software reliability;object-oriented programming;discrete event simulation;program testing;software maintenance","structure-based software reliability;component-based software development;fault detection;repair strategies;cost-effective manner;software application;discrete-event simulation","","45","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Integer Parameter Synthesis for Real-Time Systems","A. Jovanović; D. Lime; O. H. Roux","Ecole Centrale de Nantes - IRCCyN UMR CNRS 6597, Nantes, France; Ecole Centrale de Nantes - IRCCyN UMR CNRS 6597, Nantes, France; Ecole Centrale de Nantes - IRCCyN UMR CNRS 6597, Nantes, France","IEEE Transactions on Software Engineering","","2015","41","5","445","461","We provide a subclass of parametric timed automata (PTA) that we can actually and efficiently analyze, and we argue that it retains most of the practical usefulness of PTA for the modeling of real-time systems. The currently most useful known subclass of PTA, L/U automata, has a strong syntactical restriction for practical purposes, and we show that the associated theoretical results are mixed. We therefore advocate for a different restriction scheme: since in classical timed automata, real-valued clocks are always compared to integers for all practical purposes, we also search for parameter values as bounded integers. We show that the problem of the existence of parameter values such that some TCTL property is satisfied is PSPACE-complete. In such a setting, we can of course synthesize all the values of parameters and we give symbolic algorithms, for reachability and unavoidability properties, to do it efficiently, i.e., without an explicit enumeration. This also has the practical advantage of giving the result as symbolic constraints between the parameters. We finally report on a few experimental results to illustrate the practical usefulness of our approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2357445","ANR; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6895298","Timed automata;parameters;synthesis;model-checking;real-time systems;symbolic algorithms","Cost accounting;Automata;Clocks;Radiation detectors;Upper bound;Delays;Real-time systems","automata theory;reachability analysis;real-time systems","integer parameter synthesis;real-time systems;parametric timed automata;PTA;real-valued clocks;TCTL property;PSPACE-complete;symbolic algorithms;reachability;symbolic constraints","","22","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Automatically Generating Test Cases for Specification Mining","V. Dallmeier; N. Knopp; C. Mallon; G. Fraser; S. Hack; A. Zeller","Universität des Saarlandes, Saarbrücken; Universität des Saarlandes, Saarbrücken; Universität des Saarlandes, Saarbrücken; Universität des Saarlandes, Saarbrücken; Universität des Saarlandes, Saarbrücken; Universität des Saarlandes, Saarbrücken","IEEE Transactions on Software Engineering","","2012","38","2","243","257","Dynamic specification mining observes program executions to infer models of normal program behavior. What makes us believe that we have seen sufficiently many executions? The TAUTOKO (“Tautoko” is the Mãori word for “enhance, enrich.”) typestate miner generates test cases that cover previously unobserved behavior, systematically extending the execution space, and enriching the specification. To our knowledge, this is the first combination of systematic test case generation and typestate mining-a combination with clear benefits: On a sample of 800 defects seeded into six Java subjects, a static typestate verifier fed with enriched models would report significantly more true positives and significantly fewer false positives than the initial models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.105","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6044587","Specification mining;test case generation;typestate analysis.","Testing;Java;Instruments;Schedules;Software;Heuristic algorithms;Fault detection","automatic test pattern generation;data mining;formal specification;Java;program verification","automatic test case generation;dynamic specification mining;program executions;normal program behavior;TAUTOKO;typestate mining;Java;static typestate verifier","","30","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Timed Automata Modeling and Verification for Publish-Subscribe Structures Using Distributed Resources","V. Valero; G. Díaz; M. Cambronero","Department of Computer Science, University of Castilla-La Mancha, Albacete, Spain; Department of Computer Science, University of Castilla-La Mancha, Albacete, Spain; Department of Computer Science, University of Castilla-La Mancha, Albacete, Spain","IEEE Transactions on Software Engineering","","2017","43","1","76","99","In this paper we present a Timed Automata model for the Publish/Subscribe paradigm in the context of Web Service Compositions with distributed resources, on the basis of an algebraic language inspired by the WSRF standard constructions. This framework allows a set of participants in a Web Service composition to interact with one another and also to manage a collection of distributed resources. The model includes operations for clients to publish, discover and subscribe to resources, so as to be notified when the resource property values fulfill certain conditions (topic-based subscription). Simulation and model-checking techniques can therefore be applied to the obtained network of timed automata, in order to check whether certain properties of interest are satisfied. A specific case study is finally presented to illustrate the model and the verification of the relevant properties on the obtained timed automata model.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2560842","Spanish Government; FEDER; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7463051","Publish/subscribe;formal modeling;timed automata;verification;model checking","Web services;Automata;Unified modeling language;Semantics;Clocks;Probabilistic logic;Context modeling","algebra;automata theory;formal verification;message passing;middleware;resource allocation;Web services","timed automata modeling;timed automata verification;publish-subscribe structures;resource distribution;publish-subscribe paradigm;Web service compositions;algebraic language;WSRF standard constructions;resource property values;topic-based subscription;model checking","","5","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Moral dominance relations for program comprehension","S. C. Shaw; M. Goldstein; M. Munro; E. Burd","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2003","29","9","851","863","Dominance trees have been used as a means for reengineering legacy systems into potential reuse candidates. The dominance relation suggests the reuse candidates which are identified by strongly directly dominated subtrees. We review the approach and illustrate how the dominance tree may fail to show the relationship between the strongly direct dominated procedures and the directly dominated procedures. We introduce a relation of generalized conditional independence which strengthens the argument for the adoption of the potential reuse candidates suggested by the dominance tree and explains their relationship with the directly dominated vertices. This leads to an improved dominance tree, the moral dominance tree, which helps aid program comprehension available from the tree. The generalized conditional independence relation also identifies potential reuse candidates that are missed by the dominance relation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1232289","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1232289","","Ethics;Tree graphs;Software maintenance;Databases;Graphical models;Reverse engineering;Testing;Companies;Software performance;Documentation","tree data structures;reverse engineering;software reusability;systems re-engineering","moral dominance relations;program comprehension;dominance trees;legacy systems reengineering;reuse candidates;strongly directly dominated subtrees;directly dominated vertices;generalized conditional independence relation","","7","","29","","","","","","IEEE","IEEE Journals & Magazines"
"Systematic transformation of functional analysis model into OO design and implementation","H. B. K. Tan; Y. Yang; L. Bian","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore; Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore","IEEE Transactions on Software Engineering","","2006","32","2","111","135","Functional refinement is beneficial to object-oriented (OO) software development, especially for problems with more complex functions. However, the use of functional refinement in OO software development has not received much attention. This paper proposes an enhanced data flow diagram (DFD), called data flow net (DF net), for specifying use-cases through functional decomposition. It proposes a novel approach to complement existing OO software development methods with functional decomposition for realizing use-cases, especially those with more complex functions. In the requirements analysis stage, the proposed approach realizes use-cases through functional refinement and specifies them in DF nets. In the design and implementation stages, it transforms the DF nets systematically and precisely into OO design and implementation. The approach is amenable to automation and a prototype has been developed to support the transformation process. In the development of an OO system, it is seamless to realize some of the use-cases using the proposed approach and the remaining use-cases in the same target system using any existing OO software development methods.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599420","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599420","Functional model;object-oriented model;functional decomposition;requirements analysis;design;model transformation.","Functional analysis;Programming;Object oriented modeling;Design for disassembly;Automation;Software prototyping;Prototypes;Information systems;Encapsulation","object-oriented methods;flowcharting;formal specification;data flow analysis","systematic transformation;functional analysis model;object-oriented software development;functional refinement;data flow diagram;DFD;data flow net;DF net;use-case specification;functional decomposition;requirements analysis;OO design","","11","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Inferring Loop Invariants by Mutation, Dynamic Analysis, and Static Checking","J. P. Galeotti; C. A. Furia; E. May; G. Fraser; A. Zeller","Saarland University, Saarbr&#x00FC;cken, Germany; ETH Zurich, Switzerland; Google Inc, London, United Kingdom; Department of Computer Science, University of Sheffield, United Kingdom; Saarland University, Saarbr&#x00FC;cken, Germany","IEEE Transactions on Software Engineering","","2015","41","10","1019","1037","Verifiers that can prove programs correct against their full functional specification require, for programs with loops, additional annotations in the form of loop invariants-properties that hold for every iteration of a loop. We show that significant loop invariant candidates can be generated by systematically mutating postconditions; then, dynamic checking (based on automatically generated tests) weeds out invalid candidates, and static checking selects provably valid ones. We present a framework that automatically applies these techniques to support a program prover, paving the way for fully automatic verification without manually written loop invariants: Applied to 28 methods (including 39 different loops) from various java.util classes (occasionally modified to avoid using Java features not fully supported by the static checker), our DYNAMATE prototype automatically discharged 97 percent of all proof obligations, resulting in automatic complete correctness proofs of 25 out of the 28 methods-outperforming several state-of-the-art tools for fully automatic verification.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2431688","European Research Council; European Union’s Seventh Framework Programme; ERC; EU FP7; Swiss SNF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7105412","Loop invariants;inference;automatic verification;functional properties;dynamic analysis;Loop invariants;inference;automatic verification;functional properties;dynamic analysis","Heuristic algorithms;Java;Generators;Detectors;Arrays;Prototypes;Instruments","formal specification;Java;program control structures;program testing;program verification;system monitoring","loop invariant inference;mutation;dynamic analysis;static checking;functional specification;program prover;automatic verification;Java.util classes;DYNAMATE prototype;automatic complete correctness proofs;test automatic generation","","5","","80","","","","","","IEEE","IEEE Journals & Magazines"
"Oracles for Distributed Testing","R. M. Hierons","Brunel University, Middlesex","IEEE Transactions on Software Engineering","","2012","38","3","629","641","The problem of deciding whether an observed behavior is acceptable is the oracle problem. When testing from a finite state machine (FSM), it is easy to solve the oracle problem and so it has received relatively little attention for FSMs. However, if the system under test has physically distributed interfaces, called ports, then in distributed testing, we observe a local trace at each port and we compare the set of local traces with the set of allowed behaviors (global traces). This paper investigates the oracle problem for deterministic and nondeterministic FSMs and for two alternative definitions of conformance for distributed testing. We show that the oracle problem can be solved in polynomial time for the weaker notion of conformance (⊆<sub>w</sub>) but is NP-hard for the stronger notion of conformance (⊆), even if the FSM is deterministic. However, when testing from a deterministic FSM with controllable input sequences, the oracle problem can be solved in polynomial time and similar results hold for nondeterministic FSMs. Thus, in some cases, the oracle problem can be efficiently solved when using ⊆<sub>s</sub>and where this is not the case, we can use the decision procedure for ⊆<sub>w</sub>as a sound approximation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5750006","Software engineering/software/program verification;software engineering/testing and debugging;systems and software;distributed systems;finite state machine;nondeterminism;test oracle;controllability;local observability.","Testing;Controllability;Observability;Polynomials;Software;Software engineering","distributed processing;finite state machines;polynomials;program testing","distributed testing;oracle problem;finite state machine;FSM;physically distributed interfaces;polynomial time;weaker notion;NP-hard problem;sound approximation;decision procedure","","19","","37","","","","","","IEEE","IEEE Journals & Magazines"
"Hierarchical Clustering for Software Architecture Recovery","O. Maqbool; H. Babri","NA; NA","IEEE Transactions on Software Engineering","","2007","33","11","759","780","Gaining an architectural level understanding of a software system is important for many reasons. When the description of a system's architecture does not exist, attempts must be made to recover it. In recent years, researchers have explored the use of clustering for recovering a software system's architecture, given only its source code. The main contributions of this paper are given as follows. First, we review hierarchical clustering research in the context of software architecture recovery and modularization. Second, to employ clustering meaningfully, it is necessary to understand the peculiarities of the software domain, as well as the behavior of clustering measures and algorithms in this domain. To this end, we provide a detailed analysis of the behavior of various similarity and distance measures that may be employed for software clustering. Third, we analyze the clustering process of various well-known clustering algorithms by using multiple criteria, and we show how arbitrary decisions taken by these algorithms during clustering affect the quality of their results. Finally, we present an analysis of two recently proposed clustering algorithms, revealing close similarities in their apparently different clustering approaches. Experiments on four legacy software systems provide insight into the behavior of well-known clustering algorithms and their characteristics in the software domain.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70732","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339232","Software Engineering;Restructuring;reverse engineering;and reengineering;architecture recovery;hierarchical clustering;arbitrary decisions","Software architecture;Computer architecture;Software systems;Clustering algorithms;Software measurement;Software algorithms;Algorithm design and analysis;Taxonomy;Partitioning algorithms;Reverse engineering","software architecture","software architecture recovery;architectural level understanding;hierarchical clustering research;software modularization;software domain;clustering algorithms;arbitrary decisions","","155","","79","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities","Y. Shin; A. Meneely; L. Williams; J. A. Osborne","DePaul University, Chicago; North Carolina State University, Raleigh; North Carolina State University, Raleigh; North Carolina State University, Raleigh","IEEE Transactions on Software Engineering","","2011","37","6","772","787","Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.81","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5560680","Fault prediction;software metrics;software security;vulnerability prediction.","Fault diagnosis;Software security;Complexity theory;Predictive models;Charge coupled devices","Linux;online front-ends;program testing;public domain software;software fault tolerance;software metrics","code churn;software vulnerabilities;developer activity metrics;security inspection;software metrics;source code;vulnerable code locations;open-source projects;Mozilla Firefox Web browser;Red Hat enterprise Linux kernel","","113","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Examining the Potentially Confounding Effect of Class Size on the Associations between Object-Oriented Metrics and Change-Proneness","Y. Zhou; H. Leung; B. Xu","State Key Laboratory for Novel Software Technology and Nanjing University, Jiangsu; Hong Kong Polytechnic University, Hong Kong; State Key Laboratory for Novel Software Technology and Nanjing University, Jiangsu","IEEE Transactions on Software Engineering","","2009","35","5","607","623","Previous research shows that class size can influence the associations between object-oriented (OO) metrics and fault-proneness and therefore proposes that it should be controlled as a confounding variable when validating OO metrics on fault-proneness. Otherwise, their true associations may be distorted. However, it has not been determined whether this practice is equally applicable to other external quality attributes. In this paper, we use three size metrics, two of which are available during the high-level design phase, to examine the potentially confounding effect of class size on the associations between OO metrics and change-proneness. The OO metrics that are investigated include cohesion, coupling, and inheritance metrics. Our results, based on Eclipse, indicate that: 1) The confounding effect of class size on the associations between OO metrics and change-proneness, in general, exists, regardless of whichever size metric is used; 2) the confounding effect of class size generally leads to an overestimate of the associations between OO metrics and change-proneness; and 3) for many OO metrics, the confounding effect of class size completely accounts for their associations with change-proneness or results in a change of the direction of the associations. These results strongly suggest that studies validating OO metrics on change-proneness should also consider class size as a confounding variable.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4967613","Object-oriented;metrics;validation;class size;confounding;change-proneness.","Size control;Maintenance;Predictive models;Size measurement;Laboratories","inheritance;object-oriented programming;program verification;programming environments;software maintenance;software metrics;software quality","object-oriented metrics;change-proneness;fault-proneness;potentially confounding class size effect;high-level design phase;external quality attribute;cohesion metrics;coupling metrics;inheritance metrics;Eclipse;program validation","","54","","57","","","","","","IEEE","IEEE Journals & Magazines"
"An Empirical Methodology to Evaluate Vulnerability Discovery Models","F. Massacci; V. H. Nguyen","DISI, University of Trento, Trento, TN, Italy; DISI, University of Trento, Trento, TN, Italy","IEEE Transactions on Software Engineering","","2014","40","12","1147","1162","Vulnerability discovery models (VDMs) operate on known vulnerability data to estimate the total number of vulnerabilities that will be reported after a software is released. VDMs have been proposed by industry and academia, but there has been no systematic independent evaluation by researchers who are not model proponents. Moreover, the traditional evaluation methodology has some issues that biased previous studies in the field. In this work we propose an empirical methodology that systematically evaluates the performance of VDMs along two dimensions (quality and predictability) and addresses all identified issues of the traditional methodology. We conduct an experiment to evaluate most existing VDMs on popular web browsers' vulnerability data. Our comparison shows that the results obtained by the proposed methodology are more informative than those by the traditional methodology. Among evaluated VDMs, the simplest linear model is the most appropriate choice in terms of both quality and predictability for the first 6-12 months since a release date. Otherwise, logistics-based models are better choices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2354037","European Union Seventh Framework Programme; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6891367","Software security;empirical evaluation;vulnerability discovery model;vulnerability analysis","Data models;Computer security;Operating systems;Browsers;Computer bugs;Predictive models","online front-ends;security of data;software quality","empirical methodology;vulnerability discovery model evaluation;VDM;quality;predictability;Web browser vulnerability data;logistics-based model;time 6 month to 12 month","","11","","54","","","","","","IEEE","IEEE Journals & Magazines"
"Triggered Message Sequence Charts","B. Sengupta; R. Cleaveland","IBM India Research Laboratory, Block-1, Indian Institute of Technology, Delhi, Hauz Khas, New Delhi, India; Department of Computer Science, A.V. Williams Building, University of Maryland, College Park, MD","IEEE Transactions on Software Engineering","","2006","32","8","587","607","This paper introduces triggered message sequence charts (TMSCs), a graphical, mathematically well-founded framework for capturing scenario-based system requirements of distributed systems. Like message sequence charts (MSCs), TMSCs are graphical depictions of scenarios, or exchanges of messages between processes in a distributed system. Unlike MSCs, however, TMSCs are equipped with a notion of trigger that permits requirements to be made conditional, a notion of partiality indicating that a scenario may be subsequently extended, and a notion of refinement for assessing whether or not a more detailed specification correctly elaborates on a less detailed one. The TMSC notation also includes a collection of composition operators allowing structure to be introduced into scenario specifications so that interactions among different scenarios may be studied. In the first part of this paper, TMSCs are introduced and their use in support of requirements modeling is illustrated via two extended examples. The second part develops the mathematical underpinnings of the language","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.82","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703389","Message Sequence Charts;scenarios;requirements modeling;formal semantics;refinement.","Software systems;Guidelines;Design engineering;Petri nets;Terminology;Joining processes;Vehicles;Standards development;Unified modeling language;Telephony","distributed processing;flowcharting;formal specification;programming language semantics;specification languages;visual languages","triggered message sequence chart;graphical framework;scenario-based distributed system requirements modeling;formal specification;composition operators;specification language","","20","","43","","","","","","IEEE","IEEE Journals & Magazines"
"A Realistic Empirical Evaluation of the Costs and Benefits of UML in Software Maintenance","W. J. Dzidek; E. Arisholm; L. C. Briand","Simula Research Laboratory Simula Research Laboratory; Simula Research Laboratory; Simula Research Laboratory","IEEE Transactions on Software Engineering","","2008","34","3","407","432","The Unified Modeling Language (UML) is the de facto standard for object-oriented software analysis and design modeling. However, few empirical studies exist that investigate the costs and evaluate the benefits of using UML in realistic contexts. Such studies are needed so that the software industry can make informed decisions regarding the extent to which they should adopt UML in their development practices. This is the first controlled experiment that investigates the costs of maintaining and the benefits of using UML documentation during the maintenance and evolution of a real, non-trivial system, using professional developers as subjects, working with a state-of-the-art UML tool during an extended period of time. The subjects in the control group had no UML documentation. In this experiment, the subjects in the UML group had on average a practically and statistically significant 54% increase in the functional correctness of changes (p=0.03), and an insignificant 7% overall improvement in design quality (p=0.22) - though a much larger improvement was observed on the first change task (56%) - at the expense of an insignificant 14% increase in development time caused by the overhead of updating the UML documentation (p=0.35).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4459340","Maintainability;Distribution;Maintenance;and Enhancement;Software Engineering;Software/Software Engi;Design notations and documentation;Object-Oriented Programming;Maintainability;Distribution;Maintenance;and Enhancement;Software Engineering;Software/Software Engi;Design notations and documentation;Object-Oriented Programming","Unified modeling language;Software maintenance;Documentation;Costs;Software standards;Software design;Object oriented modeling;Computer industry;Electrical equipment industry;Control systems","object-oriented programming;software maintenance;software quality;systems analysis;Unified Modeling Language","realistic empirical evaluation;UML;software maintenance;Unified Modeling Language;object-oriented software analysis;design modeling;design quality","","71","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamically discovering likely program invariants to support program evolution","M. D. Ernst; J. Cockrell; W. G. Griswold; D. Notkin","Dept. of Electr. Eng. & Comput. Sci., MIT, Cambridge, MA, USA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","2","99","123","Explicitly stated program invariants can help programmers by identifying program properties that must be preserved when modifying code. In practice, however, these invariants are usually implicit. An alternative to expecting programmers to fully annotate code with invariants is to automatically infer likely invariants from the program itself. This research focuses on dynamic techniques for discovering invariants from execution traces. This article reports three results. First, it describes techniques for dynamically discovering invariants, along with an implementation, named Daikon, that embodies these techniques. Second, it reports on the application of Daikon to two sets of target programs. In programs from Gries's work (1981) on program derivation, the system rediscovered predefined invariants. In a C program lacking explicit invariants, the system discovered invariants that assisted a software evolution task. These experiments demonstrate that, at least for small programs, invariant inference is both accurate and useful. Third, it analyzes scalability issues, such as invariant detection runtime and accuracy, as functions of test suites and program points instrumented.","0098-5589;1939-3520;2326-3881","","10.1109/32.908957","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=908957","","Programming profession;Testing;Instruments;Application software;Runtime;Detectors;Computer Society;Scalability;Formal specifications;Pattern analysis","software maintenance;reverse engineering","likely program invariants;program evolution;explicitly stated program invariants;program properties;modifying code;execution traces;Daikon;program derivation;software evolution;small programs;invariant inference;scalability","","354","","88","","","","","","IEEE","IEEE Journals & Magazines"
"An Integrative Economic Optimization Approach to Systems Development Risk Management","M. Benaroch; J. Goldstein","Syracuse University, Syracuse; Syracuse University, Syracuse","IEEE Transactions on Software Engineering","","2009","35","5","638","653","Despite significant research progress on the problem of managing systems development risk, we are yet to see this problem addressed from an economic optimization perspective. Doing so entails answering the question: What mitigations should be planned and deployed throughout the life of a systems development project in order to control risk and maximize project value? We introduce an integrative economic optimization approach to solving this problem. The approach is integrative since it bridges two complementary research streams: one takes a traditional microlevel technical view on the software development endeavor alone, another takes a macrolevel business view on the entire life cycle of a systems project. Bridging these views requires recognizing explicitly that value-based risk management decisions pertaining to one level impact and can be impacted by decisions pertaining to the other level. The economic optimization orientation follows from reliance on real options theory in modeling risk management decisions within a dynamic stochastic optimization setting. Real options theory is well suited to formalizing the impacts of risk as well as the asymmetric and contingent economic benefits of mitigations, in a way that enables their optimal balancing. We also illustrate how the approach is applied in practice to a small realistic example.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815277","Risk management;systems development;economics.","Risk management;Programming;Control systems;Bridges;Costs;Risk analysis;Research and development management;Stochastic processes;Information analysis;Information management","business data processing;dynamic programming;macroeconomics;project management;risk management;software prototyping;stochastic programming","integrative economic optimization approach;system development risk management;system development project life cycle;microlevel technical view;software development;macrolevel business view;value-based risk management decision;dynamic stochastic optimization setting;real option theory","","6","","43","","","","","","IEEE","IEEE Journals & Magazines"
"A Multi-Objective Technique to Prioritize Test Cases","A. Marchetto; M. M. Islam; W. Asghar; A. Susi; G. Scanniello","independent researchers; independent researchers; independent researchers; Fondazione Bruno Kessler; DiMIE - University of Basilicata","IEEE Transactions on Software Engineering","","2016","42","10","918","940","While performing regression testing, an appropriate choice for test case ordering allows the tester to early discover faults in source code. To this end, test case prioritization techniques can be used. Several existing test case prioritization techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test case prioritization technique that determines the ordering of test cases that maximize the number of discovered faults that are both technical and business critical. In other words, our new technique aims at both early discovering faults and reducing the execution cost of test cases. To this end, we automatically recover links among software artifacts (i.e., requirements specifications, test cases, and source code) and apply a metric-based approach to automatically identify critical and fault-prone portions of software artifacts, thus becoming able to give them more importance during test case prioritization. We experimentally evaluated our technique on 21 Java applications. The obtained results support our hypotheses on efficiency and effectiveness of our new technique and on the use of automatic artifacts analysis and weighting in test case prioritization.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2510633","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7362042","Regression testing;requirements;testing;test case prioritization","Software;Fault diagnosis;Testing;Software engineering;Business;Electronic mail;Optimization","formal specification;formal verification;program testing;regression analysis;software fault tolerance;software metrics;source code (software);systems analysis","multiobjective technique;test case prioritization;regression testing;source code fault;software artifact;requirements specification;metric-based approach","","8","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Nonfunctional requirements: from elicitation to conceptual models","L. M. Cysneiros; J. C. S. do Prado Leite","Dept. of Math. & Stat., York Univ., North York, Ont., Canada; NA","IEEE Transactions on Software Engineering","","2004","30","5","328","350","Nonfunctional requirements (NFRs) have been frequently neglected or forgotten in software design. They have been presented as a second or even third class type of requirement, frequently hidden inside notes. We tackle this problem by treating NFRs as first class requirements. We present a process to elicit NFRs, analyze their interdependencies, and trace them to functional conceptual models. We focus our attention on conceptual models expressed using UML (Unified Modeling Language). Extensions to UML are proposed to allow NFRs to be expressed. We show how to integrate NFRs into the class, sequence, and collaboration diagrams. We also show how use cases and scenarios can be adapted to deal with NFRs. This work was used in three case studies and their results suggest that by using our proposal we can improve the quality of the resulting conceptual models.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291835","Software design;requirements elicitation;nonfunctional requirements;goal graphs;UML conceptual models.","Unified modeling language;Programming;Computer Society;Software design;Collaborative work;Proposals;Security;Safety;Cultural differences;Software measurement","specification languages;formal specification;object-oriented programming","nonfunctional requirements;software design;functional conceptual models;UML;Unified Modeling Language;requirements elicitation;goal graphs","","112","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Hidden Implementation Dependencies in High Assurance and Critical Computing Systems","D. Conte de Leon; J. Alves-Foss","Center for Secure and Dependable Systems, University of Idaho, Moscow, ID; Center for Secure and Dependable Systems, University of Idaho, Moscow, ID","IEEE Transactions on Software Engineering","","2006","32","10","790","811","Critical and catastrophic failures in high assurance and critical computing systems can arise from unfounded assumptions of independence between system components, requirements, and constraints (work product sections), which can stem from misunderstandings and miscommunication between system engineers, managers, and operators and from inadequate or incomplete traceability between system work products. In this article, we propose a formal framework for the effective implementation of traceability between work product sections along with a technique for discovering potential causes of critical failures in high assurance and critical computing system models. We introduce a new abstraction of interrelated work product sections called implementation meta-work product and describe how our technique finds these meta-work products. We also demonstrate how this technique can be used to help analysts discover potential causes of safety-related errors in high assurance and critical computing systems by applying it to one case study of a known critical error and to one case study where we anticipate potential safety hazards","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.103","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717472","Critical systems;data dependencies;emergent properties;formal frameworks;high assurance systems;knowledge modeling;set theory;software engineering documentation;system development;system safety;traceability;work products.","Certification;Software engineering;IEC standards;Navigation;Systems engineering and theory;Engineering management;Safety;Hazards;Set theory;Documentation","formal specification;program diagnostics;safety-critical software;software maintenance;system recovery","critical computing system;catastrophic failure;system component;meta-work product;safety-related error;data dependency;formal traceability;high assurance system","","8","","114","","","","","","IEEE","IEEE Journals & Magazines"
"Improving the precision of INCA by eliminating solutions with spurious cycles","S. F. Siegel; G. S. Avrunin","Dept. of Comput. Sci., Massachusetts Univ., Amherst, MA, USA; NA","IEEE Transactions on Software Engineering","","2002","28","2","115","128","The Inequality Necessary Condition Analyzer (INCA) is a finite-state verification tool that has been able to check properties of some very large concurrent systems. INCA checks a property of a concurrent system by generating a system of inequalities that must have integer solutions if the property can be violated. There may, however, be integer solutions to the inequalities that do not correspond to an execution violating the property. INCA thus accepts the possibility of an inconclusive result in exchange for greater tractability. We describe here a method for eliminating one of the two main sources of these inconclusive results.","0098-5589;1939-3520;2326-3881","","10.1109/32.988494","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988494","","Automata;Computer Society;Linear programming;System recovery;System testing;Explosions;State-space methods;Costs","formal verification;software tools;finite state machines;concurrency theory","Inequality Necessary Condition Analyzer;finite-state verification tool;very large concurrent systems;INCA;inequalities;integer solutions;spurious cycle solution elimination","","1","","26","","","","","","IEEE","IEEE Journals & Magazines"
"Toward a mathematical foundation of software engineering methods","M. Broy","Inst. fur Inf., Tech. Univ. Munchen, Germany","IEEE Transactions on Software Engineering","","2001","27","1","42","57","The development of large software systems consists of a sequence of modeling tasks. It requires the modeling and description of the application domain, software requirements, software architecture, software components, their internal structure, and their implementation. Technically, in software engineering, we work with a development method and description techniques with modeling, refinement, and implementation concepts. Today, much of the modeling is carried out by informal text and graphical description techniques. The development is organized in a development process and supported by CASE tools. We show how mathematics can provide a scientific foundation for the modeling aspects, description techniques, and development methods of software engineering. Such a scientific foundation leads to a deeper understanding of the development process and to a basis for a more powerful tool support.","0098-5589;1939-3520;2326-3881","","10.1109/32.895987","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=895987","","Software engineering;Object oriented modeling;Software systems;Power engineering and energy;Mathematical model;Application software;Power system modeling;Systems engineering and theory;Unified modeling language;Specification languages","software engineering;computer aided software engineering","mathematical foundation;software engineering methods;modeling tasks;software requirements;software architecture;software components;text description;graphical description;CASE","","12","","31","","","","","","IEEE","IEEE Journals & Magazines"
"ARENA: An Approach for the Automated Generation of Release Notes","L. Moreno; G. Bavota; M. D. Penta; R. Oliveto; A. Marcus; G. Canfora","University of Texas at Dallas, Richardson, TX; University of Lugano, Lugano, Switzerland; University of Sannio, Benevento, Italy; University of Molise, Pesche, IS, Italy; University of Texas at Dallas, Richardson, TX; University of Sannio, Benevento, Italy","IEEE Transactions on Software Engineering","","2017","43","2","106","127","Release notes document corrections, enhancements, and, in general, changes that were implemented in a new release of a software project. They are usually created manually and may include hundreds of different items, such as descriptions of new features, bug fixes, structural changes, new or deprecated APIs, and changes to software licenses. Thus, producing them can be a time-consuming and daunting task. This paper describes ARENA (Automatic RElease Notes generAtor), an approach for the automatic generation of release notes. ARENA extracts changes from the source code, summarizes them, and integrates them with information from versioning systems and issue trackers. ARENA was designed based on the manual analysis of 990 existing release notes. In order to evaluate the quality of the release notes automatically generated by ARENA, we performed four empirical studies involving a total of 56 participants (48 professional developers and eight students). The obtained results indicate that the generated release notes are very good approximations of the ones manually produced by developers and often include important information that is missing in the manually created release notes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2591536","National Science Foundation; Markos project; European Commission; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7513412","Release notes;software documentation;software evolution","Libraries;Licenses;Feature extraction;Documentation;Computer bugs;Open source software","application program interfaces;program debugging;software engineering;source code (software);system documentation","ARENA;automated release note generation;release note document corrections;release note document enhancements;software project;bug fixes;new features;structural changes;API;software licenses;source code change extraction;source code change summarization","","6","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing symbolic model checking for statecharts","W. Chan; R. J. Anderson; P. Beame; D. H. Jones; D. Notkin; W. E. Warner","Dept. of Comput. Sci. & Eng., Washington Univ., Seattle, WA, USA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","2","170","190","Symbolic model checking based on binary decision diagrams is a powerful formal verification technique for reactive systems. In this paper, we present various optimizations for improving the time and space efficiency of symbolic modal checking for systems specified as statecharts. We used these techniques in our analyses of the models of a collision avoidance system and a fault-tolerant electrical power distribution (EPD) system, both used on commercial aircraft. The techniques together reduce the time and space requirements by orders of magnitude, making feasible some analysis that was previously intractable. We also elaborate on the results of verifying the EPD model. The analysis disclosed subtle modeling and logical flaws not found by simulation.","0098-5589;1939-3520;2326-3881","","10.1109/32.908961","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=908961","","Power system modeling;Data structures;Boolean functions;Aerospace electronics;Binary decision diagrams;Formal verification;Collision avoidance;Power distribution;Aircraft;Hardware","binary decision diagrams;formal verification;aerospace computing;fault tolerant computing;aircraft;collision avoidance","symbolic model checking optimization;statecharts;binary decision diagrams;formal verification;reactive systems;time efficiency;space efficiency;collision avoidance system;fault-tolerant electrical power distribution system;commercial aircraft","","13","","41","","","","","","IEEE","IEEE Journals & Magazines"
"What Types of Defects Are Really Discovered in Code Reviews?","M. V. Mäntylä; C. Lassenius","Helsinki University of Technology, TKK; Helsinki University of Technology, TKK","IEEE Transactions on Software Engineering","","2009","35","3","430","448","Research on code reviews has often focused on defect counts instead of defect types, which offers an imperfect view of code review benefits. In this paper, we classified the defects of nine industrial (C/C++) and 23 student (Java) code reviews, detecting 388 and 371 defects, respectively. First, we discovered that 75 percent of defects found during the review do not affect the visible functionality of the software. Instead, these defects improved software evolvability by making it easier to understand and modify. Second, we created a defect classification consisting of functional and evolvability defects. The evolvability defect classification is based on the defect types found in this study, but, for the functional defects, we studied and compared existing functional defect classifications. The classification can be useful for assigning code review roles, creating checklists, assessing software evolvability, and building software engineering tools. We conclude that, in addition to functional defects, code reviews find many evolvability defects and, thus, offer additional benefits over execution-based quality assurance methods that cannot detect evolvability defects. We suggest that code reviews may be most valuable for software products with long life cycles as the value of discovering evolvability defects in them is greater than for short life cycle systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.71","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4604671","Code inspections and walkthroughs;enhancement;extensibility;maintainability;restructuring.;Source code organization;Code documentation;Construction QA;Methods for SQA and V&V;Measurement applied to SQA and V&V;Code inspections and walkthroughs;Maintainability","Quality assurance;Inspection;Runtime;Java;Software tools;Software engineering;Software measurement;Software quality;Guidelines;Timing","pattern classification;software maintenance;software reviews","software functionality;software evolvability;defect classification;software engineering tools;execution-based quality assurance methods;software products","","50","","73","","","","","","IEEE","IEEE Journals & Magazines"
"On the Semantics of Distributed Reactive Programming: The Cost of Consistency","A. Margara; G. Salvaneschi","Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano, Milan, Italy; Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany","IEEE Transactions on Software Engineering","","2018","44","7","689","711","The reactive programming paradigm aims to simplify the development of reactive systems. It provides abstractions to define time-changing values that are automatically updated by the runtime according to their dependencies. The benefits of reactive programming in distributed settings have been recognized for long. Yet, existing solutions for distributed reactive programming enforce the same semantics as in single processes, introducing communication and synchronization costs that hamper scalability. Establishing suitable abstractions for distributed reactive programming demands for a deeper investigation of the semantics of change propagation. This paper takes a foundational approach and defines precise propagation semantics in terms of consistency guarantees that constrain the order and isolation of value updates. We study the benefits and costs of these consistency guarantees both theoretically and empirically, using case studies and synthetic benchmarks. We show that different applications require different levels of consistency and that manually implementing the required level on a middleware that provides a lower one annuls the abstraction improvements of reactive programming. This motivates a framework that enables the developers to select the best trade-off between consistency and overhead for the problem at hand. To this end, we present DREAM, a distributed reactive programming middleware with flexible consistency guarantees.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2833109","European Research Council; German Research Foundation (DFG); DFG; AWS Cloud Credits; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8354906","Distributed reactive programming;consistency guarantees;reactive programming middleware;DREAM","Programming;Semantics;Artificial intelligence;Runtime;Middleware;Games;Computational modeling","middleware","synchronization costs;distributed reactive programming demands;foundational approach;value updates;distributed reactive programming middleware;flexible consistency guarantees;reactive programming paradigm;reactive systems;time-changing values;distributed settings","","1","","97","","","","","","IEEE","IEEE Journals & Magazines"
"Fusion: a system for business users to manage program variability","S. Weber; H. Chan; L. Degenaro; J. Diament; A. Fokoue-Nkoutche; I. Rouvellou","IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","IEEE Transactions on Software Engineering","","2005","31","7","570","587","In order to make software components more flexible and reusable, it is desirable to provide business users with facilities to assemble and control them without their needing programming knowledge. This paper describes a fully functional prototype middleware system where variability is externalized so that core applications need not be altered for anticipated changes. In this system, application behavior modification is fast and easy, making this middleware suitable for frequently changing programs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.82","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492372","Index Terms- Web site management/development tools;middleware/business logic;specialized application languages;domain-specific architectures;human factors in software design;user interfaces.","Application software;Middleware;Programming profession;Companies;Law;Logic;Business communication;Knowledge management;Software reusability;Assembly","software development management;object-oriented programming;software architecture;software reusability;middleware;Internet;user interfaces;business data processing","program variability management;software component flexibility;software component reusability;middleware system;Web site management tool;Web site development tool;business logic;software architecture;human factors;software design;user interface","","","","32","","","","","","IEEE","IEEE Journals & Magazines"
"An Ontology-Based Product Architecture Derivation Approach","H. A. Duran-Limon; C. A. Garcia-Rios; F. E. Castillo-Barrera; R. Capilla","Department of Information Systems, University of Guadalajara, CUCEA, Mexico; Department of Information Systems, University of Guadalajara, CUCEA, Mexico; School of Engineering, Autonomous University of San Luis Potosi, Mexico; Department of Computer Science, Rey Juan Carlos University of Madrid, Spain","IEEE Transactions on Software Engineering","","2015","41","12","1153","1168","Software product line (SPL) engineering has proven to improve software quality and shorten development cycles, cost and time. In product line engineering, product derivation is concerned with the realization of the variability at the implementation level. However, the majority of research works focuses on instantiating the variants selected in the final product, while the derivation at the architecture level has been poorly explored. As product line engineers often customize the product architecture by hand during the application engineering phase, the derivation and customization processes of the product line architecture (PLA) might be in some cases error-prone. Consequently, in this research we present an Ontology-based product Architecture Derivation (OntoAD) framework which automates the derivation of product-specific architectures from an SPL architecture. Our solution uses a language-independent model to specify the product line architecture and a model-driven engineering approach for architecture derivation activities. We use an ontology formalism to reason about the automatic generation of model-to-model transformation rules based on the selection of features and we illustrate our approach using a voice over IP motivating example. Finally, we report results about scalability and performance regarding the size of the variability model.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2449854","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7134799","Software Product Lines;Feature Models;Software Architecture;Product Derivation;Architecture Derivation;Ontologies;Model-Driven Engineering;Scalability;Software product lines;feature models;software architecture;product derivation;architecture derivation;ontologies;model-driven engineering;scalability","Unified modeling language;Computer architecture;Ontologies;Software product lines;Software architecture;Scalability","ontologies (artificial intelligence);software architecture;software product lines;software quality","ontology-based product architecture derivation approach;software product line engineering;SPL engineering;software quality;software development cycles;software development cost;software development time;product derivation;product customization;product line architecture;PLA;OntoAD;product-specific architectures;SPL architecture;language-independent model;model-driven engineering;architecture derivation activities;model-to-model transformation rules;features selection;voice over IP","","2","","53","","","","","","IEEE","IEEE Journals & Magazines"
"G<sc>o</sc>P<sc>rime</sc>: A Fully Decentralized Middleware for Utility-Aware Service Assembly","M. Caporuscio; V. Grassi; M. Marzolla; R. Mirandola","Department of Computer Science, Linnaeus University, Växjö, Sweden; Informatica, Sistemi e Produzione, University of Roma “Tor Vergata,”, Roma, Italy; Dipartimento di Informatica Scienza e Ingegneria, University of Bologna, Bologna, Italy; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy","IEEE Transactions on Software Engineering","","2016","42","2","136","152","Modern applications, e.g., for pervasive computing scenarios, are increasingly reliant on systems built from multiple distributed components, which must be suitably composed to meet some specified functional and non-functional requirements. A key challenge is how to efficiently and effectively manage such complex systems. The use of self-management capabilities has been suggested as a possible way to address this challenge. To cope with the scalability and robustness issues of large distributed systems, self-management should ideally be architected in a decentralized way, where the overall system behavior emerges from local decisions and interactions. Within this context, we propose GOPRIME, a fully decentralized middleware solution for the adaptive self-assembly of distributed services. The GOPRIME goal is to build and maintain an assembly of services that, besides functional requirements, fulfils also global quality-of-service and structural requirements. The key aspect of GOPRIME is the use of a gossip protocol to achieve decentralized information dissemination and decision making. To show the validity of our approach, we present results from the experimentation of a prototype implementation of GOPRIME in a mobile health application, and an extensive set of simulation experiments that assess the effectiveness of GOPRIME in terms of scalability, robustness and convergence speed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2476797","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7243346","Service-oriented architecture;pervasive computing;runtime adaptation;quality of service;gossip protocol;Service-oriented architecture;pervasive computing;runtime adaptation;quality of service;gossip protocol","Assembly;Compounds;Quality of service;Peer-to-peer computing;Scalability;Middleware;Robustness","distributed processing;middleware;ubiquitous computing","GOPRIME;fully decentralized middleware;utility-aware service assembly;pervasive computing;distributed components;self-management capabilities;gossip protocol;decentralized information dissemination;decision making;mobile health application","","4","","30","","","","","","IEEE","IEEE Journals & Magazines"
"Task construction for model-based design of embedded control software","S. Wang; K. G. Shin","Electr. & Control Integration Lab., Gen. Motors Corp., Warren, MI, USA; NA","IEEE Transactions on Software Engineering","","2006","32","4","254","264","Constructing runtime tasks, or operating system-level processes/threads, from the components of software design models is crucial to the model-based development of embedded control software. A better method should explore more design choices and reduce the overheads of the runtime system to meet the timing and resource constraints of embedded control software. This paper presents a novel, two-step method for systematic and automatic construction of runtime tasks from software design models. It uses graph transformation to construct a task set meeting system-level end-to-end (e2e) timing constraints. Its first step decomposes the system-level e2e timing constraints into the components' timing constraints, which form a necessary condition for any valid and feasible schedule. The second step iteratively merges the components into tasks and sequences their executions. A thus-constructed task set is proven to meet both intercomponent precedence and system-level e2e timing constraints and to minimize runtime overheads by minimizing the total number of resultant tasks. Our evaluation results based on randomly generated software models have shown that the proposed method outperforms commonly used methods and is also scalable.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628971","Task construction;model transformation;model-based design;embedded software.","Embedded software;Timing;Software design;Runtime;Control systems;Scheduling;Automatic control;Application software;Design optimization","embedded systems;software engineering;object-oriented programming","runtime task construction;software design model;model-based software development;embedded control software;graph transformation;system-level end-to-end timing constraint","","22","","27","","","","","","IEEE","IEEE Journals & Magazines"
"A Formal Specification and Verification Framework for Timed Security Protocols","L. Li; J. Sun; Y. Liu; M. Sun; J. Dong","ISTD, Singapore University of Technology and Design, Singapore; ISTD, Singapore University of Technology and Design, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; School of Mathematical Sciences, Peking University, Beijing, China; School of Computing, National University of Singapore, Singapore","IEEE Transactions on Software Engineering","","2018","44","8","725","746","Nowadays, protocols often use time to provide better security. For instance, critical credentials are often associated with expiry dates in system designs. However, using time correctly in protocol design is challenging, due to the lack of time related formal specification and verification techniques. Thus, we propose a comprehensive analysis framework to formally specify as well as automatically verify timed security protocols. A parameterized method is introduced in our framework to handle timing parameters whose values cannot be decided in the protocol design stage. In this work, we first propose timed applied p-calculus as a formal language for specifying timed security protocols. It supports modeling of continuous time as well as application of cryptographic functions. Then, we define its formal semantics based on timed logic rules, which facilitates efficient verification against various authentication and secrecy properties. Given a parameterized security protocol, our method either produces a constraint on the timing parameters which guarantees the security property satisfied by the protocol, or reports an attack that works for any parameter value. The correctness of our verification algorithm has been formally proved. We evaluate our framework with multiple timed and untimed security protocols and successfully find a previously unknown timing attack in Kerberos V.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2712621","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7939995","Timed security protocol;timed applied$\pi$-calculus;parameterized verification;secrecy and authentication","Protocols;Timing;Authentication;Semantics;Sun","cryptographic protocols;formal specification;formal verification;protocols","comprehensive analysis framework;timing parameters;protocol design stage;formal language;formal semantics;parameterized security protocol;security property;formal verification techniques;formal specification framework;timed security protocols","","","","50","","","","","","IEEE","IEEE Journals & Magazines"
"Usability through Software Design","L. Carvajal; A. M. Moreno; M. Sánchez-Segura; A. Seffah","Universidad Politécnica de Madrid, Madrid; Universidad Politécnica de Madrid, Madrid; Carlos III University of Madrid, Leganes; Concordia University, Montreal","IEEE Transactions on Software Engineering","","2013","39","11","1582","1596","Over the past two decades, the HCI community has proposed specific features that software applications should include to overcome some of the most common usability problems. However, incorporating such usability features into software applications may not be a straightforward process for software developers who have not been trained in usability (i.e., determining when, how, and why usability features should been considered). We have defined a set of usability guidelines for software development to help software engineers incorporate particular usability features into their applications. In this paper, we focus on the software design artifacts provided by the guidelines. We detail the structure of the proposed design artifacts and how they should be used according to the software development process and software architecture used in each application. We have tested our guidelines in an academic setting. Preliminary validation shows that the use of the guidelines reduces development time, improves the quality of the resulting designs, and significantly decreases the perceived complexity of the usability features from the developers' perspective.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6523225","Software usability;software design;software design patterns","Usability;Guidelines;Human computer interaction;Unified modeling language;Communities","human computer interaction;object-oriented methods;software architecture;software quality;user interfaces","software usability;software design patterns;HCI community;human computer interaction;software development process;software design artifacts;software architecture;integral software development quality aspect","","11","","24","","","","","","IEEE","IEEE Journals & Magazines"
"Combining a performance estimation methodology with a hardware/software codesign flow supporting multiprocessor systems","A. Baghdadi; N. -. Zergainoh; W. O. Cesario; A. A. Jerraya","TIMA Lab., Grenoble, France; TIMA Lab., Grenoble, France; TIMA Lab., Grenoble, France; TIMA Lab., Grenoble, France","IEEE Transactions on Software Engineering","","2002","28","9","822","831","This paper addresses performance estimation and architecture exploration issues within the context of hardware/software codesign. We introduce a new methodology to rapidly explore the large design space encountered in hardware/software systems. The proposed methodology is based on a fast and accurate estimation approach. This estimation approach takes advantage of both system and RT levels of abstraction, and combines both static and dynamic analysis techniques, in order to obtain the best trade-off between speed and accuracy. It has been implemented as an extension to a hardware/software codesign flow to enable the exploration of a large number of multiprocessor architecture solutions from the very start of the design process. The effectiveness of the proposed methodology is illustrated by a significant application example. Experimental results indicate strong advantages of the proposed methodology.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033223","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033223","","Hardware;Software performance;Computer architecture;Space technology;Performance analysis;Multiprocessing systems;Application software;Aerospace industry;Space exploration;Protocols","hardware-software codesign;multiprocessing systems;software performance evaluation;computer architecture;software prototyping","performance estimation;architecture exploration;hardware/software codesign;large design space;abstraction;dynamic analysis;static analysis;accuracy;speed;multiprocessor architecture","","17","","29","","","","","","IEEE","IEEE Journals & Magazines"
"DESSERT: a DividE-and-conquer methodology for identifying categorieS, choiceS, and choicE Relations for Test case generation","T. Y. Chen; P. Poon; S. Tang; T. H. Tse","Swinburne University of Technology, Melbourne; The Hong Kong Polytechnic University, Hong Kong; Swinburne University of Technology, Melbourne; The University of Hong Kong, Hong Kong","IEEE Transactions on Software Engineering","","2012","38","4","794","809","This paper extends the choce relation framework, abbreviated as choc'late, which assists software testers in the application of category/choice methods to testing. choc'late assumes that the tester is able to construct a single choice relation table from the entire specification; this table then forms the basis for test case generation using the associated algorithms. This assumption, however, may not hold true when the specification is complex and contains many specification components. For such a specification, the tester may construct a preliminary choice relation table from each specification component, and then consolidate all the preliminary tables into a final table to be processed by choc'late for test case generation. However, it is often difficult to merge these preliminary tables because such merging may give rise to inconsistencies among choice relations or overlaps among choices. To alleviate this problem, we introduce a DividE-and-conquer methodology for identifying categorieS, choiceS, and choicE Relations for Test case generation, abbreviated as dessert. The theoretical framework and the associated algorithms are discussed. To demonstrate the viability and effectiveness of our methodology, we describe case studies using the specifications of three real-life commercial software systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.69","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5963695","Black-box testing;category-partition method;choice relation framework;choice relation table;software testing;test case generation","Awards activities;Electronic mail;Software systems;Encoding;Software testing","divide and conquer methods;formal specification;program testing","DESSERT;divide-and-conquer methodology;category identification;choice identification;choice relation identification;test case generation;specification components;real-life commercial software systems;software testing;CHOC'LATE;black-box testing","","10","","18","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Literature Review and Meta-Analysis on Cross Project Defect Prediction","S. Hosseini; B. Turhan; D. Gunarathna","University of Oulu, Oulu, Finland; Department of Computer Science, Brunel University London, London, United Kingdom; Vaimo Finland (Oy), Oulu, Finland","IEEE Transactions on Software Engineering","","2019","45","2","111","147","<italic>Background:</italic> Cross project defect prediction (CPDP) recently gained considerable attention, yet there are no systematic efforts to analyse existing empirical evidence. <italic>Objective:</italic> To synthesise literature to understand the state-of-the-art in CPDP with respect to metrics, models, data approaches, datasets and associated performances. Further, we aim to assess the performance of CPDP versus within project DP models. <italic>Method:</italic> We conducted a systematic literature review. Results from primary studies are synthesised (thematic, meta-analysis) to answer research questions. <italic>Results:</italic> We identified 30 primary studies passing quality assessment. Performance measures, except precision, vary with the choice of metrics. Recall, precision, f-measure, and AUC are the most common measures. Models based on Nearest-Neighbour and Decision Tree tend to perform well in CPDP, whereas the popular naïve Bayes yields average performance. Performance of ensembles varies greatly across f-measure and AUC. Data approaches address CPDP challenges using row/column processing, which improve CPDP in terms of recall at the cost of precision. This is observed in multiple occasions including the meta-analysis of CPDP versus WPDP. NASA and Jureczko datasets seem to favour CPDP over WPDP more frequently. <italic>Conclusion:</italic> CPDP is still a challenge and requires more research before trustworthy applications can take place. We provide guidelines for further research.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2770124","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8097045","Defect prediction;fault prediction;cross project;systematic literature review;meta-analysis;within project","Object oriented modeling;Systematics;Measurement;Bibliographies;Predictive models;Context modeling;Data models","","","","7","","150","","","","","","IEEE","IEEE Journals & Magazines"
"Program Behavior Discovery and Verification: A Graph Grammar Approach","C. Zhao; J. Kong; K. Zhang","The University of Texas at Dallas, Richardson; North Dakota State University, Fargo; The University of Texas at Dallas, Richardson","IEEE Transactions on Software Engineering","","2010","36","3","431","448","Discovering program behaviors and functionalities can ease program comprehension and verification. Existing program analysis approaches have used text mining algorithms to infer behavior patterns or formal models from program execution. When one tries to identify the hierarchical composition of a program behavior at different abstraction levels, textual descriptions are not informative and expressive enough. To address this, we present a semi-automatic graph grammar approach to retrieving the hierarchical structure of the program behavior. The hierarchical structure is built on recurring substructures in a bottom-up fashion. We formulate the behavior discovery and verification problem as a graph grammar induction and parsing problem, i.e., automatically iteratively mining qualified patterns and then constructing graph rewriting rules. Furthermore, using the induced grammar to parse the behavioral structure of a new program could verify if the program has the same behavioral properties specified by the grammar.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.3","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383371","Visual language;graph grammar induction;program comprehension;reengineering.","Software maintenance;Reverse engineering;Learning automata;Software systems;Data mining;Clustering algorithms;Pattern analysis;Algorithm design and analysis;Text mining;Documentation","data mining;graph grammars;program verification","program behavior discovery;program behavior verification;program comprehension;text mining algorithms;behavior patterns;formal models;program execution;semi-automatic graph grammar approach;graph rewriting rules;mining qualified patterns","","13","","60","","","","","","IEEE","IEEE Journals & Magazines"
"Detecting, Tracing, and Monitoring Architectural Tactics in Code","M. Mirakhorli; J. Cleland-Huang","Department of Software Engineering, Rochester, NY; School of Computing, Chicago, IL","IEEE Transactions on Software Engineering","","2016","42","3","205","220","Software architectures are often constructed through a series of design decisions. In particular, architectural tactics are selected to satisfy specific quality concerns such as reliability, performance, and security. However, the knowledge of these tactical decisions is often lost, resulting in a gradual degradation of architectural quality as developers modify the code without fully understanding the underlying architectural decisions. In this paper we present a machine learning approach for discovering and visualizing architectural tactics in code, mapping these code segments to tactic traceability patterns, and monitoring sensitive areas of the code for modification events in order to provide users with up-to-date information about underlying architectural concerns. Our approach utilizes a customized classifier which is trained using code extracted from fifty performance-centric and safety-critical open source software systems. Its performance is compared against seven off-the-shelf classifiers. In a controlled experiment all classifiers performed well; however our tactic detector outperformed the other classifiers when used within the larger context of the Hadoop Distributed File System. We further demonstrate the viability of our approach for using the automatically detected tactics to generate viable and informative messages in a simulation of maintenance events mined from Hadoop's change management system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2479217","US National Science Foundation; Research Experience for Undergraduates; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7270338","Architecture;traceability;tactics;traceability information models;Architecture;traceability;tactics;traceability information models","Heart beat;Monitoring;Detectors;Reliability;Biomedical monitoring;Authentication","learning (artificial intelligence);pattern classification;program diagnostics;public domain software;software architecture;system monitoring","code architectural tactics monitoring;code architectural tactics detection;code architectural tactics tracing;machine learning;code architectural tactics visualization;tactic traceability patterns;performance-centric software systems;safety-critical open source software systems;off-the-shelf classifiers;Hadoop Distributed File System;Hadoops change management system","","17","","73","","","","","","IEEE","IEEE Journals & Magazines"
"An internally replicated quasi-experimental comparison of checklist and perspective based reading of code documents","O. Laitenberger; K. El Emam; T. G. Harbich","Fraunhofer Inst. for Exp. Software Eng., Kaiserslautern, Germany; NA; NA","IEEE Transactions on Software Engineering","","2001","27","5","387","421","The basic premise of software inspections is that they detect and remove defects before they propagate to subsequent development phases where their detection and correction cost escalates. To exploit their full potential, software inspections must call for a close and strict examination of the inspected artifact. For this, reading techniques for defect detection may be helpful since these techniques tell inspection participants what to look for and, more importantly, how to scrutinize a software artifact in a systematic manner. Recent research efforts investigated the benefits of scenario-based reading techniques. A major finding has been that these techniques help inspection teams find more defects than existing state-of-the-practice approaches, such as, ad-hoc or checklist-based reading (CBR). We experimentally compare one scenario-based reading technique, namely, perspective-based reading (PBR), for defect detection in code documents with the more traditional CBR approach. The comparison was performed in a series of three studies, as a quasi experiment and two internal replications, with a total of 60 professional software developers at Bosch Telecom GmbH. Meta-analytic techniques were applied to analyze the data.","0098-5589;1939-3520;2326-3881","","10.1109/32.922713","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=922713","","Inspection;Costs;Bioreactors;Software quality;Computer Society;Phase detection;Software performance;Telecommunications;Software engineering;Data analysis","program debugging;data analysis;software engineering","checklist-based reading;perspective based reading;code documents;software inspection;defect detection;scenario-based reading;experiment;professional software developers;Bosch Telecom;data analysis","","40","","90","","","","","","IEEE","IEEE Journals & Magazines"
"Asking and Answering Questions during a Programming Change Task","J. Sillito; G. C. Murphy; K. De Volder","University of Calgary, Calgary; University of British Columbia, Vancouver; University of British Columbia, Vancouver","IEEE Transactions on Software Engineering","","2008","34","4","434","451","Little is known about the specific kinds of questions programmers ask when evolving a code base and how well existing tools support those questions. To better support the activity of programming, answers are needed to three broad research questions: 1) What does a programmer need to know about a code base when evolving a software system? 2) How does a programmer go about finding that information? 3) How well do existing tools support programmers in answering those questions? We undertook two qualitative studies of programmers performing change tasks to provide answers to these questions. In this paper, we report on an analysis of the data from these two user studies. This paper makes three key contributions. The first contribution is a catalog of 44 types of questions programmers ask during software evolution tasks. The second contribution is a description of the observed behavior around answering those questions. The third contribution is a description of how existing deployed and proposed tools do, and do not, support answering programmers' questions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4497212","Software psychology;Programming Environments/Construction Tools;Enhancement;Software psychology;Programming Environments/Construction Tools;Enhancement","Programming profession;Data analysis;Software systems;Software tools;Genetic programming;Lab-on-a-chip;IEEE activities;Computer science","software maintenance;software tools","programming change task;code base;software system;software evolution","","98","","76","","","","","","IEEE","IEEE Journals & Magazines"
"A Systematic Review of Software Development Cost Estimation Studies","M. Jorgensen; M. Shepperd","NA; NA","IEEE Transactions on Software Engineering","","2007","33","1","33","53","This paper aims to provide a basis for the improvement of software-estimation research through a systematic review of previous work. The review identifies 304 software cost estimation papers in 76 journals and classifies the papers according to research topic, estimation approach, research approach, study context and data set. A Web-based library of these cost estimation papers is provided to ease the identification of relevant estimation research results. The review results combined with other knowledge provide support for recommendations for future software cost estimation research, including: 1) increase the breadth of the search for relevant studies, 2) search manually for relevant papers within a carefully selected set of journals when completeness is essential, 3) conduct more studies on estimation methods commonly used by the software industry, and 4) increase the awareness of how properties of the data sets impact the results when evaluating estimation methods","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256943","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027147","Systematic review;software cost estimation;software effort estimation;software cost prediction;software effort prediction;research methods.","Programming;Costs;Software libraries;Computer industry;Robustness;Laboratories;Mathematics","software cost estimation","software development cost estimation;software-estimation research;systematic review;Web-based library;software industry;data sets","","377","","320","","","","","","IEEE","IEEE Journals & Magazines"
"Towards Self-Stabilizing Operating Systems","S. Dolev; R. Yagel","Ben-Gurion University of the Negev, Beer-Sheva; Ben-Gurion University of the Negev, Beer-Sheva","IEEE Transactions on Software Engineering","","2008","34","4","564","576","This work presents several approaches for designing self-stabilizing operating systems. The first approach is based on periodical automatic reinstalling of the operating system and restart. The second reinstalls the executable portion of the operating system and uses predicates on the operating system state (content of variables) to ensure that the operating system does not diverge from its specifications. The last approach presents an example of a tailored self-stabilizing very tiny operating system. Prototypes using the Intel Pentium processor were composed.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4553720","DDistributed objects;Fault tolerance;DDistributed objects;Fault tolerance","Operating systems;Robustness;Monitoring;Error correction;Microprocessors;Application software;Resource management;Hardware;Prototypes;Fault tolerant systems","operating systems (computers);software fault tolerance","self-stabilizing operating systems;Intel Pentium processor;fault tolerance","","3","","49","","","","","","IEEE","IEEE Journals & Magazines"
"Mining Semantic Loop Idioms","M. Allamanis; E. T. Barr; C. Bird; P. Devanbu; M. Marron; C. Sutton","University of Edinburgh, Edinburgh, United Kingdom; University College London, London, United Kingdom; Microsoft Research, Redmond, WA; University of California, Davis, CA; Microsoft Research, Redmond, WA; University of Edinburgh, Edinburgh, United Kingdom","IEEE Transactions on Software Engineering","","2018","44","7","651","668","To write code, developers stitch together patterns, like API protocols or data structure traversals. Discovering these patterns can identify inconsistencies in code or opportunities to replace these patterns with an API or a language construct. We present coiling, a technique for automatically mining code for semantic idioms: surprisingly probable, semantic patterns. We specialize coiling for loop idioms, semantic idioms of loops. First, we show that automatically identifiable patterns exist, in great numbers, with a largescale empirical study of loops over 25MLOC. We find that most loops in this corpus are simple and predictable: 90 percent have fewer than 15LOC and 90 percent have no nesting and very simple control. Encouraged by this result, we then mine loop idioms over a second, buildable corpus. Over this corpus, we show that only 50 loop idioms cover 50 percent of the concrete loops. Our framework opens the door to data-driven tool and language design, discovering opportunities to introduce new API calls and language constructs. Loop idioms show that LINQ would benefit from an Enumerate operator. This can be confirmed by the exitence of a StackOverflow question with 542k views that requests precisely this feature.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2832048","Microsoft Research; Engineering and Physical Sciences Research Council; US National Science Foundation; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8355713","Data-driven tool design;idiom mining;code patterns","Semantics;Tools;Syntactics;Data mining;C# languages;Machine learning;Testing","application program interfaces;data mining;data structures","data structure traversals;automatically mining code;semantic idioms;semantic patterns;automatically identifiable patterns;concrete loops;semantic loop idiom mining","","","","68","","","","","","IEEE","IEEE Journals & Magazines"
"An Attack Surface Metric","P. K. Manadhata; J. M. Wing","Symantec Research Labs, Culver City; Carnegie Mellon University, Pittsburgh and US National Science Foundation, Arlington","IEEE Transactions on Software Engineering","","2011","37","3","371","386","Measurement of software security is a long-standing challenge to the research community. At the same time, practical security metrics and measurements are essential for secure software development. Hence, the need for metrics is more pressing now due to a growing demand for secure software. In this paper, we propose using a software system's attack surface measurement as an indicator of the system's security. We formalize the notion of a system's attack surface and introduce an attack surface metric to measure the attack surface in a systematic manner. Our measurement method is agnostic to a software system's implementation language and is applicable to systems of all sizes; we demonstrate our method by measuring the attack surfaces of small desktop applications and large enterprise systems implemented in C and Java. We conducted three exploratory empirical studies to validate our method. Software developers can mitigate their software's security risk by measuring and reducing their software's attack surfaces. Our attack surface reduction approach complements the software industry's traditional code quality improvement approach for security risk mitigation and is useful in multiple phases of the software development lifecycle. Our collaboration with SAP demonstrates the use of our metric in the software development process.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5482589","Code design;life cycle;product metrics;protection mechanisms;risk mitigation;software security.","Software measurement;Security;Programming;Software systems;Size measurement;Time measurement;Pressing;Application software;Java;Software quality","C language;Java;security;software metrics","attack surface metric;software security;security metrics;software development;implementation language;C language;Java language","","167","","53","","","","","","IEEE","IEEE Journals & Magazines"
"GEA: A Goal-Driven Approach toDiscovering Early Aspects","J. Lee; K. Hsu","Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Department of Computer Science, National Taichung University of Education, Taichung, Taiwan","IEEE Transactions on Software Engineering","","2014","40","6","584","602","Aspect-oriented software development has become an important development and maintenance approach to software engineering across requirements, design and implementation phases. However, discovering early aspects from requirements for a better integration of crosscutting concerns into a target system is still not well addressed in the existing works. In this paper, we propose a Goal-driven Early Aspect approach (called GEA) to discovering early aspects by means of a clustering algorithm in which relationships among goals and use cases are utilized to explore similarity degrees of clustering goals, and total interaction degrees are devised to check the validity of the formation of each cluster. Introducing early aspects not only enhances the goal-driven requirements modeling to manage crosscutting concerns, but also provides modularity insights into the analysis and design of software development. Moreover, relationships among goals represented numerically are more informative to discover early aspects and more easily to be processed computationally than qualitative terms. The proposed approach is illustrated by using two problem domains: a meeting scheduler system and a course enrollment system. An experiment is also conducted to evaluate the benefits of the proposed approach with Mann-Whitney U-test to show that the difference between with GEA and without GEA is statistically significant.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2322368","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6822606","Early aspects;goals;goals interaction;fuzzy logic;use cases;goal cluster","Software;Clustering algorithms;Educational institutions;Analytical models;Computer science;Software engineering;Documentation","aspect-oriented programming;pattern clustering;software maintenance","GEA;early aspect discovery;aspect-oriented software development;maintenance approach;software engineering;goal-driven early aspect approach;clustering algorithm;goal-driven requirements;crosscutting concerns;meeting scheduler system;course enrollment system;Mann-Whitney U-test","","3","","51","","","","","","IEEE","IEEE Journals & Magazines"
"CARISMA: context-aware reflective middleware system for mobile applications","L. Capra; W. Emmerich; C. Mascolo","Dept. of Comput. Sci., Univ. Coll. London, UK; Dept. of Comput. Sci., Univ. Coll. London, UK; Dept. of Comput. Sci., Univ. Coll. London, UK","IEEE Transactions on Software Engineering","","2003","29","10","929","945","Mobile devices, such as mobile phones and personal digital assistants, have gained wide-spread popularity. These devices will increasingly be networked, thus enabling the construction of distributed applications that have to adapt to changes in context, such as variations in network bandwidth, battery power, connectivity, reachability of services and hosts, etc. In this paper, we describe CARISMA, a mobile computing middleware which exploits the principle of reflection to enhance the construction of adaptive and context-aware mobile applications. The middleware provides software engineers with primitives to describe how context changes should be handled using policies. These policies may conflict. We classify the different types of conflicts that may arise in mobile computing and argue that conflicts cannot be resolved statically at the time applications are designed, but, rather, need to be resolved at execution time. We demonstrate a method by which policy conflicts can be handled; this method uses a microeconomic approach that relies on a particular type of sealed-bid auction. We describe how this method is implemented in the CARISMA middleware architecture and sketch a distributed context-aware application for mobile devices to illustrate how the method works in practice. We show, by way of a systematic performance evaluation, that conflict resolution does not imply undue overheads, before comparing our research to related work and concluding the paper.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237173","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237173","","Middleware;Mobile computing;Application software;Mobile handsets;Personal digital assistants;Context-aware services;Bandwidth;Batteries;Power engineering computing;Reflection","middleware;mobile computing;game theory;quality of service","CARISMA;middleware;personal digital assistants;mobile phones;mobile computing;context awareness;game theory;conflict resolution","","235","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Verifying Protocol Conformance Using Software Model Checking for the Model-Driven Development of Embedded Systems","Y. Moffett; J. Dingel; A. Beaulieu","CF 18 Avionics System Engineering, Ottawa; Queens University, Kingston; Royal Military College, Kingston","IEEE Transactions on Software Engineering","","2013","39","9","1307","13256","To facilitate modular development, the use of state machines has been proposed to specify the protocol (i.e., the sequence of messages) that each port of a component can engage in. The protocol conformance checking problem consists of determining whether the actual behavior of a component conforms to the protocol specifications on its ports. In this paper, we consider this problem in the context of the model-driven development (MDD) of embedded systems based on UML 2, in which UML 2 state machines are used to specify component behavior. We provide a definition of conformance which slightly extends those found in the literature and reduce the conformance check to a state space exploration. We describe a tool implementing the approach using the Java PathFinder software model checker and the MDD tool IBM Rational RoseRT, discuss its application to three case studies, and show how the tool repeatedly allowed us to find unexpected conformance errors with encouraging performance. We conclude that the approach is promising for supporting the modular development of embedded components in the context of industrial applications of MDD.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6482140","Component-based software engineering;behavioral interface specifications;software modeling;model-driven development;formal specification and verification;software model checking","Unified modeling language;Protocols;Ports (Computers);Software;Safety;Context;Java","embedded systems;formal verification;object-oriented programming;Unified Modeling Language","protocol conformance verification;software model checking;model-driven development;embedded system;protocol conformance checking problem;UML 2 state machines;Unified Modeling Language;state space exploration;Java PathFinder software model checker;IBM Rational RoseRT MDD tool;MDD industrial application","","1","","47","","","","","","IEEE","IEEE Journals & Magazines"
"Improving Multi-Objective Test Case Selection by Injecting Diversity in Genetic Algorithms","A. Panichella; R. Oliveto; M. D. Penta; A. De Lucia","Department of Mathematics and Computer Science, University of Salerno, Fisciano, Salerno, Italy; Department of Bioscience and Territory, University of Molise, Pesche, Isernia, Italy; Department of Engineering, University of Sannio, Benevento, Italy; Department of Mathematics and Computer Science, University of Salerno, Fisciano, Salerno, Italy","IEEE Transactions on Software Engineering","","2015","41","4","358","383","A way to reduce the cost of regression testing consists of selecting or prioritizing subsets of test cases from a test suite according to some criteria. Besides greedy algorithms, cost cognizant additional greedy algorithms, multi-objective optimization algorithms, and multi-objective genetic algorithms (MOGAs), have also been proposed to tackle this problem. However, previous studies have shown that there is no clear winner between greedy and MOGAs, and that their combination does not necessarily produce better results. In this paper we show that the optimality of MOGAs can be significantly improved by diversifying the solutions (sub-sets of the test suite) generated during the search process. Specifically, we introduce a new MOGA, coined as DIversity based Genetic Algorithm (DIV-GA), based on the mechanisms of orthogonal design and orthogonal evolution that increase diversity by injecting new orthogonal individuals during the search process. Results of an empirical study conducted on eleven programs show that DIV-GA outperforms both greedy algorithms and the traditional MOGAs from the optimality point of view. Moreover, the solutions (sub-sets of the test suite) provided by DIV-GA are able to detect more faults than the other algorithms, while keeping the same test execution cost.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2364175","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6936894","Test Case Selection;Regression Testing;Orthogonal Design;Singular Value Decomposition;Genetic Algorithms;Empirical Studies;Test case selection;regression testing;orthogonal design;singular value decomposition;genetic algorithms;empirical studies","Optimization;Greedy algorithms;Testing;Linear programming;Genetic algorithms;Genetics;Sociology","genetic algorithms;greedy algorithms;program testing;search problems","multiobjective test case selection improvement;regression testing cost reduction;test case subset prioritization;test case subset selection;greedy algorithms;multiobjective optimization algorithms;multiobjective genetic algorithms;MOGA optimality improvement;test suite subsets;search process;diversity-based genetic algorithm;DIV-GA;orthogonal design mechanism;orthogonal evolution mechanism;empirical analysis","","23","","76","","","","","","IEEE","IEEE Journals & Magazines"
"<sc>Cina</sc>: Suppressing the Detection of Unstable Context Inconsistency","C. Xu; W. Xi; S. C. Cheung; X. Ma; C. Cao; J. Lu","State Key Laboratory for Novel Software Technology and the Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology and the Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong, China; State Key Laboratory for Novel Software Technology and the Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology and the Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology and the Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China","IEEE Transactions on Software Engineering","","2015","41","9","842","865","Context-aware applications adapt their behavior based on contexts. Contexts can, however, be incorrect. A popular means to build dependable applications is to augment them with a set of constraints to govern the consistency of context values. These constraints are evaluated upon context changes to detect inconsistencies so that they can be timely handled. However, we observe that many context inconsistencies are unstable. They vanish by themselves and do not require handling. Such inconsistencies are detected due to misaligned sensor sampling or improper inconsistency detection scheduling. We call them unstable context inconsistencies (or STINs). STINs should be avoided to prevent unnecessary inconsistency handling and unstable behavioral adaptation to applications. In this article, we study STINs systematically, from examples to theoretical analysis, and present algorithms to suppress their detection. Our key insight is that only certain patterns of context changes can make a consistency constraint subject to the detection of STINs. We derive such patterns and proactively use them to suppress the detection of STINs. We implemented our idea and applied it to real-world applications. Experimental results confirmed its effectiveness in suppressing the detection of numerous STINs with negligible overhead, while preserving the detection of stable context inconsistencies that require inconsistency handling.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2418760","National Basic Research 973 Program; National Natural Science Foundation; Research Grants Council; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7078871","Constraint;context inconsistency;impact propagation;instability analysis;pervasive computing;Constraint;context inconsistency;impact propagation;instability analysis;pervasive computing","Context;Schedules;Delays;Sensors;Middleware;Finite element analysis","software engineering;ubiquitous computing","CINA;unstable context inconsistency detection;context-aware applications;STIN","","5","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient Consistency Measurement Based on Behavioral Profiles of Process Models","M. Weidlich; J. Mendling; M. Weske","Hasso Plattner Institute, Potsdam; Humboldt-Universit&#x0E4;t zu Berlin, Berlin; Hasso Plattner Institute, Berlin","IEEE Transactions on Software Engineering","","2011","37","3","410","429","Engineering of process-driven business applications can be supported by process modeling efforts in order to bridge the gap between business requirements and system specifications. However, diverging purposes of business process modeling initiatives have led to significant problems in aligning related models at different abstract levels and different perspectives. Checking the consistency of such corresponding models is a major challenge for process modeling theory and practice. In this paper, we take the inappropriateness of existing strict notions of behavioral equivalence as a starting point. Our contribution is a concept called behavioral profile that captures the essential behavioral constraints of a process model. We show that these profiles can be computed efficiently, i.e., in cubic time for sound free-choice Petri nets w.r.t. their number of places and transitions. We use behavioral profiles for the definition of a formal notion of consistency which is less sensitive to model projections than common criteria of behavioral equivalence and allows for quantifying deviation in a metric way. The derivation of behavioral profiles and the calculation of a degree of consistency have been implemented to demonstrate the applicability of our approach. We also report the findings from checking consistency between partially overlapping models of the SAP reference model.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.96","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5611557","Process model analysis;process model alignment;behavioral abstraction;consistency checking;consistency measures.","Unified modeling language;Business;Analytical models;Semantics;Computational modeling;Petri nets;Software","commerce;corporate modelling;Petri nets","efficient consistency measurement;behavioral profiles;process-driven business applications;business requirements;system specifications;business process modeling;behavioral constraints;Petri nets;SAP reference model","","76","","82","","","","","","IEEE","IEEE Journals & Magazines"
"Detecting Overly Strong Preconditions in Refactoring Engines","M. Mongiovi; R. Gheyi; G. Soares; M. Ribeiro; P. Borba; L. Teixeira","Department of Computing and Systems, Federal University of Campina Grande, Campina Grande, PB, Brazil; Department of Computing and Systems, Federal University of Campina Grande, Campina Grande, PB, Brazil; Department of Computing and Systems, Federal University of Campina Grande, Campina Grande, PB, Brazil; Computing Institute, Federal University of Alagoas, Maceió, AL, Brazil; Informatics Center, Federal University of Pernambuco, Recife, PE, Brazil; Informatics Center, Federal University of Pernambuco, Recife, PE, Brazil","IEEE Transactions on Software Engineering","","2018","44","5","429","452","Refactoring engines may have overly strong preconditions preventing developers from applying useful transformations. We find that 32 percent of the Eclipse and JRRT test suites are concerned with detecting overly strong preconditions. In general, developers manually write test cases, which is costly and error prone. Our previous technique detects overly strong preconditions using differential testing. However, it needs at least two refactoring engines. In this work, we propose a technique to detect overly strong preconditions in refactoring engines without needing reference implementations. We automatically generate programs and attempt to refactor them. For each rejected transformation, we attempt to apply it again after disabling the preconditions that lead the refactoring engine to reject the transformation. If it applies a behavior preserving transformation, we consider the disabled preconditions overly strong. We evaluate 10 refactorings of Eclipse and JRRT by generating 154,040 programs. We find 15 overly strong preconditions in Eclipse and 15 in JRRT. Our technique detects 11 bugs that our previous technique cannot detect while missing 5 bugs. We evaluate the technique by replacing the programs generated by JDolly with the input programs of Eclipse and JRRT test suites. Our technique detects 14 overly strong preconditions in Eclipse and 4 in JRRT.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2693982","National Institute of Science and Technology for Software Engineering (INES); CNPq; CAPES; FACEPE; FAPEAL PPGs; DEVASSES; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7898404","Refactoring;overly strong preconditions;automated testing;program generation","Engines;Computer bugs;Databases;Testing;Java;Electronic mail;Usability","automatic programming;C language;Java;object-oriented programming;program debugging;program testing;software maintenance","JRRT test suites;refactoring engine;overly strong preconditions;Eclipse;differential testing;rejected transformation;JDolly","","","","53","","","","","","IEEE","IEEE Journals & Magazines"
"The impact of institutional forces on software metrics programs","A. Gopal; T. Mukhopadhyay; M. S. Krishnan","Robert H. Smith Sch. of Bus., Maryland Univ., College Park, MD, USA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","8","679","694","Software metrics programs are an important part of a software organization's productivity and quality initiatives as precursors to process-based improvement programs. Like other innovative practices, the implementation of metrics programs is prone to influences from the greater institutional environment the organization exists in. In this paper, we study the influence of both external and internal institutional forces on the assimilation of metrics programs in software organizations. We use previous case-based research in software metrics programs as well as prior work in institutional theory in proposing a model of metrics implementation. The theoretical model is tested on data collected through a survey from 214 metrics managers in defense-related and commercial software organizations. Our results show that external institutions, such as customers and competitors, and internal institutions, such as managers, directly influence the extent to which organizations change their internal work-processes around metrics programs. Additionally, the adaptation of work-processes leads to increased use of metrics programs in decision-making within the organization. Our research informs managers about the importance of management support and institutions in metrics programs adaptation. In addition, managers may note that the continued use of metrics information in decision-making is contingent on adapting the organization's work-processes around the metrics program. Without these investments in metrics program adaptation, the true business value in implementing metrics and software process improvement is not realized.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.95","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498772","Index Terms- Product metrics;process metrics;software engineering;metrics programs;software development;institutional forces;metrics adaptation;metrics acceptance.","Software metrics;Software quality;Computer industry;Costs;Decision making;Investments;Programming;Software development management;Productivity;Software testing","software metrics;productivity;decision making;software process improvement;software development management;software houses;software quality","software metrics program;software organization productivity;quality initiatives;institutional environment;defense-related organization;commercial software organizations;decision-making;business value;software process improvement program;product metrics adaptation;software engineering;program metrics acceptance;software development;case-based research","","27","","43","","","","","","IEEE","IEEE Journals & Magazines"
"A Study of Social Interactions in Open Source Component Use","M. Palyart; G. C. Murphy; V. Masrani","Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; Department of Computer Science, University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Software Engineering","","2018","44","12","1132","1145","All kinds of software projects, whether open or closed source, rely on open source components. Repositories that serve open source components to organizations, such as the Central Repository and npmjs.org, report billions of requests per year. Despite the widespread reliance of projects on open source components, little is known about the social interactions that occur between developers of a project using a component and developers of the component itself. In this paper, we investigate the social interactions that occur for 5,133 pairs of projects, from two different communities (Java and Ruby) representing user projects that depend on a component project. We consider such questions as how often are there social interactions when a component is used? When do the social interactions occur? And, why do social interactions occur? From our investigation, we observed that social interactions typically occur after a component has been chosen for use and relied upon. When social interactions occur, they most frequently begin with creating issues or feature requests. We also found that the more use a component receives, the less likely it is that developers of project using the component will interact with the component project, and when those interactions occur, they will be shorter in duration. Our results provide insight into how socio-technical interactions occur beyond the level of an individual or small group of projects previously studied by others and identify the need for a new model of socio-technical congruence for dependencies between, instead of within, projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2756043","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8049385","Software reuse;collaboration;social interactions;OSS components","Social factors;Computer bugs;Java;Collaboration;Software reusability;Open source software;Project management","human computer interaction;Java;object-oriented programming;project management;public domain software;software development management","open source components;social interactions;component project;Ruby;Java;sociotechnical congruence;sociotechnical interactions;software projects","","","","27","","","","","","IEEE","IEEE Journals & Magazines"
"Preliminary guidelines for empirical research in software engineering","B. A. Kitchenham; S. L. Pfleeger; L. M. Pickard; P. W. Jones; D. C. Hoaglin; K. El Emam; J. Rosenberg","Dept. of Comput. Sci., Keele Univ., UK; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","8","721","734","Empirical software engineering research needs research guidelines to improve the research and reporting processes. We propose a preliminary set of research guidelines aimed at stimulating discussion among software researchers. They are based on a review of research guidelines developed for medical researchers and on our own experience in doing and reviewing software engineering research. The guidelines are intended to assist researchers, reviewers, and meta-analysts in designing, conducting, and evaluating empirical studies. Editorial boards of software engineering journals may wish to use our recommendations as a basis for developing guidelines for reviewers and for framing policies for dealing with the design, data collection, and analysis and reporting of empirical studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027796","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027796","","Guidelines;Software engineering;Computer Society;Statistics;Computer science;Data analysis;Software standards;Laboratories;Software performance;Surgery","software engineering","software engineering;software researchers","","579","","51","","","","","","IEEE","IEEE Journals & Magazines"
"A fluid model for layered queueing networks","M. Tribastone","Ludwig-Maximilians University of Munich, Munich","IEEE Transactions on Software Engineering","","2013","39","6","744","756","Layered queueing networks are a useful tool for the performance modeling and prediction of software systems that exhibit complex characteristics such as multiple tiers of service, fork/join interactions, and asynchronous communication. These features generally result in nonproduct form behavior for which particularly efficient approximations based on mean value analysis (MVA) have been devised. This paper reconsiders the accuracy of such techniques by providing an interpretation of layered queueing networks as fluid models. Mediated by an automatic translation into a stochastic process algebra, PEPA, a network is associated with a set of ordinary differential equations (ODEs) whose size is insensitive to the population levels in the system under consideration. A substantial numerical assessment demonstrates that this approach significantly improves the quality of the approximation for typical performance indices such as utilization, throughput, and response time. Furthermore, backed by established theoretical results of asymptotic convergence, the error trend shows monotonic decrease with larger population sizes-a behavior which is found to be in sharp contrast with that of approximate mean value analysis, which instead tends to increase.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6314480","Modeling and prediction;Markov processes;PEPA;ordinary differential equations;queueing networks;mean value analysis","Approximation methods;Unified modeling language;Stochastic processes;Sociology;Statistics;Servers;Accuracy","approximation theory;differential equations;queueing theory;software performance evaluation","performance modeling;performance prediction;software systems;layered queueing networks;nonproduct form behavior;mean value analysis;MVA;fluid models;automatic translation;stochastic process algebra;PEPA;ordinary differential equations;numerical assessment;approximation quality;performance indices;asymptotic convergence","","17","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Local versus Global Lessons for Defect Prediction and Effort Estimation","T. Menzies; A. Butcher; D. Cok; A. Marcus; L. Layman; F. Shull; B. Turhan; T. Zimmermann","West Virginia University, Morgantown; West Virginia University, Morgantown; GrammaTech, Ithaca; Wayne State University, Detroit; Fraunhofer Center, College Park; Fraunhofer Center, College Park; University of Oulu, Oulu; Microsoft Research, Redmond","IEEE Transactions on Software Engineering","","2013","39","6","822","834","Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.83","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363444","Data mining;clustering;defect prediction;effort estimation","Estimation;Data models;Context;Java;Telecommunications;Measurement;Software","automatic test pattern generation;data mining;pattern clustering","defect prediction;effort estimation;global lessons;local lessons;automated clustering tools;PROMISE repository;data source;learned lesson generated rule;defect dataset","","70","","71","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal project feature weights in analogy-based cost estimation: improvement and limitations","M. Auer; A. Trendowicz; B. Graser; E. Haunschmid; S. Biffl","Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Austria; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","2","83","92","Cost estimation is a vital task in most important software project decisions such as resource allocation and bidding. Analogy-based cost estimation is particularly transparent, as it relies on historical information from similar past projects, whereby similarities are determined by comparing the projects' key attributes and features. However, one crucial aspect of the analogy-based method is not yet fully accounted for: the different impact or weighting of a project's various features. Current approaches either try to find the dominant features or require experts to weight the features. Neither of these yields optimal estimation performance. Therefore, we propose to allocate separate weights to each project feature and to find the optimal weights by extensive search. We test this approach on several real-world data sets and measure the improvements with commonly used quality metrics. We find that this method 1) increases estimation accuracy and reliability, 2) reduces the model's volatility and, thus, is likely to increase its acceptance in practice, and 3) indicates upper limits for analogy-based estimation quality as measured by standard metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599418","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599418","Software cost estimation;analogy-based cost estimation;project clustering;project features.","Cost function;Spatial databases;Resource management;Portfolios;Risk management;User interfaces;Yield estimation;Testing;Measurement standards;Human factors","software cost estimation;software process improvement;software reliability;software quality;project management;software metrics;software management","optimal project feature weights;analogy-based cost estimation;software project decisions;software quality metrics;software cost estimation;software process improvement;software reliability;software project management","","61","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Fluid stochastic Petri nets augmented with flush-out arcs: a transient analysis technique","M. Gribaudo; A. Horvath","Dipt. di Inf., Torino Univ., Italy; NA","IEEE Transactions on Software Engineering","","2002","28","10","944","955","Fluid stochastic (or hybrid) Petri nets with flush-out arcs are Petri net-based models with two classes of places: discrete places that carry a natural number of distinct objects (tokens), and fluid places that hold a positive amount of fluid, represented by a real number. For this kind of formalisms, equations can be automatically derived from the model. Such equations, however, are often too complex to be solved analytically and simple discretization techniques usually can be successfully applied only to simple cases. In this paper, we present a particular solution technique for transient solution that makes use of the Kronecker-algebra.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041051","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041051","","Stochastic processes;Petri nets;Transient analysis;Differential equations;Laplace equations;Matrix decomposition;Fires;Steady-state;Linear systems;Finite element methods","Petri nets;transient analysis;probability;matrix algebra","fluid stochastic Petri nets;discretization;Kronecker-algebra;transient analysis;real number;formalisms;probability","","11","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Class Schema Evolution for Persistent Object-Oriented Software: Model, Empirical Study, and Automated Support","M. Piccioni; M. Oriol; B. Meyer","ETH Zurich, Zurich; ABB Corporate Research, Industrial Software Systems, Baden-Dättwil and University of York, York; ETH Zurich, Zurich","IEEE Transactions on Software Engineering","","2013","39","2","184","196","With the wide support for object serialization in object-oriented programming languages, persistent objects have become commonplace and most large object-oriented software systems rely on extensive amounts of persistent data. Such systems also evolve over time. Retrieving previously persisted objects from classes whose schema has changed is, however, difficult, and may lead to invalidating the consistency of the application. The ESCHER framework addresses these issues through an IDE-integrated approach that handles class schema evolution by managing versions of the code and generating transformation functions automatically. The infrastructure also enforces class invariants to prevent the introduction of potentially corrupt objects. This paper describes a model for class attribute changes, a measure for class evolution robustness, four empirical studies, and the design and implementation of the ESCHER system.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.123","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6122034","Versioning;persistence;serialization;object-oriented class schema evolution;IDE integration","Object oriented modeling;Java;Databases;Software;Robustness;Dictionaries;Atomic measurements","object-oriented languages;object-oriented programming;persistent objects","persistent object-oriented software;object serialization;object-oriented programming languages;object-oriented software systems;IDE-integrated approach;class schema evolution;automatic transformation function generation;potentially corrupt objects;class evolution robustness;ESCHER system implementation;ESCHER system design","","5","","43","","","","","","IEEE","IEEE Journals & Magazines"
"A controlled experiment in maintenance: comparing design patterns to simpler solutions","L. Prechelt; B. Unger; W. F. Tichy; P. Brossler; L. G. Votta","Fakultalt fur Informatik, Univ. Karlsruhe, Germany; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","12","1134","1144","Software design patterns package proven solutions to recurring design problems in a form that simplifies reuse. We are seeking empirical evidence whether using design patterns is beneficial. In particular, one may prefer using a design pattern even if the actual design problem is simpler than that solved by the pattern, i.e., if not all of the functionality offered by the pattern is actually required. Our experiment investigates software maintenance scenarios that employ various design patterns and compares designs with patterns to simpler alternatives. The subjects were professional software engineers. In most of our nine maintenance tasks, we found positive effects from using a design pattern: either its inherent additional flexibility was achieved without requiring more maintenance time or maintenance time was reduced compared to the simpler alternative. In a few cases, we found negative effects: the alternative solution was less error-prone or required less maintenance time. Overall, we conclude that, unless there is a clear reason to prefer the simpler solution, it is probably wise to choose the flexibility provided by the design pattern because unexpected new requirements often appear. We identify several questions for future empirical research.","0098-5589;1939-3520;2326-3881","","10.1109/32.988711","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988711","","Computer Society;Packaging;Software maintenance;Books;Delay effects;Runtime;Terminology;Testing;Solids;Guidelines","software maintenance;software reusability;professional aspects;object-oriented programming","controlled experiment;design patterns;software design patterns;recurring design problems;proven solutions;reuse;design problem;software maintenance scenarios;professional software engineers;maintenance tasks;Decorator pattern;new requirements","","84","","11","","","","","","IEEE","IEEE Journals & Magazines"
"Construction and Validation of an Instrument for Measuring Programming Skill","G. R. Bergersen; D. I. K. Sjøberg; T. Dybå","Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo and SINTEF, Norway","IEEE Transactions on Software Engineering","","2014","40","12","1163","1184","Skilled workers are crucial to the success of software development. The current practice in research and industry for assessing programming skills is mostly to use proxy variables of skill, such as education, experience, and multiple-choice knowledge tests. There is as yet no valid and efficient way to measure programming skill. The aim of this research is to develop a valid instrument that measures programming skill by inferring skill directly from the performance on programming tasks. Over two days, 65 professional developers from eight countries solved 19 Java programming tasks. Based on the developers' performance, the Rasch measurement model was used to construct the instrument. The instrument was found to have satisfactory (internal) psychometric properties and correlated with external variables in compliance with theoretical expectations. Such an instrument has many implications for practice, for example, in job recruitment and project allocation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2348997","Research Laboratory and the Research Council of Norway through; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6882243","Skill;programming;performance;instrument;measurement;empirical software engineering","Programming profession;Software development;Personnel;Software quality;Software design","Java;personnel;software development management","programming skill measurement;skilled workers;software development;proxy variables;multiple-choice knowledge tests;programming tasks;professional developers;Java programming tasks;Rasch measurement model;psychometric properties;job recruitment;project allocation","","8","","130","","","","","","IEEE","IEEE Journals & Magazines"
"Hybrid Program Dependence Approximation for Effective Dynamic Impact Prediction","H. Cai","School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA","IEEE Transactions on Software Engineering","","2018","44","4","334","364","Impact analysis determines the effects that program entities of interest, or changes to them, may have on the rest of the program for software measurement, maintenance, and evolution tasks. Dynamic impact analysis could be one major approach to impact analysis that computes smaller impact setsthan static alternatives for concrete sets of executions. However, existing dynamic approaches often produce impact sets that are too large to be useful, hindering their adoption in practice. To address this problem, we propose to exploit static program dependencies to drastically prune false-positive impacts that are not exercised by the set of executions utilized by the analysis, via hybrid dependence approximation. Further, we present a novel dynamic impact analysis called Diver which leverages both the information provided by the dependence graph and method-execution events to identify runtimemethod-level dependencies, hence dynamic impact sets, much more precisely without reducing safety and at acceptable costs. We evaluate Diver on ten Java subjects of various sizes and application domains against both arbitrary queries covering entire programs and practical queries based on changes actually committed by developers to actively evolving software repositories. Our extensive empirical studies show that Diver can significantly improve the precision of impact prediction, with 100-186 percent increase, with respect to a representative existing alternative thus provide a far more effective option for dynamic impact prediction. Following a similar rationale to Diver, we further developed and evaluated an online dynamic impact analysis called DiverOnline which produces impact sets immediately upon the termination of program execution. Our results show that compared to the offline approach, for the same precision, the online approach can reduce the time by 50 percent on average for answering all possible queries in the given program at once albeit at the price of possibly significant increase in runtime overhead. For users interested in one specific query only, the online approach may compute the impact set for that query during runtime without much slowing down normal program operation. Further, the online analysis, which does not incur any space cost beyond the static-analysis phase, may be favored against the offline approach when trace storage and/or related file-system resource consumption becomes a serious challenge or even stopper for adopting dynamic impact prediction. Therefore, the online and offline analysis together offer complementary options to practitioners accommodating varied application/task scenarios and diverse budget constraints.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2692783","ONR; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7895205","Static program dependence;dynamic analysis;impact prediction;online impact analysis;precision;efficiency","Performance analysis;Runtime;Software;Java;Concrete;Software measurement;Maintenance engineering","graph theory;Java;program diagnostics;software maintenance;system monitoring","dynamic impact prediction;software measurement;software maintenance;software evolution;Java;software repositories;DiverOnline;runtime method-level dependencies;dependence graph;false-positive impacts;static program dependencies;hybrid program dependence approximation;offline analysis;static-analysis phase;program execution;online dynamic impact analysis;Diver","","","","92","","","","","","IEEE","IEEE Journals & Magazines"
"FAML: A Generic Metamodel for MAS Development","G. Beydoun; G. Low; B. Henderson-Sellers; H. Mouratidis; J. J. Gomez-Sanz; J. Pavon; C. Gonzalez-Perez","University of Wollongong, Wollongong; The University of New South Wales, Sydney; University of Technology, Sydney; University of East London, London; Universidad Complutense de Madrid, Madrid; Universidad Complutense de Madrid, Madrid; Instituto de Estudios Gallegos Padre Sarmiento (CSIC), Santiago de Compostela","IEEE Transactions on Software Engineering","","2009","35","6","841","863","In some areas of software engineering research, there are several metamodels claiming to capture the main issues. Though it is profitable to have variety at the beginning of a research field, after some time, the diversity of metamodels becomes an obstacle, for instance to the sharing of results between research groups. To reach consensus and unification of existing metamodels, metamodel-driven software language engineering can be applied. This paper illustrates an application of software language engineering in the agent-oriented software engineering research domain. Here, we introduce a relatively generic agent-oriented metamodel whose suitability for supporting modeling language development is demonstrated by evaluating it with respect to several existing methodology-specific metamodels. First, the metamodel is constructed by a combination of bottom-up and top-down analysis and best practice. The concepts thus obtained and their relationships are then evaluated by mapping to two agent-oriented metamodels: TAO and Islander. We then refine the metamodel by extending the comparisons with the metamodels implicit or explicit within five more extant agent-oriented approaches: Adelfe, PASSI, Gaia, INGENIAS, and Tropos. The resultant FAML metamodel is a potential candidate for future standardization as an important component for engineering an agent modeling language.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4967615","Modeling;metamodel;multiagent systems.","Software engineering;Australia;Application software;Multiagent systems;Management information systems;Best practices;Standardization;Software architecture;Natural languages","languages;multi-agent systems;software engineering","MAS development;metamodel;software language engineering;multiagent systems;FAML","","68","","72","","","","","","IEEE","IEEE Journals & Magazines"
"Locating Need-to-Externalize Constant Strings for Software Internationalization with Generalized String-Taint Analysis","X. Wang; L. Zhang; T. Xie; H. Mei; J. Sun","Peking University, Beijing; Peking University, Beijing; North Carolina State University, Raleigh; Peking University, Beijing; Peking University, Beijing","IEEE Transactions on Software Engineering","","2013","39","4","516","536","Nowadays, a software product usually faces a global market. To meet the requirements of different local users, the software product must be internationalized. In an internationalized software product, user-visible hard-coded constant strings are externalized to resource files so that local versions can be generated by translating the resource files. In many cases, a software product is not internationalized at the beginning of the software development process. To internationalize an existing product, the developers must locate the user-visible constant strings that should be externalized. This locating process is tedious and error-prone due to 1) the large number of both user-visible and non-user-visible constant strings and 2) the complex data flows from constant strings to the Graphical User Interface (GUI). In this paper, we propose an automatic approach to locating need-to-externalize constant strings in the source code of a software product. Given a list of precollected API methods that output values of their string argument variables to the GUI and the source code of the software product under analysis, our approach traces from the invocation sites (within the source code) of these methods back to the need-to-externalize constant strings using generalized string-taint analysis. In our empirical evaluation, we used our approach to locate need-to-externalize constant strings in the uninternationalized versions of seven real-world open source software products. The results of our evaluation demonstrate that our approach is able to effectively locate need-to-externalize constant strings in uninternationalized software products. Furthermore, to help developers understand why a constant string requires translation and properly translate the need-to-externalize strings, we provide visual representation of the string dependencies related to the need-to-externalize strings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6216383","Software internationalization;need-to-externalize constant strings;string-taint analysis","Software;Graphical user interfaces;Prototypes;Java;Libraries;Production;Globalization","globalisation;graphical user interfaces;public domain software;software engineering","need-to-externalize constant string location;software internationalization;generalized string-taint analysis;user requirement;internationalized software product;user-visible hard-coded constant string;software development process;graphical user interface;GUI;data flow;API method;application program interface;string argument variable;open source software product;string dependency","","4","","31","","","","","","IEEE","IEEE Journals & Magazines"
"On the Accuracy, Efficiency, and Reusability of Automated Test Oracles for Android Devices","Y. Lin; J. F. Rojas; E. T. -. Chu; Y. Lai","Department of Computer Science, National Chiao Tung University, University Road, Hsinchu, Taiwan; Department of Computer Science, National Chiao Tung University, University Road, Hsinchu, Taiwan; Department of Computer Science and Information Engineering, National Yunlin University of Science and Technology, Yunlin, Taiwan; Department of Information Management, National Taiwan University of Science and Technology, Keelung Road, Taipei, Taiwan","IEEE Transactions on Software Engineering","","2014","40","10","957","970","Automated GUI testing consists of simulating user events and validating the changes in the GUI in order to determine if an Android application meets specifications. Traditional record-replay testing tools mainly focus on facilitating the test case writing process but not the replay and verification process. The accuracy of testing tools degrades significantly when the device under test (DUT) is under heavy load. In order to improve the accuracy, our previous work, SPAG, uses event batching and smart wait function to eliminate the uncertainty of the replay process and adopts GUI layout information to verify the testing results. SPAG maintains an accuracy of up to 99.5 percent and outperforms existing methods. In this work, we propose smart phone automated GUI testing tool with camera (SPAG-C), an extension of SPAG, to test an Android hardware device. Our goal is to further reduce the time required to record test cases and increase reusability of the test oracle without compromising test accuracy. In the record stage, SPAG captures screenshots from device's frame buffer and writes verification commands into the test case. Unlike SPAG, SPAG-C captures the screenshots from an external camera instead of frame buffer. In the replay stage, SPAG-C automatically performs image comparison while SPAG simply performs a string comparison to verify the test results. In order to make SPAG-C reusable for different devices and to allow bettersynchronization at the time of capturing images, we develop a new architecture that uses an external camera and Web services to decouple the test oracle. Our experiments show that recording a test case using SPAG-C's automatic verification is as fast as SPAG's but more accurate. Moreover, SPAG-C is 50 to 75 percent faster than SPAG in achieving the same test accuracy. With reusability, SPAG-C reduces the testing time from days to hours for heterogeneous devices.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2331982","National Science Council (NSC) and Institute of Information Industry (III) in Taiwan; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6840332","Reusable software;test execution;testing tools;user interfaces","Graphical user interfaces;Testing;Androids;Humanoid robots;Accuracy;Smart phones;Performance evaluation","graphical user interfaces;program testing;smart phones;software reusability;Web services","automated test oracles;Android devices;user events;Android application;traditional record-replay testing tools;test case writing process;replay process;verification process;device under test;DUT;event batching;smart wait function;GUI layout information;smart phone automated GUI testing tool;with camera;SPAG-C;Android hardware device;test accuracy;frame buffer;replay stage;image comparison;capturing images;Web services;automatic verification","","11","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Hierarchical GUI test case generation using automated planning","A. M. Memon; M. E. Pollack; M. L. Soffa","Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","2","144","155","The widespread use of GUIs for interacting with software is leading to the construction of more and more complex GUIs. With the growing complexity come challenges in testing the correctness of a GUI and its underlying software. We present a new technique to automatically generate test cases for GUIs that exploits planning, a well-developed and used technique in artificial intelligence. Given a set of operators, an initial state, and a goal state, a planner produces a sequence of the operators that will transform the initial state to the goal state. Our test case generation technique enables efficient application of planning by first creating a hierarchical model of a GUI based on its structure. The GUI model consists of hierarchical planning operators representing the possible events in the GUI. The test designer defines the preconditions and effects of the hierarchical operators, which are input into a plan-generation system. The test designer also creates scenarios that represent typical initial and goal states for a GUI user. The planner then generates plans representing sequences of GUI interactions that a user might employ to reach the goal state from the initial state. We implemented our test case generation system, called Planning Assisted Tester for Graphical User Interface Systems (PATHS) and experimentally evaluated its practicality and effectiveness. We describe a prototype implementation of PATHS and report on the results of controlled experiments to generate test cases for Microsoft's WordPad.","0098-5589;1939-3520;2326-3881","","10.1109/32.908959","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=908959","","Graphical user interfaces;Automatic testing;Computer aided software engineering;Software testing;System testing;Artificial intelligence;Prototypes;Software measurement;Path planning;Automatic generation control","graphical user interfaces;planning (artificial intelligence);program testing;automatic testing","hierarchical GUI test case generation;automated planning;correctness testing;software;automatic test case generation;artificial intelligence;operators;initial state;goal state;plan-generation system;Planning Assisted Tester for Graphical User Interface Systems;Microsoft WordPad","","139","","30","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Design and Code Reviews on Software Quality: An Empirical Study Based on PSP Data","C. F. Kemerer; M. C. Paulk","University of Pittsburgh, Pittsburgh; Carnegie Mellon University, Pittsburgh","IEEE Transactions on Software Engineering","","2009","35","4","534","550","This research investigates the effect of review rate on defect removal effectiveness and the quality of software products, while controlling for a number of potential confounding factors. Two data sets of 371 and 246 programs, respectively, from a personal software process (PSP) approach were analyzed using both regression and mixed models. Review activities in the PSP process are those steps performed by the developer in a traditional inspection process. The results show that the PSP review rate is a significant factor affecting defect removal effectiveness, even after accounting for developer ability and other significant process variables. The recommended review rate of 200 LOC/hour or less was found to be an effective rate for individual reviews, identifying nearly two-thirds of the defects in design reviews and more than half of the defects in code reviews.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4815279","Code reviews;design reviews;inspections;software process;software quality;defects;software measurement;mixed models;personal software process (PSP).","Software quality;Inspection;Business continuity;Software performance;Computer Society;Lab-on-a-chip;Software design;Software measurement;Costs;Job shop scheduling","program debugging;program testing;regression analysis;software metrics;software process improvement;software quality","design review impact;code review impact;software product quality;empirical study;PSP review rate;personal software process improvement approach;defect removal effectiveness;regression model;mixed model;inspection process;software measurement;business chain reaction","","39","","55","","","","","","IEEE","IEEE Journals & Magazines"
"ZPL: a machine independent programming language for parallel computers","B. L. Chamberlain; Sung-Eun Choi; C. Lewis; C. Lin; L. Snyder; W. D. Weathersby","Dept. of Comput. Sci. & Eng., Washington Univ., Seattle, WA, USA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","3","197","211","The goal of producing architecture-independent parallel programs is complicated by the competing need for high performance. The ZPL programming language achieves both goals by building upon an abstract parallel machine and by providing programming constructs that allow the programmer to ""see"" this underlying machine. This paper describes ZPL and provides a comprehensive evaluation of the language with respect to its goals of performance, portability, and programming convenience. In particular, we describe ZPt's machine-independent performance model, describe the programming benefits of ZPL's region-based constructs, summarize the compilation benefits of the language's high-level semantics, and summarize empirical evidence that ZPL has achieved both high performance and portability on diverse machines such as the IBM SP-2, Cray T3E, and SGI Power Challenge.","0098-5589;1939-3520;2326-3881","","10.1109/32.842947","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842947","","Computer languages;Concurrent computing;Parallel programming;Parallel machines;High performance computing;Hardware;Laboratories;Buildings;Programming profession;Parallel processing","parallel programming;parallel languages","ZPL;machine independent programming language;parallel computers;architecture-independent parallel programs;abstract parallel machine;programming constructs;performance;portability;programming convenience;programming benefits;high-level semantics","","26","","52","","","","","","IEEE","IEEE Journals & Magazines"
"Statistical Debugging: A Hypothesis Testing-Based Approach","Chao Liu; Long Fei; Xifeng Yan; Jiawei Han; S. P. Midkiff","Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801; School of Electronic and Computer Engineering, Purdue University, West Lafayette, IN 47907; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801; School of Electronic and Computer Engineering, Purdue University, West Lafayette, IN 47907","IEEE Transactions on Software Engineering","","2006","32","10","831","848","Manual debugging is tedious, as well as costly. The high cost has motivated the development of fault localization techniques, which help developers search for fault locations. In this paper, we propose a new statistical method, called SOBER, which automatically localizes software faults without any prior knowledge of the program semantics. Unlike existing statistical approaches that select predicates correlated with program failures, SOBER models the predicate evaluation in both correct and incorrect executions and regards a predicate as fault-relevant if its evaluation pattern in incorrect executions significantly diverges from that in correct ones. Featuring a rationale similar to that of hypothesis testing, SOBER quantifies the fault relevance of each predicate in a principled way. We systematically evaluate SOBER under the same setting as previous studies. The result clearly demonstrates the effectiveness: SOBER could help developers locate 68 out of the 130 faults in the Siemens suite by examining no more than 10 percent of the code, whereas the cause transition approach proposed by Holger et al. [2005] and the statistical approach by Liblit et al. [2005] locate 34 and 52 faults, respectively. Moreover, the effectiveness of SOBER is also evaluated in an ""imperfect world"", where the test suite is either inadequate or only partially labeled. The experiments indicate that SOBER could achieve competitive quality under these harsh circumstances. Two case studies with grep 2.2 and bc 1.06 are reported, which shed light on the applicability of SOBER on reasonably large programs","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.105","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717474","Debugging aids;statistical methods;statistical debugging.","Debugging;Testing;Runtime;Statistical analysis;Fault location;Chaos;Costs;Labeling;History","program debugging;program diagnostics;program testing;statistical analysis","software fault localization;program testing;statistical debugging;hypothesis testing","","119","","40","","","","","","IEEE","IEEE Journals & Magazines"
"Toward reference models for requirements traceability","B. Ramesh; M. Jarke","Dept. of Comput. Inf. Syst., Georgia State Univ., Atlanta, GA, USA; NA","IEEE Transactions on Software Engineering","","2001","27","1","58","93","Requirements traceability is intended to ensure continued alignment between stakeholder requirements and various outputs of the system development process. To be useful, traces must be organized according to some modeling framework. Indeed, several such frameworks have been proposed, mostly based on theoretical considerations or analysis of other literature. This paper, in contrast, follows an empirical approach. Focus groups and interviews conducted in 26 major software development organizations demonstrate a wide range of traceability practices with distinct low-end and high-end users of traceability. From these observations, reference models comprising the most important kinds of traceability links for various development tasks have been synthesized. The resulting models have been validated in case studies and are incorporated in a number of traceability tools. A detailed case study on the use of the models is presented. Four kinds of traceability link types are identified and critical issues that must be resolved for implementing each type and potential solutions are discussed. Implications for the design of next-generation traceability methods and tools are discussed and illustrated.","0098-5589;1939-3520;2326-3881","","10.1109/32.895989","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=895989","","Costs;Mathematical model;Engineering management;Programming;Design engineering;Prototypes;Computer science;Best practices;Standards organizations","systems analysis;software engineering","reference models;requirements traceability;stakeholder requirements;system development process;software development organizations;case study;traceability link types;requirements engineering","","372","","62","","","","","","IEEE","IEEE Journals & Magazines"
"Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts","D. Kim; X. Wang; S. Kim; A. Zeller; S. C. Cheung; S. Park","Sogang University, Seoul; The Hong Kong University of Science and Technology, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong; Saarland University, Saarbrücken; The Hong Kong University of Science and Technology, Hong Kong; Sogang University, Seoul","IEEE Transactions on Software Engineering","","2011","37","3","430","447","Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these “top crashes” thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5711013","Top crash;machine learning;crash reports;social network analysis;data mining.","Fires;Feature extraction;Software;Testing;Computer bugs;Training","program debugging;software maintenance;software quality;system recovery","debugging;software systems;software failures;Firefox crash report databases;Thunderbird crash report databases;software quality;software maintenance","","36","","59","","","","","","IEEE","IEEE Journals & Magazines"
"Interactive fault localization techniques in a spreadsheet environment","J. R. Ruthruff; M. Burnett; G. Rothermel","Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","4","213","239","End-user programmers develop more software than any other group of programmers, using software authoring devices such as multimedia simulation builders, e-mail filtering editors, by-demonstration macro builders, and spreadsheet environments. Despite this, there has been only a little research on finding ways to help these programmers with the dependability of the software they create. We have been working to address this problem in several ways, one of which includes supporting end-user debugging activities through interactive fault localization techniques. This paper investigates fault localization techniques in the spreadsheet domain, the most common type of end-user programming environment. We investigate a technique previously described in the research literature and two new techniques. We present the results of an empirical study to examine the impact of two individual factors on the effectiveness of fault localization techniques. Our results reveal several insights into the contributions such techniques can make to the end-user debugging process and highlight key issues of interest to researchers and practitioners who may design and evaluate future fault localization techniques.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628969","Fault localization;debugging;end-user software engineering;spreadsheets;end-user programming.","Programming profession;Software engineering;Debugging;Electronic mail;Filtering;Programming environments;Fault detection;Feedback;Computer science;Employment","authoring systems;spreadsheet programs;program debugging;program diagnostics;fault diagnosis","software authoring device;spreadsheet environment;end-user debugging;interactive fault localization technique;end-user programming;end-user software engineering","","9","","68","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting the probability of change in object-oriented systems","N. Tsantalis; A. Chatzigeorgiou; G. Stephanides","Dept. of Appl. Informatics, Univ. of Macedonia, Thessaloniki, Greece; Dept. of Appl. Informatics, Univ. of Macedonia, Thessaloniki, Greece; Dept. of Appl. Informatics, Univ. of Macedonia, Thessaloniki, Greece","IEEE Transactions on Software Engineering","","2005","31","7","601","614","Of all merits of the object-oriented paradigm, flexibility is probably the most important in a world of constantly changing requirements and the most striking difference compared to previous approaches. However, it is rather difficult to quantify this aspect of quality: this paper describes a probabilistic approach to estimate the change proneness of an object-oriented design by evaluating the probability that each class of the system will be affected when new functionality is added or when existing functionality is modified. It is obvious that when a system exhibits a large sensitivity to changes, the corresponding design quality is questionable. The extracted probabilities of change can be used to assist maintenance and to observe the evolution of stability through successive generations and identify a possible ""saturation"" level beyond which any attempt to improve the design without major refactoring is impossible. The proposed model has been evaluated on two multiversion open source projects. The process has been fully automated by a Java program, while statistical analysis has proved improved correlation between the extracted probabilities and actual changes in each of the classes in comparison to a prediction model that relies simply on past data.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.83","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492374","Index Terms- Object-oriented programming;product metrics;object-oriented design methods;quality analysis and evaluation.","Computer Society;Object oriented modeling;Stability;Java;Statistical analysis;Data mining;Probability;Predictive models;Object oriented programming;Design methodology","object-oriented programming;object-oriented methods;software quality;software maintenance;Java;public domain software;configuration management;software metrics;probability;statistical analysis","object-oriented system;software flexibility;software quality;object-oriented design method;probability prediction model;software maintenance;multiversion open source project;Java program;object-oriented programming;product metrics","","46","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Reconfigurable instruction set processors from a hardware/software perspective","F. Barat; R. Lauwereins; G. Deconinck","Dept. of Electr. Eng., Katholieke Univ., Leuven, Belgium; NA; NA","IEEE Transactions on Software Engineering","","2002","28","9","847","862","This paper presents the design alternatives for reconfigurable instruction set processors (RISP) from a hardware/software point of view. Reconfigurable instruction set processors are programmable processors that contain reconfigurable logic in one or more of its functional units. Hardware design of such a type of processors can be split in two main tasks: the design of the reconfigurable logic and the design of the interfacing mechanisms of this logic to the rest of the processor. Among the most important design parameters are: the granularity of the reconfigurable logic, the structure of the configuration memory, the instruction encoding format, and the type of instructions supported. On the software side, code generation tools require new techniques to cope with the reconfigurability of the processor. Aside from traditional techniques, code generation requires the creation and evaluation of new reconfigurable instructions and the selection of instructions to minimize reconfiguration time. The most important design alternative on the software side is the degree of automatization present in the code generation tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1033225","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1033225","","Hardware;Reconfigurable logic;Application software;Application specific processors;Software tools;Microprocessors;Energy consumption;Logic design;Encoding;Computer bugs","reconfigurable architectures;hardware-software codesign;microprocessor chips;instruction sets;program compilers;embedded systems;formal logic","reconfigurable instruction set processors;RISP;programmable processors;reconfigurable logic;microprocessor;program compiler;embedded systems;hardware software codesign;hardware design;configuration memory;instruction encoding format;code generation tools","","51","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Exception Handling Patterns for Process Modeling","B. S. Lerner; S. Christov; L. J. Osterweil; R. Bendraou; U. Kannengiesser; A. Wise","Mount Holyoke College, South Hadley; University of Massachusetts, Amherst; University of Massachusetts, Amherst; Université Pierre & Marie Curie, Paris; NICTA, Alexandria and University of New South Wales, Sydney; University of Massachusetts, Amherst","IEEE Transactions on Software Engineering","","2010","36","2","162","183","Process modeling allows for analysis and improvement of processes that coordinate multiple people and tools working together to carry out a task. Process modeling typically focuses on the normative process, that is, how the collaboration transpires when everything goes as desired. Unfortunately, real-world processes rarely proceed that smoothly. A more complete analysis of a process requires that the process model also include details about what to do when exceptional situations arise. We have found that, in many cases, there are abstract patterns that capture the relationship between exception handling tasks and the normative process. Just as object-oriented design patterns facilitate the development, documentation, and maintenance of object-oriented programs, we believe that process patterns can facilitate the development, documentation, and maintenance of process models. In this paper, we focus on the exception handling patterns that we have observed over many years of process modeling. We describe these patterns using three process modeling notations: UML 2.0 Activity Diagrams, BPMN, and Little-JIL. We present both the abstract structure of the pattern as well as examples of the pattern in use. We also provide some preliminary statistical survey data to support the claim that these patterns are found commonly in actual use and discuss the relative merits of the three notations with respect to their ability to represent these patterns.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383369","Exception handling patterns;process modeling;process modeling languages.","Object oriented modeling;Documentation;Humans;Pattern analysis;Collaborative work;Unified modeling language;Collaboration;Data processing;Manufacturing processes;Medical services","exception handling;groupware;object-oriented programming;software maintenance;system documentation","exception handling patterns;process modeling;collaboration process;normative process;object-oriented design patterns;object-oriented programs;process development;process documentation;process maintenance;UML 2.0 activity diagrams;BPMN;Little-JIL","","47","","51","","","","","","IEEE","IEEE Journals & Magazines"
"A comprehensive evaluation of capture-recapture models for estimating software defect content","L. C. Briand; K. El Emam; B. G. Freimut; O. Laitenberger","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; NA; NA; NA","IEEE Transactions on Software Engineering","","2000","26","6","518","540","An important requirement to control the inspection of software artifacts is to be able to decide, based on more objective information, whether the inspection can stop or whether it should continue to achieve a suitable level of artifact quality. A prediction of the number of remaining defects in an inspected artifact can be used for decision making. Several studies in software engineering have considered capture-recapture models to make a prediction. However, few studies compare the actual number of remaining defects to the one predicted by a capture-recapture model on real software engineering artifacts. The authors focus on traditional inspections and estimate, based on actual inspections data, the degree of accuracy of relevant state-of-the-art capture-recapture models for which statistical estimators exist. In order to assess their robustness, we look at the impact of the number of inspectors and the number of actual defects on the estimators' accuracy based on actual inspection data. Our results show that models are strongly affected by the number of inspectors, and therefore one must consider this factor before using capture-recapture models. When the number of inspectors is too small, no model is sufficiently accurate and underestimation may be substantial. In addition, some models perform better than others in a large number of conditions and plausible reasons are discussed. Based on our analyses, we recommend using a model taking into account that defects have different probabilities of being detected and the corresponding Jackknife Estimator. Furthermore, we calibrate the prediction models based on their relative error, as previously computed on other inspections. We identified theoretical limitations to this approach which were then confirmed by the data.","0098-5589;1939-3520;2326-3881","","10.1109/32.852741","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=852741","","Inspection;Biological system modeling;Software quality;Software engineering;Predictive models;Robustness;Computer Society;State estimation;Decision making;Animals","software quality;software performance evaluation;inspection;probability;software development management","capture-recapture models;software defect content estimation;traditional inspections;objective information;artifact quality;software engineering;real software engineering artifacts;inspections data;statistical estimators;estimator accuracy;probabilities;Jackknife Estimator;relative error","","85","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Foundations of sequence-based software specification","S. J. Prowell; J. H. Poore","Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA; Dept. of Comput. Sci., Tennessee Univ., Knoxville, TN, USA","IEEE Transactions on Software Engineering","","2003","29","5","417","429","Rigorous specification early in the software development process can greatly reduce the cost of later development and maintenance, as well as provide an explicit means to manage risk and identify and meet safety requirements. Sequence-based software specification is a collection of techniques for implementing rigorous, practical software specification. The primary result of this research is the sequence enumeration method of specification writing. Straightforward, systematic enumeration of all sequences to produce an arguably complete, consistent, and traceably correct specification is made practical by controlling the growth of the process.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1199071","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1199071","","Software systems;Software safety;Software maintenance;Computer Society;Programming;Costs;Risk management;Software development management;Writing;Control systems","formal specification;software engineering","sequence-based software specification;software development process;software development cost reduction;software maintenance cost reduction;sequence enumeration method;specification writing;complete specification;consistent specification;traceably correct specification","","47","","15","","","","","","IEEE","IEEE Journals & Magazines"
"A Genetic Algorithm-Based Stress Test Requirements Generator Tool and Its Empirical Evaluation","V. Garousi","University of Calgary, Calgary","IEEE Transactions on Software Engineering","","2010","36","6","778","797","Genetic algorithms (GAs) have been applied previously to UML-driven stress test requirements generation with the aim of increasing chances of discovering faults relating to network traffic in distributed real-time systems. However, since evolutionary algorithms are heuristic, their performance can vary across multiple executions, which may affect robustness and scalability. To address this, we present the design and technical detail of a UML-driven, GA-based stress test requirements generation tool, together with its empirical analysis. The main goal is to analyze and improve the applicability, efficiency, and effectiveness and also to validate the design choices of the GA used in the tool. Findings of the empirical evaluation reveal that the tool is robust and reasonably scalable when it is executed on large-scale experimental design models. The study also reveals the main bottlenecks and limitations of the tools, e.g., there is a performance bottleneck when the system under test has a large number of sequence diagrams which could be triggered independently from each other. In addition, issues specific to stress testing, e.g., the impact of variations in task arrival pattern types, reveal that the tool generally generates effective test requirements, although the features of those test requirements might be different in different runs (e.g., different stress times from the test start time might be chosen). While the use of evolutionary algorithms to generate software test cases has been widely reported, the extent, depth, and detail of the empirical findings presented in this paper are novel and suggest that the proposed approach is effective and efficient in generating stress test requirements. It is hoped that the findings of this empirical study will help other SBSE researchers with the empirical evaluation of their own techniques and tools.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.5","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5383373","Index Term—Search-based testing;genetic algorithms;stress testing;test tools;test automation;empirical analysis.","Stress;System testing;Robustness;Genetic algorithms;Telecommunication traffic;Real time systems;Evolutionary computation;Scalability;Large-scale systems;Design for experiments","distributed algorithms;genetic algorithms;program testing;real-time systems;Unified Modeling Language","genetic algorithm;UML;distributed real time system;empirical analysis;software test cases;stress test requirement generation","","14","","32","","","","","","IEEE","IEEE Journals & Magazines"
"An efficient algorithm for aggregating PEPA models","S. Gilmore; J. Hillston; M. Ribaudo","Lab. for Found. of Comput. Sci., Edinburgh Univ., UK; NA; NA","IEEE Transactions on Software Engineering","","2001","27","5","449","464","Performance Evaluation Process Algebra (PEPA) is a formal language for performance modeling based on process algebra. It has previously been shown that, by using the process algebra apparatus, compact performance models can be derived which retain the essential behavioral characteristics of the modeled system. However, no efficient algorithm for this derivation was given. We present an efficient algorithm which recognizes and takes advantage of symmetries within the model and avoids unnecessary computation. The algorithm is illustrated by a multiprocessor example.","0098-5589;1939-3520;2326-3881","","10.1109/32.922715","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=922715","","Algebra;Stochastic processes;State-space methods;Partitioning algorithms;Timing;Performance analysis;Context modeling;Formal languages;Explosions;Prototypes","performance evaluation;process algebra;formal languages;multiprocessing systems","PEPA models;Performance Evaluation Process Algebra;formal language;performance modeling;multiprocessor;model aggregation","","51","","21","","","","","","IEEE","IEEE Journals & Magazines"
"An Enhanced Bailout Protocol for Mixed Criticality Embedded Software","I. Bate; A. Burns; R. I. Davis","Department of Computer Science, University of York, York, United Kingdom; Department of Computer Science, University of York, York, United Kingdom; Department of Computer Science, University of York, York, United Kingdom","IEEE Transactions on Software Engineering","","2017","43","4","298","320","To move mixed criticality research into industrial practice requires models whose run-time behaviour is acceptable to systems engineers. Certain aspects of current models, such as abandoning lower criticality tasks when certain situations arise, do not give the robustness required in application domains such as the automotive and aerospace industries. In this paper a new bailout protocol is developed that still guarantees high criticality software but minimises the negative impact on lower criticality software via a timely return to normal operation. We show how the bailout protocol can be integrated with existing techniques, utilising both offline slack and online gain-time to further improve performance. Static analysis is provided for schedulability guarantees, while scenario-based evaluation via simulation is used to explore the effectiveness of the protocol.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2592907","ESPRC; MCC; EU FP7 IP PROXIMA; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7516652","Real-time systems;mixed criticality;fixed priority scheduling;mode changes","Protocols;Standards;Software;Analytical models;Job shop scheduling;Software engineering;Safety","embedded systems;safety-critical software;scheduling","scenario-based evaluation;schedulability guarantees;online gain-time;lower criticality software;high criticality software;mixed criticality embedded software;enhanced bailout protocol","","4","","44","","","","","","IEEE","IEEE Journals & Magazines"
"EnergyPatch: Repairing Resource Leaks to Improve Energy-Efficiency of Android Apps","A. Banerjee; L. K. Chong; C. Ballabriga; A. Roychoudhury","School of Computing, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore; University of Lille, Villeneuve, France; School of Computing, National University of Singapore, Singapore","IEEE Transactions on Software Engineering","","2018","44","5","470","490","Increased usage of mobile devices, such as smartphones and tablets, has led to widespread popularity and usage of mobile apps. If not carefully developed, such apps may demonstrate energy-inefficient behaviour, where one or more energy-intensive hardware components (such as Wifi, GPS, etc) are left in a high-power state, even when no apps are using these components. We refer to such kind of energy-inefficiencies as energy bugs. Executing an app with an energy bug causes the mobile device to exhibit poor energy consumption behaviour and a drastically shortened battery life. Since mobiles apps can have huge input domains, therefore exhaustive exploration is often impractical. We believe that there is a need for a framework that can systematically detect and fix energy bugs in mobile apps in a scalable fashion. To address this need, we have developed EnergyPatch, a framework that uses a combination of static and dynamic analysis techniques to detect, validate and repair energy bugs in Android apps. The use of a light-weight, static analysis technique enables EnergyPatch to quickly narrow down to the potential program paths along which energy bugs may occur. Subsequent exploration of these potentially buggy program paths using a dynamic analysis technique helps in validations of the reported bugs and to generate test cases. Finally, EnergyPatch generates repair expressions to fix the validated energy bugs. Evaluation with real-life apps from repositories such as F-droid and Github, shows that EnergyPatch is scalable and can produce results in reasonable amount of time. Additionally, we observed that the repair expressions generated by EnergyPatch could bring down the energy consumption on tested apps up to 60 percent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2689012","Singapore MoE Tier 2; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7889026","Mobile apps;energy bugs;non-functional testing;energy-aware test generation","Computer bugs;Androids;Humanoid robots;Maintenance engineering;Mobile handsets;Energy consumption;Batteries","Android (operating system);energy consumption;mobile computing;power aware computing;program debugging;program diagnostics;program testing;smart phones","EnergyPatch;energy-efficiency;Android apps;mobile device;mobile apps;energy-inefficient behaviour;energy-intensive hardware components;energy-inefficiencies;energy bug;poor energy consumption behaviour;mobiles apps;validated energy bugs;real-life apps;tested apps;smartphones;tablets;static analysis techniques;dynamic analysis techniques;buggy program paths;F-droid;Github","","2","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Verifying Synchronization for Atomicity Violation Fixing","Q. Shi; J. Huang; Z. Chen; B. Xu","State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China; Texas A&M University, College Station, TX; State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China; State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Software Engineering","","2016","42","3","280","296","Atomicity is a fundamental property to guarantee the isolation of a work unit (i.e., a sequence of related events in a thread) from concurrent threads. However, ensuring atomicity is often very challenging due to complex thread interactions. We present an approach to help developers verify whether such work units, which have triggered bugs due to certain violations of atomicity, are sufficiently synchronized or not by locks introduced for fixing the bugs. A key feature of our approach is that it combines the fortes of both bug-driven and change-aware techniques, which enables it to effectively verify synchronizations by testing only a minimal set of suspicious atomicity violations without any knowledge on the to-be-isolated work units, thus being more efficient and practical than other approaches. Besides, unlike existing approaches, our approach effectively utilizes all the inferred execution traces even they may not be completely feasible, such that the verification algorithm can converge much faster. We demonstrate via extensive evaluation that our approach is much more effective and efficient than the state-of-the-arts. Besides, we show that although there have existed sound automatic fixing techniques for atomicity violations, our approach is still necessary and useful for quality assurance of concurrent programs, because the assumption behind our approach is much weaker. We have also investigated one of the largest bug databases and found that insufficient synchronizations are common and difficult to be found in software development.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2477820","National Basic Research Program of China; National Natural Science Foundation of China; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7254228","Atomicity violations;insufficient synchronization;fix;dynamic trace analysis;maximal sound verification;Atomicity violations;insufficient synchronization;fix;dynamic trace analysis;maximal sound verification","Synchronization;Schedules;Optimization;Java;Computer bugs;Runtime;Instruction sets","program debugging;software quality;synchronisation","synchronization verification;atomicity violation fixing;work unit isolation;concurrent threads;complex thread interactions;triggered bugs;bug-driven techniques;change-aware techniques;suspicious atomicity violations;to-be-isolated work units;verification algorithm;extensive evaluation;automatic fixing techniques;concurrent programs;quality assurance;bug databases;software development","","2","","48","","","","","","IEEE","IEEE Journals & Magazines"
"The Impact of Irrelevant and Misleading Information on Software Development Effort Estimates: A Randomized Controlled Field Experiment","M. Jorgensen; S. Grimstad","University of Oslo and Simula Research Laboratory, Lysaker; University of Oslo and Simula Research Laboratory, Lysaker","IEEE Transactions on Software Engineering","","2011","37","5","695","707","Studies in laboratory settings report that software development effort estimates can be strongly affected by effort-irrelevant and misleading information. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from various countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: 1) reduced length of requirement specification with no change of content, 2) information about the low effort spent on the development of the old system to be replaced, 3) information about the client's unrealistic expectations about low cost, and 4) a restriction of a short development period with start up a few months ahead. We found that the effect sizes in the field settings were much smaller than those found for similar manipulations in laboratory settings. Our findings suggest that we should be careful about generalizing to field settings the effect sizes found in laboratory settings. While laboratory settings can be useful to demonstrate the existence of an effect and better understand it, field studies may be needed to study the size and importance of these effects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.78","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5551161","Cost estimation;software psychology;requirements/specifications.","Estimation;Software;Companies;Laboratories;Programming;Materials;Project management","formal specification;software cost estimation","irrelevant information impact;misleading information impact;software development effort estimates;randomized controlled field experiment;software development projects;original requirement specification;laboratory settings;software psychology","","18","","31","","","","","","IEEE","IEEE Journals & Magazines"
"Control-Theoretical Software Adaptation: A Systematic Literature Review","S. Shevtsov; M. Berekmeri; D. Weyns; M. Maggio","Linnaeus University, Växjö, Sweden; Grenoble Institute of Technology, Grenoble, France; Katholieke Universiteit Leuven, Leuven, Belgium; Lund University, Lund, Sweden","IEEE Transactions on Software Engineering","","2018","44","8","784","810","Modern software applications are subject to uncertain operating conditions, such as dynamics in the availability of services and variations of system goals. Consequently, runtime changes cannot be ignored, but often cannot be predicted at design time. Control theory has been identified as a principled way of addressing runtime changes and it has been applied successfully to modify the structure and behavior of software applications. Most of the times, however, the adaptation targeted the resources that the software has available for execution (CPU, storage, etc.) more than the software application itself. This paper investigates the research efforts that have been conducted to make software adaptable by modifying the software rather than the resource allocated to its execution. This paper aims to identify: the focus of research on control-theoretical software adaptation; how software is modeled and what control mechanisms are used to adapt software; what software qualities and controller guarantees are considered. To that end, we performed a systematic literature review in which we extracted data from 42 primary studies selected from 1,512 papers that resulted from an automatic search. The results of our investigation show that even though the behavior of software is considered non-linear, research efforts use linear models to represent it, with some success. Also, the control strategies that are most often considered are classic control, mostly in the form of Proportional and Integral controllers, and Model Predictive Control. The paper also discusses sensing and actuating strategies that are prominent for software adaptation and the (often neglected) proof of formal properties. Finally, we distill open challenges for control-theoretical software adaptation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2704579","Assurances for Decentralized Self-Adaptive Systems Vetenskapsradet Sweden; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7929422","Self-adaptive software;control theory;software adaptation","Software;Control theory;Adaptation models;Runtime;Bibliographies;Mathematical model;Knowledge based systems","predictive control;resource allocation;reverse engineering;software engineering;software performance evaluation;software quality","control-theoretical software adaptation;systematic literature review;modern software applications;runtime changes;software application;software adaptable;software qualities;model predictive control","","","","116","","","","","","IEEE","IEEE Journals & Magazines"
"An investigation of graph-based class integration test order strategies","L. C. Briand; Y. Labiche; Yihong Wang","Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada; Dept. of Syst. & Comput. Eng., Carleton Univ., Ottawa, Ont., Canada","IEEE Transactions on Software Engineering","","2003","29","7","594","607","The issue of ordering class integration in the context of integration testing has been discussed by a number of researchers. More specifically, strategies have been proposed to generate a test order while minimizing stubbing. Recent papers have addressed the problem of deriving an integration order in the presence of dependency cycles in the class diagram. Such dependencies represent a practical problem as they make any topological ordering of classes impossible. Three main approaches, aimed at ""breaking"" cycles, have been proposed. The first one was proposed by Tai and Daniels (1999) and is based on assigning a higher-level order according to aggregation and inheritance relationships and a lower-level order according to associations. The second one was proposed by Le Traon et al. (2000) and is based on identifying strongly connected components in the dependency graph. The third one was proposed by Briand et al. (2000); it combines some of the principles of the two previous approaches and addresses some of their shortcomings (e.g., the first approach may result into unnecessary stubbing whereas the second may lead to breaking cycles by ""removing"" aggregation or inheritance dependencies, thus leading to complex stubbing). This paper reviews these strategies (principles are described, advantages and drawbacks are precisely investigated) and provides both analytical and empirical comparisons based on five case studies.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1214324","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1214324","","Software testing;Costs;Sorting;Software engineering;System testing;Object oriented modeling;Performance evaluation;Graph theory;Performance analysis","integrated software;program testing;object-oriented programming","graph-based class integration test order strategies;test order;strongly connected components;object-oriented software engineering;dependency graph","","55","","25","","","","","","IEEE","IEEE Journals & Magazines"
"On the Use of Hidden Markov Model to Predict the Time to Fix Bugs","M. Habayeb; S. S. Murtaza; A. Miranskyy; A. B. Bener","Department of Mechanical and Industrial Engineering, Ryerson University, Toronto, Ontario, Canada; Department of Mechanical and Industrial Engineering, Ryerson University, Toronto, Ontario, Canada; Department of Computer Science, Ryerson University, Toronto, Ontario, Canada; Department of Mechanical and Industrial Engineering, Ryerson University, Toronto, Ontario, Canada","IEEE Transactions on Software Engineering","","2018","44","12","1224","1244","A significant amount of time is spent by software developers in investigating bug reports. It is useful to indicate when a bug report will be closed, since it would help software teams to prioritise their work. Several studies have been conducted to address this problem in the past decade. Most of these studies have used the frequency of occurrence of certain developer activities as input attributes in building their prediction models. However, these approaches tend to ignore the temporal nature of the occurrence of these activities. In this paper, a novel approach using Hidden Markov Models and temporal sequences of developer activities is proposed. The approach is empirically demonstrated in a case study using eight years of bug reports collected from the Firefox project. Our proposed model correctly identifies bug reports with expected bug fix times. We also compared our proposed approach with the state of the art technique in the literature in the context of our case study. Our approach results in approximately 33 percent higher F-measure than the contemporary technique based on the Firefox project data.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2757480","NSERC; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8052546","Bug repositories;temporal activities;time to fix a bug;hidden markov model","Computer bugs;Hidden Markov models;Predictive models;Software quality;Data science;Stochastic processes","hidden Markov models;program debugging;public domain software;software maintenance;software quality","bug report;fix bugs;hidden Markov model;time prediction;temporal sequences;Firefox project","","","","42","","","","","","IEEE","IEEE Journals & Magazines"
"PerLa: A Language and Middleware Architecture for Data Management and Integration in Pervasive Information Systems","F. A. Schreiber; R. Camplani; M. Fortunato; M. Marelli; G. Rota","Politecnico di Milano, Milano; Politecnico di Milano, Milano; Politecnico di Milano, Milano; Politecnico di Milano, Milano; Politecnico di Milano, Milano","IEEE Transactions on Software Engineering","","2012","38","2","478","496","A declarative SQL-like language and a middleware infrastructure are presented for collecting data from different nodes of a pervasive system. Data management is performed by hiding the complexity due to the large underlying heterogeneity of devices, which can span from passive RFID(s) to ad hoc sensor boards to portable computers. An important feature of the presented middleware is to make the integration of new device types in the system easy through the use of device self-description. Two case studies are described for PerLa usage, and a survey is made for comparing our approach with other projects in the area.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5728831","Declarative language;device heterogeneity;functionality proxy;middleware infrastructure;pervasive system;SQL;wireless sensor networks.","Middleware;Monitoring;Software;Context;Wireless sensor networks;Databases;Hardware","data integration;information systems;middleware;software architecture;SQL;ubiquitous computing","PerLa;language architecture;middleware architecture;data management;data integration;pervasive information systems;declarative SQL-like language;data collection;passive RFID;ad hoc sensor boards;portable computers;device self-description","","19","","53","","","","","","IEEE","IEEE Journals & Magazines"
"Embedding Polychrony into Synchrony","J. Brandt; M. Gemünde; K. Schneider; S. K. Shukla; J. Talpin","University of Kaiserslautern, Kaiserslautern; University of Kaiserslautern, Kaiserslautern; University of Kaiserslautern, Kaiserslautern; Virginia Polytechnic and State University, Blacksburg; INRIA Rennes-Bretagne-Atlantique, Rennes","IEEE Transactions on Software Engineering","","2013","39","7","917","929","This paper presents an embedding of polychronous programs into synchronous ones. Due to this embedding, it is not only possible to deepen the understanding of these different models of computation, but, more importantly, it is possible to transfer compilation techniques that were developed for synchronous programs to polychronous programs. This transfer is nontrivial because the underlying paradigms differ more than their names suggest: Since synchronous systems react deterministically to given inputs in discrete steps, they are typically used to describe reactive systems with a totally ordered notion of time. In contrast, polychronous system models entail a partially ordered notion of time, and are most suited to interface a system with an asynchronous environment by specifying input/output constraints from which a deterministic controller may eventually be refined and synthesized. As particular examples for the mentioned cross fertilization, we show how a simulator and a verification backend for synchronous programs can be made available to polychronous specifications, which is a first step toward integrating heterogeneous models of computation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.85","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6381420","Model-driven embedded software;synchronous programming;polychronous programming;programming models;synchrony hypothesis;synchronous guarded commands","Clocks;Computational modeling;Synchronization;Embedded systems;Hardware;Unified modeling language","embedded systems;program compilers;program verification","polychronous program specification;compilation techniques;reactive systems;polychronous system model;asynchronous environment;input-output constraints;deterministic controller;synchronous program verification;heterogeneous model","","4","","42","","","","","","IEEE","IEEE Journals & Magazines"
"Finding and Evaluating the Performance Impact of Redundant Data Access for Applications that are Developed Using Object-Relational Mapping Frameworks","T. Chen; W. Shang; Z. M. Jiang; A. E. Hassan; M. Nasser; P. Flora","Software Analysis and Intelligence Lab, School of Computing, Queen's University, Kingston, ON, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada; Software Analysis and Intelligence Lab, School of Computing, Queen's University, Kingston, ON, Canada; BlackBerry, Waterloo, ON, Canada; BlackBerry, Waterloo, ON, Canada","IEEE Transactions on Software Engineering","","2016","42","12","1148","1161","Developers usually leverage Object-Relational Mapping (ORM) to abstract complex database accesses for large-scale systems. However, since ORM frameworks operate at a lower-level (i.e., data access), ORM frameworks do not know how the data will be used when returned from database management systems (DBMSs). Therefore, ORM cannot provide an optimal data retrieval approach for all applications, which may result in accessing redundant data and significantly affect system performance. Although ORM frameworks provide ways to resolve redundant data problems, due to the complexity of modern systems, developers may not be able to locate such problems in the code; hence, may not proactively resolve the problems. In this paper, we propose an automated approach, which we implement as a Java framework, to locate redundant data problems. We apply our framework on one enterprise and two open source systems. We find that redundant data problems exist in 87 percent of the exercised transactions. Due to the large number of detected redundant data problems, we propose an automated approach to assess the impact and prioritize the resolution efforts. Our performance assessment result shows that by resolving the redundant data problems, the system response time for the studied systems can be improved by an average of 17 percent.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2553039","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7451264","Performance;object-relational mapping (ORM);program analysis;database","Databases;System performance;Java;Computer bugs;Complexity theory;Time factors;Object tracking","database management systems;Java;program diagnostics;public domain software;software performance evaluation","performance impact evaluation;redundant data access;object-relational mapping framework;ORM framework;database abstraction;database management system;DBMS;Java framework;open source system;program analysis","","4","","84","","","","","","IEEE","IEEE Journals & Magazines"
"Automated Checking of Conformance to Requirements Templates Using Natural Language Processing","C. Arora; M. Sabetzadeh; L. Briand; F. Zimmer","SnT Centre for Security, Reliability, and Trust, Luxembourg; SnT Centre for Security, Reliability, and Trust, Luxembourg; SnT Centre for Security, Reliability, and Trust, Luxembourg; SES TechCom, Luxembourg","IEEE Transactions on Software Engineering","","2015","41","10","944","968","Templates are effective tools for increasing the precision of natural language requirements and for avoiding ambiguities that may arise from the use of unrestricted natural language. When templates are applied, it is important to verify that the requirements are indeed written according to the templates. If done manually, checking conformance to templates is laborious, presenting a particular challenge when the task has to be repeated multiple times in response to changes in the requirements. In this article, using techniques from natural language processing (NLP), we develop an automated approach for checking conformance to templates. Specifically, we present a generalizable method for casting templates into NLP pattern matchers and reflect on our practical experience implementing automated checkers for two well-known templates in the requirements engineering community. We report on the application of our approach to four case studies. Our results indicate that: (1) our approach provides a robust and accurate basis for checking conformance to templates; and (2) the effectiveness of our approach is not compromised even when the requirements glossary terms are unknown. This makes our work particularly relevant to practice, as many industrial requirements documents have incomplete glossaries.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2428709","National Research Fund-Luxembourg; Validation Laboratory and AFR; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7100933","Requirements Templates;Natural Language Processing (NLP);Case Study Research;Requirements templates;natural language processing (NLP);case study research","Terminology;Natural language processing;Ear;Safety;Pipelines;Pattern matching","formal specification;natural language processing;pattern matching","conformance automated checking;requirements templates;natural language processing;generalizable method;NLP pattern matcher;requirements engineering community","","25","","71","","","","","","IEEE","IEEE Journals & Magazines"
"Formalizing and integrating the dynamic model for object-oriented modeling","B. H. C. Cheng; E. Y. Wang","Software Eng. & Network Syst. Lab., Michigan State Univ., East Lansing, MI, USA; NA","IEEE Transactions on Software Engineering","","2002","28","8","747","762","The Object Modeling Technique (OMT), a commonly used object-oriented development technique, comprises the object, dynamic, and functional models to provide three complementary views that graphically describe different aspects of systems. The lack of a well-defined semantics for the integration of the three models hinders the overall development process, particularly during the design phase. Previously, we formalized the object model in terms of algebraic specifications. However, the algebraic specifications only capture the static, structural aspects of a system. They do not explicitly describe the behavior, which is critical for system development especially for the design phase. It is necessary to formalize the dynamic model in terms of the structural descriptions in order to specify and verify the system behavior using rigorous techniques. This paper presents a well-defined formal model for both the object and dynamic models and their integration. The formal model is described in terms of a well-known specification language, LOTOS. Formalization of the graphical notation enables numerous automated processing and analysis tasks, such as behavior simulation and consistency checks between levels of specifications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027798","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027798","","Object oriented modeling;Formal specifications;Unified modeling language;Specification languages;Analytical models;Software engineering;Graphical models;ISO standards;Concurrent computing","algebraic specification;specification languages;formal specification;object-oriented programming","dynamic model;object-oriented modeling;object modeling technique;object-oriented development;functional models;object model;algebraic specifications;specification language;LOTOS;graphical notation;behavior simulation;consistency checks","","10","","38","","","","","","IEEE","IEEE Journals & Magazines"
"Expanding Queries for Code Search Using Semantically Related API Class-names","F. Zhang; H. Niu; I. Keivanloo; Y. Zou","Queen's University, Kingston, ON, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, ON, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, ON, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2018","44","11","1070","1082","When encountering unfamiliar programming tasks (e.g., connecting to a database), there is a need to seek potential working code examples. Instead of using code search engines, software developers usually post related programming questions on online Q&A forums (e.g., Stack Overflow). One possible reason is that existing code search engines would return effective code examples only if a query contains identifiers (e.g., class or method names). In other words, existing code search engines do not handle natural-language queries well (e.g., a description of a programming task). However, developers may not know the appropriate identifiers at the time of the search. As the demand of searching code examples is increasing, it is of significant interest to enhance code search engines. We conjecture that expanding natural-language queries with their semantically related identifiers has a great potential to enhance code search engines. In this paper, we propose an automated approach to find identifiers (in particular API class-names) that are semantically related to a given natural-language query. We evaluate the effectiveness of our approach using 74 queries on a corpus of 23,677,216 code snippets that are extracted from 24,666 open source Java projects. The results show that our approach can effectively recommend semantically related API class-names to expand the original natural-language queries. For instance, our approach successfully retrieves relevant code examples in the top 10 retrieved results for 76 percent of 74 queries, while it is 36 percent when using the original natural-language query; and the median rank of the first relevant code example is increased from 22 to 7.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2750682","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8031055","Query expansion;code search;neural network language model;API class-name","Search engines;Programming;Java;Software engineering;IEEE transactions;Software;Joining processes","application program interfaces;Internet;natural language processing;query processing;search engines;software engineering","semantically related API class-names;natural-language query;code search engines;online Q&A forums;software developers","","","","46","","","","","","IEEE","IEEE Journals & Magazines"
"Formulating Cost-Effective Monitoring Strategies for Service-Based Systems","Q. He; J. Han; Y. Yang; H. Jin; J. Schneider; S. Versteeg","School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia 3122; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia 3122; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia 3122; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan 430074, China; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia 3122; CA Technologies, Melbourne, Australia 3004","IEEE Transactions on Software Engineering","","2014","40","5","461","482","When operating in volatile environments, service-based systems (SBSs) that are dynamically composed from component services must be monitored in order to guarantee timely and successful delivery of outcomes in response to user requests. However, monitoring consumes resources and very often impacts on the quality of the SBSs being monitored. Such resource and system costs need to be considered in formulating monitoring strategies for SBSs. The critical path of a composite SBS, i.e., the execution path in the service composition with the maximum execution time, is of particular importance in cost-effective monitoring as it determines the response time of the entire SBS. In volatile operating environments, the critical path of an SBS is probabilistic, as every execution path can be critical with a certain probability, i.e., its criticality. As such, it is important to estimate the criticalities of different execution paths when deciding which parts of the SBS to monitor. Furthermore, cost-effective monitoring also requires management of the trade-off between the benefit and cost of monitoring. In this paper, we propose CriMon, a novel approach to formulating and evaluating monitoring strategies for SBSs. CriMon first calculates the criticalities of the execution paths and the component services of an SBS and then, based on those criticalities, generates the optimal monitoring strategy considering both the benefit and cost of monitoring. CriMon has two monitoring strategy formulation methods, namely local optimisation and global optimisation. In-lab experimental results demonstrate that the response time of an SBS can be managed cost-effectively through CriMon-based monitoring. The effectiveness and efficiency of the two monitoring strategy formulation methods are also evaluated and compared.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.48","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6642029","Service-based system;web service;QoS;response time;monitoring;criticality;cost of monitoring;value of monitoring","Monitoring;Scattering;Probability;Time factors;Runtime;Probabilistic logic;Quality of service","service-oriented architecture;system monitoring;Web services","cost-effective monitoring strategy;service-based systems;SBSs;component services;system costs;service composition;global optimisation;local optimisation;CriMon-based monitoring approach;monitoring strategy formulation methods;service-oriented computing;Web service","","13","","65","","","","","","IEEE","IEEE Journals & Magazines"
"Product form solution for generalized stochastic Petri nets","G. Balbo; S. C. Bruell; M. Sereno","Dipt. di Inf., Torino Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","2002","28","10","915","932","In this paper, we show the structural characteristics that a particular class of generalized stochastic Petri nets must exhibit in order for their stationary probabilities to have a product-form. Sufficient conditions for identifying such a class are derived and proven with the development of a series of transformations that can also be used to construct, for any GSPN of the class, an equivalent SPN. These resulting SPNs represent the structures that can be analyzed with standard methods for product-form SPNs to establish whether the original GSPNs have product-form solutions and to compute their performance indices with effective approaches based on computationally efficient algorithms that avoid the generation of their underlying state spaces.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041049","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041049","","Stochastic processes;Petri nets;Equations;Routing;State-space methods;Traffic control;Sufficient conditions;Performance analysis;Algorithm design and analysis;Stochastic systems","Petri nets;software performance evaluation;probability;matrix algebra","generalized stochastic Petri nets;product form solution;stationary probability;priority levels;routing process;incidence matrix","","9","","15","","","","","","IEEE","IEEE Journals & Magazines"
"Inner Source in Platform-Based Product Engineering","D. Riehle; M. Capraro; D. Kips; L. Horn","Computer Science Department, Friedrich-Alexander University Erlangen-N&#x00FC;rnberg, Erlangen, Germany; Computer Science Department, Friedrich-Alexander University Erlangen-N&#x00FC;rnberg, Erlangen, Germany; Develop Group, Erlangen, Germany; e-solutions, Erlangen, Germany","IEEE Transactions on Software Engineering","","2016","42","12","1162","1177","Inner source is an approach to collaboration across intra-organizational boundaries for the creation of shared reusable assets. Prior project reports on inner source suggest improved code reuse and better knowledge sharing. Using a multiple-case case study research approach, we analyze the problems that three major software development organizations were facing in their product line engineering efforts. We find that a root cause, the separation of product units as profit centers from a platform organization as a cost center, leads to delayed deliveries, increased defect rates, and redundant software components. All three organizations assume that inner source can help solve these problems. The article analyzes the expectations that these companies were having towards inner source and the problems they were experiencing in its adoption. Finally, the article presents our conclusions on how these organizations should adapt their existing engineering efforts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2554553","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7452676","Inner source;product line engineering;product families;platform-based product engineering;open source;open collaboration;case study research","Collaboration;Product design;Software product lines;Best practices;Open source software","asset management;public domain software;software product lines","inner source;platform-based product line engineering;shared reusable asset creation;software development organization;product unit separation;profit center","","","","73","","","","","","IEEE","IEEE Journals & Magazines"
"Recovering traceability links between code and documentation","G. Antoniol; G. Canfora; G. Casazza; A. De Lucia; E. Merlo","Res. Centre on Software Technol., Univ. of Sannio, Roma, Italy; Res. Centre on Software Technol., Univ. of Sannio, Roma, Italy; NA; NA; NA","IEEE Transactions on Software Engineering","","2002","28","10","970","983","Software system documentation is almost always expressed informally in natural language and free text. Examples include requirement specifications, design documents, manual pages, system development journals, error logs, and related maintenance reports. We propose a method based on information retrieval to recover traceability links between source code and free text documents. A premise of our work is that programmers use meaningful names for program items, such as functions, variables, types, classes, and methods. We believe that the application-domain knowledge that programmers process when writing the code is often captured by the mnemonics for identifiers; therefore, the analysis of these mnemonics can help to associate high-level concepts with program concepts and vice-versa. We apply both a probabilistic and a vector space information retrieval model in two case studies to trace C++ source code onto manual pages and Java code to functional requirements. We compare the results of applying the two models, discuss the benefits and limitations, and describe directions for improvements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041053","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041053","","Documentation;Programming profession;Information retrieval;Natural languages;Context modeling;Information resources;Writing;Java;Inspection;Mathematics","system documentation;information retrieval;object-oriented programming;natural languages;probability","software system documentation;information retrieval;object orientation;traceability;program comprehension;traceability link recovery;source code;free text documents;vector space","","450","","55","","","","","","IEEE","IEEE Journals & Magazines"
"A Risk Management Methodology for Project Risk Dependencies","T. W. Kwan; H. K. N. Leung","The Hong Kong Polytechnic University, Hong Kong; The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Software Engineering","","2011","37","5","635","648","Project risks are not always independent, yet current risk management practices do not clearly manage dependencies between risks. If dependencies can be explicitly identified and analyzed, project managers will be able to develop better risk management strategies and make more effective risk planning decisions. This paper proposes a management methodology to address risk dependency issues. Through the study of three IT projects, we confirm that risk dependencies do exist in projects and can be identified and systematically managed. We also observed that, as project teams needed to deal with risk dependency issues, communications between projects were improved, and there were synergetic effects in managing risks and risk dependencies among projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.108","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5696725","Project risk management;risk dependencies;risk assessment;metrics.","Risk management;Delta modulation;Analytical models;Monitoring;Measurement;Lead;Fault trees","project management;risk analysis","project risk management;risk planning decisions;IT projects;risk dependencies","","41","","33","","","","","","IEEE","IEEE Journals & Magazines"
"An operational process for goal-driven definition of measures","L. C. Briand; S. Morasca; V. R. Basili","Syst. & Comput. Eng. Dept., Carleton Univ., Ottawa, Ont., Canada; NA; NA","IEEE Transactions on Software Engineering","","2002","28","12","1106","1125","We propose an approach (GOM/MEDEA) for defining measures of product attributes in software engineering. The approach is driven by the experimental goals of measurement, expressed via the GQM paradigm, and a set of empirical hypotheses. To make the empirical hypotheses quantitatively verifiable, GQM/MEDEA supports the definition of theoretically valid measures for the attributes of interest based on their expected mathematical properties. The empirical hypotheses are subject to experimental verification. This approach integrates several research contributions from the literature into a consistent, practical, and rigorous approach.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158285","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158285","","Software measurement;Software engineering;Software quality;Size measurement;Humans;Application software;Computer Society;Programming;Process planning;Monitoring","software metrics;software quality;software development management","GQM/MEDEA;goal-driven measure definition;operational process;product attributes;software engineering;measurement;GQM paradigm;empirical hypotheses;expected mathematical properties;experimental verification","","61","","43","","","","","","IEEE","IEEE Journals & Magazines"
"Generating software test data by evolution","C. C. Michael; G. McGraw; M. A. Schatz","Cigital Corp., Dulles, VA, USA; NA; NA","IEEE Transactions on Software Engineering","","2001","27","12","1085","1110","This paper discusses the use of genetic algorithms (GAs) for automatic software test data generation. This research extends previous work on dynamic test data generation where the problem of test data generation is reduced to one of minimizing a function. In our work, the function is minimized by using one of two genetic algorithms in place of the local minimization techniques used in earlier research. We describe the implementation of our GA-based system and examine the effectiveness of this approach on a number of programs, one of which is significantly larger than those for which results have previously been reported in the literature. We also examine the effect of program complexity on the test data generation problem by executing our system on a number of synthetic programs that have varying complexities.","0098-5589;1939-3520;2326-3881","","10.1109/32.988709","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988709","","Software testing;Automatic testing;Genetic algorithms;Performance evaluation;Minimization methods;System testing;Heuristic algorithms;Instruments;Simulated annealing","program testing;automatic testing;genetic algorithms","automatic software test data generation;evolution;genetic algorithms;dynamic test data generation;program complexity","","218","","36","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient Software Verification: Statistical Testing Using Automated Search","S. Poulding; J. A. Clark","University of York, York; University of York, York","IEEE Transactions on Software Engineering","","2010","36","6","763","777","Statistical testing has been shown to be more efficient at detecting faults in software than other methods of dynamic testing such as random and structural testing. Test data are generated by sampling from a probability distribution chosen so that each element of the software's structure is exercised with a high probability. However, deriving a suitable distribution is difficult for all but the simplest of programs. This paper demonstrates that automated search is a practical method of finding near-optimal probability distributions for real-world programs, and that test sets generated from these distributions continue to show superior efficiency in detecting faults in the software.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5406530","Software/program verification;testing strategies;test coverage of code;optimization.","Statistical analysis;Software testing;Automatic testing;Probability distribution;Software engineering;Fault detection;Sampling methods;Software algorithms;Application software;Flow graphs","program testing;program verification;statistical distributions;statistical testing","software verification;statistical testing;automated search;software fault detection;dynamic testing;random testing;structural testing;test data;near-optimal probability distribution","","26","","48","","","","","","IEEE","IEEE Journals & Magazines"
"Software cost estimation with incomplete data","K. Strike; K. El Emam; N. Madhavji","Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada; NA; NA","IEEE Transactions on Software Engineering","","2001","27","10","890","908","The construction of software cost estimation models remains an active topic of research. The basic premise of cost modeling is that a historical database of software project cost data can be used to develop a quantitative model to predict the cost of future projects. One of the difficulties faced by workers in this area is that many of these historical databases contain substantial amounts of missing data. Thus far, the common practice has been to ignore observations with missing data. In principle, such a practice can lead to gross biases and may be detrimental to the accuracy of cost estimation models. We describe an extensive simulation where we evaluate different techniques for dealing with missing data in the context of software cost modeling. Three techniques are evaluated: listwise deletion, mean imputation, and eight different types of hot-deck imputation. Our results indicate that all the missing data techniques perform well with small biases and high precision. This suggests that the simplest technique, listwise deletion, is a reasonable choice. However, this will not necessarily provide the best performance. Consistent best performance (minimal bias and highest precision) can be obtained by using hot-deck imputation with Euclidean distance and a z-score standardization.","0098-5589;1939-3520;2326-3881","","10.1109/32.962560","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=962560","","Costs;Predictive models;Size measurement;Productivity;Databases;Context modeling;Project management;Software performance;Euclidean distance;Standardization","software cost estimation;data integrity;database management systems","software cost estimation;cost modeling;database;software project cost data;quantitative model;missing data;simulation;listwise deletion;mean imputation;hot-deck imputation;incomplete data;Euclidean distance;z-score standardization;data quality","","91","","90","","","","","","IEEE","IEEE Journals & Magazines"
"Predicting Future Developer Behavior in the IDE Using Topic Models","K. Damevski; H. Chen; D. C. Shepherd; N. A. Kraft; L. Pollock","Department of Computer Science, Virginia Commonwealth University, Richmond, VA; Department of Computer and Information Science, University of New York, Brooklyn, NY; ABB Corporate Research, Raleigh, NC; ABB Corporate Research, Raleigh, NC; Department of Computer and Information Sciences, University of Delaware, Newark, DE","IEEE Transactions on Software Engineering","","2018","44","11","1100","1111","While early software command recommender systems drew negative user reaction, recent studies show that users of unusually complex applications will accept and utilize command recommendations. Given this new interest, more than a decade after first attempts, both the recommendation generation (backend) and the user experience (frontend) should be revisited. In this work, we focus on recommendation generation. One shortcoming of existing command recommenders is that algorithms focus primarily on mirroring the short-term past,-i.e., assuming that a developer who is currently debugging will continue to debug endlessly. We propose an approach to improve on the state of the art by modeling future task context to make better recommendations to developers. That is, the approach can predict that a developer who is currently debugging may continue to debug OR may edit their program. To predict future development commands, we applied Temporal Latent Dirichlet Allocation, a topic model used primarily for natural language, to software development interaction data (i.e., command streams). We evaluated this approach on two large interaction datasets for two different IDEs, Microsoft Visual Studio and ABB Robot Studio. Our evaluation shows that this is a promising approach for both predicting future IDE commands and producing empirically-interpretable observations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2748134","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8024001","Command recommendation systems;IDE interaction data","Natural languages;Data models;Analytical models;Predictive models;Visualization;Adaptation models;Data analysis","program debugging;recommender systems;software engineering","future developer behavior;early software command recommender systems;negative user reaction;unusually complex applications;command recommendations;recommendation generation;user experience;command recommenders;future task context;debug OR;future development commands;software development interaction data;predicting future IDE commands;empirically-interpretable observations","","","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Parallaxis-III: architecture-independent data parallel processing","T. Braunl","Dept. of Electr. & Electron. Eng., Western Australia Univ., Nedlands, WA, Australia","IEEE Transactions on Software Engineering","","2000","26","3","227","243","Parallaxis-III is an architecture-independent data parallel programming language based on Modula-2. It has been designed for teaching data parallel concepts and is in use at a large number of institutions. Compilers exist for data parallel systems, as well as for a sequential simulation system. A data parallel graphics debugger allows efficient source level analysis for parallel programs.","0098-5589;1939-3520;2326-3881","","10.1109/32.842949","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842949","","Parallel processing;Parallel programming;Computational modeling;Concurrent computing;Education;Data visualization;Computer simulation;Computer languages;Iterative algorithms;Parallel languages","parallel programming;parallel languages;program debugging;data visualisation;virtual machines","Parallaxis-III;architecture-independent data parallel processing;data parallel programming language;Modula-2;compilers;sequential simulation system;data parallel graphics debugger;source level analysis;parallel programs","","1","","21","","","","","","IEEE","IEEE Journals & Magazines"
"Simplifying and isolating failure-inducing input","A. Zeller; R. Hildebrandt","Lehrstuhl fur Softwaretechnik, Saarlandes Univ., Saarbrucken, Germany; NA","IEEE Transactions on Software Engineering","","2002","28","2","183","200","Given some test case, a program fails. Which circumstances of the test case are responsible for the particular failure? The delta debugging algorithm generalizes and simplifies the failing test case to a minimal test case that still produces the failure. It also isolates the difference between a passing and a failing test case. In a case study, the Mozilla Web browser crashed after 95 user actions. Our prototype implementation automatically simplified the input to three relevant user actions. Likewise, it simplified 896 lines of HTML to the single line that caused the failure. The case study required 139 automated test runs or 35 minutes on a 500 MHz PC.","0098-5589;1939-3520;2326-3881","","10.1109/32.988498","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988498","","Vehicle crash testing;Debugging;Automatic testing;HTML;Computer crashes;Computer Society;Prototypes;Databases;Computer bugs;Turning","program debugging;program testing;online front-ends","test case;delta debugging algorithm;failure-inducing input;Mozilla web browser;user actions;HTML;500 MHz","","313","","14","","","","","","IEEE","IEEE Journals & Magazines"
"Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines","C. Henard; M. Papadakis; G. Perrouin; J. Klein; P. Heymans; Y. Le Traon","Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg","IEEE Transactions on Software Engineering","","2014","40","7","650","670","Large Software Product Lines (SPLs) are common in industry, thus introducing the need of practical solutions to test them. To this end, t-wise can help to drastically reduce the number of product configurations to test. Current t-wise approaches for SPLs are restricted to small values of t. In addition, these techniques fail at providing means to finely control the configuration process. In view of this, means for automatically generating and prioritizing product configurations for large SPLs are required. This paper proposes (a) a search-based approach capable of generating product configurations for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of product configurations. Both these techniques employ a similarity heuristic. The ability of the proposed techniques is assessed in an empirical study through a comparison with state of the art tools. The comparison focuses on both the product configuration generation and the prioritization aspects. The results demonstrate that existing t-wise tools and prioritization techniques fail to handle large SPLs. On the contrary, the proposed techniques are both effective and scalable. Additionally, the experiments show that the similarity heuristic can be used as a viable alternative to t-wise.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2327020","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6823132","Software product lines;testing;T-wise Interactions;search-based approaches;prioritization;similarity","Testing;Frequency modulation;Context;Scalability;Software;Linux;Arrays","combinatorial mathematics;program testing;software product lines","combinatorial explosion;test configurations;software product lines;SPL;product configurations;configuration process;search based approach;similarity heuristic;product configuration generation","","57","","64","","","","","","IEEE","IEEE Journals & Magazines"
"Static Analysis for Extracting Permission Checks of a Large Scale Framework: The Challenges and Solutions for Analyzing Android","A. Bartel; J. Klein; M. Monperrus; Y. Le Traon","Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, 4, rue Alphonse Weicker, Luxembourg, Kirchberg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, 4, rue Alphonse Weicker, Luxembourg, Kirchberg; University of Lille and Inria, Villeneuve d'Ascq, France; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, 4, rue Alphonse Weicker, Luxembourg, Kirchberg","IEEE Transactions on Software Engineering","","2014","40","6","617","632","A common security architecture is based on the protection of certain resources by permission checks (used e.g., in Android and Blackberry). It has some limitations, for instance, when applications are granted more permissions than they actually need, which facilitates all kinds of malicious usage (e.g., through code injection). The analysis of permission-based framework requires a precise mapping between API methods of the framework and the permissions they require. In this paper, we show that naive static analysis fails miserably when applied with off-the-shelf components on the Android framework. We then present an advanced class-hierarchy and field-sensitive set of analyses to extract this mapping. Those static analyses are capable of analyzing the Android framework. They use novel domain specific optimizations dedicated to Android.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2322867","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6813664","Large scale framework;permissions;call-graph;Android;security;Soot;Java;static analysis","Androids;Humanoid robots;Sparks;Cameras;Java;Servers;Security","Android (operating system);optimisation;program diagnostics;security of data","static analysis;permission checks;large scale framework;common security architecture;permission-based framework;API methods;Android framework;novel domain specific optimizations;advanced class-hierarchy analysis;field-sensitive set analysis","","23","","32","","","","","","IEEE","IEEE Journals & Magazines"
"Group communication in partitionable systems: specification and algorithms","O. Babaoglu; R. Davoli; A. Montresor","Dept. of Comput. Sci., Bologna Univ., Italy; NA; NA","IEEE Transactions on Software Engineering","","2001","27","4","308","336","Gives a formal specification and an implementation for a partitionable group communication service in asynchronous distributed systems. Our specification is motivated by the requirements for building ""partition-aware"" applications that can continue operating without blocking in multiple concurrent partitions and can reconfigure themselves dynamically when partitions merge. The specified service guarantees liveness and excludes trivial solutions, it constitutes a useful basis for building realistic partition-aware applications, and it is implementable in practical asynchronous distributed systems where certain stability conditions hold.","0098-5589;1939-3520;2326-3881","","10.1109/32.917522","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=917522","","Partitioning algorithms;Application software;Mobile computing;Computer networks;Degradation;Computer Society;Formal specifications;Stability;Fault tolerant systems;Computer applications","formal specification;distributed processing;stability criteria;fault tolerant computing","partitionable group communication service;asynchronous distributed systems;specification;algorithms;formal specification;partition-aware applications;concurrent partitions;dynamic reconfiguration;partition merging;liveness;stability conditions;view synchrony;fault tolerance","","27","","34","","","","","","IEEE","IEEE Journals & Magazines"
"REPENT: Analyzing the Nature of Identifier Renamings","V. Arnaoudova; L. M. Eshkevari; M. D. Penta; R. Oliveto; G. Antoniol; Y. Guéhéneuc","Polytechnique Montréal, Québec, Canada; Polytechnique Montréal, Québec, Canada; University of Sannio, Benevento, Italy; University of Molise, Pesche (IS), Italy; Polytechnique Montréal, Québec, Canada; Polytechnique Montréal, Québec, Canada","IEEE Transactions on Software Engineering","","2014","40","5","502","532","Source code lexicon plays a paramount role in software quality: poor lexicon can lead to poor comprehensibility and even increase software fault-proneness. For this reason, renaming a program entity, i.e., altering the entity identifier, is an important activity during software evolution. Developers rename when they feel that the name of an entity is not (anymore) consistent with its functionality, or when such a name may be misleading. A survey that we performed with 71 developers suggests that 39 percent perform renaming from a few times per week to almost every day and that 92 percent of the participants consider that renaming is not straightforward. However, despite the cost that is associated with renaming, renamings are seldom if ever documented-for example, less than 1 percent of the renamings in the five programs that we studied. This explains why participants largely agree on the usefulness of automatically documenting renamings. In this paper we propose REanaming Program ENTities (REPENT), an approach to automatically document-detect and classify-identifier renamings in source code. REPENT detects renamings based on a combination of source code differencing and data flow analyses. Using a set of natural language tools, REPENT classifies renamings into the different dimensions of a taxonomy that we defined. Using the documented renamings, developers will be able to, for example, look up methods that are part of the public API (as they impact client applications), or look for inconsistencies between the name and the implementation of an entity that underwent a high risk renaming (e.g., towards the opposite meaning). We evaluate the accuracy and completeness of REPENT on the evolution history of five open-source Java programs. The study indicates a precision of 88 percent and a recall of 92 percent. In addition, we report an exploratory study investigating and discussing how identifiers are renamed in the five programs, according to our taxonomy.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2312942","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6776542","Identifier renaming;refactoring;program comprehension;mining software repositories;empirical study","Taxonomy;Semantics;Java;Grammar;Software;History;Documentation","data flow analysis;pattern classification;software fault tolerance;software quality;source code (software)","identifier renaming analysis;REPENT;source code lexicon;software quality;software fault-proneness;program entity renaming;entity identifier;software evolution;reanaming program entities;data flow analysis;natural language tools;taxonomy dimensions;public API;open-source Java programs","","19","","58","","","","","","IEEE","IEEE Journals & Magazines"
"Performance Analysis for Object-Oriented Software: A Systematic Mapping","D. Maplesden; E. Tempero; J. Hosking; J. C. Grundy","Department of Computer Science, University of Auckland, Private Bag 92019, Auckland 1142, New Zealand; Department of Computer Science, University of Auckland, Private Bag 92019, Auckland 1142, New Zealand; Faculty of Science, University of Auckland, Private Bag 92019, Auckland 1142, New Zealand; Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn, Vic. 3122, Australia","IEEE Transactions on Software Engineering","","2015","41","7","691","710","Performance is a crucial attribute for most software, making performance analysis an important software engineering task. The difficulty is that modern applications are challenging to analyse for performance. Many profiling techniques used in real-world software development struggle to provide useful results when applied to large-scale object-oriented applications. There is a substantial body of research into software performance generally but currently there exists no survey of this research that would help identify approaches useful for object-oriented software. To provide such a review we performed a systematic mapping study of empirical performance analysis approaches that are applicable to object-oriented software. Using keyword searches against leading software engineering research databases and manual searches of relevant venues we identified over 5,000 related articles published since January 2000. From these we systematically selected 253 applicable articles and categorised them according to ten facets that capture the intent, implementation and evaluation of the approaches. Our mapping study results allow us to highlight the main contributions of the existing literature and identify areas where there are interesting opportunities. We also find that, despite the research including approaches specifically aimed at object-oriented software, there are significant challenges in providing actionable feedback on the performance of large-scale object-oriented applications.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2396514","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7024167","Systematic review;survey;performance;object-oriented;Systematic review;survey;performance;object-oriented","Performance analysis;Systematics;Software performance;Databases;Mathematical model;Runtime","database management systems;object-oriented methods;program diagnostics;software performance evaluation","performance analysis;object-oriented software;systematic mapping;software engineering task;profiling techniques;software engineering research database","","2","","17","","","","","","IEEE","IEEE Journals & Magazines"
"When and Why Your Code Starts to Smell Bad (and Whether the Smells Go Away)","M. Tufano; F. Palomba; G. Bavota; R. Oliveto; M. D. Penta; A. De Lucia; D. Poshyvanyk","College of William and Mary, Williamsburg, VA; University of Salerno, Fisciano, SA, Italy; Università della Svizzera italiana (USI), Lugano, Switzerland; University of Molise, Pesche, (IS), Italy; University of Sannio, Benevento, BN, Italy; University of Salerno, Fisciano, SA, Italy; College of William and Mary, Williamsburg, VA","IEEE Transactions on Software Engineering","","2017","43","11","1063","1088","Technical debt is a metaphor introduced by Cunningham to indicate “not quite right code which we postpone making it right”. One noticeable symptom of technical debt is represented by code smells, defined as symptoms of poor design and implementation choices. Previous studies showed the negative impact of code smells on the comprehensibility and maintainability of code. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced, what is their survivability, and how they are removed by developers. To empirically corroborate such anecdotal evidence, we conducted a large empirical study over the change history of 200 open source projects. This study required the development of a strategy to identify smell-introducing commits, the mining of over half a million of commits, and the manual analysis and classification of over 10K of them. Our findings mostly contradict common wisdom, showing that most of the smell instances are introduced when an artifact is created and not as a result of its evolution. At the same time, 80 percent of smells survive in the system. Also, among the 20 percent of removed instances, only 9 percent are removed as a direct consequence of refactoring operations.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2653105","NSF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7817894","Code smells;empirical study;mining software repositories","Ecosystems;History;Androids;Humanoid robots;Software systems;Maintenance engineering","data mining;public domain software;software maintenance;software quality;source code (software)","open source projects;code smells;code comprehensibility;code maintainability;commits mining;smell instances;smell-introducing commits;code quality;technical debt","","8","","91","","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Automatic Summarization of Bug Reports","S. Rastkar; G. C. Murphy; G. Murray","Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver, Canada; Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver, Canada; Computer Information Systems Department, University of the Fraser Valley, 33844 King Road, Abbotsford, Canada","IEEE Transactions on Software Engineering","","2014","40","4","366","380","Software developers access bug reports in a project's bug repository to help with a number of different tasks, including understanding how previous changes have been made and understanding multiple aspects of particular defects. A developer's interaction with existing bug reports often requires perusing a substantial amount of text. In this article, we investigate whether it is possible to summarize bug reports automatically so that developers can perform their tasks by consulting shorter summaries instead of entire bug reports. We investigated whether existing conversation-based automated summarizers are applicable to bug reports and found that the quality of generated summaries is similar to summaries produced for e-mail threads and other conversations. We also trained a summarizer on a bug report corpus. This summarizer produces summaries that are statistically better than summaries produced by existing conversation-based generators. To determine if automatically produced bug report summaries can help a developer with their work, we conducted a task-based evaluation that considered the use of summaries for bug report duplicate detection tasks. We found that summaries helped the study participants save time, that there was no evidence that accuracy degraded when summaries were used and that most participants preferred working with summaries to working with original bug reports.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.2297712","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6704866","Empirical software engineering;summarization of software artifacts;bug report duplicate detection","Software;Electronic mail;Computer bugs;Natural languages;Feature extraction;Detectors","electronic mail;program debugging;software engineering","automatic summarization;bug reports;software developers;bug repository;conversation-based automated summarizers;e-mail threads;bug report corpus;conversation-based generators;bug report summaries;task-based evaluation;bug report duplicate detection tasks","","41","","41","","","","","","IEEE","IEEE Journals & Magazines"
"Extending the UML Statecharts Notation to Model Security Aspects","M. El-Attar; H. Luqman; P. Kárpáti; G. Sindre; A. L. Opdahl","Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Kingdom of Saudi Arabia; Information and Computer Science Department, King Fahd University of Petroleum and Minerals, Dhahran, Kingdom of Saudi Arabia; Institute for Energy Technology, Halden, Norway; Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway; Department of Information Science and Media, University of Bergen, Bergen, Norway","IEEE Transactions on Software Engineering","","2015","41","7","661","690","Model driven security has become an active area of research during the past decade. While many research works have contributed significantly to this objective by extending popular modeling notations to model security aspects, there has been little modeling support for state-based views of security issues. This paper undertakes a scientific approach to propose a new notational set that extends the UML (Unified Modeling Language) statecharts notation. An online industrial survey was conducted to measure the perceptions of the new notation with respect to its semantic transparency as well as its coverage of modeling state based security aspects. The survey results indicate that the new notation encompasses the set of semantics required in a state based security modeling language and was largely intuitive to use and understand provided very little training. A subject-based empirical evaluation using software engineering professionals was also conducted to evaluate the cognitive effectiveness of the proposed notation. The main finding was that the new notation is cognitively more effective than the original notational set of UML statecharts as it allowed the subjects to read models created using the new notation much quicker.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2396526","Deanship of Scientific Research; King Fahd University of Petroleum and Minerals; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7042284","Statecharts;Security Modeling;Extended Notation;Industrial Survey;Subject-Based Experiment;Statecharts;security modeling;extended notation;industrial survey;subject-based experiment","Unified modeling language;Security;Software engineering;Object oriented modeling;Semantics;Proposals;Educational institutions","security of data;software engineering;Unified Modeling Language","UML statecharts notation;model driven security;scientific approach;Unified Modeling Language statecharts notation;semantic transparency;modeling state based security aspect coverage;state based security modeling language;subject-based empirical evaluation;software engineering professionals;notation cognitive effectiveness","","5","","80","","","","","","IEEE","IEEE Journals & Magazines"
"CTTE: support for developing and analyzing task models for interactive system design","G. Mori; F. Paterno; C. Santoro","ISTI, Nat. Res. Council of Italy, Pisa, Italy; ISTI, Nat. Res. Council of Italy, Pisa, Italy; ISTI, Nat. Res. Council of Italy, Pisa, Italy","IEEE Transactions on Software Engineering","","2002","28","8","797","813","While task modeling and task-based design are entering into current practice in the design of interactive software applications, there is still a lack of tools supporting the development and analysis of task models. Such tools should provide developers with ways to represent tasks, including their attributes and objects and their temporal and semantic relationships, to easily create, analyze, and modify such representations and to simulate their dynamic behavior. In this paper, we present a tool, CTTE, that provides thorough support for developing and analyzing task models of cooperative applications, which can then be used to improve the design and evaluation of interactive software applications. We discuss how we have designed this environment and report on trials of its use.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1027801","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1027801","","Interactive systems;Application software;User interfaces;Software tools;Context modeling;Performance analysis;Software design;Analytical models;Software systems","interactive systems;task analysis;user interfaces;systems analysis","CTTE;task models;interactive system design;task-based design;interactive software;attributes;semantic relationships;dynamic behavior;user interfaces","","133","","33","","","","","","IEEE","IEEE Journals & Magazines"
"Listening to the Crowd for the Release Planning of Mobile Apps","S. Scalabrino; G. Bavota; B. Russo; M. D. Penta; R. Oliveto","University of Molise, Campobasso, Italy; Università della Svizzera Italiana (USI), Lugano, Switzerland; Free University of Bozen-Bolzano, Bolzano, South Tyrol, Italy; University of Sannio, Benevento, Italy; University of Molise, Campobasso, Italy","IEEE Transactions on Software Engineering","","2019","45","1","68","86","The market for mobile apps is getting bigger and bigger, and it is expected to be worth over 100 Billion dollars in 2020. To have a chance to succeed in such a competitive environment, developers need to build and maintain high-quality apps, continuously astonishing their users with the coolest new features. Mobile app marketplaces allow users to release reviews. Despite reviews are aimed at recommending apps among users, they also contain precious information for developers, reporting bugs and suggesting new features. To exploit such a source of information, developers are supposed to manually read user reviews, something not doable when hundreds of them are collected per day. To help developers dealing with such a task, we developed CLAP (Crowd Listener for releAse Planning), a web application able to (i) categorize user reviews based on the information they carry out, (ii) cluster together related reviews, and (iii) prioritize the clusters of reviews to be implemented when planning the subsequent app release. We evaluated all the steps behind CLAP, showing its high accuracy in categorizing and clustering reviews and the meaningfulness of the recommended prioritizations. Also, given the availability of CLAP as a working tool, we assessed its applicability in industrial environments.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2759112","SNF; ","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8057860","Release planning;mobile apps;mining software repositories","Computer bugs;Planning;Tools;Google;Mobile communication;Security;Electronic mail","data mining;Internet;mobile computing;pattern clustering","mobile app marketplaces;user reviews;CLAP;clustering reviews;crowd listener for release planning;Web application;categorizing reviews","","2","","42","","","","","","IEEE","IEEE Journals & Magazines"
"New Transactions Issue Alerts","","","IEEE Transactions on Software Engineering","","2011","37","6","879","879","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.118","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6095283","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society celebrates two 60-year anniversaries","","","IEEE Transactions on Software Engineering","","2006","32","3","212","212","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610612","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society Career Center","","","IEEE Transactions on Software Engineering","","2008","34","6","863","863","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.99","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4709492","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"The IEEE Computer Society Celebrates Two 60-Year Anniversaries","","","IEEE Transactions on Software Engineering","","2006","32","7","527","527","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.72","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677537","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"The IEEE Computer Society celebrates two 60-Year Anniversaries","","","IEEE Transactions on Software Engineering","","2006","32","6","432","432","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.58","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650218","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: AE Farewell","J. Knight","NA","IEEE Transactions on Software Engineering","","2004","30","5","281","281","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291831","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Signing Off: The State of the Journal","B. Nuseibeh","Open University, UK","IEEE Transactions on Software Engineering","","2014","40","1","1","3","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2298171","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6757048","","Editorials;Software engineering;Communities;Educational institutions;Software;Bibliometrics;Production","","","","","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editor's Introduction: 10th Working Conference on Reverse Engineering","A. van Deursen; E. Stroulia","NA; NA","IEEE Transactions on Software Engineering","","2005","31","2","97","98","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401926","","Reverse engineering;Application software;Hardware;Software maintenance;Program processors;Cloning;Computer architecture;Service oriented architecture;Image reconstruction;Computer Society","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","12","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.135","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566610","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: New Associate Editors Introduction","J. Kramer","NA","IEEE Transactions on Software Engineering","","2006","32","3","137","139","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610606","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: A New Editor-in-Chief and the State of TSE","J. Knight","NA","IEEE Transactions on Software Engineering","","2006","32","1","1","1","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583597","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","8","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.93","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498776","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction to the Special Section on Evaluation and Improvement of Software Dependability","K. Goseva-Popstojanova; K. Kanoun","Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV; LAAS-CNRS, 31077 Toulouse, France","IEEE Transactions on Software Engineering","","2010","36","3","306","308","The four papers in this special section present new findings on different aspects of software dependability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5473900","","Software systems;Information security;Safety;Software testing;Medical control systems;Control systems;Humans;Software quality;Availability;Airplanes","","","","1","","51","","","","","","IEEE","IEEE Journals & Magazines"
"State of the Journal","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2012","38","1","1","2","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6141070","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editorial: Special Issue on Software Maintenance and Evolution","M. Harman; B. Korel; P. K. Linos","NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","10","801","803","In systems developed without aspect-oriented programming, code implementing a crosscutting concern may be spread over many different parts of a system. Identifying such code automatically could be of great help during maintenance of the system. First of all, it allows a developer to more easily find the places in the code that must be changed when the concern changes and, thus, makes such changes less time consuming and less prone to errors. Second, it allows the code to be refactored to an aspect-oriented solution, thereby improving its modularity. In this paper, we evaluate the suitability of clone detection as a technique for the identification of crosscutting concerns. To that end, we manually identify five specific crosscutting concerns in an industrial C system and analyze to what extent clone detection is capable of finding them. We consider our results as a stepping stone toward an automated ""aspect miner” based on clone detection.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.113","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542063","Index Terms- Clone detection;reverse engineering;aspect-oriented programming;crosscutting concerns;aspect mining.","Software maintenance;Conferences;Computer science;Software engineering;Software algorithms;Programming;Cloning;Software systems;Costs;Humans","","Index Terms- Clone detection;reverse engineering;aspect-oriented programming;crosscutting concerns;aspect mining.","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"In Memoriam: Robin Milner and Amir Pnueli","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2010","36","3","305","305","Provides the biographies for two members of the computing community, Robin Milner and Amir Pnueli, who recently passed away. Both were Turing Award winners and both contributed in fundamental ways to the foundations of software engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5473899","","Obituary;Robin Milner and Amir Pnueli","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2004","30","7","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318611","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2004","30","9","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324650","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: AE introduction","T. Ball; M. Chechik; D. Hoffman","Microsoft Research; NA; NA","IEEE Transactions on Software Engineering","","2004","30","1","1","2","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265731","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265731","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction: Special Section on the Socio-Technical Environment of Software Development Projects","M. Cataldo; K. Ehrlich; A. Mockus","NA; NA; NA","IEEE Transactions on Software Engineering","","2011","37","3","305","306","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5779016","","Special issues and sections;Software development;Social factors;Software architecture;Collaboration","","","","1","","4","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial","J. Knight","NA","IEEE Transactions on Software Engineering","","2002","28","1","3","3","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.979985","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979985","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","10","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70737","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302783","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Call for Papers","","","IEEE Transactions on Software Engineering","","2004","30","7","488","488","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318609","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: State of the Journal Addrress","","","IEEE Transactions on Software Engineering","","2005","31","1","1","1","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392715","","","","","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","3","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610614","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2012","38","6","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.80","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363460","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction: 2003 International Conference on Software Engineering","L. K. Dillon; W. F. Tichy","Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824; Institut fu¨r Programmstrukturen und Datenorganisation (IPD), Universita¨t Karlsruhe, Postfach 6980, D-76128 Karlsruhe, Germany","IEEE Transactions on Software Engineering","","2004","30","6","353","354","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321058","","Software engineering;Educational programs;Automotive engineering;Testing;Computer Society;Software systems;Silver;Paper technology;Engineering management;Discrete event simulation","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction to the Special Issue on the International Conference on Software Maintenance and Evolution","T. Gyimothy; V. Rajlich","NA; NA","IEEE Transactions on Software Engineering","","2006","32","9","625","626","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.89","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707663","","Software maintenance;Java;Conferences;Reverse engineering;Application software;Documentation;Programming;Visualization;Statistical analysis","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","2","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401936","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","2","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052590","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","11","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.111","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015516","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Improving Timeliness and Visibility in Publishing Software Engineering Research","M. B. Dwyer","NA","IEEE Transactions on Software Engineering","","2017","43","3","205","206","Reports on initiatives to improve and enhance the IEEE Transactions on Software Engineering. ","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2663918","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7876880","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: New Associate Editors Introduction","J. Knight","NA","IEEE Transactions on Software Engineering","","2005","31","3","185","186","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423990","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: New Associate Editors Introduction","J. Kramer","NA","IEEE Transactions on Software Engineering","","2008","34","6","721","722","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.97","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4709490","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","7","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.070704","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4227831","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Transactions on Software Engineering","","","IEEE Transactions on Software Engineering","","2004","30","2","C2","C2","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265812","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265812","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: New Associate Editors Introduction","J. Kramer","NA","IEEE Transactions on Software Engineering","","2009","35","4","449","449","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.48","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5186361","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2004","30","8","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316873","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2013","39","9","1187","1189","IT is my pleasure to introduce a number of distinguished researchers to the Editorial Board of IEEE Transactions on Software Engineering (TSE) this month. Their expertise covers a range of of areas that have seen consistently large numbers of submissions in recent times, and each new associate editor (AE) brings a track record of significant contributions to their field. Their short biographies are provided. Additionally, I am happy to report in the meantime that the latest journal Impact Factors have recently been published, and TSE's has risen to 2.6. It continues to be the highest of all software engineering and related journals.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6587460","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: New AE Introduction","","","IEEE Transactions on Software Engineering","","2004","30","11","713","714","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.85","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359766","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial from the New Editor in Chief","N. Medvidović","NA","IEEE Transactions on Software Engineering","","2018","44","1","3","4","Presents the introductory editorial for this issue of the publication.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2778899","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8249710","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society Information","","","IEEE Transactions on Software Engineering","","2004","30","1","82","82","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265739","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265739","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial [new associate editors]","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2013","39","5","588","590","It is the Editor-in-Chief's (EiC's) pleasure to welcome a number of new associate editors to the editorial board of the IEEE Transactions on Software Engineering. They are: Luciano Baresi, Daniela Damian, Robert DeLine, Audris Mockus, Gail Murphy, Mauro Pezze, Gian Pietro Pico, Helen Sharp, and Paolo Tonella. They bring a wealth of expertise in a broad range of research areas within software engineering, consolidating traditional strengths in areas such as software testing, and strengthening areas such as empirical studies of software development, mobile computing, and adaptive systems. Short professional biographies are included. At the same time, the EiC would like to bid farewell to those associate editors whose terms of service have ended: Martin Robillard, Peggy Storey, and Tetsuo Tamai. He thanks them for their distinguished contributions over a number of years, and for continuing to handle submitted manuscripts already on their editorial stack.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6509892","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editor's Introduction: 2004 IEEE International Symposium on Software Testing and Analysis","G. Rothermel","Department of Computer Science and Engineering, 360 Avery Hall, University of Nebraska—Lincoln, Lincoln, NE 68588","IEEE Transactions on Software Engineering","","2005","31","4","273","274","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.48","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435349","","Software testing;System testing;Java;Computer industry;Web services;Meetings;Algorithm design and analysis;Fault diagnosis;Analytical models","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","8","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70716","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267030","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2004","30","6","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321068","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society - Information","","","IEEE Transactions on Software Engineering","","2004","30","4","280","280","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274048","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274048","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: What Makes a Publication Archival?","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2011","37","2","145","145","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5739157","","","","","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","9","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707673","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","1","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392727","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","6","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650220","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"E-ditorial: State of the Journal","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2013","39","2","145","146","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.5","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6419732","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction: Special Section on Software Engineering for Secure Systems","P. McDaniel; B. Nuseibeh","NA; IEEE Computer Society","IEEE Transactions on Software Engineering","","2008","34","1","3","4","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444343","","Software engineering;Software systems;Software design;Privacy;Data security;Web services;Protection;Computer Society;Internet","","","","1","","5","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: State of the Journal","J. Kramer","NA","IEEE Transactions on Software Engineering","","2007","33","1","1","1","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256940","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027144","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: State of the Journal","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2011","37","1","1","3","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5704236","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","3","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423998","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editor's Introduction: Special Issue on Mining Software Repositories","A. E. Hassan; A. Mockus; R. C. Holt; P. M. Johnson","School of Computer Science, University of Waterloo, 200 University Avenue West Waterloo, ON N2L 3G1 Canada; Software Technology Research Department, Avaya Labs Research, 233 Mt. Airy Road, Basking Ridge, NJ 07920; School of Computer Science, University of Waterloo, 200 University Avenue West Waterloo, ON N2L 3G1 Canada; Department of Information and Computer Sciences, University of Hawaii, 1680 East-West Road Honolulu, HI 96822","IEEE Transactions on Software Engineering","","2005","31","6","426","428","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463227","","Conferences;Programming;Software engineering;Software tools;Books;Control systems;Software systems;Data mining;Communication system control;Personnel","","","","6","","","","","","","","IEEE","IEEE Journals & Magazines"
"Introduction: The Best Papers of ISSTA","B. G. Ryder; A. Zeller","NA; NA","IEEE Transactions on Software Engineering","","2010","36","4","451","452","We present the best papers of the International Symposium on Software Testing and Analysis (ISSTA) 2008.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5532338","","Software testing;System testing;Sections;Electronic equipment testing;Electronic voting;Humans;Computer science;Computer industry;Security;Electronic voting systems","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: Journal-First Publication for the Software Engineering Community","M. B. Dwyer; D. S. Rosenblum","NA; NA","IEEE Transactions on Software Engineering","","2016","42","1","1","1","Presents the introductory editorial for this issue of the publication.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2500318","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7374796","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","4","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435359","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","8","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.78","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703392","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial","A. A. Andrews","NA","IEEE Transactions on Software Engineering","","2001","27","5","385","386","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.922712","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=922712","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2010","36","6","735","736","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.104","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5644734","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","11","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.126","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556559","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"State of the Journal","M. Dwyer","NA","IEEE Transactions on Software Engineering","","2018","44","1","1","2","Presents the state of the journal for this issue of the publication.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2778898","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8249614","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"State of the Journal Editorial","M. B. Dwyer","NA","IEEE Transactions on Software Engineering","","2015","41","1","1","2","Reports on the state of the journal.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2380479","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7004121","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","1","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256948","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027152","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction to the Special Issue on Quantitative Evaluation of Computer Systems","J. Hillston; M. Kwiatkowska; M. Telek","NA; NA; NA","IEEE Transactions on Software Engineering","","2009","35","2","145","147","The 10 items in this special issue focus on quantitative evaluation of computer systems.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4809711","","Stochastic processes;Performance analysis;Petri nets;Queueing analysis;Stochastic systems;Discrete event simulation;Protocols;Performance evaluation;Air traffic control;Computational modeling","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","2","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599423","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599423","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: AE Introduction and Farewell","J. Knight","NA","IEEE Transactions on Software Engineering","","2004","30","10","633","633","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.65","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339275","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Connecting and Serving the Software Engineering Community","M. B. Dwyer; E. Bodden; B. Fitzgerald; M. Kim; S. Kim; A. J. Ko; E. Mendes; R. Mirandola; A. Moreira; F. Shull; S. Siegel; T. Xie; C. Zhang","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2016","42","3","203","280","Presents an editorial discusses the current status and activities supported by this publication.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2532379","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7432058","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","6","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.67","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463237","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","7","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.81","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492377","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: New Associate Editors Introduction","","","IEEE Transactions on Software Engineering","","2007","33","10","641","642","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70738","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302776","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: A Message from the New Editor-in-Chief","J. Kramer","NA","IEEE Transactions on Software Engineering","","2006","32","1","2","3","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583598","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editor's Introduction: International Conference on Software Engineering","J. M. Atlee; P. Inverardi","NA; NA","IEEE Transactions on Software Engineering","","2012","38","1","3","4","The papers in this special section contain extended versions of selected papers from the 31st ACM/IEEE International Conference on Software Engineering (ICSE), held 20-22 May 2009 in Vancouver, British Columbia, Canada.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6141071","","Special issues and sections;Meetings;Software engineering","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editortial: New EIC Introduction","A. A. Andrews","NA","IEEE Transactions on Software Engineering","","2002","28","1","1","1","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.979984","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=979984","","Feedback;Biographies","","","","5","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: State of the Journal","N. Medvidovic","NA","IEEE Transactions on Software Engineering","","2019","45","1","1","1","Presents the introductory editorial for this issue of the publication.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2885501","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8605418","","","","","","","","0","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction: 2008 Conference on the Foundations of Software Engineering","G. C. Murphy; W. Schafer","NA; NA","IEEE Transactions on Software Engineering","","2010","36","5","591","592","The four papers in this special section are extended versions of selected papers from the 16th ACM International Symposium on the Foundations of Software Engineering, held in Atlanta, Georgia, 11-13 November 2008.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.88","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5593044","","Special issues and sections;Meetings;Software engineering","","","","","","45","","","","","","IEEE","IEEE Journals & Magazines"
"[Front inside cover]","","","IEEE Transactions on Software Engineering","","2012","38","1","c2","c2","Provides a listing of current society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.5","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6141069","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction: International Symposium on Software Testing and Analysis","S. Elbaum; D. S. Rosenblum","NA; NA","IEEE Transactions on Software Engineering","","2008","34","5","577","578","The five papers in this special section are based on five of the best papers of ISSTA 2007.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.85","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4636876","","Software testing;Object oriented modeling;System testing;Data mining;Space technology;Computer science;Computer industry;Semiconductor optical amplifiers;Cities and towns","","","","","","51","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","6","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.070604","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181712","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Call for Papers for the IEEE Transactions on Software Engineering Special Issue on Mining Software Repositories","","","IEEE Transactions on Software Engineering","","2004","30","10","712","712","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339285","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Introduction to the Special Section on the ACM SIGSOFT Foundations of Software Engineering Conference","P. Devanbu; M. Young","NA; NA","IEEE Transactions on Software Engineering","","2008","34","4","433","433","The two papers in this special section were originally presented at the 14th ACM SIGSOFT Foundations of Software Engineering Conference (SIGSOFT FSE-14), held in Portland, Oregon, during November 2006.  These revised and extended papers are summarized here.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4585925","","Special issues and sections;Software engineering;Software testing;Software maintenance;Computer science;Programming profession;System testing;Computer Society;Meetings","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","4","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123331","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: Associate Editor Introduction and Farewell","","","IEEE Transactions on Software Engineering","","2005","31","1","2","2","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392716","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","9","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.103","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514448","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: New AEIC introduction and AE farewell","J. Knight","School of Electrical Engineering and Computer Science at Oregon State University","IEEE Transactions on Software Engineering","","2003","29","11","961","961","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1245297","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1245297","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","1","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583605","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Workshop on software and performance [Guest Editors' Introduction]","A. M. K. Cheng; P. Clements; M. Woodside","University of Houston; NA; NA","IEEE Transactions on Software Engineering","","2000","26","12","1121","1121","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.888626","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=888626","","Software performance;Software testing;System testing;Management information systems;Conferences;Software quality;Iron;Scattering;Maintenance;Databases","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: AE Introduction","J. Knight","NA","IEEE Transactions on Software Engineering","","2004","30","8","489","490","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316866","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2004","30","10","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.63","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339287","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","5","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1009","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160975","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2012","38","1","c4","c4","Provides a listing of current society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6141077","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","10","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.111","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542074","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2004","30","12","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.98","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377201","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: the state of TSE","J. Knight","NA","IEEE Transactions on Software Engineering","","2004","30","2","81","81","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265813","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265813","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","12","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.121","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016577","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","3","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084139","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction to the Special Section from the International Conference on Software Maintenance","G. Canfora; L. Tahvildari; H. A. Muller","NA; NA; NA","IEEE Transactions on Software Engineering","","2009","35","4","450","451","The two papers in this special section are extended and enhanced versions of ones presented at the International Conference on Software Maintenance (ICSM), held in Paris, France, on 2-5 October 2007.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5186362","","Software maintenance;Computer Society;Conferences;Computer science;Software testing;Tagging;Programming profession;Navigation;Cities and towns;Sections","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","9","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70730","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288199","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","12","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70764","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4375383","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"State of the Journal address","J. Kramer","NA","IEEE Transactions on Software Engineering","","2008","34","1","1","2","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444342","","Software engineering;Production;Frequency measurement;Particle measurements;IEEE online publications;Delay;Subscriptions;Calendars;Biographies;Engineering management","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society Information","","","IEEE Transactions on Software Engineering","","2004","30","3","208","208","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271176","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271176","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","5","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642683","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: New Associate Editor Introduction","J. Knight","NA","IEEE Transactions on Software Engineering","","2005","31","6","425","425","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463226","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","7","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.65","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677540","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: How Special Should Issues Be?","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2010","36","4","449","450","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.75","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5532337","","","","","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2005","31","5","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1438378","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: New Associate Editors Introduction","J. Kramer","NA","IEEE Transactions on Software Engineering","","2006","32","10","769","770","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.101","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717470","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: New Associate Editor Introduction","J. Kramer","NA","IEEE Transactions on Software Engineering","","2008","34","2","161","161","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4476754","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editorial: Special Section on Interaction and State-Based Modeling","S. Uchitel; M. Broy; I. H. Kruger; J. Whittle","IEEE Computer Society; NA; NA; NA","IEEE Transactions on Software Engineering","","2005","31","12","997","998","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.139","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566602","","Object oriented modeling;Computer Society;Software engineering;Protocols;Systems engineering and theory;Computer architecture;Computational modeling;History;Real time systems;System analysis and design","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: State of the Journal Address","J. Kramer","NA","IEEE Transactions on Software Engineering","","2009","35","1","1","1","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4771848","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2004","30","11","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.84","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359783","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Society Journals Seek Editors in Chief for 2009-2010 Terms","","","IEEE Transactions on Software Engineering","","2008","34","1","160","160","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444348","","Bioinformatics;Computational biology;Computer vision;Computer Society;Biology computing;Transaction databases;Image storage;Qualifications;Computer industry;Government","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Search Based Software Engineering: Introduction to the Special Issue of the IEEE Transactions on Software Engineering","M. Harman; A. Mansouri","NA; NA","IEEE Transactions on Software Engineering","","2010","36","6","737","741","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.106","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5644735","","Special issues and sections;Search engines;Search problems;Search methods","","","","13","","22","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editorial: Special Section on the International Symposium on Software Testing and Analysis 2010","A. Orso; P. Tonella","Georgia Institute of Technology, 266 Ferst Drive, Atlanta, GA 30332-0765; Fondazione Bruno Kessler, Via Sommarive, 18, 38123 Povo, Trento, Italy","IEEE Transactions on Software Engineering","","2012","38","2","241","242","The articles in this special section contain selected papers from the International Symposium on Software Testing and Analysis 2010.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6173077","","Special issues and section;Meetings;Software testing","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2007","33","11","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.4339236","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339236","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2004","30","5","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291839","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","10","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.100","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717476","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2006","32","4","c4","c4","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628975","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction to the Special Section on Software Language Engineering","J. Favre; D. Gasević; R. Lammel; A. Winter","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2009","35","6","737","741","The six articles in this special section are devoted to software language engineering.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.78","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5353438","","History;Natural languages;Writing;Humans;Production;Food technology;Information technology;Systems engineering and theory;Software engineering;Domain specific languages","","","","2","","8","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","10","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.118","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542061","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","10","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70735","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302775","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2011","37","6","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.114","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6095281","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2010","36","3","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5473897","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","4","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123322","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE information for authors [inside back cover]","","","IEEE Transactions on Software Engineering","","2012","38","5","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311393","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2011","37","3","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.48","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5779014","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2012","38","2","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6173079","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Author index","","","IEEE Transactions on Software Engineering","","2002","28","12","1194","1196","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158291","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158291","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","6","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.070601","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181705","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","10","709","709","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339282","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","2","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052582","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","2","182","182","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401932","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"7 Great Reasons for Joining the IEEE Computer Society [advertisement]","","","IEEE Transactions on Software Engineering","","2010","36","1","144","144","Advertisement: 7 Great Reasons for Joining the IEEE Computer Society.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401367","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","5","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1007","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160966","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2009","35","6","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.74","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5353436","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2010","36","2","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5439566","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Call for Papers for New IEEE Transactions on Affective Computing","","","IEEE Transactions on Software Engineering","","2009","35","6","879","879","The new IEEE Transactions on Affective Computing seeks original manuscripts for publication. This new online only journal will publish cross disciplinary and international archival results of research on the design of systems that can recognize, interpret, and simulate human emotions and related affective phenomena. The journal will publish original research on the principles and theories explaining why and how affective factors condition interaction between humans and technology, on how affective sensing and simulation techniques can inform our understanding of human affective processes, and on the design, implementation and evaluation of systems that carefully consider affect among the factors that influence their usability. Surveys of existing work will be considered for publication when they propose a new viewpoint on the history and the perspective on this domain.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5353440","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","1","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392713","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2010","36","2","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5439570","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","11","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.109","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015508","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2010","36","3","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5473902","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2010","36","5","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.85","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5593043","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Table of Contents [Front cover]","","","IEEE Transactions on Software Engineering","","2012","38","1","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6141068","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2011 Reviewers List","","","IEEE Transactions on Software Engineering","","2012","38","1","236","238","Lists the reviewers who contributed to IEEE Transactions on Software Engineering in 2011.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.3","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6141072","","IEEE publishing","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2004","30","6","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321056","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","3","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084132","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2008","34","5","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.82","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4636875","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2005 Reviewers List","","","IEEE Transactions on Software Engineering","","2006","32","1","51","53","The publication offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.2","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583602","","IEEE","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","9","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70727","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288190","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society's - Staff List","","","IEEE Transactions on Software Engineering","","2004","30","4","0_2","0_2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274040","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274040","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside back cover]","","","IEEE Transactions on Software Engineering","","2011","37","6","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.115","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6095285","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2018 Reviewers List","","","IEEE Transactions on Software Engineering","","2019","45","1","107","110","The publication offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2884373","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8605391","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","6","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.070603","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181711","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","10","711","711","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339284","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2008","34","4","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4585924","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","4","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123330","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","9","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.84","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707661","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","2","184","184","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401934","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2010","36","1","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401369","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2009","35","6","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5353442","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2011","37","3","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5779018","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","9","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.102","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514447","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"In this issue","","","IEEE Transactions on Software Engineering","","2006","32","11","928","928","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.107","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015514","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2013 Reviewers List","","","IEEE Transactions on Software Engineering","","2014","40","1","103","106","The publication offers a note of thanks and lists its reviewers. Reviewers' comments are an essential part of the process of creating a well written article capable of expressing theoretical content well with effective illustrations, tables and graphs.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2298173","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6755631","","IEEE publishing","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Introduction to OnlinePlus Video [advertisement]","","","IEEE Transactions on Software Engineering","","2012","38","1","239","239","Advertisement: Now available: A video introducing the IEEE Computer Society's new OnlinePius publication model for Transactions. Viewers will see an overview of the great features and benefits included with an OnlinePlus subscription and will take a tour of the user-friendly interface included on the accompanying disc. Go to www.computer.org/onlineplus to view the video and learn all about it today.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6141074","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","8","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70713","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267020","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","12","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.118","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016568","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2008","34","5","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.83","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4636877","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","1","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583604","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2004","30","8","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316865","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","10","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.62","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339286","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"In this issue - Technically","","","IEEE Transactions on Software Engineering","","2007","33","4","269","269","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123328","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2008","34","4","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4585926","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","5","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1008","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160974","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","7","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.070701","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4227825","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front inside cover]","","","IEEE Transactions on Software Engineering","","2012","38","6","c2","c2","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.78","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363458","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2010","36","5","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5593047","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Cover3]","","","IEEE Transactions on Software Engineering","","2012","38","1","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6141076","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","10","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.110","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542073","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","5","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1438371","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2004","30","11","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.81","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359764","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","6","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650210","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Join the IEEE Computer Society!","","","IEEE Transactions on Software Engineering","","2007","33","2","142","144","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052588","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2003 Reviewers list","","","IEEE Transactions on Software Engineering","","2004","30","1","78","80","The publication offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265737","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265737","","IEEE","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Transactions on Software Engineering","","","IEEE Transactions on Software Engineering","","2004","30","3","0_1","0_1","Presents the front cover for this issue of the publication.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271167","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271167","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","4","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435347","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","12","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.97","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377200","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Call For Papers","","","IEEE Transactions on Software Engineering","","2005","31","1","94","94","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming conference described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392723","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","12","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.120","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016576","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","3","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084138","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Annual Index","","","IEEE Transactions on Software Engineering","","2004","30","12","1084","1096","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.93","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377199","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","12","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70761","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4375376","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Call-for-Papers: Special Issue on Software Language Engineering","","","IEEE Transactions on Software Engineering","","2007","33","12","891","891","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70760","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4375380","","Software engineering;Design engineering;Application software;Domain specific languages;Ontologies;Software maintenance;Genetic programming;Engineering management;Software systems;Computer languages","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","3","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423988","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2011","37","2","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.30","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5739155","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2015 reviewers list","","","IEEE Transactions on Software Engineering","","2016","42","1","100","102","The conference offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2507218","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7374786","","IEEE publishing","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2008","34","6","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.93","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4709488","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","8","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.75","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703382","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","1","96","96","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.3","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392725","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2009","35","5","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.62","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5275151","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Author index","","","IEEE Transactions on Software Engineering","","2003","29","12","1135","1137","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265527","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265527","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2009","35","4","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5186359","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","9","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70729","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288198","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Call for Papers for Special Issue on Software Services and Service-Based Systems","","","IEEE Transactions on Software Engineering","","2009","35","4","592","592","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5186363","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","5","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642676","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","12","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70763","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4375382","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","7","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.62","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677529","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside back cover]","","","IEEE Transactions on Software Engineering","","2012","38","3","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6205671","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2011","37","4","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.72","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5966993","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2008","34","6","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.95","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4709494","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2008","34","1","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444341","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","3","207","207","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271175","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271175","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","4","356","358","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435355","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","11","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.123","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556549","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2004","30","5","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.5","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291830","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2009","35","5","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.63","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5275153","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society Information to Authors","","","IEEE Transactions on Software Engineering","","2004","30","2","142","142","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265820","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265820","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Join the IEEE Computer Society","","","IEEE Transactions on Software Engineering","","2007","33","8","576","576","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70717","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267028","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Table of Contents [Front cover]","","","IEEE Transactions on Software Engineering","","2012","38","5","c1","c1","Presents the cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.57","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311484","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","1","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256938","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027142","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside back cover]","","","IEEE Transactions on Software Engineering","","2012","38","4","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6249693","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2009","35","4","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5186365","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","5","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642682","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2011","37","1","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.13","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5704234","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2011","37","2","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5739159","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","6","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.65","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463225","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2012","38","3","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6205669","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2010","36","4","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.72","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5532336","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","5","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1438377","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2011","37","4","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5966995","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2010","36","4","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.74","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5532340","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","11","833","833","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.70","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359774","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","4","360","360","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435357","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2008","34","2","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4476753","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"New Transactions Newsletter [advertisement]","","","IEEE Transactions on Software Engineering","","2012","38","4","992","992","Advertisement: Stay connected with the IEEE Computer Society Transactions by signing up for our new Transactions Connection newsletter. It is free and contains valuable information.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6249695","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2012 Reviewers List","","","IEEE Transactions on Software Engineering","","2013","39","1","141","144","The publication offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.2","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6397559","","IEEE publishing","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2011","37","1","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5704240","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","11","835","835","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359776","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","2","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599415","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599415","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","11","839","839","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359780","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"In this issue - Technically","","","IEEE Transactions on Software Engineering","","2008","34","1","157","157","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.3","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444345","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2010","36","6","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.100","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5644732","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","11","996","996","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.121","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556557","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","7","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.78","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492367","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"In this issue","","","IEEE Transactions on Software Engineering","","2007","33","1","64","64","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256946","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027150","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","12","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.133","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566601","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2010 Reviewers List","","","IEEE Transactions on Software Engineering","","2011","37","1","142","144","Lists the reviewers who contributed to IEEE Transactions on Software Engineering in 2010.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5704238","","IEEE publishing","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","7","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677539","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","8","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.90","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498767","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2011","37","5","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.97","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6030115","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","11","837","837","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.71","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359778","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2009","35","1","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4771847","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","11","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.83","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359782","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2006","32","2","136","136","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599421","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599421","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","11","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.4339229","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339229","","Materials;Book reviews;Computer Society;Educational institutions;Software;Permission;Subscriptions","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2009","35","1","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4771851","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"In this issue - Technically","","","IEEE Transactions on Software Engineering","","2008","34","1","159","159","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444347","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Author index","","","IEEE Transactions on Software Engineering","","2002","28","11","1194","1196","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049408","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049408","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2008","34","1","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.9","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444351","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","10","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.98","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717469","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editors' introduction: special issues on architecture-independent languages and software tools for parallel processing","D. Talia; P. K. Srimani; M. Jazayeri","ISI-CNR, DEIS, UNICAL; NA; NA","IEEE Transactions on Software Engineering","","2000","26","3","193","196","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.842946","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=842946","","Software tools;Concurrent computing;Parallel processing;Computer architecture;Parallel machines;Parallel architectures;Computational modeling;Standardization;Parallel programming;High level languages","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2016 Reviewers List*","","","IEEE Transactions on Software Engineering","","2017","43","1","100","103","The publication offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2636498","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7809238","","IEEE publishing","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","4","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628968","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2009","35","3","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5061641","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2004","30","12","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.95","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377182","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2004","30","7","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.27","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318602","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2004","30","10","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339273","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2006 Reviewers List","","","IEEE Transactions on Software Engineering","","2007","33","1","54","56","Lists, in alphabetic order, reviewers who contributed to the IEEE Transactions on Software Engineering in 2006.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256944","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027148","","IEEE","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2008","34","2","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4476757","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2010","36","1","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401362","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2008","34","3","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4536053","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2011","37","5","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.98","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6030117","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","6","528","528","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.62","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463235","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","9","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.100","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514440","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2008 Reviewers List","","","IEEE Transactions on Software Engineering","","2009","35","1","138","141","Lists the reviewers who contributed to the IEEE Transactions on Software Engineering from 09 January 2008 through 13 January 2009.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4771849","","IEEE","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Cover2]","","","IEEE Transactions on Software Engineering","","2012","38","2","c2","c2","Provides a listing of current society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6173076","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Author index","","","IEEE Transactions on Software Engineering","","2001","27","12","1145","1147","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.988712","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988712","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","11","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.4339235","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339235","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2007 Annual Index","","","IEEE Transactions on Software Engineering","","2008","34","1","Online-only content","","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444349","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society CSDP [advertisement]","","","IEEE Transactions on Software Engineering","","2010","36","6","878","878","Advertisement: The IEEE Computer Society Certified Software Development Professional (CSDP) credential.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.105","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5644736","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","5","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291838","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","10","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.99","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717475","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2009","35","3","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5061639","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","4","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628974","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","10","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70734","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302774","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2011","37","6","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.113","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6095280","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2004","30","9","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324643","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2014 Index IEEE Transactions on Software Engineering Vol. 40","","","IEEE Transactions on Software Engineering","","2015","41","1","104","112","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2382474","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7014339","","Indexes","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","2","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401925","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Transaction on Software Engineering - Table of Contents","","","IEEE Transactions on Software Engineering","","2004","30","1","0_1","0_1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265724","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265724","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2008","34","3","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.42","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4536055","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2012","38","5","c2","c2","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.58","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311392","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","3","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610605","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","1","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.5","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583596","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","8","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.92","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498775","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2009","35","2","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4809709","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2009","35","2","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4809713","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside back cover]","","","IEEE Transactions on Software Engineering","","2012","38","2","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.23","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6173078","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2010","36","6","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.103","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5644738","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","10","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.119","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542062","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society Magazines and Transactions available in ePUB format [advertisement]","","","IEEE Transactions on Software Engineering","","2011","37","6","878","878","Advertisement: IEEE Computer Society Magazines and Transactions in ePUB format.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.117","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6095282","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2010","36","3","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5473898","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","7","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.29","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318610","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","4","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123323","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Transactions on Software Engineering - Staff List","","","IEEE Transactions on Software Engineering","","2004","30","1","0_2","0_2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265730","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265730","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2009 Reviewers List","","","IEEE Transactions on Software Engineering","","2010","36","1","141","143","Lists the reviewers who contributed to the IEEE Transactions on Software Engineering from 14 January 09 through 07 January 2010.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401366","","IEEE publications","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","5","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1006","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160965","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society [Back cover]","","","IEEE Transactions on Software Engineering","","2012","38","5","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311394","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2011","37","3","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.49","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5779015","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","11","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.108","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015507","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2010","36","3","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.54","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5473901","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2010","36","5","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.84","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5593042","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Subject index","","","IEEE Transactions on Software Engineering","","2002","28","12","1196","1200","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1158292","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1158292","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","3","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4084131","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2008","34","5","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.81","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4636874","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2016 Index IEEE Transactions on Software Engineering Vol. 42","","","IEEE Transactions on Software Engineering","","2017","43","1","1","8","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2638761","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7809252","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"What's new in Transactions [advertisement]","","","IEEE Transactions on Software Engineering","","2011","37","6","880","880","Advertisement: IEEE periodicals.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.119","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6095284","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","6","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.070602","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4181706","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","10","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70736","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4302782","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","10","710","710","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.56","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339283","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2008","34","4","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4585923","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","2","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052583","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","2","183","183","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401933","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2010","36","1","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401368","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","12","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.134","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566609","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Call for papers","P. Diaz; I. Aedo; F. Panetsos","Universidad Complutense de Madrid; NA; NA","IEEE Transactions on Software Engineering","","2001","27","8","766","767","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.940729","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=940729","","Laboratories;Engineering management","","","","","","1","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2009","35","6","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.75","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5353437","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2010","36","2","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5439567","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"7 Great Reasons for Joining the IEEE Computer Society [advertisement]","","","IEEE Transactions on Software Engineering","","2009","35","6","880","880","Advertisement: 7 Great Reasons for Joining the IEEE Computer Society.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5353441","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Open Access Publishing [advertisement]","","","IEEE Transactions on Software Engineering","","2012","38","5","1232","1232","Advertisement: What does IEEE Open Access mean to an author? Top quality publishing with established impact factors; Increased exposure and recognition as a thought leader; A consistent IEEE peer-review standard of excellence; Unrestricted access for readers to discover your publications; and Great way to fulfill a requirement to publish open access.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.61","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6311396","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Distinguish yourself with the CSDP [advertisement]","","","IEEE Transactions on Software Engineering","","2011","37","3","448","448","Advertisement: The IEEE Computer Society Certified Software Development Professional (CSDP) credential.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5779017","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","1","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392714","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2010","36","2","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5439571","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","3","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.26","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610613","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2011 Annual Index","","","IEEE Transactions on Software Engineering","","2012","38","1","Online Only","","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.2","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6141073","","Indexes","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2004","30","6","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321057","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2005 Annual Index","","","IEEE Transactions on Software Engineering","","2006","32","1","54","64","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583603","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","9","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70728","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4288191","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2011","37","6","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.116","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6095286","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2004","30","8","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.36","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316864","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","9","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.48","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324649","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","9","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.85","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707662","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","2","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401935","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Join the IEEE Computer Society!","","","IEEE Transactions on Software Engineering","","2007","33","5","366","368","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.1010","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4160973","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2009","35","6","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.81","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5353443","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2011","37","3","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5779019","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","11","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.110","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4015515","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Table of contents - Front cover]","","","IEEE Transactions on Software Engineering","","2012","38","6","c1","c1","Presents the cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363457","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2010","36","5","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5593046","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"What's new in Transactions [advertisement]","","","IEEE Transactions on Software Engineering","","2012","38","1","240","240","Advertisement: Our new ""What's New in Transactions"" webpage provides an overview of our 14 peer-reviewed scholarly journals. Visit http://www.computer.org.ezproxy.gsu.edu/whats-new today.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6141075","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","8","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70714","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267021","","Materials;Book reviews;Computer Society;Educational institutions;Software;Permission;Subscriptions","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","12","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.119","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016569","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2008","34","5","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.84","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4636878","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Transactions on Software Engineering","","","IEEE Transactions on Software Engineering","","2004","30","4","0_1","0_1","Presents the front cover for this issue of the publication.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274039","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274039","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","10","912","912","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.108","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1542072","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Join the IEEE Computer Society","","","IEEE Transactions on Software Engineering","","2007","33","4","270","272","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.35","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4123329","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2008","34","4","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.59","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4585927","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2014 Reviewers List*","","","IEEE Transactions on Software Engineering","","2015","41","1","100","103","The publication offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2376651","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=7004099","","IEEE publishing","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","7","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.070702","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4227826","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2004 Reviewers list","","","IEEE Transactions on Software Engineering","","2005","31","1","91","93","The publication offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392722","","IEEE","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","7","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.070703","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4227830","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back inside cover]","","","IEEE Transactions on Software Engineering","","2012","38","6","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363459","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Transactions on Software Engineering - Table of contents","","","IEEE Transactions on Software Engineering","","2004","30","2","C1","C1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265811","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265811","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"OnlinePlus Coming Soon to TSE [advertisement]","","","IEEE Transactions on Software Engineering","","2012","38","6","1488","1488","Advertisement: Recognizing the need for quicker access to research, TSE will transition to the new OnlinePlus publication model beginning in 2013.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.81","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6363463","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2017 Reviewers List*","","","IEEE Transactions on Software Engineering","","2018","44","1","100","102","Presents a list of reviewers who contributed to this publication in 2017.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2775290","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8249563","","IEEE publishing","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Join the IEEE Computer Society","","","IEEE Transactions on Software Engineering","","2006","32","12","1006","1008","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.124","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016575","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2009","35","4","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5186360","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Reviewers list","","","IEEE Transactions on Software Engineering","","2003","29","1","95","96","The publication offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1166592","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1166592","","IEEE","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","8","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1316872","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","7","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.63","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677530","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","5","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.55","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1438372","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2004","30","11","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.82","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359765","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","6","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.52","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650211","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","2","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.15","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4052589","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE: Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","1","81","81","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265738","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265738","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society 2009 Membership Application","","","IEEE Transactions on Software Engineering","","2008","34","6","860","862","Advertisement: IEEE Computer Society 2009 Membership Application.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.98","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4709491","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society Staff List","","","IEEE Transactions on Software Engineering","","2004","30","3","0_2","0_2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1271168","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1271168","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","4","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435348","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","1","95","95","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.2","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392724","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","11","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.124","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556550","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2009","35","5","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.61","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5275150","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","6","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.18","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1321067","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE: Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","4","279","279","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1274047","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1274047","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","5","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.43","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1642675","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","12","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70762","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4375377","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Join the IEEE Computer Society","","","IEEE Transactions on Software Engineering","","2007","33","12","892","892","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70766","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4375381","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","3","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423989","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2011","37","2","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.31","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5739156","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society OnlinePlus Coming Soon to TSE","","","IEEE Transactions on Software Engineering","","2012","38","3","736","736","Advertisement: Recognizing the need for quicker access to research, TSE will transition to the new OnlinePlus publication model beginning in 2013.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6205670","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2011","37","4","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.71","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5966992","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","9","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.86","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1707672","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2008","34","6","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.94","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4709489","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society CSDP [advertisement]","","","IEEE Transactions on Software Engineering","","2008","34","6","864","864","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.100","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4709493","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2008","34","1","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.6","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444340","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","8","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703383","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","1","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1392726","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Call for Papers: Special Issue on Software Services and Service-Based Systems","","","IEEE Transactions on Software Engineering","","2009","35","5","736","736","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.60","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5275152","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2017 Index IEEE Transactions on Software Engineering Vol. 43","","","IEEE Transactions on Software Engineering","","2018","44","1","1","9","Presents the 2017 subject/author index for this publication.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2784782","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8249567","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"180,000 aritlces in the IEEE Computer Society Digital Library [advertisement]","","","IEEE Transactions on Software Engineering","","2007","33","8","575","575","A critical computer science and information technology rlesource for academic, government, and corporate libraries around the world ... does your organization subscribe?","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70711","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267027","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Subject index","","","IEEE Transactions on Software Engineering","","2003","29","12","1137","1142","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265528","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265528","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2012","38","4","c2","c2","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.48","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6249692","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"1999 reviewers list","","","IEEE Transactions on Software Engineering","","2000","26","1","94","96","The publication offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.825768","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=825768","","IEEE","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2009","35","4","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5186364","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2011","37","2","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.32","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5739158","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","6","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463224","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Table of Contents [Front cover]","","","IEEE Transactions on Software Engineering","","2012","38","3","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.34","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6205668","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2010","36","4","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.71","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5532335","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2012","38","3","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.37","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6205672","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Distinguish yourself with the CSDP [advertisement]","","","IEEE Transactions on Software Engineering","","2011","37","4","592","592","Advertisement: The IEEE Computer Society Certified Software Development Professional (CSDP) credential.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.75","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5966994","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2008","34","6","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.96","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4709495","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","4","359","359","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435356","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Table of Contents [Front cover]","","","IEEE Transactions on Software Engineering","","2012","38","4","c1","c1","Presents the cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.47","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6249798","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2009","35","5","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.64","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5275154","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","8","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70715","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4267029","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2007","33","1","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256939","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027143","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2008","34","2","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4476752","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2012","38","4","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.50","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6249694","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2012 Annual Index","","","IEEE Transactions on Software Engineering","","2013","39","1","Online-only content","","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.1","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6397558","","Indexes","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2011","37","1","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.14","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5704235","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","3","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1423997","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2011","37","4","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.74","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5966996","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","11","834","834","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.76","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359775","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2007 Reviewer's List","","","IEEE Transactions on Software Engineering","","2008","34","1","154","156","Lists, in alphabetical order, the reviewers who contributed to the IEEE Transactions on Software Engineering from 18 November 06 through 31 December 07.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.2","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444344","","IEEE","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","4","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1435358","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2004","30","5","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1291829","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","8","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.77","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703391","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Information for authors","","","IEEE Transactions on Software Engineering","","2004","30","2","141","141","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.1265819","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265819","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","12","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.132","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1566600","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Join the IEEE Computer Society!","","","IEEE Transactions on Software Engineering","","2006","32","7","528","528","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.68","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1677538","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2011","37","1","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5704241","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2011","37","5","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.96","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6030114","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2010","36","4","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.73","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5532339","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","6","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.53","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1650219","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","11","836","836","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.74","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359777","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2006","32","2","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599416","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599416","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2009","35","1","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.7","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4771846","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","11","840","840","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.75","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359781","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2007","33","11","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.4339228","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339228","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society 2009 Membership Application","","","IEEE Transactions on Software Engineering","","2009","35","1","142","144","Advertisement: IEEE Computer Society 2009 Membership Application.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.12","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4771850","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"In this issue - Technically","","","IEEE Transactions on Software Engineering","","2008","34","1","158","158","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.5","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444346","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2008","34","1","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.8","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4444350","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2010","36","6","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.101","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5644733","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","10","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.97","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1717468","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","11","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.125","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1556558","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","7","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.79","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492368","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2000 reviewers list","","","IEEE Transactions on Software Engineering","","2001","27","1","94","96","The publication offers a note of thanks and lists its reviewers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.895991","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=895991","","IEEE","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","4","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.33","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1628967","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2009","35","3","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.38","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5061640","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Call for Papers: Search-Based Optimization for Software Engineering","","","IEEE Transactions on Software Engineering","","2008","34","2","304","304","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.16","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4476756","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2007","33","1","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256947","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027151","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Call for papers","J. Offutt; J. Jezequel","NA; NA","IEEE Transactions on Software Engineering","","2002","28","4","432","432","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.995443","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=995443","","Software testing;Object oriented modeling;Context modeling;Software engineering;Computer science;Information technology;Information analysis;Application software;Reliability engineering;Software systems","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2010 Annual Index","","","IEEE Transactions on Software Engineering","","2011","37","1","Online-only content","","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.11","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5704239","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2008","34","3","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.39","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4536052","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","8","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.91","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498768","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"IEEE Computer Society Magazines and Transactions available in ePUB format [advertisement]","","","IEEE Transactions on Software Engineering","","2011","37","5","736","736","Advertisement: IEEE Computer Society Magazines and Transactions in ePUB format.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.100","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6030116","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","6","527","527","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.61","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463234","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2004","30","11","838","838","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.72","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1359779","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2009","35","2","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4809710","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2006","32","2","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.1599422","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1599422","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Table of Contents [Front cover]","","","IEEE Transactions on Software Engineering","","2012","38","2","c1","c1","Presents the front cover/table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6173075","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2009","35","1","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.10","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4771852","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"180,000 aritlces in the IEEE Computer Society Digital Library [advertisement]","","","IEEE Transactions on Software Engineering","","2007","33","11","796","796","A critical computer science and information technology rlesource for academic, government, and corporate libraries around the world ... does your organization subscribe?","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.4339234","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4339234","","Software libraries;Keyboards;Computer Society;Computer science;Information technology;Government","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Subject index","","","IEEE Transactions on Software Engineering","","2002","28","11","1196","1200","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1049409","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1049409","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2009","35","3","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.40","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5061642","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2004","30","12","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.96","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1377183","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2004","30","7","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.28","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1318603","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2004","30","10","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.61","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1339274","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2004","30","9","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2004.46","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1324642","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2006 Annual Index","","","IEEE Transactions on Software Engineering","","2007","33","1","57","63","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.256945","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4027149","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editors' introduction: special issues on architecture-independent languages and software tools for parallel processing","D. Talia; P. K. Srimani; M. Jazayeri","Colorado State University; NA; NA","IEEE Transactions on Software Engineering","","2000","26","4","289","292","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.844490","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=844490","","Software tools;Concurrent computing;Parallel processing;Computer architecture;Costs;Computational modeling;Parallel machines;Standardization;Parallel programming","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2008","34","2","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4476758","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2005","31","2","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.19","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1401924","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2010","36","1","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.17","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401363","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2008","34","3","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2008.41","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4536054","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","3","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.24","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1610604","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Back cover]","","","IEEE Transactions on Software Engineering","","2011","37","5","c4","c4","Provides a listing of current staff, committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2011.99","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6030118","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","6","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.66","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1463236","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Front cover]","","","IEEE Transactions on Software Engineering","","2006","32","1","c1","c1","Presents the table of contents for this issue of the periodical.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.4","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1583595","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Advertisement]","","","IEEE Transactions on Software Engineering","","2005","31","8","712","712","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.87","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1498774","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"[Inside front cover]","","","IEEE Transactions on Software Engineering","","2005","31","9","c2","c2","Provides a listing of current committee members and society officers.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.101","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1514441","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2009","35","2","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.22","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4809712","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Subject index","","","IEEE Transactions on Software Engineering","","2001","27","12","1147","1151","This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.988713","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988713","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2010","36","6","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.102","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5644737","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"TSE Information for authors","","","IEEE Transactions on Software Engineering","","2005","31","7","c3","c3","Provides instructions and guidelines to prospective authors who wish to submit manuscripts.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2005.80","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1492376","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: A New Decade of TSE","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2010","36","1","3","6","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.20","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401365","","","","","","1","","1","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction to the Special Section on the International Conference on Software Engineering","W. G. Griswold; B. Nuseibeh","Department of Computer Science and Engineering, University of California, San Diego, La Jolla, CA 92093- 0404; Department of Computing, Faculty of Maths and Computing, The Open University, Walton Hall, Milton Keynes MK7 6AA, UK","IEEE Transactions on Software Engineering","","2006","32","12","929","930","THE 27th International Conference on Software Engineering (ICSE '05) was held in St. Louis, Missouri, 15-21 May 2005. Of the 313 papers submitted to the conference, 44 were published in the conference proceedings, and, of those, four of the best were selected and invited for revision and extension. The papers cover four diverse topics but share rigorous technical presentations of novel work, substantially validated or evaluated. The papers are briefly summarized.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.123","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4016570","","Software engineering;Data structures;Genetic mutations;Fault detection;Computer science;Testing;Conference proceedings;Fault tolerance;Data engineering","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: Readers, Writers, Reviewers, and Editors","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2010","36","2","145","156","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.44","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5439568","","","","","","1","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editor's introduction: seventh international software metrics symposium","C. Wohlin","Blekinge Institute of Technology","IEEE Transactions on Software Engineering","","2001","27","11","961","962","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.965337","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=965337","","Software metrics;Software measurement;Software engineering;Application software;Costs;Computer Society;Software quality;Pressure measurement;Time measurement;Monitoring","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editors' introduction: workshop on software and performance","A. M. K. Cheng; P. Clements; M. Woodside","Carnegie Mellon University; NA; NA","IEEE Transactions on Software Engineering","","2000","26","11","1025","1026","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.881715","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=881715","","Software performance;Object oriented modeling;Computer architecture;Conferences;Performance analysis;Concurrent computing;Hardware;Design engineering;Spatial databases;Transaction databases","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction to the Special Section on the First International Conference on the Quantitative Evaluation of SysTems (QEST)","G. Franceschinis; J. -. Katoen; M. Woodside","NA; NA; NA","IEEE Transactions on Software Engineering","","2006","32","8","529","530","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2006.80","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1703384","","Logic;Algorithm design and analysis;Web pages;State-space methods;Sections;Software engineering;Communication systems;Stochastic systems;Computational modeling;Analytical models","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editors introduction: next generation software reuse","P. T. Devanbu; D. E. Perry; J. S. Poulin","University of California; NA; NA","IEEE Transactions on Software Engineering","","2000","26","5","423","424","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2000.846299","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=846299","","Object oriented modeling;Containers;Technological innovation;Domain specific languages;Software libraries;Data structures;Algorithms;Software engineering;Costs;Software architecture","","","","6","","4","","","","","","IEEE","IEEE Journals & Magazines"
"In Memoriam: Mary Jean Harrold (1947-2013)","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2013","39","11","1466","1466","Recounts the career and contributions of Mary Jean Harrold.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.51","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6648565","","Obituaries;Harrold,  Mary Jean","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editors' introduction: 1999 international conference on software engineering","J. Kramer; D. Garlan; D. S. Rosenblum","Imperial College; NA; NA","IEEE Transactions on Software Engineering","","2001","27","2","97","98","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2001.908956","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=908956","","Software engineering;Packaging;Software systems;Application software;Software packages;Computer science;Computer Society;User interfaces;Software architecture;Computer industry","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction: 2000 International Symposium on Software Testing and Analysis","M. J. Harrold; A. Bertolino","George Institute of Technology; NA","IEEE Transactions on Software Engineering","","2002","28","2","113","114","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.988493","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=988493","","Software testing;Software tools;Software quality;System testing;Real time systems;Object oriented modeling;Java;Analytical models;Monitoring;Computer Society","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editor's introduction: 2002 conference on the foundations of software engineering","W. G. Griswold","NA","IEEE Transactions on Software Engineering","","2003","29","10","865","865","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1237168","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1237168","","Software engineering;Computer science;Software tools;Mathematics;Programming;Information technology;Ubiquitous computing;Software design;Visualization;Drives","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Introduction to the special section on petri nets and performance models","G. Ciardo; R. German; B. R. Haverkort","College of William and Mary; NA; NA","IEEE Transactions on Software Engineering","","2002","28","10","913","914","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2002.1041048","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1041048","","Special issues and sections;Petri nets;Stochastic processes;Conferences;Algebra;Transient analysis;Computer science;State-space methods;Australia;Probability distribution","","","","","","3","","","","","","IEEE","IEEE Journals & Magazines"
"Editorial: A New Editor in Chief and the State of the Journal","J. Kramer","NA","IEEE Transactions on Software Engineering","","2010","36","1","1","2","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.21","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5401364","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editors' introduction: 2001 international conference on software engineering","M. J. Harrold; W. Schafer","University of Paderborn; NA","IEEE Transactions on Software Engineering","","2003","29","2","97","98","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1178047","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1178047","","Software engineering;Software reusability;Programming;Data structures;Computer Society;Educational programs;Pulp and paper industry;Computer industry;Conferences;Animation","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editors' introduction: 2001 international conference on software maintenance","G. Canfora; A. A. Andrews","University of Sannio; NA","IEEE Transactions on Software Engineering","","2003","29","3","193","194","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1183926","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1183926","","Software maintenance;System testing;Software testing;Software systems;Performance analysis;Lattices;Sections;Discussion forums;Computer industry;Application software","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest editors' introduction: software engineering for the wireless internet","M. Morisio; M. Oivo","Dipartimento Automatica e Informatica, Italy; NA","IEEE Transactions on Software Engineering","","2003","29","12","1057","1058","","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2003.1265520","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=1265520","","Software engineering;Internet;Middleware;Application software;Context awareness;Peer to peer computing;Batteries;Bandwidth;Quality of service;Computer Society","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Guest Editors' Introduction to the Special Section on Exception Handling: From Requirements to Software Maintenance","A. Garcia; A. Romanovsky; V. Issarny","NA; NA; NA","IEEE Transactions on Software Engineering","","2010","36","2","147","149","The four papers in this special section focus on topics related to exception handling.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.45","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=5439569","","Software maintenance;Application software;Software systems;Computer languages;Programming;Software quality;Software engineering;Protection;Pressing;Reflection","","","","","","70","","","","","","IEEE","IEEE Journals & Magazines"
"In Memoriam - David Notkin (1953-2013)","B. Nuseibeh","NA","IEEE Transactions on Software Engineering","","2013","39","6","742","743","David Samuel Notkin, whose technical, educational, and social contributions to computer science and software engineering research made him a major figure in the field, died on 22 April 2013, at his home in Seattle, Washington. He was 58 years old. The cause of his death was cancer. David is best known for his research, with his many graduate students, on software evolution. He asked why software is often so hard and expensive to change, and he worked to reduce the difficulty of software evolution to an essential minimum. This focus came from his belief that the ability to change software - its softness - is where its true but under-realized potential resides. He asked questions such as whether we can identify and close the gap between Brooks' notions of accidental and essential software complexity? How much should rather than does it cost to develop, test, and evolve software? Can we make the cost of change proportionate rather than disproportionate to the apparent complexity of changes to be made? Can we design software analysis methods that realize the best properties of both static and dynamic analysis techniques? Beyond technical contributions, David is widely recognized and admired for his exceptional skill as a research mentor for graduate students and as a powerful and unwavering advocate for improving gender diversity in computer science. A brief biography is given highlighting Notkin's professional achievements.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.25","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=6519247","","Obituaries;Notkin, David","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"An Extensible Metamodel for Program Analysis (abstract only)","D. Strein; R. Lincke; J. Lundberg; W. Löwe","NA; NA; NA; NA","IEEE Transactions on Software Engineering","","2007","33","12","799","799","Software maintenance tools for program analysis and refactoring rely on a metamodel capturing the relevant properties of programs. However, what is considered relevant may change when the tools are extended with new analyses, refactorings, and new programming languages. This paper proposes a language independent metamodel and an architecture to construct instances thereof, which is extensible for new analyses, refactorings, and new front-ends of programming languages. Due to the loose coupling between analysis, refactoring, and front-end components, new components can be added independently and reuse existing ones. Two maintenance tools implementing the metamodel and the architecture, VlZZANALYZER and X-DEVELOP, serve as proof of concept.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2007.70759","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=4375379","Programming environments;program analysis;metamodels.","Independent component analysis;Computer languages;Computer architecture;Software libraries","high level languages;software maintenance;software tools","program analysis;software maintenance tools;refactoring rely;programming languages;language independent metamodel;VlZZANALYZER;X-DEVELOP","","","","","","","","","","IEEE","IEEE Journals & Magazines"
"2018 Index IEEE Transactions on Software Engineering Vol. 44","","","IEEE Transactions on Software Engineering","","2019","45","1","1","9","Presents the 2018 subject/author index for this publication.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2018.2887195","","https://ieeexplore-ieee-org.ezproxy.gsu.edu/stamp/stamp.jsp?arnumber=8605397","","","","","","","","","","","","","","IEEE","IEEE Journals & Magazines"
